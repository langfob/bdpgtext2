---
params:
#--------------------  START of setting params variables  --------------------# 
  initialDate: "2025-06-12"
#-----  Rmd file and chunk control variables  -----# 
  chunk_echo: FALSE    #  Variable to allow turning echo of chunk code on/off for multiple, specified chunks.
  chunk_include: FALSE    #  Variable to allow turning echo of chunk output on/off for multiple, specified chunks.
  build_appendices: FALSE
#-----  predicted dominance variables  -----# 
  use_predicted_dominance_values: FALSE    ##  2024 01 21 ##FALSE
  use_predicted_FP_dominant: FALSE    ##  2024 01 21 ##FALSE
  use_predicted_FN_dominant: FALSE
  evaluate_dominance_prediction_model: FALSE    ##  2024 01 21 ##FALSE
#-----  input inclusion/exclusion and modification variables  -----# 
  exclude_ZL: TRUE
  bdpg_p_needs_fixing: TRUE
  exclude_imperfect_wraps: FALSE    #  2020 06 09 unfinished fails w/ both T & F
  gurobi_problem_filter: "all"    # "all" OR "completed" OR "unfinished"
  exclude_APP_0_inErr: TRUE    #  Necessary to avoid Inf and NaN magnifications
  remove_zero_output_errors: FALSE
  use_gurobi_optimal_runs_only: FALSE    #  Is this different from option "gurobi_problem_filter"?  2022 12 25 - BTL
  use_perfect_wraps_only: FALSE    #  Different from option "exclude_imperfect_wraps"?  2022 12 25 - BTL
  use_FN_dominant: TRUE
  use_FP_dominant: TRUE
  remove_probs_with_gt_10_pct_input_err: TRUE
  separate_by_redundancy: FALSE
#-----  file writing variables  -----# 
  write_tibs_to_csv: FALSE
  write_most_important_tibs_to_csv: TRUE
  file_type_to_write: "csv"    #"rds"
  add_gen_time_to_csv_name: FALSE
#-----  path variables  -----# 
  relative_path_to_input_data: "Data/Clean/All_batches/cln_exp."
  relative_path_to_data_out_loc: "Data/TempOutput"
  ggplot_save_path: "Paper_9_heavily_abridged_version_of_p8/Saved_plots"
#-----  plotting variables  -----# 
  exclude_greedy_rs_in_fit_plots: FALSE
  show_only_test_results: TRUE
  force_colors: TRUE
  display_train_as_final_pred_using_plot: FALSE    #  Whether prediction plots in pdf should show train or test data
#-----  prep for fitting variables  -----# 
  do_BoxCox: TRUE
#-----  fitting function variables  -----# 
  do_lm: TRUE    ## 2024 01 30 ##
  fitting_model_str: "lm"     ## 2024 01 30 ##
  show_lm_model_fit_coefficients_and_summary: FALSE
  echoExamplefitResultsCoefficient: FALSE    # <<<<<<<<<<==========
  do_rf: FALSE    ## 2024 01 30 ##
  ## 2024 01 30 ##fitting_model_str: "rf"    
  do_lm_cv: FALSE
  use_party_pkg_for_rf: FALSE    ## 2024 01 30 ##  This was FALSE already.  When I set it to TRUE, it failed with "Line 2019 Error in eval(predvars, data, env) : object 'train_y_vec' not found.
  do_glmnet_caret: FALSE
  do_glmnet_UC: FALSE    ## 2024 01 30 ##
  ## 2024 01 30 ## fitting_model_str: "glmnet_UC"        ## 2024 01 30 ## 
  SHOW_ALL_GLMNET_UC_PLOTS: FALSE
  VERBOSE_GLMNET_UC: FALSE        ## 2024 01 30 ## 
  VERBOSE_LM: FALSE
  VERBOSE_LM_CATS: FALSE
#-----  misc variables  -----# 
  mag_base_col_name_str: "max_TOT_FN_FP_rate"    #"max_TOT_FN_FP_rate"    #  "rsp_euc_realized_Ftot_and_cost_in_err_frac"
#-----  vestigial variables  -----# 
#  near_1_tol: 0.05
#  do_all_batches: TRUE  #FIRST ADDED FROM P6    #  Still used somewhere?  2024 01 23
#  create_p1_COR_data: TRUE
#  training_split_denom: 2
#--------------------  END of setting params variables  --------------------# 
#title: |  
#    | Learning to predict reserve selection optimization errors 
#    | under uncertainty - p8 v6
title: |  
    | Learning to predict reserve selection error under uncertainty 
    | p9 v01
author: |  
    | William T. Langford^1,3^, Ascelin Gordon^2^
    | 
    | 1 Bellingen NSW 2454, Australia
    | 2 School of Global, Urban and Social Studies, RMIT University, Melbourne, VIC 3000, Australia
    | 3 Corresponding author email: btlangford.work\@gmail.com
date: "`r paste(params$initialDate,'thru', Sys.time())`"
header-includes: 
- \usepackage{longtable}
output:
  bookdown::pdf_document2: 
    keep_tex: true
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: '6'
    fig_caption: yes
    number_sections: yes
    extra_dependencies: ["float"]
  bookdown::word_document2: 
    reference_docx: ../rmarkdownWordTemplate.docx
    keep_tex: true
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: '6'
    fig_caption: yes
    number_sections: yes
#    extra_dependencies: ["booktabs"]
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '6'
    toc_float: yes
  html_notebook: 
    toc: yes
    toc_depth: 6
bibliography: ../btl_zotero_lib.bib
csl: ../Zotero/styles/ecological-monographs.csl
---

```{r include=FALSE}

    # | `r paste0 ('#-------------------------  RMD FILE AND CHUNK CONTROL VARIABLES   -------------------------#')`
    # | `r paste0 ('[chunk_echo: ', params$chunk_echo, ']')`
    # | `r paste0 ('[chunk_include: ', params$chunk_include, ']')`
    # | `r paste0 ('[build_appendices: ', params$build_appendices, ']')`
    # | `r paste0 ('#------------------------------  PREDICTED DOMINANCE VARIABLES   ------------------------------#')`
    # | `r paste0 ('[use_predicted_dominance_values: ', params$use_predicted_dominance_values, ']')`
    # | `r paste0 ('[use_predicted_FP_dominant: ', params$use_predicted_FP_dominant, ']')`
    # | `r paste0 ('[use_predicted_FN_dominant: ', params$use_predicted_FN_dominant, ']')`
    # | `r paste0 ('[evaluate_dominance_prediction_model: ', params$evaluate_dominance_prediction_model, ']')`
    # | `r paste0 ('#--------------------  INPUT INCLUSION/EXCLUSION AND MODIFICATION VARIABLES   --------------------#')`
    # | `r paste0 ('[exc ZL: ', params$exclude_ZL, ']')`
    # | `r paste0 ('[bdpg_p_needs_fixing: ', params$bdpg_p_needs_fixing, ']')`
    # | `r paste0 ('[exc imperfect: ', params$exclude_imperfect_wraps, ']')`
    # | `r paste0 ('[gur probs: ', params$gurobi_problem_filter, ']')` 
    # | `r paste0 ('[exc APP 0 inErr: ', params$exclude_APP_0_inErr, ']')` 
    # | `r paste0 ('[remove_zero_output_errors: ', params$remove_zero_output_errors, ']')`
    # | `r paste0 ('[use_gurobi_optimal_runs_only: ', params$use_gurobi_optimal_runs_only, ']')`
    # | `r paste0 ('[use_perfect_wraps_only: ', params$use_perfect_wraps_only, ']')`
    # | `r paste0 ('[use_FN_dominant: ', params$use_FN_dominant, ']')`
    # | `r paste0 ('[use_FP_dominant: ', params$use_FP_dominant, ']')`
    # | `r paste0 ('[remove_probs_with_gt_10_pct_input_err: ', params$remove_probs_with_gt_10_pct_input_err, ']')`
    # | `r paste0 ('[separate_by_redundancy: ', params$separate_by_redundancy, ']')`
    # | `r paste0 ('#-----------------------------------  FILE WRITING VARIABLES   -----------------------------------#')`
    # | `r paste0 ('[write_tibs_to_csv: ', params$write_tibs_to_csv, ']')`
    # | `r paste0 ('[write_most_important_tibs_to_csv: ', params$write_most_important_tibs_to_csv, ']')`
    # | `r paste0 ('[file_type_to_write: ', params$file_type_to_write, ']')`
    # | `r paste0 ('[add_gen_time_to_csv_name: ', params$add_gen_time_to_csv_name, ']')`
    # | `r paste0 ('#----------------------------------------  PATH VARIABLES   ----------------------------------------#')`
    # | `r paste0 ('[relative_path_to_input_data: ', params$relative_path_to_input_data, ']')`
    # | `r paste0 ('[relative_path_to_data_out_loc: ', params$relative_path_to_data_out_loc, ']')`
    # | `r paste0 ('[ggplot_save_path: ', params$ggplot_save_path, ']')`
    # | `r paste0 ('#----------------------------------------  PLOTTING VARIABLES   ----------------------------------------#')`
    # | `r paste0 ('[exclude_greedy_rs_in_fit_plots: ', params$exclude_greedy_rs_in_fit_plots, ']')`
    # | `r paste0 ('[show_only_test_results: ', params$show_only_test_results, ']')`
    # | `r paste0 ('[force_colors: ', params$force_colors, ']')`
    # | `r paste0 ('[display_train_as_final_pred_using_plot: ', params$display_train_as_final_pred_using_plot, ']')`
    # | `r paste0 ('#-----------------------------------  PREP FOR FITTING VARIABLES   -----------------------------------#')`
    # | `r paste0 ('[do_BoxCox: ', params$do_BoxCox, ']')`
    # | `r paste0 ('#-----------------------------------  FITTING FUNCTION VARIABLES   -----------------------------------#')`
    # | `r paste0 ('[do_lm: ', params$do_lm, ']')`
    # | `r paste0 ('[fitting_model_str: ', params$fitting_model_str, ']')`
    # | `r paste0 ('[show_lm_model_fit_coefficients_and_summary: ', params$show_lm_model_fit_coefficients_and_summary, ']')`
    # | `r paste0 ('[echoExamplefitResultsCoefficient: ', params$echoExamplefitResultsCoefficient, ']')`
    # | `r paste0 ('[do_rf: ', params$do_rf, ']')`
    # | `r paste0 ('[do_lm_cv: ', params$do_lm_cv, ']')`
    # | `r paste0 ('[use_party_pkg_for_rf: ', params$use_party_pkg_for_rf, ']')`
    # | `r paste0 ('[do_glmnet_caret: ', params$do_glmnet_caret, ']')`
    # | `r paste0 ('[do_glmnet_UC: ', params$do_glmnet_UC, ']')`
    # | `r paste0 ('[SHOW_ALL_GLMNET_UC_PLOTS: ', params$SHOW_ALL_GLMNET_UC_PLOTS, ']')`
    # | `r paste0 ('[VERBOSE_GLMNET_UC: ', params$VERBOSE_GLMNET_UC, ']')`
    # | `r paste0 ('[VERBOSE_LM: ', params$VERBOSE_LM, ']')`
    # | `r paste0 ('[VERBOSE_LM_CATS: ', params$VERBOSE_LM_CATS, ']')` 
    # | `r paste0 ('#----------------------------------------  MISC VARIABLES   ----------------------------------------#')`
    # | `r paste0 ('[mag_base_col_name_str: ', params$mag_base_col_name_str, ']')`

```

\newpage

```{r version_history, eval=FALSE, echo=FALSE, cache=TRUE}

#  Version history

##  p9 v01 June 12, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v17_all_combined__body.Rmd.  

Need to greatly abridge p8, so I'm creating p9 to do that.  

##  p8 v17 June 12, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v16_all_combined__body.Rmd.  

Need to change all references to Gurobi, Marxan_SA, and Marxan_SA_SS to ILP, SA, and SA_SS.  I've done a couple of these in the version 16, but I want to separate this action out as much as possible in case it causes any problems.  So, I'm going to do only this in version 17.  This could also be done using a fork for this name changing and another for many other changes, but merging the name change fork with all kinds of other changes made on another fork will be just as messy, if not more so.  I'm putting it in a separate version number as a compromise to make it easier to revert back to the end of v16 if there are unanticipated issues down the road after changing the names.

##  p8 v16 April 16, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v15_all_combined__body.Rmd.  

Made a fairly complete, somewhat reduced version of the fully body of the paper in v15, however, there are a fair number of small details that need to be repaired.  Creating v16 to make those repairs, such as entering the correct definitions of variables in the tables of variables.  

I'm also going to remove all uses of and references to the Latapy variables in both the text and the data loading, since they were of almost no use and greatly complicate the writing up.

##  p8 v15 August 4, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v14_all_combined__body.Rmd.  

Edited v14 with respect to Ascelin comments up through most of Discussion but stopped at beginning of Predictions section of Discussion.  I've decided to try to shrink the paper and as much as possible and get rid of as much controversial stuff as possible.  If I change my mind later about doing that, I should go back to the latest version of v14 and continue with the editing I had been doing there.  At the moment, continuing with those edits seems like a waste of time if I'm going to slash things, so I'm creating a new version 15 for slashing.  

##  p8 v14 June 6, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v13_all_combined__body.Rmd.  

Froze v13 at the point where I gave it to Ascelin for review as p8_v13_all_combined__body__2024-05-01-for-ascelin.docx.  Starting v14 to make changes resulting from Ascelin review.

##  p8 v13 April 22, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v12_all_combined__body.Rmd.  

In v12, I did a lot of editing to Discussion and Methods.  Discussion is down now to needing some moderate changes but is largely there in a long form.  I'm creating this version to freeze the things that need changing before I do those changes, in case I screw up the change and want to have an easy fallback point.

##  p8 v12 February 9, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v11_all_combined__body.Rmd.  

In v11, I stripped out nearly everything in the Results section to get down to something useful for a simplified final paper form, i.e., just a simple set of 4 input feature sets and no learning of dominance, etc.  I also finished writing a rough draft for the prediction section of the Results, so that there is now a full Results section, though it will no doubt need lots of editing later.  I'm creating a v12 so that there is a frozen, fallback v11 version of the finished Results and the many extraneous bits of Discussion that I'm going to strip down now in v12.

##  p8 v11 February 1, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v10_all_combined__body.Rmd.  

In v10, I reinstated learning separately for sets based on predicted FP/FN-dominance.  In the end, it showed very little difference from all other forms of learning (e.g., using rf or glm or lm with non-pred dominance).  So, in v11 I'm going to strip out nearly everything to get down to something useful for a simplified final paper form, i.e., just a simple set of roughly 4 input feature sets and no learning of dominance, etc.

##  p8 v10 January 29, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v09_all_combined__body.Rmd.  

In v9, I deleted many input feature sets that were fed to the prediction methods.  I also deleted the section about using logistic regression to learn to predict whether a problem was FP or FN-dominated.  I've decided to reinstate that section and do all the downstream learning of error prediction using data that is split based on the learned prediction of whether a problem is FP or FN-dominated.  In case this goes awry somehow, I'm starting a new v10 so that I can just fall back to the last v9 version if necessary.

##  p8 v09 January 27, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v08_all_combined__body.Rmd.  

In v8, I split the data preparation out from the main body of the paper and added code to the main body to read the files containing the prepared data.  That was a fairly major change, so I've decided to make a new version number.  

##  p8 v08 January 27, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v07_all_combined__body.Rmd.  

In v7, I cleaned up much of the code for learning to predict and the selection of bipartite measures for prediction.  The last version of the p8 v7 Rmd files and resulting pdf represent a verbose version of what will be in v8, i.e., I've left many notes in the file and I've run the lm code with reporting of fitting coefficients turned on (params$show_lm_model_fit_coefficients_and_summary set to TRUE).  In v8, I will remove or silence many of the prediction input feature sets that were tested in v7 to make the text less complicated and because there was very little performance difference among many of the feature sets.  The pdf for the end of p8 v7 should serve as a good reference though, if you need to look at more details of fitting and correlations, etc.  I'm hoping that v8 will be a fairly strict subset of v7 in terms of computation.  The text of Methods, Results, Discussion, and Conclusions will not be such a strict subset though.

Decided to split the data loading and preparation out into a separate Rmd file so that I can leave all kinds of headings and notes in the text without making a mess out of the body of the paper.  The intention is for the new file to be run first and generate one or more files that the main body can read in as the fully prepared data.  I will then remove all the prep code from the main p8 v8 body Rmd file and replace it with one or more file read statements.  The new prep file is called p8_v08_prep_data_for_p8_to_load_from_files.Rmd.

##  p8 v07 December 13, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v06_all_combined__body.Rmd.  

In v6, I condensed and rewrote many sections of the Discussion, but I'm still missing much of anything to say about the predictions and the bipartite graph measures.  So, in v7, I'm going to work on cleaning up the code for learning to predict and the selection of bipartite measures for prediction.  To make things more structured and defensible, I'm going to use the exploration and regression procedures from a couple of Zuur and Ieno papers.  

##  p8 v06 November 26, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v05_all_combined__body.Rmd.  

After adding Study Characteristics section in v5 Discussion to organize the many different topics, I'm now going to move many Discussion sections under those new organizing headings in the Discussion.  I'm making a new version number because this is likely to totally mess with the flow of various existing Discussion sections and I want to be able to fall back to v5 and recover that flow if this restructuring of the Discussion just makes a big mess of it.  

##  p8 v05 October 13, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v04_all_combined__body.Rmd.  

Preparing to add substantial changes to Intro and Discussion.  

##  p8 v04 January 7, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v03_all_combined__body.Rmd.  

v3 deleted all mag prediction plotting and hid all but 4 of the feature sets for predicting output errors.  It also added 2 facetted bar plots to summarize all the rmse and adj R2 results from the 4 feature sets.  It also pulled most of the TODO and notes out into a separate file so that body Rmd file for p8 is now ready to edit as a paper.  However, I think that I want to delete all of the feature set prediction chunks that are currently hidden so that the pdf will build faster.  I'm going to leave those hidden chunks as the last version of v3 so that if I need any of them, I can easily go back and get them after I delete them in v4.  

##  p8 v02 January 6, 2023  
##  p8 v03 January 6, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v02_all_combined__body.Rmd.  

Finished getting v02 working with all of the p1-p3/6 data loading and prediction code mixed with pasting in nearly all of the body text from p6.  In v03, I'm going to strip out many of the prediction plots and that sort of thing so that it gets down to a manageable size.  However, I wanted to freeze the v02 version at the point of working with every single plot plus all the p6 text.  That way, after I cut a bunch of things out in v03, I can still go back to v02 to grab anything that I shouldn't have cut out in v03.

##  p8 v02 January 6, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v01_all_combined.Rmd.  Also renaming to include "__body" in the file name since that had somehow been dropped in v01.  

Everything was working in v01 with unified data loading from p1-p3, 13 different input feature sets predicting rep shortfall and solution cost errors and error magnifications, and bar plotting the adjusted R2 and rmse for all 13 datasets and each predicted error variable.  So, I'm freezing that version and moving on to a new version to incorporate all the text from paper 6 (though probably using the model generator text from paper 1 instead of the reduced paper 6 version of that text).  I'll probably also get rid of all the predicting of magnifications because they're pretty similar to the raw error predictions (though a little worse) and don't really advance the story we're trying to tell.  They're more useful in the paper 2/6 story that's characterizing the errors made in optimization.  

##  p8 v01 December 20, 2022  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/v01_Paper_8_all_combined__body.Rmd

Even after stripping out most of the p6 stuff to make the first cut at v01_Paper_8_all_combined__body.Rmd, there was still too much noise in the file for the early going where I'm trying to get code working instead of writing text.  So, I'm cutting out all the text, appendices, etc, so that I can visually focus on getting the basic code right before I mess with the text again.  This will mostly be about getting the code from paper 3 set up correctly and stripped down.

Last but not least, in all the bdpgtext directories, the file names have started with the version number and used the full word "Paper" in them.  The full word is unnecessary and having the version before the paper number in the file name is often confusing.  So, I'm switching the name to lead with "p8_v01" instead.  I've taken the file that this file is based on (the one beginning "v01_Paper_8" and put it in bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/older_Rmd_versions.

##  v01 (of paper 8) December 17, 2022  

Cloned from bdpgtext/Paper_6_p1p2_combined/v13_Paper_6_p1p2_combined__body.Rmd

Combining papers 1, 2, and 3 by cannibalizing:
- paper 6, which combined papers 1 and 2, and 
-  paper 5, which was the first try at the long form of combining all 3 papers but has been had the first 2 parts of it superseded by paper 6. 

I've copied in the entire p6 v13 Rmd file, but I'm going to strip out nearly everything to make sure that I'm starting with a clean slate and getting rid of any cruft that has built up in p6. 

```

```{r include=FALSE}
#  Header code chunks (option setting, history, etc.)

##  Set latex and knitr options
```

```{cat, engine.opts = list(file = "header.tex")}
%  This chunk and the pdf_document header lines: 
%
%    keep_tex: true
%    includes:
%      in_header: header.tex
%
%  are hacks to keep knitting to a pdf from blowing up with the 
%  following message due to a known bug in pandoc:
%
%  This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded %  format=pdflatex)
%   restricted \write18 enabled.
%  entering extended mode
%  ! Missing number, treated as zero.
%  <to be read again> 
%                     \protect 
%  l.514 ...nter}\rule{0.5\linewidth}{\linethickness}
%                                                    \end{center} 
%
%  I found this solution at:
%      Horizontal rule in R Markdown / Bookdown causing errors
%          https://stackoverflow.com/questions/58587918/horizontal-rule-in-r-markdown-bookdown-causing-errors 
%
%  On 2020 01 05, I tried replacing the current pandoc that knitr uses (version 2.3.1) 
%  with the latest version of pandoc (version 2.9.1).  While that does 
%  fix this problem, it blows up in a completely different way that I have 
%  not been able to find a way to fix, so I'm going with this hack.  
%  I will have to use this hack in EVERY file I want to knit that uses  
%  markdown's "---" to specify a horizontal line.
%
%  Note that the comment lines in this chunk must be marked with "%" instead 
%  of "#" because they're going to be included in the latex header.tex file 
%  and "%" is the latex comment marker.
%  Some documentation for the "cat engine" that drives this chunk 
%  can be found at:
%      https://bookdown.org/yihui/rmarkdown-cookbook/eng-cat.html

\renewcommand{\linethickness}{0.05em}
```

```{r global_options, include=FALSE}

    #  Global options are set here based on some examples in:
    #      https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
    #      https://kbroman.org/knitr_knutshell/pages/figs_tables.html

knitr::opts_chunk$set(#fig.width=12, 
                      #fig.height=8, 
                      fig.path='Figs/',  #  Save figures to indiv files in Figs/
                      echo=FALSE,        #  Don't echo code to output
                      
                      warning=FALSE,  
                      message=FALSE, 

                      fig.pos = "H", out.extra = ""    #  Keep latex from floating figures
                     )
```

```{r loadLibraries, echo=FALSE}

##  Load R libraries  

     #===========================================================================
    #  2022 12 20 - BTL
    #  THIS LONG COMMENT ABOUT THE message=FALSE CHUNK OPTION COMES FROM THE 
    #  LIBRARY LOADING SECTION OF P3 V6, HOWEVER THAT CHUNK IN THAT FILE 
    #  NO LONGER INCLUDES THAT CHUNK OPTION. IT JUST SAYS include=FALSE NOW.  
    #  SO, MAYBE THE message OPTION DOESN'T HAVE TO BE EXPLICITLY SET NOW 
    #  BECAUSE IT'S SET IN THE GLOBAL CHUNK OPTIONS IN THE BROMAN CHUNK ABOVE.
    #  -------------------------------------------------------------------------
    #  Note that "message=FALSE" is necessary for this chunk if you want to 
    #  generate pdfs.  When the tidyverse package is loaded, it writes puts 
    #  out a message that includes some unicode that the normal latex engine 
    #  (used to produce the pdf) can't handle and it crashes with the following 
    #  message:
    #       ! Package inputenc Error: Unicode character [sqrt symbol goes here] (U+221A)
    #       (inputenc)                not set up for use with LaTeX.
    #  Note that I've also had to remove the sqrt symbol from the error message 
    #  when embedding it in the comment here, because even inside the comment, 
    #  latex tried to render that and crashed.
    #  More information about this can be found at:
    #   - https://community.rstudio.com/t/tidyverse-1-2-1-knitting-to-pdf-issue/2880/4
    #   - https://community.rstudio.com/t/cant-render-tidyverse-1-2-startup-message-in-latex/2811/5
    #   - https://chrisbeeley.net/?p=1037
    #===========================================================================

   #===========================================================================
    #  Suppress startup message for tidyverse package because when it's loaded, 
    #  it puts out a message that includes some unicode that the normal latex  
    #  engine (used to produce the pdf) can't handle and it crashes with the  
    #  following message:
    #       ! Package inputenc Error: Unicode character [sqrt symbol goes here] (U+221A)
    #       (inputenc)                not set up for use with LaTeX.
    #  Note that I've also had to remove the sqrt symbol from the error message 
    #  when embedding it in the comment here, because even inside the comment, 
    #  latex tried to render that and crashed.
    #  More information about this can be found at:
    #   - https://community.rstudio.com/t/tidyverse-1-2-1-knitting-to-pdf-issue/2880/4
    #   - https://community.rstudio.com/t/cant-render-tidyverse-1-2-startup-message-in-latex/2811/5
    #   - https://chrisbeeley.net/?p=1037
    #===========================================================================

suppressPackageStartupMessages (library (tidyverse))
library("tidylog", warn.conflicts = FALSE)    #  Load AFTER tidyverse packages

library (here)
library (glue)

library (knitr)    #  For include_graphics(), kable()

library (ggplot2)
library (scales)    #  For percent in "scale_y_continuous (labels = percent, ..."
library(ggthemes)

library (patchwork)    #  To make combinations of ggplots.
library (cowplot)    #  For background_grid() function (at least).

#library(viridis)    #  For color scale.  Not sure if necessary in the end.

#library(GGally)  #  For ggpairs() function.  Probably won't need this in the end.  

#library (readr)

###  Libraries from p3 v6

#library (tidymodels)

library (corrplot)  #  For correlation plots
library (corrr)    #  For correlate() function

library (ranger)    #  For random forests

library (caret)  #  For data preprocessing functions, e.g., scaling and BoxCox

#library (DMwR)    #  For regr.eval()  #  2024 03 06 - No longer necessary.
                   #  Have copied and slightly modified regr.eval() source 
                   #  code into the plotting and evaluation code R file for 
                   #  this project since it was the only thing used from DMwR 
                   #  and the DMwR library has been updated to DMwR2, so it 
                   #  might be a problem for other people to get at some point.
##library (DMwR2)  #  DON'T LOAD THIS - SOME EXAMPLES FROM BOOK FAIL SINCE THEIR 
                  #  TYPES HAVE CHANGED.  FOR EXAMPLE, algae IS A TIBBLE IN THE 
                  #  REVISED LIBRARY AND A DATA FRAME IN THE ORIGINAL.
                  #  THIS CAUSED SOME OF MY CODE TO CHOKE.

#library (stringr)    #  For str_replace()

#library (e1071)
library (factoextra)    #  Related to principal components

library (glmnet)   #  implementing regularized regression approaches

#library (lindia)   #  diagnostic ggplots for linear regression

#library (cowplot)    #  To plot multiple plots together


#library (usethis)    #  For echo functions like ui_done().

library (bdpg)    #  For safe_sample().

library (broom)    #  For tidy() to apply to glm prediction results.

library (latex2exp)    #  For Tex() function to use latex math formatting in ggplot title
```

```{r setFilePaths, include=params$chunk_include}

##  Set file paths
 
proj_dir = here()
cat ("\n\nproj_dir = here() = ", proj_dir, "\n", sep='')
```

```{r setSeeds, include=params$chunk_include}

###  Set random number seeds for reproducibility

seed1 = 12345
seed2 = 8910
seed3 = 1230

# seed1 = 54321
# seed2 = 198
# seed3 = 321

# seed1 = 101
# seed2 = 17
# seed3 = 83

seed = set.seed (seed1)
```

```{r setBdpgOptionsThatAreHardToSetInParams, include=FALSE}

##  Set bdpg options  

    #  Two options are either used globally or are hard to set in 
    #  the header params list, so set them here.

    #  This is both hard to set in params header and used globally.
if (as.logical (params$exclude_ZL))
    {
    rs_method_names_list = c("ILP", "SA", "UR_Forward", 
                             "SA_SS")
    } else
    {
    rs_method_names_list = c("ILP", "SA", "UR_Forward", 
                             "ZL_Backward", "SA_SS")
    }

```

```{r checkParamsAndSetDefaults, include=FALSE}

    #  Check the params list for errors and missing default values.
    #  Either set values appropriately or crash if that's more appropriate.

cat ("VALIDATION CODE FOR PARAMS IS STILL MISSING HERE !!")

```

```{r echoOptions, include=FALSE}

#  Option settings  

#  Params should all be properly set at this point.  

#-------------------------------------------------------------------------------

    #  Echo the values of the main options.

cat ("\nMain options are:\n")

for (idx in 1:length(params))   
  cat ("\n", names(params)[idx], ": ", params[[idx]], sep="")

cat ("\nrs_method_names_list = ", rs_method_names_list, "\n")
```

```{r loadP1andP2FunctionDefns, include=FALSE}

#  Load R functions

#-------------------------------------------------------------------------------
#  2020 08 20 - BTL
#  "Rmarkdown Cookbook" section	"16.1 Source external R scripts" says to 
#  include the "local" argument.
#       https://bookdown.org/yihui/rmarkdown-cookbook/source-script.html
#       "We recommend that you use the argument local in source() or envir in 
#        sys.source() explicitly to make sure the code is evaluated in the 
#        correct environment, i.e., knitr::knit_global(). The default values 
#        for them may not be the appropriate environment: you may end up 
#        creating variables in the wrong environment, and being surprised 
#        that certain objects are not found in later code chunks."
#  I haven't noticed a problem with this, but maybe it's been there and 
#  I just haven't had it affect something important enough to notice.
#-------------------------------------------------------------------------------

# source (file.path (proj_dir,         #  For ggplot w/ magrays, etc
#                    "R/v3_Paper_2_bdpg_analysis_scripts_function_defns.paper_2.R"), 
#         local = knitr::knit_global())  
# 
# source (file.path (proj_dir, "R/v1_p5_unifiedDataLoading.R"), 
#         local = knitr::knit_global())

## 2022 12 17  ##source (file.path (proj_dir, 
## 2022 12 17  ##"R/v1_p6_load_libraries_and_R_source_code.R"), 
## 2022 12 17  ##        local = knitr::knit_global())



source (file.path (proj_dir, "/R_new/p8.unifiedDataLoading.v01.R"), 
         local = knitr::knit_global())

#--------------------

source (file.path (proj_dir, "/R_new/p8.preprocessingForLearning.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/p8.BoxCoxAndNormalizingFunctions.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_plotting_and_evaluation_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_utility_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_fitting_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/p8.matildaFunctions.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v2_paper_2_func_defns_for_plotting.R"), 
         local = knitr::knit_global())

# 2025 11 29 - BTL
# Move source file from R to R_new since it is referenced in current code, 
# which assumes R code is in R_new.

# source (file.path (proj_dir, "/R/v2_paper_3_cv_test_train_splitting_functions.R"), 
#          local = knitr::knit_global())
source (file.path (proj_dir, "/R_new/v2_paper_3_cv_test_train_splitting_functions.R"), 
         local = knitr::knit_global())
```

```{r loadPreppedDataFromFiles, include=FALSE}

#  Load prepped data from files
#  These are all data structures that are referenced in the paper but 
#  have been prepared elsewhere to simplify this file, e.g., in
#   p8_v??_prep_data_for_p8_to_load_from_files.Rmd.

#--------------------  

filtered_full_initial_exp_tib = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "filtered_full_initial_exp_tib.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p2_app_wrap_tib = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p2_app_wrap_tib.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p3_working_train_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_train_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

p3_working_test_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_test_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p3_train_aux_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_train_aux_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

p3_test_aux_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_test_aux_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

```

```{r load_p3_working_train_data, include=FALSE}

p3_working_train_df__before_any_preprocessing = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_train_df__before_any_preprocessing.gurobi__all.exclude_imperfect_wraps__FALSE.csv")
```

```{r load_p3_working_test_data, include=FALSE}

p3_working_test_df__before_any_preprocessing = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_test_df__before_any_preprocessing.gurobi__all.exclude_imperfect_wraps__FALSE.csv")
```
















```{r convertRSmethodNamesInLoadedData, include=FALSE}

#  Replace all occurrences of "Gurobi" in loaded data with "ILP" to satisfy 
#  reviewer.

#--------------------  

filtered_full_initial_exp_tib <- filtered_full_initial_exp_tib %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p2_app_wrap_tib <- p2_app_wrap_tib %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p3_working_train_df <- p3_working_train_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))



p3_working_test_df <- p3_working_test_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p3_train_aux_df <- p3_train_aux_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))



p3_test_aux_df <- p3_test_aux_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

```

```{r convertRSmethodNamesInp3_working_train_data, include=FALSE}

p3_working_train_df__before_any_preprocessing <- p3_working_train_df__before_any_preprocessing %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

```

```{r convertRSmethodNamesInp3_working_test_data, include=FALSE}

p3_working_test_df__before_any_preprocessing <- p3_working_test_df__before_any_preprocessing %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

```



\newpage

#  Abstract  

1. Mathematical optimization methods are commonly used for conservation reserve selection.  However, the true accuracy of these methods under input uncertainty is unknown for several reasons, including: i) methods are generally tested on small numbers of problems and uncertainties, ii) there is little variation in test problem structure and difficulty, and iii) the correct solutions for the uncertain test problems are often unknown, implying the true amount of reserve selection error is also unknown.  Consequently, it is difficult to predict a reserve selector's error on any specific problem.  This is important because  unexpectedly large error can drastically increase the already large risks to species covered by the plans.  Moreover, overly optimistic beliefs about reserve selector accuracy hinders development of better methods.   

2. We use graph theory and solution planting to build a three step sequence to learn to predict optimizer error on individual problems under input uncertainty.   First, we introduce and extend solution planting methods from computational complexity theory for generating reserve selection problems with known solutions and theoretically-guided variation in difficulty.  Second, we apply these generators to produce 18,000 problems of varying difficulty, known uncertainty, and known correct solutions, then compare the errors of four reserve selectors on all problems.  Third, we convert each problem into a bipartite graph and compute graph structure measures to use as inputs to statistical models that predict the amount of each reserve selector's error on each individual problem under uncertainty.  

3. The new generators provide publicly available, easily generated, statistically useful numbers of experiments with realistic size, known solutions, varying difficulty, and structural variety.  The method evaluations demonstrated large variation and large reserve selector output error even with small input errors (median output error ~5 times input error).  Learning statistical models from graph measures over this data led to a roughly 3-fold improvement over using problem size alone in predicting the amount and nature of optimizer error under uncertainty on individual reserve selection problems.  

4. This work provides a framework for improving reserve selector development and for identifying problems where optimizer risk may be unacceptably large, implying a need for other actions or method improvements.  

#  Keywords    

systematic conservation planning, uncertainty, reserve selection method comparison, 
computational solution planting, synthetic species data, problem difficulty, graph theory

\newpage

#  Introduction  

The natural world has been significantly altered by multiple anthropogenic drivers, resulting in growing loss of biodiversity (@diaz2019). Mathematical approaches to optimally allocating conservation actions represent a promising avenue for addressing this problem.  These methods are often referred to as systematic conservation planning (SCP) (@margules2000n). *Reserve selection* is one important subfamily of these SCP approaches where the action to be optimized is the selection of locations (referred to as "planning units") to implement new protected areas or "reserves".  An example of reserve selection is to combine maps of species distributions, land parcels (i.e., planning units), and costs as inputs to determine the most cost-effective set of parcels to reserve to protect a specified proportion of habitat for a range of species.  

These methods have shown great promise and have been widely adopted. However, the development and evaluation of reserve selection methods face several fundamental challenges that the research community continues to address. Even the simplest mathematical formulations of reserve selection fall into the class of provably difficult computational problems known as NP-hard problems (@garey1979).  While many problems in this class allow finding an optimal solution within a reasonable time, other problems are intractable using current methods.  Crucially, this variation in individual problem difficulty relates to the structure of the problem, not just its size and can be difficult to predict.  As Sarkar et al. noted twenty years ago regarding "place prioritization algorithms" (PPAs) (Sarkar et al. 2004):

```  
    We could not identify any set of features that predicted the 
    accuracy of an algorithm. Even size was not an unequivocal 
    predictor. However, the various PPAs, both optimal and heuristic, 
    tracked the problems in qualitatively similar ways.  All found 
    the same ones difficult or easy which suggests that there are 
    definite characteristics of the data sets that explain the 
    performance of various PPAs. This is a problem that will merit 
    further study.  
```  

In the intervening twenty years, improved SCP methods have made it possible to exactly solve most reserve selection problems when the input data is correct.  However, similar problems remain in developing tools to understand and predict method performance on specific individual problems under the uncertainty that is pervasive in real-world SCP inputs.  Many input values can be difficult to determine accurately and may also change over time, e.g., presence of rare species, land costs, land availability, as well as  large-scale effects such as climate, economics, and land-use change.  

The research community has made significant progress in developing methods that account for varying degrees of uncertainties, providing optimal or near-optimal solutions for specific problem reformulations (e.g., @beechStochasticApproachMarine2008, @andoOptimalPortfolioDesign2012,  @tullochIncorporatingUncertaintyAssociated2013, @billionnet2015ema, @haider2018em, @runtingReducingRiskReserve2018, @eatonSpatialConservationPlanning2019,  @sierra-altamiranda2020em, @ghasemisaghand2021e).  However, each approach necessarily makes assumptions about the inclusion and quantification of uncertainties (e.g., declaring upper bounds on the magnitude of input errors or on the number of input variables containing error).  Given the complexity of addressing many common uncertainties simultaneously, many SCP problem formulations have necessarily focused on subsets of uncertainties to maintain mathematical tractability (@moilanen2008bc).  For example, methods may assume that costs are known with certainty (@haider2018em) or that there is no overprediction of species presence in the input (@velazco2020bc).  Another  difficult issue is ensuring the correctness of uncertainty quantifications themselves, e.g., producing well-calibrated probabilities of species presence/absence - which is a difficult problem given the scarcity of data (@valavi2022em , @muscatello2021cb).  Together, these assumptions have the effect of shifting uncertainties from input variables to the variables characterizing their uncertainties.

Given these inherent challenges, understanding how reserve selectors are likely to perform on real-world problems with uncertainty remains an important open question.  In particular, the magnitudes of errors in the solution for any specific problem are difficult to predict. Currently, we have two primary approaches for demonstrating method reliability: theoretical guarantees and empirical evaluations.  

In the first case, methods such as robust integer linear programming (@billionnetSolvingProbabilisticReserve2011,  @billionnetMathematicalOptimizationIdeas2013,  @haider2018em) have theoretical guarantees to produce an optimal solution assuming that i) all of the problem uncertainties and problem characteristics can be and have been correctly captured in the problem definition and method assumptions, as well as ii) a feasible solution exists, and iii) sufficient computational resources are available.  These represent important theoretical advances, but meeting all of these theoretical criteria is demanding in practice and we currently lack the tools to know the consequences of failing to meet them for individual real-world problems.    

In the second case, where methods lack theoretical guarantees (e.g., heuristic methods or methods where assumptions may not hold perfectly), we must rely on empirical evaluations.  However, the field faces several well-recognized obstacles to providing strong evidence for generalizing empirical results to specific, previously unseen problems:

1)  **Sample size**: Comprehensive evaluation requires substantial time and computational resources, so evaluations usually rely on single real-world case studies or small ad-hoc synthetic data sets  that provide limited coverage of the full space of problems.  

2)  **Problem difficulty**:  We have few theoretical or empirical studies characterizing problem attributes that control problem difficulty under uncertainty other than problem size.  Consequently, most evaluations involving more than a single case study vary problems in the test set primarily based on problem size rather than structural diversity, though some notable exceptions explore specific aspects of problem structure (e.g., @presseyEffectsDataCharacteristics1999, @viscontiBuildingRobustConservation2015).

3)  **Data sharing**: Sharing of test problems between researchers is limited, which hampers comparative evaluation and the development of shared benchmarks for understanding problem attributes.  

4)  **Uncertainty**:  Evaluating methods under uncertainty requires careful experimental design to capture the range of uncertainty types and magnitudes found in practice.  Current evaluation problems often include no uncertainty or only include a single type and amount of uncertainty, e.g., studies where all uncertainties are 10% false negatives.

5)  **Known solution**: The true optimum is generally unknown in real-world problems with uncertain inputs, making it difficult to quantify solution error. 

These obstacles create a shared problem for the entire research community.  They can  make it difficult to give us clear direction about where to focus improvement efforts for methods whose true strengths and weaknesses are unknown.  Moreover, they make it difficult to provide practitioners with reliable guidance about how methods will perform on their specific problems and can increase the risks to species covered by plans with unknown amounts of error.  

Lacking reliable predictions of solution error, practitioners often can and do take  actions to address suspected uncertainties in their input data and problem formulation.  For example, they might improve the sampling and modelling of species or they might increase possibilities for redundancy in solutions by increasing representation targets.  However, determining the consequences and appropriate magnitudes of such actions remains difficult without better predictive evidence for solution error types and amounts resulting from the intrinsic difficulty of specific problem structures such as species co-occurrence and abundance patterns.  

To address these shared issues, we propose developing systematic approaches for making explicit, generalizable, and testable predictions of reserve selector performance based on individual problem characteristics.  This would enable practitioners to estimate how difficult their problem is likely to be for a specific reserve selector and to determine whether it would be useful and cost-effective to gather more data to reduce uncertainty, i.e., the value of information (@canessa2015mee, @raymond2020joaea).  It would also give us more information on which problem structures methods perform best and worst, facilitating targeted improvements to methods.  While such prediction is challenging, explicit attempts would help identify knowledge gaps in our understanding of both methods and problems.

Making predictions requires us to address the five difficulties listed earlier.  We need large sets of publicly available reserve selection problems with known optimal solutions as well as systematic variation in controlled uncertainties and structural difficulties.  In this paper, we contribute to this goal by adapting techniques from computational complexity theory and graph theory to present a sequence of interconnected actions addressing the five difficulties above:

1) We introduce and extend a *solution planting* method for generating test problems with known correct solutions, controllable problem difficulty as a function of problem structure, and ecologically relevant patterns of species abundance (Sections \ref{modelRBexplanation} and \ref{modelWrapExplanation}),  

2) We use this method to generate a publicly available benchmark set of 3,000 independent problems and 18,000 variants with wide variation in size and difficulty, including two types of input error across a range of error magnitudes. We demonstrate its use by evaluating four reserve selectors across this problem set (Section  \ref{comparisonResultsSection}),

3) Based on those results, we learn models to predict the amount of error different reserve selectors will make on individual problems under uncertainty.  We do this by reframing each problem as a bipartite graph and computing a set of predictive input features based on graph-theoretic measures characterizing structural attributes of that problem (Section  \ref{predictionResultsSection})

Our goal is to help method developers understand their methods' performance characteristics and to help practitioners make informed and reliable decisions about method selection. 

#  Definitions  

In what follows, "LongformSI", "AppSI", "CorSI" and "ShinySI" are supplemental information files with appendices containing more detail and further results.  

Throughout this paper, we use terminology from multiple fields that may be unfamiliar to some readers in other fields, so we first specify ecological and graph theory terms before giving detailed examples illustrating the problem generators.

**Reserve Selection Method**  For brevity, we will use the terms *reserve selector* or *reserve selection method*  to refer to the combination of i) the optimization algorithm, ii) the chosen parameter settings, iii) the chosen mathematical formulation of the problem, and iv) any assumptions made about the data used by the algorithm.    

**Problem difficulty and/or Method performance**  The amount of error in the solution given a known correct optimum solution rather than the amount of time required to solve the problem.  

**Problem structure**  In this paper, problem structure is primarily the combination of species co-occurrence pattern and species rank abundance distribution.  In more complex problems, structure would also include other information, e.g., distributions of representation targets and PU costs.  

**Planning units (PUs) and Conservation features**  The conservation planning literature commonly refers to *planning units* (*PUs*) and *conservation features*. Without loss of generality, we use *PUs* and "*species*" because our examples use species distributions. Our results apply equally to any non-species features that can be associated with PUs.  We refer to the full set of PUs as the "*landscape*", but there is no spatial arrangement of PUs assumed. 

**Rank abundance distribution**  In ecology, species *abundance* refers to counts of organisms, but here we use the number of PUs where a species occurs as a surrogate for abundance. By *species rank abundance distributions*, we refer to the number of PUs each species occurs on, ranked in decreasing order from species occurring on the most PUs to those on the fewest (Fig. \ref{fig:buildExampleWrapDistHist}).

**Graph theory terms**  The graph theory literature often uses *vertices* and *edges*, but here we use *nodes* and *links* since they are more common in ecology, except when we refer to the *minimum vertex cover* problem since that is its usual name in computational literature (@garey1979).  In what follows, *nodes correspond to PUs and links correspond to species*. Graph theory terms required to understand our implementation of model RB are:  

- **_clique_**: A set of nodes where every node is connected to every other node in that set.  There can be many cliques within a single graph.
- **_independent set_**: A subset of the graph's nodes where no pair of 
nodes inside that subset has a link connecting them, though each may connect to nodes outside the subset.
- **_maximum independent set_**: An independent set where no other independent set in the graph contains more nodes though others could be equal size.
- **_vertex cover_**: A subset of nodes in a graph where every link in the full graph has at least one end connected to a node in the cover.
- **_minimum vertex cover_**: A vertex cover such that no other cover contains fewer nodes though others could be equal size.  

See Fig. \ref{fig:BaseProbExampleDiagram}b) for an example of both a maximum independent set and a minimum vertex cover.

#  Problem generators  

Our need to generate problems with a range of difficulty and known correct solutions is similar to the need in the computational complexity literature for building problems to evaluate approximations to solutions of NP-hard problems.  There, researchers have developed methods to generate difficult constraint satisfaction problems with known optimal solutions using a technique called *solution planting* (see @zhou2021acm for overview of planting literature).  Solution planting involves generating a structured set of elements that is designated to be the optimal answer to a problem (e.g., a set of planning units to reserve and the species occurrences on them), then adding misleading elements (such as other planning units and species occurrences) in a way that hides this correct answer but is guaranteed not to alter what is the optimal solution.  

In Section \ref{modelRBexplanation}  below, we describe a solution planting method based on *model RB* borrowed from computational complexity theory (@xu2005ipnijcai) for quickly generating large sets of problems with known optimal solutions, whose difficulty varies widely even within problems of the same size.  An important advantage of the method is that it provides a theoretical basis for which input parameter combinations will generate problems with different levels of difficulty (Section \ref{modelRBexplanation}).  

However, an ecological issue arising from the theoretical origins of model RB is that it uses a uniform distribution of problem elements (in our case, species) that is unlikely in real-world reserve selection. To make it useful in reserve selection evaluation, we have developed a new extension of the method to generate what we call "Wrap" problems with more ecologically relevant, user-specified distributions of species occurrence wrapped around the Base problem while preserving its planted solution (Section \ref{modelWrapExplanation}). This enables the rigorous evaluation of the performance of target-based reserve selection algorithms across a wide range of problems with varying difficulties and levels of uncertainty where we know the correct solution.  

##  Base problem using model RB {#modelRBexplanation}

Using model RB to generate reserve selection problems relies on the fact that for some hard problems, the solution to one type of problem can be translated into the solution for a different type of problem. We can translate a vertex cover problem into a reserve selection problem by  designating (i) each node as a PU, and (ii) each link as a species that occupies the two PUs connected by the link and occupying no other PUs. With this correspondence, a minimum vertex cover then represents the smallest number of PUs that contain at least one occurrence of each species.  Next, as explained in @xuke2014, we rely on two facts: First, it is easy to generate a graph with a known maximum independent set and second, a maximum independent set is the complement of a minimum vertex cover.  For example, nodes above the dashed line in Fig. \ref{fig:BaseProbExampleDiagram}b) constitute a maximum independent set in that graph, while all nodes outside that set automatically make up a minimum vertex cover (nodes below dashed line in Fig. \ref{fig:BaseProbExampleDiagram}b).

@xuke2014 briefly describes how to use model RB to easily generate a graph and a maximum independent set for that graph (and therefore, a reserve selection problem with a known solution). Quoting from @xuke2014 :  

1. Generate n disjoint cliques, each of which has $n\alpha$ vertices (where $\alpha > 0$ is a constant);
2. Randomly select two different cliques and then generate without repetitions $pn2\alpha$ random edges between these two cliques (where $0 < p < 1$ is a constant);
3. Run Step 2 (with repetitions) for another $rn~ln(n)$ times (where $r>0$ is a constant).

###  Example of creating base problem using model RB {#modelRBexample}

To make this clearer in the context of reserve selection, we now provide a simple example with diagrams in Fig. \ref{fig:BaseProbExampleDiagram} to illustrate the process of deriving a reserve selection problem with a known optimal solution.  The essence of the algorithm is the following four steps: (i) choose parameters; (ii) generate core of graph (cliques); (iii) designate independent set and reserve solution; and (iv) hide solution using multiple rounds of linking between cliques. Steps ii and iii plant a solution, i.e., they create a problem with a known solution.  Step iv hides the solution by adding extra links (species) that leave the original solution intact but add noise that the search must wade through. Pseudocode for the algorithm is given in CorSI Appendix B and we now step through a simple example illustrated in Fig.  \ref{fig:BaseProbExampleDiagram}.

**Choose parameters.** Four parameters are required to specify each reserve selection problem.  The first two are the number of cliques and the number of nodes per clique used in planting the solution.  The second two parameters relate to hiding the solution, which is done in a series of rounds where in each round, a pair of cliques is randomly chosen and then several random links are made between those two cliques.  This means that the final two parameters we need are the number of rounds of linking and the number of links per round.  

We can designate those four variables directly, but if we do, we have no connection to the theoretical predictions made in the development of model RB.  It was derived in a theoretical setting of constraint satisfaction problems in general and requires the user to specify four different input parameters related to the computational theory behind predicting the problem's difficulty (@xu2005ipnijcai): 

- $n$: number of cliques
- $\alpha$: exponent driving number of nodes per clique
- $r$: constraint tightness (multiplier driving number of rounds of linking)
- $p$: constraint density (proportion driving number of links per round)

These variables are now used to derive variables with a direct interpretation for building reserve selection problems:  

- $n$: number of cliques = $n$
- $d$: number of nodes per clique = $n^\alpha$
- $m$: number of rounds of linking = $rn~ln(n)$
- $\theta$: number of links per round = $pd^2$

For our example, we can arbitrarily choose parameter values that will lead to a graph that is small and simple to understand, i.e., $n=3,d=4,m=4,\theta=2$.

**Generate core of graph.**  Based on those parameter choices, we define 3 cliques containing 4 nodes in each clique, initially with no node linked to any node outside of its clique (Fig. \ref{fig:BaseProbExampleDiagram}a).  Since cliques are fully connected by definition, each 4-node clique has 6 links, giving 18 links in the graph.  This corresponds to 18 species spread across 12 PUs with each species occurring on exactly 2 PUs.  

**Designate independent set and reserve solution.**  Randomly choose one node from each clique to be part of the independent set, making a total of 3 nodes (arbitrarily chosen to be nodes above dashed line in Fig. \ref{fig:BaseProbExampleDiagram}b). Because we have chosen one node from each clique and the cliques have been initialized with no connections between them, we know these 3 nodes have no connections between them and therefore, comprise an independent set. We also know that the independent set cannot be any larger than this because each of these independent nodes is connected to every other node in its clique, so none of the other nodes in those cliques are independent of the chosen nodes or independent of any other nodes in their clique.

Since the cover and the independent set are complements of each other, we now know that the minimum vertex cover consists of all nodes not in the maximum independent set (nodes below dashed line in Fig. \ref{fig:BaseProbExampleDiagram}b).  Thus, the 9 PUs in the minimum vertex cover set are known to be a correct solution: the smallest reserve selection that will provide at least one occurrence of all 18 species.  Note that no node in the cover set could be brought into the independent set because that node would violate independence by being connected to a node already in the independent set.

**Hide the solution.**  model RB says that we can make the problem more difficult by hiding this solution through adding more links in a specific way. We do 4 rounds of random linking (given by $m$).  In each round, we randomly choose 2 cliques and add 2 links (given by $\theta$), each between randomly chosen pairs of nodes having one node in each clique (Fig. \ref{fig:BaseProbExampleDiagram}c,d). Note that there is no significance in there being 2 links.  We could have chosen any number of links between the 2 cliques, but chose 2 to keep the example simple.

There is one caveat in the linking; no link can violate the independent set condition by linking the independent node of one clique to the independent node of another clique.  We *could* connect the independent node of one clique to a non-independent node of another clique without violating the independent set, but for simplicity, we restrict all added links to go between nodes not in the independent set.  

After 4 rounds we now have 8 new links for a total of 26 links in this graph.  Since we have added no new nodes, the final problem still has 12 PUs but now has 26 species, each occurring on exactly 2 PUs.  Finally, the smallest possible set of PUs covering at least one instance of all species still must contain 9 PUs.  

```{r BaseFig, echo=FALSE, fig.align="center", out.height = "40%", fig.cap = "\\label{fig:BaseProbExampleDiagram} Diagram depicting the steps to generate a synthetic reserve selection problem using model RB. The dashed line indicates separation between maximum independent set nodes and minimum vertex cover nodes.  The three nodes above the line are maximum independent set PUs while the nine nodes below the line are minimum vertex cover PUs (an optimal reserve selection). Each link represents a species that occurs on the two connected PU nodes and nowhere else. The colors of links in c) and d) distinguish different rounds of linking."}

base_img_path = file.path (proj_dir, "Figures/Figure_RB.png")
include_graphics(base_img_path)
```

##  Wrapped problem {#modelWrapExplanation}

There are two issues with using model RB to generate useful synthetic reserve selection problems. First, model RB produces problems with ecologically unrealistic abundance distributions of exactly 2 occurrences of every species, thus we need to extend the method to generate more realistic distributions of species occurrences. Second, in preliminary testing we found that model RB generally generates its most difficult problems where the correct solution is a large proportion of the total number of PUs. Since realistic reserve selection problems generally require selecting a small proportion of the landscape, we need to be able to generate test problems with solutions requiring smaller proportions of the landscape.

We address these issues by developing a procedure for wrapping one rank abundance distribution around another in a way that guarantees that the original base problems solution is still the solution to the *wrapped* problem but allows the final problem to have an arbitrary rank abundance distribution. Moreover, we can specify the proportion of the wrapped problems landscape taken up by the correct solution. This may mean that we are unable to generate problems as difficult as the hardest benchmark problems, but our results show that we can still generate problems with a wide range of difficulty for some reserve selectors.

The key idea in wrapping is that any rank abundance distribution will have a section of the distribution specifying species that occur on exactly 2 PUs. To generate a wrapped problem, we simply build a rank abundance curve of a desired form that contains a part of the distribution corresponding to the set of species on exactly two PUs in the base problem.

Consider a new example Base problem to be wrapped (Fig.  \ref{fig:WrapProbExampleDiagram}a).  For simplicity, this example has only two base cliques, each containing three nodes and three links.  Suppose that additional rounds of linking between those cliques have added four more links, resulting in a total of ten species in this Base problem.  If we label those nodes with PU IDs A-F and number the links as species IDs 5-14 (the reason for not starting these IDs at 1 will become clear below), then we can move from a graph representation to a landscape representation of 6 PUs containing species occurrences (Fig. \ref{fig:WrapProbExampleDiagram}b) with 10 species occurring on exactly 2 PUs each.  

```{r WrapFig, echo=FALSE, fig.align="center", out.height = "80%", fig.cap = "\\label{fig:WrapProbExampleDiagram}Diagram of the wrapping process for increasing the number of species and patches to allow arbitrary rank abundance distributions. Here, letters are PUs, numbers are species, red numbers are species added by the wrapping procedure, and black numbers are species generated by the original model RB generator. (a) Original model RB problem as nodes and links. Nodes are PUs and links are species that occupy the 2 PUs connected by the link. The dashed line separates the independent set from the solution set. (b) Same Base problem as a) but converted from a graph representation to a set representation, with each PU containing species IDs of all species occupying the PU. (c) Wrapped problem using set representation. The bottom dashed line separates PUs in the solution set from PUs that were added in wrapping."}
#{r Wrap, echo=FALSE, out.height = "80%"}
#{r Wrap, echo=FALSE, out.height = "80%", fig.cap = "\\label{fig:Wrap}Diagram for example wrapped distribution.  Letters are PUs.  Numbers are species."}

#  Had to:
#  - draw in powerpoint
#  - copy and paste each figure into Word using the text inline setting for the image once it's pasted in
#  - from word, "File/Save as Adobe PDF"
#  - open the resulting pdf in Preview and then Export it to PNG, using 600 dpi

#wrap_img_path = file.path (proj_dir, "Figures/figure wrap from powerpoint.png")
    #  2022 07 03
    #  Reviewer 3 requested that this diagram have the same styling as the Base 
    #  diagram, so I did that, but it has somehow it now has much more space 
    #  between it and the caption.
    #  Need to use a different program to rebuild it.  
wrap_img_path = file.path (proj_dir, "Figures/figure wrap from powerpoint v2 2022 07 03.png")
include_graphics(wrap_img_path)
```

Now consider the rank abundance distribution of species occurrence counts in Fig. \ref{fig:buildExampleWrapDistHist}. Here, four new species have been added and we designate these as wrapping species, labelling them as species 1-4. Looking at the way they have been distributed on the PUs we can see species 1-4 occur on 5, 4, 4, and 3 PUs, respectively, and the base species (5-14) occur on exactly 2 PUs each. So, we have a rank abundance distribution that fully includes the species from the base problem and includes 4 new species. Note that the choice of the wrapping species and their occurrence counts could follow any distribution, as long as it contains the original set of species that occur on exactly 2 PUs. Note that the wrapping species could also contain more species that occur on exactly 2 PUs.

```{r buildExampleWrapDistHistogram, echo=FALSE, fig.align="center", out.width = "50%", fig.cap = "\\label{fig:buildExampleWrapDistHist}Species rank abundance distribution for the example Wrapped distribution in Fig. \\ref{fig:WrapFig}c with species sorted in decreasing order by number of PUs occupied by each species.  Grey bars indicate number of PUs occupied for species generated in the model RB Base problem.  Red bars indicate number of PUs occupied for species generated during wrapping."}

SPP_TYPE = as.factor (
            c ("wrap_spp","wrap_spp","wrap_spp","wrap_spp",
            "base_spp","base_spp","base_spp","base_spp","base_spp","base_spp",
            "base_spp","base_spp","base_spp","base_spp")
            )
bar_colors = c(wrap_spp="red", base_spp="grey")

spp_ID = 1:14
abundance = rep (2, 14)
abundance [1:4] = 0

df <- data.frame (spp_ID = spp_ID,
                  abundance = abundance)
#head(df)

# p<-ggplot(df, aes(x=spp_ID, y=abundance)) +
#   geom_bar(stat="identity", 
#             aes (fill = SPP_TYPE)
#             ) +
#   ylim(0, 5) + 
#   scale_fill_manual(values=bar_colors) + 
#   labs (x = "Species ID", y="Num PUs Occupied") +
#   theme(legend.position="right") + 
#   ggtitle("BASE Spp Rank Abundance Distribution    ") + 
#     theme_classic() + 
#     theme(plot.title = element_text(margin = margin(t = 10, b = -35)))     + 
#   theme(plot.title=element_text( hjust=1, vjust=0.5, face='bold'))
# p


abundance [1:4] = c(5,4,4,3)

df <- data.frame (spp_ID = spp_ID,
                  abundance = abundance)
#head(df)

p <- ggplot (data=df, aes(x=spp_ID, y=abundance)) +
  geom_bar(stat="identity", 
            aes (fill = SPP_TYPE)) + 
  
#  ylim(0, 5) + 
  coord_cartesian (ylim = c(0, 5)) + 
                  
  scale_fill_manual(values=bar_colors) + 
  labs (x = "Species ID", y="Num PUs Occupied") +
  theme(legend.position="right") + 
  ggtitle("WRAPPED Spp Rank Abundance Distribution    ") + 
    theme_classic() + 
    theme(plot.title = element_text(margin = margin(t = 10, b = -35))) + 
  theme(plot.title=element_text( hjust=1, vjust=0.5, face='bold')) 

p

#ggsave(plot=plot_1, filename="plot_1.png", height=5, width=5)
```
 
Next, we examine how to create a problem where the correct solution occupies a specified proportion of the wrapped problems landscape, e.g., 40% to keep this example small. In this case, we need to have 10 PUs in the wrapped problem landscape since there were 4 PUs in the base problems solution. So, we need to add 4 more PUs to the base problems 6 and decide how to spread the 16 total occurrences of the 4 new species across the new, larger landscape.

If we take one occurrence of each of the four new species and randomly assign each to any PU in the base solution, then we are guaranteed that the base solution is a solution to the full wrapped problem. If we then randomly distribute the remaining occurrences of each of the new species across the four newly added PUs (without replacement), then we have a full wrapped problem with the desired rank abundance distribution (Fig. \ref{fig:WrapProbExampleDiagram}c). Finally, we know the optimal solution because there are no occurrences of the base problem species outside of the original base problem and the four new species also each have at least one occurrence in the base problem solution.

There are two things to note.  First, while the wrapping distribution could contain species that occur on only 1 PU, we do not allow that because any PU containing such a species would automatically have to be included in the solution and therefore, would reduce the problem to an equivalent, smaller problem.  Second, the wrapped problem loses the theoretical guarantees that applied to the original model RB since we have now violated some of the RB assumptions.  However, in practice we still find wide variation in problem difficulty for the wrapped problems.

Pseudocode for the wrapping algorithm is given in CorSI Appendix C.

###  More realistic species distribution

We now have a method for generating wrapped problems, but that does not tell us what we should use as a more ecologically realistic wrapping abundance distribution.  As there are no rules governing what species are included in a real-world reserve selection, it is possible for the chosen species set to reflect almost any distribution.  We know of no studies that characterize the rank abundance distributions of species that have been included in real-world reserve selection problems. 

Here, we used lognormal distributions as an example wrapping distribution. While there is no consensus on what is a theoretically correct rank abundance distribution, we used lognormal distributions as our example distribution because they have at least been discussed by ecologists in relation to theoretical models for rank abundance distributions (@stevens2009). Note that while the lognormal distribution is undoubtedly a more realistic rank abundance distribution over the *full* set of species in a particular location than model RBs uniform distribution, the set of species considered in a reserve selection problem is never the full set of species occurring at that location.  Moreover, the practitioner can arbitrarily choose *any* subset of the species to include and therefore, the abundance distribution shape could be arbitrary. However, our wrapping method will accommodate any distribution shape that the practitioner thinks is appropriate, as long as it includes the base problems set of species that occur on exactly two PUs.

Once we have chosen the lognormal as our demonstration distribution, the question becomes which lognormal to use in each problem, i.e., what values are given to the lognormal's two parameters that control its shape? We address this choice by using an optimizer to search for the two parameters of a particular lognormal that i) doesnt exceed a user-specified upper bound on total number of species, and ii) includes the base problems given number of species that occur on exactly two PUs (though it can include more species that occur on exactly two PUs).  

We guide the parameter search by providing an evaluation function that penalizes parameter choices that yield a lognormal exceeding a given maximum number of all species (so that problems dont become unrealistically large) or that fall below the base problems number of species on two PUs. The choice of evaluation function to use is flexible in that it just needs to embody whatever constraints satisifies the user's beliefs about the appropriate function shape and somehow penalizes for violating them.  However, its form does have to differ from the squared error form used in regression because we only have data for the section of the curve containing species occurring on exactly two patches.  Everything else about the curve is underconstrained.  Though we use a lognormal as the target and our specific evaluation function to guide fitting, the same procedure works for choosing parameters of any wrapping distribution. Similarly, any desired evaluation function and optimization algorithm could be used. More details of the fitting procedure are given in CorSI.  

###  More realistic representation targets 

The wrapping method can be extended to allow a larger range of targets for any wrapping species as long as the target for the wrapping species is no larger than the size of the Base solution set and the Base species still have a target of 1.  We just need to drop at least the target number of occurrences of each wrap species on that number of PUs in the Base solution set instead of dropping just one occurrence as we had done previously.  More details are given in CorSI.

While this now allows known solutions for problems with a much greater range of targets, the disadvantage is that it moves even farther from the model RB theory that points to where the hard problems are.  We need more theory about this new kind of problem and how to avoid building only easy problems.  For example, the closer the target values are to their abundances and the closer those are to the size of the Base solution, it seems the easier the problem would be since those Base solution PUs will be forced to be part of the reserve selector's solution choice.  Other constraint relations may have similar effects.  

Since we know of no real-world data on what would be a reasonable distribution of representation targets and because it most closely matches the model RB theory, our method demonstration results below are only for problems with a target of 1 occurrence for all species.  
 
#  Methods  {#methodsSection}

Here, we describe the main elements of the methods used in the paper, however there are many more details required to fully describe the methods.  These details are given in the LongformSI, AppSI, and CorSI files, and in our R code for the R library `bdpg` at: https://github.com/langfob/bdpg.

##  Problem set  

We generated 3,000 sets of problems with 6 problems in each set.  First, a "correct" Base problem without uncertainty (designated "COR Base") is generated using model RB method and then a Wrap problem without uncertainty (designated "COR Wrap") is generated from the Base problem using a lognormal as the wrapping distribution.   Then, four "APP" versions of the COR Wrap problem are derived by adding four different kinds of error to it (Section  \ref{inputErrorGeneration}).  These are designated "APP Wrap" problems since they are the data that would be "apparent" to a reserve selector which would have no knowledge of the correct input values in a real-world setting.  Since we used Model RB to generate the problems, the representation target for all species was one occurrence and all patches have equal cost.  

```{r}
cor_data_retvals = create_p1_COR_data (filtered_full_initial_exp_tib, params)

    #  Dimensions of spp and PUs to embed in text describing the bounds of 
    #  spp and PUs created in COR problems.
min_base_spp = cor_data_retvals$min_base_spp
max_base_spp = cor_data_retvals$max_base_spp

min_wrap_spp = cor_data_retvals$min_wrap_spp
max_wrap_spp = cor_data_retvals$max_wrap_spp

min_base_PUs = cor_data_retvals$min_base_PUs
max_base_PUs = cor_data_retvals$max_base_PUs

min_wrap_PUs = cor_data_retvals$min_wrap_PUs
max_wrap_PUs = cor_data_retvals$max_wrap_PUs

# min_base_spp = 18
# max_base_spp = 541
# 
# min_wrap_spp = 39
# max_wrap_spp = 1479
# 
# min_base_PUs = 14
# max_base_PUs = 120
# 
# min_wrap_PUs = 59
# max_wrap_PUs = 765
```

Input parameter values used in problem generation are detailed in LongformSI and CorSI.  The resulting range of the number of species for Base problems was [`r min_base_spp`, `r max_base_spp`].  For Wrap problems, it was [`r min_wrap_spp`, `r max_wrap_spp`]. The range of the number of PUs was [`r min_base_PUs`, `r max_base_PUs`] for Base problems and [`r min_wrap_PUs`, `r max_wrap_PUs`] for Wrap problems.  Total Wrap solution sizes were in the range [7.48, 12.5]% of the Wrap landscape.   

##  Input error generation {#inputErrorGeneration}

In what follows, a *Positive* is defined to be an occurrence of a species on a PU and a *Negative* is the absence of a species on a PU, *FP* means False Positives, *FN* means False Negatives, *TP* means True Positives, and *TN* means True Negatives.  Appending *ct* to any of these indicates the integer count of the corresponding variable, e.g., *TPct* means the number of True Positives.

For each APP Wrap problem, we drew a uniform random value $v$ between 0 and 0.1 to simulate an input error rate of up to 10%.  Using this number, we generated four different types of input error, yielding four different APP Wrap problems corresponding to each COR Wrap problem: FP-only = $v$, FN-only = $v$, Unmatched (both FP = $v$ and FN = $v$), and Matched (both FP count and FN count matching the FN error *count* derived from FN error proportion  = $v$).  We used the same rate $v$ in all four cases so that comparisons can be made for the effects of the same error rate across different error types on the same underlying problem.  Both matched and unmatched error models are included because each species has far more PUs where it doesnt occur (True Negatives) than PUs where it does occur (True Positives).  That imbalance means that the same proportional error rate for FP as FN will produce a far larger number of False Positives occurrences than False Negatives. This turns out to make the reserve selector error on Unmatched problems be nearly the same as for the FP-only case.  Similarly, for the Matched case, making the number of False Positives match the number of False Negatives drastically reduces the number of False Positives and leads to behavior similar to the FN-only case.

In most results, we label the combined set of results from FN-only cases and Matched cases as **FN-dominated** for plotting and labeling convenience.  We say "dominated" because there are still a few False Positives in a Matched problem, but it is the existence of the False Negatives that dominate the behavior.  Similarly, we label the set of FP-only problems merged with  Unmatched problems as **FP-dominated** even though the Unmatched problems contain a relatively small number of False Negatives that generally have little effect on the outcome.  

```{r ConfusionMatrixFig, echo=FALSE, fig.align="center", out.height = "20%", fig.cap = "\\label{fig:ConfusionMatrixTableImg} Confusion matrix"}

cm_img_path = file.path (proj_dir, "Figures/confusionMatrixDiagram_PPscreenGrab.png")
include_graphics(cm_img_path)
```

Given the confusion matrix of counts in Fig. \ref{fig:ConfusionMatrixTableImg}, we then define the **False Negative rate** as the proportion of cells that should correctly be labeled as Positive but are labeled as Negative:

\begin{equation}
  \label{eq:FNrateEqn}
  FNrate = \frac{FNct}{TPct + FNct}
\end{equation}

Similarly, we define the **False Positive rate** as the proportion of cells that should correctly be labeled as Negative but are labeled as Positive:

\begin{equation}
  \label{eq:FPrateEqn}
  FPrate = \frac{FPct}{FPct + TNct}
\end{equation}

Also from the confusion matrix, we define  ***total input error rate*** $\boldsymbol{E_{ti}}$ (the proportion of all cells that are incorrectly labelled) as:  

\begin{equation}
  \label{eq:totInputErrEqn}
  E_{ti} = \frac{FPct + FNct}{TPct + FPct + FNct + TNct}
\end{equation}

##  Problem formulation  {#problemFormulationSection}  

In our experiments, we formally define the optimization problem to be solved using a formulation that is identical to that used in @beyer2016em (equation 1).  

\begin{equation}
  \label{eq:ilpProblemFormulation}
  \begin{aligned}
    min &\sum_{i=1}^{N} c_i x_i  \\
    \text{subject to} &\sum_{i=1}^{N} r_{ik} x_i \geq T_k, \; \; k \in K  \\
    &x_i \in \{0,1\}, \; \; i \in N
  \end{aligned}
\end{equation}

where  

- $N$ is the number of planning units, 
- $K$ is the number of species in the problem, 
- $x_i$ is a binary decision variable indicating whether $PU_i$ is included in the solution ($x_i = 1$) or not included ($x_1 = 0$), 
- $c_i$ is the cost of $PU_i$, 
- $r_{ik}$ is the contribution of $PU_i$ to species $k$, and 
- $T_k$ is the minimum target value to be achieved for species $k$ across all PUs in the problem.  

In our experiments, all COR costs are identical (i.e., $c_i = 1$) and all targets are identical (i.e., $T_k = 1$).  Our source code allows APP costs to have positive values other than 1 and to not be identical (to allow cost error), but in the experiments shown here, they are all 1 for simplicity.  Note that there are also no spatial constraints in our problem formulation.  

##  Output error measures  {#outputErrorMeasuresMethodsSection}  

Our primary performance interest in the reserve selectors was not in their speed, but rather in the accuracy of their solution under input uncertainty and the degree to which they violated any specified constraints.  

***Representation shortfall error*** $\boldsymbol{E_{rep}}$ is the proportion of the number of species that fail to at least meet their representation target in the selector's solution:  

\begin{equation}
  \label{eq:repShortfallErrEqn}
  E_{rep} = \frac{K_f}{K}
\end{equation}

where $K_f$ is the number of species that fall short of their representation target, and $K$ is the total number of species.  

***Signed solution cost error*** $\boldsymbol{E_{sc}}$ is the error in the cost of the reserve selector's solution as a proportion of the known optimal cost, with negative values indicating underestimation of the optimal solution cost and positive values indicating overestimation:   

\begin{equation}
  \label{eq:solutionCostErrEqn}
  E_{sc} = \frac{C_{rs} - C_{opt}}{C_{opt}}
\end{equation}

where $C_{rs}$ is the reserve selector's solution cost, and $C_{opt}$ is the optimal solution cost.  

***Error magnification*** $\boldsymbol{ErrMag_{out}}$ is the ratio of any of the errors above to the total input error rate on the same problem.  Magnification values greater than 1 imply error amplification, less than 1 imply error correction, and equal 1 implies no change to the total input error amount.  Magnification provides an intuitive way to standardize error measurement independent of the amount of input error.

\begin{equation}
  \label{eq:errMagEqn}
  ErrMag_{out} = \frac{E_{out}}{E_{ti}}
\end{equation}  

where $E_{out} \in \{E_{rep}, E_{sc}\}$.  

##  Reserve selectors  {#reserveSelectorsMethodsSection}  

```{r computeGurobiUnfinishedProportion}

gur_temp_tib = filter (p2_app_wrap_tib, rs_method_name == "ILP")
gur_status_cts = count (gur_temp_tib, gurobi_status)
gur_unfinished_proportion = gur_status_cts$n[2] / sum (gur_status_cts$n)
```

We explore the consequences of the common practice of ignoring uncertainty in reserve selection by comparing the accuracy of four reserve selection methods that ignore uncertainty in their inputs (Section \ref{resultsSection}).  Note that as stated earlier, when we refer to a "reserve selector", we are using it as a shorthand for the combination of the algorithm, its software implementation, its problem formulation, and its parameterization.  Changing any of those elements would likely affect its performance and is therefore considered a different "reserve selector".  In this work, we are not trying to make any claims about which of these methods are better than the other in general.  We are only using them as examples for our proposed approach to method evaluation and error prediction.  

We ran four different reserve selectors on each problem in 3,000 independent groups of six problems each for a total of 72,000 reserve selector runs.  The four reserve selectors we tested were:  

- Integer linear programming (hereafter, ILP)
- Simulated annealing (hereafter, SA) 
- Simulated annealing summed solution (hereafter, SA_SS)
- Greedy search using unprotected richness (hereafter, UR_Forward) 

We used Gurobi version 8.00  (@gurobioptimizationllc2020) as our integer linear programming solver.  Gurobi is proprietary linear programming software that is guaranteed to find the exact, optimal solution when given enough resources (time and memory) and no violations of its assumptions (e.g., input data does not contain errors).  We used Marxan version 2.43 (@watts2009em&s) as our simulated annealing solver.  Marxan is a stochastic heuristic method that is not guaranteed to find the exact, correct solution, but often finds good or even exact solutions when the input data is correct.  We also used Marxan version 2.43 (@watts2009em&s) plus our own R source code (R Core Team, 2022)) in our ensemble (voting) method of solution.  SA_SS is a voting method based on greedy selection of PUs in order by the number of Marxan restart solutions that the PU appears in.  We implemented this method in R using the outputs of 100 Marxan random restarts.  We used UR_Forward as our greedy, complementarity-based heuristic implemented in R.  It selects PUs in order by the number of species on the patch that have not yet met their representation target (i.e., Unprotected Richness).  Details of parameter settings used for running each reserve selector are given in LongformSI and CorSI Appendix F.

##  Model fitting    {#learnFunctionsToPredictMethodsSection}  

We used simple linear models (R function `lm`) and the data gathered in the reserve selector test problems described above to fit models predicting how much error a given reserve selector is likely to have on each individual problem given its apparent species co-occurrence patterns as captured in bipartite graph measures.  During the data exploration phase, we also tried other models beyond the simple linear model (e.g., random forest and generalized linear model with elastic net).  However, all of the models we tried had very similar performance, so we only present results for the simple linear model.  

To enable the use of bipartite graph measures for each reserve selection problem, we encoded each problem as a bipartite graph where one level of the graph has a node for each species in the problem and the other level has a node for each PU in the problem. ^[This bipartite graph has no relation to the graph used in model RB (which was not a bipartite graph).  It is strictly coincidence that there happen to be multiple different uses for graphs in this research.] For each species in the problem, we added a link between the node for that species and each PU that it occupies in the data.  We represented this node and link data as an adjacency matrix where the columns are species and the rows are PUs.  If a given species is linked to a given PU (i.e., the species occupies that PU), then there is a 1 in the adjacency matrix at the corresponding species row PU column pair.  All PUs not occupied by a given species have a 0 at the corresponding location in the adjacency matrix.  

We used the R package `bipartite` (@dormann2008rn, @dormannIndicesGraphsNull2009) to compute many different measures of the structure of the bipartite graph represented by the adjacency matrix.  We only computed measures that are relatively quick to compute, ignoring for now, some costly but potentially very useful measures such as nestedness.  We also included a few other input variables such as the number of PUs and the number of species.  The list of all measured variables and their definitions is shown in AppSI Appendix J.   All variables were Box-Cox transformed before input to model fitting.  

We computed 42 input features for each problem and learned linear models for four different subsets of these features:  i) problem size features (2 variables), ii) problem size and density features (5 variables), iii) graph features (27 variables), iv) all features (42 variables).  The features used in each set are shown in Table \ref{tab:makeFeatureSetsVarsTable}.  More detail about each variable appears in AppSI.  Nine of the input features can only be computed on our simulated problems and cannot be known for real-world problems (e.g., the model RB control parameters and the FP and FN error rates on the input data).   These variables are only included in subset iv), the "all features" subset.  The table includes a column to indicate whether each variable can be known or not for a real-world problem.    
 
```{r setAllVarsTableParams}
vars_used_str = "All" 
#source_string = paste0 (vars_used_str, "_", fitting_model_str) 

#include_median_redundancies=FALSE

allVars = c(                                           #  42 variables
            "rsp_num_occupied_PUs", 
            "rsp_num_spp", 

            "links_per_PUsAndSpp", 
            "edge_frac_of_possible",    #  Same as bip_connectance?
            "sppPUprod", 
            
            "ig_num_edges_m", 
            "web_asymmetry", 
            "cluster_coefficient", 
            "specialisation_asymmetry", 
            "linkage_density", 
            "weighted_connectance", 
            "Shannon_diversity", 
            "interaction_evenness", 
            "Alatalo_interaction_evenness", 
            "mean.number.of.shared.partners.PUs", 
            "mean.number.of.shared.partners.Spp", 
            "cluster.coefficient.PUs", 
            "cluster.coefficient.Spp", 
            "niche.overlap.PUs", 
            "niche.overlap.Spp", 
            "togetherness.PUs", 
            "togetherness.Spp", 
            "C.score.PUs", 
            "C.score.Spp", 
            "V.ratio.PUs", 
            "V.ratio.Spp", 
            "functional.complementarity.PUs", 
            "functional.complementarity.Spp", 
            "partner.diversity.PUs", 
            "partner.diversity.Spp", 
            "generality.PUs", 
            "vulnerability.Spp", 

            "gurobi_mipgap", 
            
            "rsp_alpha__", 
            "rsp_n__num_groups", 
            "rsp_p__prop_of_links_between_groups", 
            "rsp_r__density", 
            "rsp_d__number_of_nodes_per_group", 
            
            "actual_sol_frac_of_landscape", 
            
            "rsp_realized_FP_rate", 
            "rsp_realized_FN_rate", 
            "rsp_realized_Ftot_rate" 
            )
```

&nbsp;  

```{r makeFeatureSetsVarsTable, eval=TRUE}
#xxx

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

#  NOTE:  2024 03 28 
#  I have temporarily truncated all long lines in these definitions with a "..." 
#  so that the table fits on the page.  When kable() runs it off of the page, 
#  it also shifts that caption and table number completely off the page and 
#  I don't want that.  I need to figure out how to get the tables to wrap 
#  long lines of text within a fixed column width.  
  
all_var_defns = c(
                "number of occupied PUs", 				 #"rsp_num_occupied_PUs", 
                "number of species", 				 #"rsp_num_spp", 
                "average number of links per node, where species ...",    #  "and PUs are nodes", 				 #"links_per_PUsAndSpp", 
                "proportion of links made out of all possible links ...",    #  "between species and PUs"	ig_num_edges_m / sppPUprod  (Same as bip_connectance and ig_bidens.)	   #  Same as bip_connectance?		#"edge_frac_of_possible",
                "product of number of species and number ...",    #  "of PUs, i.e, size of adjacency matrix", 				#"sppPUprod", 
                "Total number of adjacency matrix cells containing ...",    #  "a 1, i.e., total number of links between spp and PUs.", 				#"ig_num_edges_m", 
                "Balance between numbers in the two levels: ...",    #  "positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2", 				#"web_asymmetry", 
                "Mean, across all nodes of the number of realized ...",    #  "links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 				#"cluster_coefficient", 
                "Asymmetry (PU level vs. spp level) of specialisation ...",    #  "now based on d' (see dfun), which is insensitive to the dimensions of the web." , 				#"specialisation_asymmetry", 
                "Marginal totals-weighted diversity of interactions ...",    #  "per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 				#"linkage_density", 
                "Linkage density divided by number of nodes in the n...",    #  "etwork (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 				#"weighted_connectance", 
                "Shannon's diversity of interactions (i.e. network entries)." , 				#"Shannon_diversity", 
                "Shannon's evenness for the web entries." , 				#"interaction_evenness", 
                "A different measure for web entry evenness, as ...",    #  "proposed by Muller et al. (1999)." , 				#"Alatalo_interaction_evenness", 
                "Based on the distance matrix between PUs counting ...",    #  "the number of spp that both PUs interact with." , 				#"mean.number.of.shared.partners.PUs", 
                "Based on the distance matrix between spp counting ...",    #  "the number of PUs that both spp interact with." , 				#"mean.number.of.shared.partners.Spp", 
                "Mean, across all PUs of the number of realized links ...",    #  "divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 				#"cluster.coefficient.PUs", 
                "Mean, across all spp of the number of realized links ...",    #  "divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 				#"cluster.coefficient.Spp", 
                "Mean similarity in interaction pattern between PUs, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 				#"niche.overlap.PUs", 
                "Mean similarity in interaction pattern between spp, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 				#"niche.overlap.Spp", 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 				#"togetherness.PUs", 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 				#"togetherness.Spp", 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 				#"C.score.PUs", 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 				#"C.score.Spp", 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level" , 				#"V.ratio.PUs", 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level." , 				 #"V.ratio.Spp", 
                "UNCLEAR - Functional complementarity? for a ...",    #  "given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 				 #"functional.complementarity.PUs", 
                "UNCLEAR - Functional complementarity? for a given ...",    #  "level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 				#"functional.complementarity.Spp", 
                "Mean Shannon diversity of the number of interactions ...",    #  "for PUs." , 				 #"partner.diversity.PUs", 
                "Mean Shannon diversity of the number of interactions ...",    #  "for spp." , 				#"partner.diversity.Spp", 
                "Mean number of spp per PU.", 				#"generality.PUs", 
                "Mean number of PUs per spp.", 				#"vulnerability.Spp", 
                "Relative measure of the upper bound on the distance ...",    #  "to the correct solution as a proportion of the correct solution.", 				#"gurobi_mipgap"   
                "Exponent driving number of nodes per clique ...",    #  "in model RB generator for current Base problem.", 				#"rsp_alpha__",     
                "Number of cliques in model RB generator of ...",    #  "current Base problem.", 				#"rsp_n__num_groups", 
                "Constraint density, i.e., proportion driving ...",    #  "number of links per round in model RB generator of current Base problem.  number of links per round = p*d^2, where d is number of nodes per clique.", 				#rsp_p__prop_of_links_between_groups", 
                "Constraint tightness, i.e., multiplier driving ...",    #  "number of rounds of linking in model RB generator of current Base problem.  Number of rounds of linking = r*n*ln(n).", 				#rsp_r__density", 
                "Number of nodes per clique.", 				#"rsp_d__number_of_nodes_per_group", 
                "Fraction of the landscape contained in correct ...",    #  "solution, i.e., correct solution cost / rsp_num_occupied_PUs", 				#"actual_sol_frac_of_landscape", 
                "Proportion of false positives in the APP adjacency ...",    #  "matrix.", 				 #"rsp_realized_FP_rate", 
                "Proportion of false negatives in the APP adjacency ...",    #  "matrix.", 				#"rsp_realized_FN_rate", 
                "Combined proportion of false positives and false ..."    #  "negatives in the APP adjacency matrix.", 				 #"rsp_realized_Ftot_rate", 
                )

#-----  

allVars_table_caption = "Input feature sets used in each of the four learned error prediction models.  An 'x' in the 'Knowable' column indicates that the corresponding variable can be known for real-world problems."

allVars_short_names = c(                                 #  42 variables
  
            "num of occupied PUs",                      #"rsp_num_occupied_PUs", 
            "num of spp",                               #"rsp_num_spp", 
            "avg num links per bipartite node",         #"links_per_PUsAndSpp", 
            "edge fraction of possible",                #"edge_frac_of_possible",   #  Same as bip_connectance?
            "product of num spp & PUs",                 #"sppPUprod", 
            "num of links in bipartite graph",          #"ig_num_edges_m", 
            "web asymmetry",                            #"web_asymmetry", 
            "cluster coefficient",                      #"cluster_coefficient", 
            "specialisation asymmetry",                 #"specialisation_asymmetry", 
            "linkage density",                          #"linkage_density", 
            "weighted connectance",                     #"weighted_connectance", 
            "Shannon diversity",                        #"Shannon_diversity", 
            "interaction evenness",                     #"interaction_evenness", 
            "Alatalo interaction evenness",             #"Alatalo_interaction_evenness", 
            "PU mean num shared partners",              #"mean.number.of.shared.partners.PUs", 
            "spp mean num shared partners",             #"mean.number.of.shared.partners.Spp", 
            "PU cluster coefficient",                   #"cluster.coefficient.PUs", 
            "spp cluster coefficient",                  #"cluster.coefficient.Spp", 
            "PU niche overlap ",                        #"niche.overlap.PUs", 
            "spp niche overlap ",                       #"niche.overlap.Spp", 
            "PU togetherness",                          #"togetherness.PUs", 
            "spp togetherness",                         #"togetherness.Spp", 
            "PU C score",                               #"C.score.PUs", 
            "spp C score",                              #"C.score.Spp", 
            "PU V ratio",                               #"V.ratio.PUs", 
            "spp V ratio",                              #"V.ratio.Spp", 
            "PU functional complementarity",            #"functional.complementarity.PUs", 
            "spp functional complementarity",           #"functional.complementarity.Spp", 
            "PU partner diversity",                     #"partner.diversity.PUs", 
            "spp partner.diversity",                    #"partner.diversity.Spp", 
            "PU generality",                            #"generality.PUs", 
            "spp vulnerability",                        #"vulnerability.Spp", 
            "gurobi mipgap",                            #"gurobi_mipgap" 
            "alpha for model RB",                       #"rsp_alpha__",     
            "n for model RB",                           #"rsp_n__num_groups", 
            "p for model RB",                           #rsp_p__prop_of_links_between_groups", 
            "r for model RB",                           #rsp_r__density", 
            "d for model RB",                           #"rsp_d__number_of_nodes_per_group", 
            "actual solution fraction of landscape",    #"actual_sol_frac_of_landscape", 
            "realized FP rate",                         #"rsp_realized_FP_rate", 
            "realized FN rate",                         #"rsp_realized_FN_rate", 
            "realized F total rate"                     #"rsp_realized_Ftot_rate" 
            )

#-----  

everything_length = length (allVars)

knowable = rep ("x", everything_length)
knowable [34:everything_length] = rep (" ", everything_length)

included_in_feature_set_1 = rep (" ", everything_length)
included_in_feature_set_1 [1:2] = rep ("x", 2)

included_in_feature_set_2 = rep (" ", everything_length)
included_in_feature_set_2 [1:5] = rep ("x", 5)

included_in_feature_set_3 = rep (" ", everything_length)
included_in_feature_set_3[6:32] = rep ("x", 27)

included_in_feature_set_4 = rep ("x", everything_length)

allVars_table = data.frame (inputFeatures = allVars_short_names
                           , knowable = knowable
                           , included_in_feature_set_1 = included_in_feature_set_1
                           , included_in_feature_set_2 = included_in_feature_set_2
                           , included_in_feature_set_3 = included_in_feature_set_3
                           , included_in_feature_set_4 = included_in_feature_set_4
#                           , inputFeatures = allVars
#                           , inputFeatureDefns = all_var_defns 
                           )

names (allVars_table) = c("Input features", "Knowable", "Size Only", "Size & Density", "Graph", "All"
#                         , "Input variables", "Defns"
                         )

kable (allVars_table, 
       #align = 'lccccll', 
       align = 'lcccc', 
       
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = allVars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       ) 

```

To measure the quality of each model fit, we computed both adjusted $R^2$ and root mean square error (RMSE) for predictions of representation shortfall and for predictions of solution cost error.  All reported values for $R^2$ and RMSE are computed on a separate test data set after model parameters were chosen on a training set.  

#  Results {#resultsSection}

#  Results - Reserve selection comparisons {#comparisonResultsSection}

ILP solved all problems without uncertainty (COR Base and COR Wrap) without any errors, as did SA.  SA_SS and UR_Forward solved most COR problems without error.  Details of results on COR problems are given in CorSI Appendix H.  

The rest of this section describes results for the reserve selectors on the APP Wrap problems, i.e., problems with uncertainty.  All reserve selectors showed both large errors and large amounts of variation at every level of input error greater than 1% (up to our maximum sampled input uncertainty of 10%).  Results for the methods that did not involve voting (ILP, SA, and UR_Forward) were visually almost indistinguishable on most plots, so we will generally refer to them together as the **Non-Voting (NV)** group.  Note that in nearly all plots shown here, positive y-axis values are truncated well below maximum error values to make the plots clearer.  

In AppSI Appendix F, we show plots of the difference between error scores for each pair of reserve selectors on each problem to make it easier to compare selectors.  

```{r countMags, include=FALSE, echo=params$chunk_include}
### May want to move this to where it's used in the text.

#  I think that `mag_frac` is only used in one place in the document and that's 
#  to just embed its value in a sentence.  Having the calculation here is 
#  something of a non-sequitir, so it might be better to just compute it 
#  right before the line of text that uses it.

mag_base_col_name_str = params$mag_base_col_name_str

p2_app_wrap_mag_frac_retvals = compute_mag_frac (p2_app_wrap_tib, params)

#####mag_base_col_name_str = p2_app_wrap_mag_frac_retvals$p2_app_wrap_mag_frac_retvals
mag_col_name_str = p2_app_wrap_mag_frac_retvals$mag_col_name_str
shortfall_mag_str = p2_app_wrap_mag_frac_retvals$shortfall_mag_str
mag_frac = p2_app_wrap_mag_frac_retvals$mag_frac
cat ("mag_frac = ", mag_frac)
```

##  Signed solution cost error  

Signed solution cost errors are shown in Fig.  \ref{fig:InVsCostErrLE125inResults} and represent how much more or less cost the reserve selectors think they need to meet each species' target when compared to the optimal solution.  The NV group underestimated the optimal cost on FP-dominated problems and overestimated on FN-dominated problems.  In FP-dominated problems, reserve selectors can choose PUs where species appear present but are absent, resulting in an underestimation of the optimal cost. In FN-dominated problems, the cost was overestimated because the reserve selectors dont have information about some locations where species are present, which could allow more efficient solutions.  SA_SS overestimated cost on nearly all problems, with greater overestimation for FN-dominated problems. All selectors showed a wide range of error and error  magnification at all levels of input error.

```{r plotInVsCostErrLE125inResults, fig.cap = "\\label{fig:InVsCostErrLE125inResults}Signed solution cost error as a function of input error, showing only problems with input errors up to 10\\% and solution cost errors truncated at 125\\%. Problems are colored by their dominant error type.  Black lines above the x-axis indicate levels where solution cost error is the same as the input error (1x), 5 times the input error (5x), and 10 times the input error (10x).  Black lines below the x-axis are the negative equivalents of those above the axis."}

#  Extra text removed from caption:
#    FN-dominated problems are in blue.  FP-dominated problems are in red.  The 1x, 5x, and 10x rays indicate the level where output error is 1 times input error, 5 times input error, and 10 times input error (for both underestimation and overestimation).  The y-axis (output error) is truncated below at -100% because that's the maximum underestimation of cost possible.  It's truncated above at 125% because only ZL_Backward had many values above 125% (up to 500%).  Truncation at 125% avoids compressing resolution for the 4 other methods.  Full display of FN-dominated values for ZL_Backward can be seen in Appendix.

ggplot_faceted_with_mag_rays (rs_method_names_list,
                              plot_title_str = paste0 ("Signed Solution Cost Error"),
                              filter (p2_app_wrap_tib, rs_solution_cost_err_frac <= 1.25), 

                              x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",
                              y_var     = "rs_solution_cost_err_frac", 
                              
                              color_var = dom_err_type,    #"id",
                              facet_var = "rs_method_name_fac",    #"rs_method_name" 
                                  
                              y_min = -1,
                              y_max = 1.25,
                              
                              x_axis_label = "Input Error", 
                              y_axis_label = "Signed Solution Cost Error",
                  
                              plot_rep_shortfall_err_bound = FALSE, 
                              plot_rays = TRUE, 
                              plot_neg_rays_too = TRUE
                                                   ) + 
                        
                              background_grid (major = c("y"), 
                                               minor = c("y"), 
                                               size.major = 0.5, 
                                               size.minor = 0.2, 
                                               color.major = "grey85", 
                                               color.minor = "grey85") 

```

##  Representation shortfall

Representation shortfall errors are shown in Fig.  \ref{fig:inVsRepShortfallinResults}, which represents the proportion of species missing from the solution due to the FPs and FNs in the input data. As expected, FN-dominated problems showed virtually no representation shortfalls for any reserve selector, as in most cases, this resulted in greater areas than necessary being selected. In FP-dominated problems, all reserve selectors showed a range of shortfalls due to selecting locations where species are not present.  For the NV group, shortfalls ranged from 0 to approximately 80% once input error was at least 5%.  Values for SA_SS had a smaller range (40-70%) once input error was at least 5% and tended to have a much larger concentration of values near 0.  

```{r plotInVsRepShortfallinResults, fig.cap = "\\label{fig:inVsRepShortfallinResults}Representation shortfall error showing only problems with input errors up to 10\\%. Problems are colored by their dominant error type.  The black lines indicate levels where output error is the same as the input error (1x), is 5 times the input error (5x), and is 10 times the input error (10x)."}

#  Extra text removed from caption:
#  FP-dominated problems are in red.  FN-dominated problems are in blue.  Since there is very little representation shortfall in FN-dominated problems,the blue FN dots on the graph are very localized and overplot each other.  The 1x, 5x, and 10x rays indicate the level where output error is 1 times input error, 5 times input error, and 10 times input error.


ggplot_faceted_with_mag_rays (rs_method_names_list, 
        plot_title_str = paste0 ("Representation Shortfall Error"),
    p2_app_wrap_tib, 
        x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",    #"rsp_euc_realized_Ftot_and_cost_in_err_frac", 
        y_var     = "rsr_COR_spp_rep_shortfall", 
#        color_var = "id",
      color_var = "dom_err_type",
        facet_var = "rs_method_name_fac",    #"rs_method_name"
    
                              y_min = 0,
                              y_max = 1,
                              
        x_axis_label = "Input Error", 
        y_axis_label = "Representation Shortfall",
                             ) + 
  
  background_grid (major = c("y"),
  minor = c("y"), size.major = 0.5,
  size.minor = 0.2, color.major = "grey85", color.minor = "grey85")

```

Because representation shortfall error can be no more than 100%, the upper bound on shortfall magnification at any input error level is *100/inputErrorPercent*.  Maximum shortfall magnifications for reserve selectors in the NV group approached this upper bound as input error increased (see AppSI Appendix G).  In contrast, the maximum magnifications for SA_SS were lower and flatter across the input error range, since SA_SS tended to have lower representation shortfalls for a given input error (Fig.  \ref{fig:inVsRepShortfallinResults}).

## Representation shortfall vs. Signed solution cost error

The relationship between representation shortfall and signed solution cost error on each problem is shown in Fig.  \ref{fig:repShortfallVsCostErrLE125inResults}.  All four reserve selectors had similar behavior on FN-dominated problems, i.e., little or no representation shortfall and a wide range of optimal solution cost error.  On FP-dominated problems, the NV group of selectors all showed mostly underestimation of optimal solution cost and a wide range of representation shortfall.  The SA_SS selector showed less representation shortfall than the NV group and tended to overestimate optimal solution cost rather than underestimating it.  

The large black rectangles in Fig.  \ref{fig:repShortfallVsCostErrLE125inResults} identify  particularly difficult problems where both representation shortfall and cost underestimate were greater than or equal to 50% on the same problem.  The small rectangles near the origin identify the region where all output errors were less than 10%.  This is where all output errors would fall if output errors were no larger than any input error.  A large proportion of output errors fell outside this region, showing that across our simulated problems, input errors tended to be magnified by the optimization process.  

```{r plotRepShortfallVsCostErrLE125inResults, fig.cap = "\\label{fig:repShortfallVsCostErrLE125inResults}Representation shortfall vs. Signed solution cost error showing only problems where absolute value of cost error is less than 125\\%.  Problems are colored by their dominant error type.  The large rectangle in the lower right corner of the plot identifies problems where the reserve selector has more than 50\\% error in both cost and representation shortfall.  The small rectangle on the 0\\% line identifies values where all output errors are less than the maximum input error of 10\\%."}

#  Extra text removed from caption:
#    FP-dominated problems are in red.  FN-dominated problems are in blue.  

# repShortfallVsCostErrLE125inResultsPlot = 
  
ggplot_faceted_with_mag_rays (rs_method_names_list,
                              plot_title_str = paste0 ("Representation shortfall vs. Signed solution cost Error"), 
                              filter (p2_app_wrap_tib, rs_solution_cost_err_frac <= 1.25), 

                              x_var     = "rsr_COR_spp_rep_shortfall", 
                              y_var     = "rs_solution_cost_err_frac", 
                              
                              color_var = dom_err_type,    #"id",
                              facet_var = "rs_method_name_fac", 
                                  
                              x_min = 0,
                              x_max = 1,
                              
                              y_min = -1,
                              y_max = 1.25,
                              
                              x_axis_label = "Representation Shortfall", 
                              y_axis_label = "Solution Cost Error",
                  
                              plot_rep_shortfall_err_bound = FALSE, 
                              plot_rays = FALSE, 
                              plot_neg_rays_too = FALSE
                                                   ) + 
                        
                              background_grid (major = c("y"), 
                                               minor = c("y"), 
                                               size.major = 0.5, 
                                               size.minor = 0.2, 
                                               color.major = "grey85", 
                                               color.minor = "grey85") + 
  
    geom_rect (mapping=aes (xmin=0.5, xmax=1.0, ymin=-0.5, ymax=-1.0),
     color="black", alpha=0, size=0.05) + 

    geom_rect (mapping=aes (xmin=0, xmax=0.1, ymin=-0.1, ymax=0.1),
     color="black", alpha=0, size=0.1)
        
# repShortfallVsCostErrLE125inResultsPlot
# 
# ggsave (plot=repShortfallVsCostErrLE125inResultsPlot,
#         device="png",
#         filename="Figs/repShortfallVsCostErrLE125inResultsPlot.png",
#         height=5, width=5)

```

##  Summary of behavior of individual reserve selectors  

The NV group of reserve selectors all had similar behavior across all error measures and input errors.  All showed a strong bias toward underestimating cost of the optimal solution and overestimating the proportion of species reaching their targets.  They also often showed considerable magnification of input errors, with magnification on many problems approaching the theoretical upper limit of representation shortfall magnification for input errors above 4 or 5%.  

While all four reserve selectors had large errors and widely varying behavior on FN-dominated problems, SA_SS had more moderated behavior on FP-dominated problems than the other selectors.  In particular, it had less extreme representation shortfall (Fig.  \ref{fig:inVsRepShortfallinResults}) and shortfall magnification, but at the expense of greater overestimation of optimal solution cost (Fig.  \ref{fig:InVsCostErrLE125inResults}).  However, SA_SS representation shortfalls still reached nearly 75% for some problems (Fig. \ref{fig:inVsRepShortfallinResults}).   

#  Results - Learning to predict output errors {#predictionResultsSection}

We now describe the outcomes of using evaluation results shown in the previous section to learn to predict output errors for each reserve selector.  We only report the results for learning to predict using linear regression.  We also did early exploratory experiments using several other learning methods such as random forest and elastic net, but found miniscule differences in performance when compared with linear regression.  Much greater differences in performance were visible when we changed the set of input features given to the fitting function, so that is what we report here.  

For each set of input features, we show one grid of plots for representation shortfall and a second grid of plots for solution cost error.  In each of the grids, we show a subplot for each reserve selector.  In each subplot, the x-axis corresponds to the true value of the output error made by that reserve selector on each problem in the test set and the y-axis corresponds to the output error value predicted by the learned model for the given fitting method and reserve selector.  

For brevity, we only show full plots of predictive results here for sets i) and iii).  The plots all look qualitatively similar, with the fits getting closer to the line of perfect fit as the number of input variables increases. Full plots for all four feature sets are shown in LongformSI, however, at the end of this Results section, we show a Summary plot of the adjusted $R^2$ values for all four input variable sets.   

```{r eval=FALSE}
This text may or may not be useful, but I'm going to leave it out for now.  If someone makes the same mistake that I did, then I can include it again.  

One thing to note in the plots that follow is that spread of x values differs between reserve selectors since each reserve selector had different output errors on the same problems.  This is in contrast to the previous Results section, where the x values usually represented input error, which was identical across reserve selectors for the same problem.  This means that in the previous section, the rightmost points for each reserve selector in each plot aligned with each other, while in the plots that follow, the rightmost points don't necessarily align.  For example, in Fig.  \ref{fig:predRepShortfallUsingPUsAndSppOnly} the representation shortfall true values for SA extend out beyond 0.8 while the values for SA_SS finish around 0.65.  
```

```{r settingsThatApplyToAllPredPlots}

    #  Settings that apply to all prediction plots

fitting_model_str = params$fitting_model_str    #  "rf"

display_train_as_final_pred_using_plot = 
                          params$display_train_as_final_pred_using_plot

include_median_redundancies = FALSE
```

```{r creacyanlFittingScoresDF}

#  The fitting summary bar charts for rmse and R2 need access to the 
#  rmse and R2 values that are buried in the fitting routines, so we need 
#  to create a data frame to hold all of them.  This data frame will be passed 
#  down into each call to do the fitting/predicting 
#  (fit_and_predict_output_error_using_feature_set()).  The rmse and R2 values 
#  for each input feature set rep shortfall and solution cost error are set 
#  in that routine and appended to the this all_fitting_scores_df.  Then, 
#  the data frame is returned to the calling code to be passed down in again 
#  for the next error type and input feature set.  
#  So, even though in this Rmd file it looks liek all_fitting_scores_df is not 
#  being used, it *is* being used lower down in the calling chain.  
 
all_fitting_scores_df = data.frame (test_or_train     = NULL, 
                                    fitting_model_str = NULL, 
                                    vars_used_str     = NULL, 
                                    measure_name_str  = NULL, 
                                    rs_method_name    = NULL, 
                                    rmse              = NULL, 
                                    R2                = NULL, 
                                    adj_R2            = NULL)
```

##  Predict using Problem Size features only (2 input variables)  

The primary structural attributes of problems that are reported in reserve selection studies are the number of planning units and the number of species.  In most studis, no other problem attributes are reported, so we start by considering how well these two attributes allow us to predict output errors with a linear regression model.  (Input variables used are listed in LongformSI.)  Results for predicting representation shortfall are shown in Fig. \ref{fig:predRepShortfallUsingPUsAndSppOnly} and for solution cost error are shown in Fig.  \ref{fig:predSolCostErrorUsingPUsAndSppOnly}.    Prediction was poor for all reserve selectors on both reserve selection measures.  For both measures, there was large variation in predicted values at every level of true values and predictions did not even show an upward trend as true values increased. The primary difference between the reserve selectors was that predictions for the NV reserve selectors had much more variation at every level of true error value than predictions for SA_SS.  In short, problem size alone predicts very little about method performance on either representation shortfall or cost error.  

```{r setPUsAndSppOnlyParams}
vars_used_str = "PUsAndSppOnly" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

inVars = c(                                #  2 variables
            "rsp_num_spp", 
            "rsp_num_occupied_PUs")
```

&nbsp;  

```{r makePUsAndSppOnlyVarsTable, eval=FALSE}
###2025 07 25###```{r makePUsAndSppOnlyVarsTable, eval=TRUE}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                "number of species", 
                "number of occupied PUs" 
                )

invars_table_caption = "Input features for model based on number of PUs and species only."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingPUsAndSppOnlyFiles, include=FALSE, eval=FALSE}

###  Write Matilda files using PUs And Spp only features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildPUsAndSppOnlyTestAndTrain, include=FALSE}

###  Prep for PUsAndSppOnly learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoPUsAndSppOnlyNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Representation shortfall using PUsAndSppOnly data

```{r predictRepShortfallUsingPUsAndSppOnly, fig.cap = "\\label{fig:predRepShortfallUsingPUsAndSppOnly}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing only the number of planning units and the number of species.  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )
```

### Solution cost error using PUsAndSppOnly data 

```{r predictSolCostErrorUsingPUsAndSppOnly, fig.cap = "\\label{fig:predSolCostErrorUsingPUsAndSppOnly}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing only the number of planning units and the number of species.  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin"}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )
```

##  Predict using Problem Size and Density features (5 input variables)    

For brevity, we omit the detailed plots for this input feature set, but briefly describe the outcomes here.  Measuring only the number of species and PUs ignores any information about the occurrence and co-occurrence patterns of the species on the PUs.  The next step is to compute two simple measures of the relative density of species occurrences based on the adjacency matrix that describes all species occurrences on PUs in the problem.  

The two measures are the problem's realized fraction of all edges possible were all species on all PUs, and the average number of links per node in the bipartite network representation of the problem.  Both use the total number of realized edges/occurrences in the problem, but the first computes the density with respect to the number of species times the number of PUs while the second computes the density with respect to the sum of the number of bipartite network nodes, where each species is a node and each PU is a node.  In addition to the two density measures, we also add a feature for the product of the number of species and number of PUs as a measure of total possible size of the problem.   

The inclusion of the three extra size and density variables produces large gains in our ability to predict representation shortfall across all four reserve selectors.  (Input variables and full plots are given in LongformSI.)  

```{r setProbSizeAndDensityParams}

#  "All prob size" is probably not a good name for this.  
#  Most papers give the number of species and the number of PUs, which are 
#  commonly interpreted as problem size. 
#  However, there are other variables that can be computed just from arithmetic 
#  combinations of those 2 variables and they relate to things that are more 
#  like measures of density and potential for density.  
#  I'm lumping them all together here to represent everything that can be 
#  derived from just knowing the number of species and PUs and the marginal 
#  totals of the adjacency matrix.

vars_used_str = "ProbSizeAndDensity" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

inVars = c(                               #  5 variables
            "rsp_num_spp"
            , "rsp_num_occupied_PUs"
            
# #         #  Not sure whether to lump these in with problem size variables or not.
# #         #  It's more like they're density variables that can be computed from simple 
# #         #  counts without any notion of a graph being involved, but they 
# #         #  don't fit with the notion of problems get harder as they include 
# #         #  more spp and PUs, which is what most papers report.
            
            , "links_per_PUsAndSpp"
            , "edge_frac_of_possible"
            , "sppPUprod"
          )
```

&nbsp;  

```{r makeProbSizeAndDensityVarsTable, eval=FALSE}
###2025 07 25###```{r makeProbSizeAndDensityVarsTable, eval=TRUE}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                "number of species", 
                "number of occupied PUs", 
                "average number of links per node, where species and PUs are nodes", 
                "proportion of links made out of all possible links between species and PUs", 
                "product of number of species and number of PUs, i.e, size of adjacency matrix"
                )

invars_table_caption = "Input features for model based on all variables related to problem size and link density."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")

kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingProbSizeAndDensityFiles, include=FALSE, eval=FALSE}

###  Write Matilda files using ProbSizeAndDensity features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildProbSizeAndDensityTestAndTrain, include=FALSE}

###  Prep for ProbSizeAndDensity learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoProbSizeAndDensityNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

```{r predictRepShortfallUsingProbSizeAndDensity, include=FALSE}

### Representation shortfall using ProbSizeAndDensity data

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r predictSolCostErrorUsingProbSizeAndDensity, include=FALSE}

### Solution cost error using ProbSizeAndDensity data 

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

##  Predict using Graph features (27 input variables)

Extending the exploration of the structure of the reserve selection problem, we next compute many more complexity measures over the adjacency matrix of the bipartite graph representing the problems and use those as inputs in the prediction of reserve selection errors.  (Input variables used are listed in LongformSI.)  Here, there is continuing improvement in prediction accuracy compared to the ProbSizeAndDensity variable set, but not nearly the step up provided by ProbSizeAndDensity over using only the number of species and PUs.  

For representation shortfall, rmse values for all of the reserve selectors have improved by only 0.01 or 0.02 compared to ProbSizeAndDensity predictions (Fig.  \ref{fig:predRepShortfallUsingNonLatapyGRAPH}).  For the NV reserve selectors, there has also been a small reduction in the underestimation of shortfall in the middle range of true values.  

Rmse values for solution cost error predictions on NV reserve selectors have improved by 0.04 (25% reduction) while SA_SS predictions have improved from an rmse value of 0.31 to a new value of 0.25 (19% reduction) (Fig.  \ref{fig:predSolCostErrorUsingNonLatapyGRAPH}).  The most notable change for all reserve selectors is that predictions for FN-dominated problems are no longer effectively flat.  While they are far from a perfect fit to the true values, the predicted values now have an upward trend that is closer to the 1:1 line.  However, for NV reserve selectors, there is still considerable underestimation of solution cost error as true solution cost error increases.  While predictions for SA_SS have improved somewhat, they are still low quality, showing large variation at all true values, with overestimation for small true error values and understimation for large true error values.  

```{r setNonLatapyGraphParams}
#vars_used_str = "nonLatapyGraph" 
vars_used_str = "Graph" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

include_median_redundancies=FALSE

#inVars = c("rsp_num_occupied_PUs", "rsp_num_spp")
inVars = c(                                                  #  27 variables                             
#                                         "ig_lcctop" ,  
#                                         "ig_lccbottom" ,  
#                                         "ig_distop" ,  
#                                         "ig_disbottom" ,  
#                                         "ig_cctop" ,  
#                                         "ig_ccbottom" ,  
#                                         "ig_cclowdottop" ,  
#                                         "ig_cclowdotbottom" ,  
#                                         "ig_cctopdottop" ,  
#                                         "ig_cctopdotbottom" ,  
#                                         
#                                         "ig_mean_bottom_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_bottom_bg_redundancy" ,  
#                                         "ig_mean_top_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_top_bg_redundancy" ,  
                    
                                    #  bipartite package metrics
#2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
                            
                                        "web_asymmetry" , 
                                        "links_per_PUsAndSpp" , 
                                        "cluster_coefficient" , 
# "weighted_NODF" , 
# "interaction_strength_asymmetry" , 
                                        "specialisation_asymmetry" , 
                                        "linkage_density" , 
                                        "weighted_connectance" , 
                                        "Shannon_diversity" , 
                                        "interaction_evenness" , 
                                        "Alatalo_interaction_evenness" , 
                        
                                        "mean.number.of.shared.partners.PUs" , 
                                        "mean.number.of.shared.partners.Spp" , 
                                        "cluster.coefficient.PUs" , 
                                        "cluster.coefficient.Spp" , 
                                        "niche.overlap.PUs" , 
                                        "niche.overlap.Spp" , 
                                        "togetherness.PUs" , 
                                        "togetherness.Spp" , 
                                        "C.score.PUs" , 
                                        "C.score.Spp" , 
                                        "V.ratio.PUs" , 
                                        "V.ratio.Spp" , 
                                        "functional.complementarity.PUs" , 
                                        "functional.complementarity.Spp" , 
                                        "partner.diversity.PUs" , 
                                        "partner.diversity.Spp" , 
                                        "generality.PUs" , 
                                        "vulnerability.Spp"
                         )
```

&nbsp;  

```{r makeNonLatapyGraphVarsTable, eval=FALSE}
###2025 07 25###```{r makeNonLatapyGraphVarsTable, eval=TRUE}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                # "number of species", 
                # "number of occupied PUs", 
                # "average number of links per node, where species and PUs are nodes", 
                # "proportion of links made out of all possible links between species and PUs", 
                # "product of number of species and number of PUs, i.e, size of adjacency matrix", 
                
#                                         "ig_lcctop" ,  
#                                         "ig_lccbottom" ,  
#                                         "ig_distop" ,  
#                                         "ig_disbottom" ,  
#                                         "ig_cctop" ,  
#                                         "ig_ccbottom" ,  
#                                         "ig_cclowdottop" ,  
#                                         "ig_cclowdotbottom" ,  
#                                         "ig_cctopdottop" ,  
#                                         "ig_cctopdotbottom" ,  
#                                         
#                                         "ig_mean_bottom_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_bottom_bg_redundancy" ,  
#                                         "ig_mean_top_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_top_bg_redundancy" ,  

  
  
                    #  bipartite package metrics

                                          #2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
                          #"connectance",   #  Shouldn't this be removed?  Same as edge_frac_of_possible?
                          #                    "SHOULD REMOVE FROM EVERYTHING SET?  bip_connectance:  Realised proportion of possible links (Dunne et al. 2002): num_links/(num_spp*num_PUs). In reserve selection, this is the proportion of the occupancy matrix (bpm) that contains 1s, i.e., 'positives'.  Same as ig_bidens (renamed to ig_realized_frac_of_all_possible_links?) and edge_frac_of_possible.  This is the standardised number of species combinations often used in co-occurrence analyses (Gotelli & Graves 1996).", 

                "Balance between numbers in the two levels: ...",    #  "positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2", 
                "average number of links per node, where species ...",    #  "and PUs are nodes", 

                        
                "Mean, across all nodes of the number of realized ...",    #  "links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
# "weighted_NODF" , 
# "interaction_strength_asymmetry" , 
                "Asymmetry (PU level vs. spp level) of specialisation ...",    #  "now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
                "Marginal totals-weighted diversity of interactions ...",    #  "per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
                "Linkage density divided by number of nodes in the n...",    #  "etwork (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
                "Shannon's diversity of interactions (i.e. network entries)." , 
                "Shannon's evenness for the web entries." , 
                "A different measure for web entry evenness, as ...",    #  "proposed by Muller et al. (1999)." , 

#"number.of.Spp",  #  Shouldn't this be removed?  Same as rsp_num_spp?
                    
                "Based on the distance matrix between PUs counting ...",    #  "the number of spp that both PUs interact with." , 
                "Based on the distance matrix between spp counting ...",    #  "the number of PUs that both spp interact with." , 
                "Mean, across all PUs of the number of realized links ...",    #  "divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
                "Mean, across all spp of the number of realized links ...",    #  "divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
                "Mean similarity in interaction pattern between PUs, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "Mean similarity in interaction pattern between spp, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level" , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level." , 
                "UNCLEAR - Functional complementarity? for a ...",    #  "given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "UNCLEAR - Functional complementarity? for a given ...",    #  "level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for PUs." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for spp." , 
                "Mean number of spp per PU." ,    #  REMOVE, since it duplicates rsp_num_spp_per_PU
                "Mean number of PUs per spp." 
      # 
      #   
      #                       
      #                                     #  bipartite package metrics
      # #2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
      #                             
      #                                         "Balance between numbers in the two levels: positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2" , 
      #                                         "[links_per_species in bipartite package].  Mean number of links per node (qualitative): sum of links divided by number of nodes." , 
      #                                         "Mean, across all nodes of the number of realized links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
      # # "weighted_NODF" , 
      # # "interaction_strength_asymmetry" , 
      #                                         "Asymmetry (PU level vs. spp level) of specialisation now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
      #                                         "Marginal totals-weighted diversity of interactions per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
      #                                         "Linkage density divided by number of nodes in the network (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
      #                                         "Shannon's diversity of interactions (i.e. network entries)." , 
      #                                         "Shannon's evenness for the web entries." , 
      #                                         "A different measure for web entry evenness, as proposed by Muller et al. (1999)." , 
      #                         
      #                                         "Based on the distance matrix between PUs counting the number of spp that both PUs interact with." , 
      #                                         "Based on the distance matrix between spp counting the number of PUs that both spp interact with." , 
      #                                         "Mean, across all PUs of the number of realized links divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
      #                                         "Mean, across all spp of the number of realized links divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
      #                                         "Mean similarity in interaction pattern between PUs, calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
      #                                         "Mean similarity in interaction pattern between spp, calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
      #                                         "UNCLEAR - Mean number of co-occupancies across all species-combinations." , 
      #                                         "UNCLEAR - Mean number of co-occupancies across all species-combinations." , 
      #                                         "UNCLEAR -  (Normalised) mean number of checkerboard combinations across all species of the level." , 
      #                                         "UNCLEAR -  (Normalised) mean number of checkerboard combinations across all species of the level." , 
      #                                         "UNCLEAR - Variance-ratio of species numbers to interaction numbers within species of a level" , 
      #                                         "UNCLEAR - Variance-ratio of species numbers to interaction numbers within species of a level." , 
      #                                         "UNCLEAR - Functional complementarity? for a given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
      #                                         "UNCLEAR - Functional complementarity? for a given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
      #                                         "Mean Shannon diversity of the number of interactions for PUs." , 
      #                                         "Mean Shannon diversity of the number of interactions for spp." , 
      #                                         "Mean number of spp per PU." , 
      #                                         "Mean number of PUs per spp."
                )

invars_table_caption = "Input features for model based on non-Latapy graph variables."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingNonLatapyGraphFiles, include=FALSE, eval=FALSE}

###  Write Matilda files using GRAPH features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildNonLatapyGraphTestAndTrain, include=FALSE}

###  Prep for GRAPH learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoNonLatapyGRAPHNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Representation shortfall using Graph data

```{r predictRepShortfallUsingNonLatapyGRAPH, fig.cap = "\\label{fig:predRepShortfallUsingNonLatapyGRAPH}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing all bipartite graph variables (listed in LongformSI).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

### Solution cost error using Graph data 

```{r predictSolCostErrorUsingNonLatapyGRAPH, fig.cap = "\\label{fig:predSolCostErrorUsingNonLatapyGRAPH}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing all bipartite graph variables (listed in LongformSI).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

##  Predict using All features (42 input variables)

For brevity, we omit the detailed plots for this input feature set, but briefly describe the outcomes here.  This final set of input variables is meant to suggest an upper bound on our ability to predict with variables that we are currently aware of.   This set contains all variables that we have measured about each problem, which is cheating in the sense that some of these variables are unknowable at the time of prediction in non-synthetic, real-world problems.  In particular, we include the true realized input error rates (FP, FN, etc.), the correct solution's fraction of the landscape, and the model RB input parameters.  The point here is to ask, if we were able to know all of these things about each problem, how much better could we predict output errors than using just the variables already tested.  However, this does not represent a theoretical upper bound on predictive performance since there may be other, more informative variables that we are not aware of or have not measured, however, it does give us a sense of whether it is worth getting good estimates of other variables such as the input error rates.  The results do not show large improvements over just using the graph variables.  (Input variables and full plots are given in LongformSI.)  

```{r setEverythingParams}
vars_used_str = "All" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

include_median_redundancies=FALSE

inVars = c(                                           #  42 variables
            "rsp_num_occupied_PUs", 
            "rsp_num_spp", 

            "links_per_PUsAndSpp", 
            "edge_frac_of_possible",    #  Same as bip_connectance?
            "sppPUprod", 
            
            "ig_num_edges_m", 
            "web_asymmetry", 
            "cluster_coefficient", 
            "specialisation_asymmetry", 
            "linkage_density", 
            "weighted_connectance", 
            "Shannon_diversity", 
            "interaction_evenness", 
            "Alatalo_interaction_evenness", 
            "mean.number.of.shared.partners.PUs", 
            "mean.number.of.shared.partners.Spp", 
            "cluster.coefficient.PUs", 
            "cluster.coefficient.Spp", 
            "niche.overlap.PUs", 
            "niche.overlap.Spp", 
            "togetherness.PUs", 
            "togetherness.Spp", 
            "C.score.PUs", 
            "C.score.Spp", 
            "V.ratio.PUs", 
            "V.ratio.Spp", 
            "functional.complementarity.PUs", 
            "functional.complementarity.Spp", 
            "partner.diversity.PUs", 
            "partner.diversity.Spp", 
            "generality.PUs", 
            "vulnerability.Spp", 

            "gurobi_mipgap", 
            
            "rsp_alpha__", 
            "rsp_n__num_groups", 
            "rsp_p__prop_of_links_between_groups", 
            "rsp_r__density", 
            "rsp_d__number_of_nodes_per_group", 
            
            "actual_sol_frac_of_landscape", 
            
            "rsp_realized_FP_rate", 
            "rsp_realized_FN_rate", 
            "rsp_realized_Ftot_rate" 
            )
```

&nbsp;  

```{r makeEverythingVarsTable, eval=FALSE}
###2025 07 25###```{r makeEverythingVarsTable, eval=TRUE}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

#  NOTE:  2024 03 28 
#  I have temporarily truncated all long lines in these definitions with a "..." 
#  so that the table fits on the page.  When kable() runs it off of the page, 
#  it also shifts that caption and table number completely off the page and 
#  I don't want that.  I need to figure out how to get the tables to wrap 
#  long lines of text within a fixed column width.  
  
in_var_defns = c(
                "number of occupied PUs", 
                "number of species", 
                "product of number of species and number ...",    #  "of PUs, i.e, size of adjacency matrix", 
                "Total number of adjacency matrix cells containing ...",    #  "a 1, i.e., total number of links between spp and PUs.", 

                "Balance between numbers in the two levels: ...",    #  "positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2", 
                "average number of links per node, where species ...",    #  "and PUs are nodes", 

                        
                "Mean, across all nodes of the number of realized ...",    #  "links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
                "Asymmetry (PU level vs. spp level) of specialisation ...",    #  "now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
                "Marginal totals-weighted diversity of interactions ...",    #  "per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
                "Linkage density divided by number of nodes in the n...",    #  "etwork (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
                "Shannon's diversity of interactions (i.e. network entries)." , 
                "Shannon's evenness for the web entries." , 
                "A different measure for web entry evenness, as ...",    #  "proposed by Muller et al. (1999)." , 

                "Based on the distance matrix between PUs counting ...",    #  "the number of spp that both PUs interact with." , 
                "Based on the distance matrix between spp counting ...",    #  "the number of PUs that both spp interact with." , 
                "Mean, across all PUs of the number of realized links ...",    #  "divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
                "Mean, across all spp of the number of realized links ...",    #  "divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
                "Mean similarity in interaction pattern between PUs, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "Mean similarity in interaction pattern between spp, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level" , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level." , 
                "UNCLEAR - Functional complementarity? for a ...",    #  "given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "UNCLEAR - Functional complementarity? for a given ...",    #  "level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for PUs." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for spp." , 
                "Mean number of spp per PU.", 
                "Mean number of PUs per spp.", 

                "Relative measure of the upper bound on the distance ...",    #  "to the correct solution as a proportion of the correct solution.", 
                "proportion of links made out of all possible links ...",     #  "between species and PUs"

                "gurobi mipgap", 
                
                "Exponent driving number of nodes per clique ...",    #  "in model RB generator for current Base problem.", 
                "Number of cliques in model RB generator of ...",    #  "current Base problem.", 
                "Constraint density, i.e., proportion driving ...",    #  "number of links per round in model RB generator of current Base problem.  number of links per round = p*d^2, where d is number of nodes per clique.", 
                "Constraint tightness, i.e., multiplier driving ...",    #  "number of rounds of linking in model RB generator of current Base problem.  Number of rounds of linking = r*n*ln(n).", 
                "Number of nodes per clique.", 
                "Fraction of the landscape contained in correct ...",    #  "solution, i.e., correct solution cost / rsp_num_occupied_PUs", 

                "Proportion of false positives in the APP adjacency ...",    #  "matrix.", 
                "Proportion of false negatives in the APP adjacency ...",    #  "matrix.", 
                "Combined proportion of false positives and false ..."    #  "negatives in the APP adjacency matrix.", 
                )

invars_table_caption = "Input features for model based on ALL variables."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingEverythingFiles, include=FALSE, eval=FALSE}

###  Write Matilda files using Everything features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildEverythingTestAndTrain, include=TRUE}

###  Prep for Everything learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoEverythingNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

```{r predictRepShortfallUsingEverything, include=FALSE}

### Representation shortfall using Everything data

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r predictSolCostErrorUsingEverything, include=FALSE}

### Solution cost error using Everything data 

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

##  Summary of learning to predict output errors    

As with the method comparison results, the results for learning to predict output error of the reserve selectors in the NV set were similar for both representation shortfall and solution cost error.  The prediction results for SA_SS were considerably worse than for predicting the results of any of the NV methods, especially for solution cost error prediction.  

To make it easier to compare all of these predictive results, we show summary plots of adjusted $R^2$ (Fig. \ref{fig:barPlotAdjR2valuesForPreds}) for both output error types for SA and SA_SS.  (A similar plot for $RMSE$ values is given in LongformSI.)  We only show the SA and SA_SS reserve selectors for visual simplicity and because all of the NV reserve selector results are so similar to the SA results.  We chose SA as the representative NV selector because it's the most like SA_SS and so, seems like the most informative comparison in relation to the effects of using a voting method over a non-voting method.  

The first overall result is that representation shortfall is much easier to predict than solution cost error for all of the reserve selectors.  The second overall result is that predicting solution cost error for SA_SS is much harder than predicting solution cost error for any of the other reserve selectors.  Finally, the problem size variables (number of species and number of PUs) were almost completely uninformative on their own.  

All four panels in Fig. \ref{fig:barPlotAdjR2valuesForPreds} show similar patterns of increasing performance with one exception.  The increase in adjusted $R^2$ when adding density variables to problem size variables is between 0.37 and 0.61 for three of the panels.  However, when predicting for SA_SS the jump is negligible, at only 0.02.     

```{r filterToSmallSetOfFittingScores, include=FALSE}
all_fitting_scores_df %>% 
    filter (
            # (vars_used_str == "PCA1thru10") | 
            # (vars_used_str == "DormannProbSize") | 
        (vars_used_str == "PUsAndSppOnly") | 
            # (vars_used_str == "sppPUprod") | 
        ##2024 02 01##(vars_used_str == "LinksPerNodeOnly") | 
            # (vars_used_str == "PUs_Spp_SppPUprod") | 
        (vars_used_str == "ProbSizeAndDensity") | 
            # (vars_used_str == "FifthCut") | 
            # (vars_used_str == "SixthRemoval") | 
            # (vars_used_str == "FifthRemoval") | 
            # (vars_used_str == "FourthRemoval") | 
            # (vars_used_str == "SecondRemoval") | 
            # (vars_used_str == "FirstRemoval") | 
#            (vars_used_str == "spp_PUs_only") | 

        ##2024 02 01##(vars_used_str == "Glmnet7var") | 
            # (vars_used_str == "Glmnet9var") | 
        (vars_used_str == "Graph") | 
        ##2024 02 01##(vars_used_str == "graph")
        ##
        (vars_used_str == "Everything")  
             
             ) -> all_fitting_scores_df 

all_fitting_scores_df$ordered_vars_used = 
    factor (all_fitting_scores_df$vars_used, levels = c(

            # "sppPUprod",                     #  1 variable
      "PUsAndSppOnly",                 #  2 variables
            # "PUs_Spp_SppPUprod",             #  3 variables
      ##2024 02 01##"LinksPerNodeOnly",              #  1 variable
            # "DormannProbSize",               #  3 variables
            # 
            # "SixthRemoval",                  #  6 variables
      "ProbSizeAndDensity",                   #  5 variables
            # "FifthCut",                      #  8 variables
            # "PCA1thru10",                    #  10 variables
            # "FifthRemoval",                  #  9 variables
            
            
            
      ##2024 02 01##"Glmnet7var",                    #  7 variables
            # "Glmnet9var",                    #  9 variables
            # "FourthRemoval",                 #  20 variables 
            # "SecondRemoval",                 #  23 variables 
      "Graph",                #  27 variables
            
      ##2024 02 01##"graph",                         #  39 variables
            # "FirstRemoval",                  #  37 variables
            
      "Everything"                     #  69 variables 
            
             ))

rs_names_to_bar_plot = c("SA", "SA_SS")
```

```{r buildMarxanReducedFittingScoresSet, include=FALSE}

###  Filter the set of fitting scores down to include only the scores for SA and SA_SS on the test set.  

all_fitting_scores_df %>% 
    filter ((rs_method_name   == "SA") | 
              (rs_method_name == "SA_SS")) %>% 
    filter (train_or_test == "TEST") %>% 
    select (vars_used_str, measure_name_str, rs_method_name, rmse, adj_R2, ordered_vars_used) %>% 
    mutate (rmse = round (rmse, 2), 
            adj_R2 = round (adj_R2, 2)) -> 
  only_marxans_fitting_scores_df

only_marxans_fitting_scores_df %>% 
    mutate (label_measure_name_str = 
            ifelse (measure_name_str == "abs_rep_shortfall_resid", 
                    "Representation Shortfall", "Solution Cost Error")) -> 
    only_marxans_fitting_scores_df            
```

```{r eval=FALSE}
###  Summary of RMSE scores for predictions of representation shortfall and solution cost error for SA and SS
```

```{r barPlotRMSEvaluesForPredictions, eval=FALSE,         fig.cap = "\\label{fig:barPlotRMSEvaluesForPreds}Summary of RMSE scores on test set for linear model predictions of representation shortfall and solution cost error for SA and and SA SS using four different input feature sets.  These are the RMSE values from the upper left corner of the SA and SA SS results in Figs. \\ref{fig:predRepShortfallUsingPUsAndSppOnly} through \\ref{fig:predSolCostErrorUsingNonLatapyGRAPH}.  Columns in the figure correspond to the output errors being predicted and rows correspond to the two reserve selectors whose performance is being summarized in the plot."}

  #  Based on facetting examples from 
  #  http://zevross.com/blog/2019/04/02/easy-multi-panel-plots-in-r-using-facet_wrap-and-facet_grid-from-ggplot2/

   #  Got some of this ggplot stuff from https://appsilon.com/ggplot2-bar-charts/
   #  
   #  Got factor ordering from https://sebastiansauer.github.io/ordering-bars/
   #  
   #  Also, some good sites for getting names of colors:
   #    - This one also has colorblind palettes:
   #      http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
   #    - Lots of colors
   #      https://www.datanovia.com/en/blog/awesome-list-of-657-r-color-names/

ggplot(data = only_marxans_fitting_scores_df, aes (x=ordered_vars_used, y=rmse)) +
                geom_col (position = position_dodge(), fill = "deepskyblue2") +  #"#3db5ff") +
                geom_text (aes(label = rmse), hjust = 1.4, size = 2.2) +
                coord_flip() + 
                labs (y = "RMSE", x = "Input Feature Set") + 
                ggtitle ("RMSE values for predicting output errors\nby Feature Set") + 
                theme(plot.title = element_text(hjust = 0.5)) + 
#    facet_grid (label_measure_name_str ~ rs_method_name) 
    facet_grid (rs_method_name ~ label_measure_name_str) 
```

###  Summary of Adjusted $R^2$ scores for error predictions for SA and SS

```{r barPlotAdjR2valuesForPredictions, fig.cap = "\\label{fig:barPlotAdjR2valuesForPreds}Summary of adjusted $R^2$ scores on test set for linear model predictions of representation shortfall and solution cost error for SA and and SA SS using four different input feature sets.  These are the $R^2$ values from the upper left corner of the SA and SA SS results in Figs. \\ref{fig:predRepShortfallUsingPUsAndSppOnly} through \\ref{fig:predSolCostErrorUsingNonLatapyGRAPH}.  Columns in the figure correspond to the output errors being predicted and rows correspond to the two reserve selectors whose performance is being summarized in the plot."}

ggplot(data = only_marxans_fitting_scores_df, aes (x=ordered_vars_used, y=adj_R2)) +
                geom_col (position = position_dodge(), fill = "goldenrod1") +  #"#0099f9") +
                geom_text (aes(label = adj_R2), hjust = 1.4, size = 2.2) +    # hjust = -0.2
                coord_flip() + 

#----------
#  2025 07 25 - BTL
#  These labels and titles using latex formatting of R^2 use R library latex2exp 
#  and are cloned from this very useful web page:
#    https://uliniemann.com/blog/2022-02-21-math-annotations-in-ggplot2-with-latex2exp/

#   OLD:        labs (y = "Adj R^2", 
                labs (y = TeX(r"( Adj $R^2$ )"), 
                      x = "Input Feature Set") + 
  
#                   #  Note that trying to include the newline in the latex expression fails 
#                   #  since it doesn't support newlines.  Haven't found a good 
#                   #  way to do it.
#   OLD:        ggtitle ("Adj R^2 values for predicting output errors\nby Feature Set") + 
                ggtitle (TeX(r"( Adj $R^2$ values for fitted models )")) + 
#----------
  
                theme(plot.title = element_text(hjust = 0.5)) + 
#    facet_grid (label_measure_name_str ~ rs_method_name) 
    facet_grid (rs_method_name ~ label_measure_name_str) 
```

#  Discussion  

##  Principal findings  {#principalFindingsDiscussionSection}   

This paper addresses a fundamental challenge facing the systematic conservation planning community in understanding when and why different reserve selection methods perform well or poorly on specific problems under uncertainty. Rather than evaluating which methods are "best," our aim has been to develop tools that can help both method developers and practitioners better understand and predict method performance across diverse problem types by i) demonstrating the underexplored importance of problem structure to method accuracy under uncertainty and ii) demonstrating the value of changing the focus of SCP evaluation away from case studies toward explicitly predicting method accuracy on individual problems under uncertainty. 

We have used three complementary ideas to enhance both prediction and understanding.  First, we have borrowed theoretically grounded **solution planting** methods from computational theory to explore synthetic reserve design problems with known correct solutions over a greater range of problem difficulty than has been previously examined in the SCP literature.  Second, we have shown how we can improve the ecological realism of these synthetic test problems by **wrapping** their species distribution inside whatever distribution a user considers more ecologically realistic to form a larger problem that is guaranteed to have the same optimal solution as the original problem. Third, we have shown how these reserve selection problem structures can be represented as a **bipartite graph** to enable us to borrow measures from graph theory as inputs for fitting explicit predictive models of the accuracy of a given reserve selector on a given individual problem under uncertainty.  

Our results showed that problem structure can create substantial variation in method difficulty that goes well beyond problem size. This was consistent across all tested reserve selectors.  Even methods that performed perfectly on problems without uncertainty (ILP and Simulated Annealing) showed substantial variation in error magnitude when small amounts of input uncertainty ($\leq 10\%$) were introduced.  Moreover, optimization nearly always magnified input error in our data set (`r mag_frac`% of all problems with input error had larger output error than input error).  

The predictive modeling component of our study demonstrated that graph-theoretic measures of problem structure substantially improved our ability to predict method performance compared to using problem size alone (roughly a 2/3 reduction in prediction error). The amount of representation shortfall was roughly twice as easy to predict with almost every predictor set (other than problem size) as solution cost error was.  One noticeable finding was that solution cost error was much more difficult to predict correctly for SA_SS, the only reserve selector that used a voting method.  We have no explanation for this.  

While our predictions were not perfect, our findings complement the ongoing development of more sophisticated uncertainty-handling methods by providing tools to understand when and why different approaches succeed or struggle.  The large jump in predictive power from using graph-based measures indicates the value of pursuing better understanding of structural attributes of individual real-world problems that influence method performance under uncertainty.  Another important point is that the predictions may also be a way of providing practitioners with quantitative estimates of the value of gathering new information (@canessa2015mee) or of other actions such as modifying the values of representation targets for their own individual problems.    

##  Methods tested  

The fact that our tested reserve selectors had error in their outputs is unsurprising given that none of them attempted to address uncertainty in the inputs.  However, both the *amount* and *variation* of error magnification, was surprisingly large at nearly every level of input error (Figs. \ref{fig:InVsCostErrLE125inResults} and  \ref{fig:inVsRepShortfallinResults}).  

It is important to evaluate the accuracy of methods that ignore uncertainty because it is still common to see real-world and published reserve selections that do not explicitly address input uncertainties.  For example, a systematic survey of reserve selection papers between 2007 and 2019 found that 90% of papers in their study did not account for overprediction of species presence (i.e., false positives) (@velazco2020bc).  Furthermore, our results come from using only small amounts of input error compared to what may be found in real-world data.  For example,  @tullochIncorporatingUncertaintyAssociated2013 tested their method for addressing uncertainty on real-world benthic habitat maps where error ranged from 0 to 72.5%  with a mean of 33%, while all our input errors were no greater than 10%.  

In recent years, the research community has made significant advances in developing robust methods that explicitly account for specific input uncertainties (e.g., @billionnet2015ema, @haider2018em, @beechStochasticApproachMarine2008, @tullochIncorporatingUncertaintyAssociated2013, @runtingReducingRiskReserve2018, @sierra-altamiranda2020em).  These methods represent important progress and likely improve performance when their assumptions are well-met. However, even these sophisticated approaches face ongoing difficulties.      

First, methods like these have introduced new uncertainties by way of variables required by these methods, such as variance estimates and probability estimates, which are often not well-calibrated (@valavi2022em).  Our findings that small input errors can lead to much larger output errors raise questions about whether even small errors in probability or variance estimates in robust methods can have similarly outsized effects on reserve selector results.  

Second, there are so many different forms of uncertainty in reserve selection inputs that methods addressing uncertainty are unlikely to assign uncertainty values to all of the many variables affecting real problems.  For example, PU cost uncertainties are rarely addressed, even though they are often uncertain or lacking strong empirical support (@armsworth2014aotnyaos).  Other omissions may be forms of uncertainty that are more difficult to measure and/or represent in a typical problem formulation, e.g., spatial correlation in risks to species survival after being reserved (e.g., fire or climate change (@albers2016po) and other examples such as those discussed in @drira2019, @robillardAssessingShelfLife2017, and @viscontiBuildingRobustConservation2015).  Consequently, like the methods that we have evaluated in our study, there are always likely to be uncertainties not addressed or inaccurately addressed in the inputs of  current methods applied to real problems.  

Our study definitely leaves questions unanswered about the behavior of robust methods and future testing should move beyond the small set of methods that we have examined here.  However, changing to robust forms of ILP does not eliminate violations of their assumptions.  It does make the assumptions about input errors more transparent and is likely to be more robust than ignoring input uncertainty altogether, however, it still requires that all input uncertainties are included and accurately calibrated, which may be a significant challenge.   

##  Synthetic vs. Real-world data  

A common concern about synthetic studies such as ours is that synthetic problems may not adequately reflect real-world problems. Real-world case studies are often considered "a more realistic representation of outcomes expected in real-world applications" (@cheokSympathyDevilDetailing2016).  We see two important issues with this view.  First, since the uncertainties in real-world data are not known exactly, then the degree to which the real-world data correctly represents the true underlying data is unknown.  

Second, while there are hundreds of individual SCP studies, there are currently no large, shared collections of SCP problems that would allow us to characterize real-world reserve selection problem traits such as problem difficulty, species co-occurrence patterns, or distributions of representation targets, species abundance, and PU costs. Without knowledge of problem attribute distributions, it is difficult to assess how representative any individual problem of the real world is or how well a method's performance generalizes.

Determining the statistical distributions of structural attributes that describe the universe of real-world reserve selection problems is arguably one of the most useful projects that could help improve the accuracy of reserve selection methods.  A good starting point for this approach could be the large pool of problems that have been addressed with Marxan and its variants, since these share a simple input file format that could be anonymized to protect sensitive data.

Meanwhile, synthetic problems generated with theoretically grounded methods like those we introduce here provide a way to systematically explore the space of attributes that control problem difficulty. Additionally, researchers in other fields have demonstrated methods for generating synthetic problems that have structural attributes that are similar to given real-world problems.  For example, You and Wu use deep learning to learn to generate bipartite networks whose bipartite graph properties match training set graph properties (@you2019anips3acnips2n2d82vbc).  Many other authors address identification and creation of sets of problems of varying difficulty under terms like data complexity, meta-learning, algorithm selection, and instance space (e.g., @tinkamho2002itpami, @basu2006, @smith-milesCrossdisciplinaryPerspectivesMetalearning2008, @macia2009hais, @macia2010p1acgec, @maciaUCIMindfulRepository2014,  @lorenaDataComplexityMetafeatures2018, @munozGeneratingNewSpaceFilling2019,  @torquette2022).  

##  Extending to more complex problems  {#externalProbDefDiscussionSection}  

The reserve selection problems tested here use a simpler formulation than is typically employed in real-world applications. However, we found substantial variation in problem difficulty and input error magnification even in these comparatively simple problems. This raises the question of whether these results are predictive of method accuracy on more complex real-world problem formulations.

There is a straightforward path to extending our work to more complex problem formulations by composing elements of different problems. For example, while our problems have no spatial structure, any of our problems can be overlaid on any landscape having at least the same number of PUs by randomly assigning IDs of synthetic PUs to spatially defined real landscape PUs.  Then, all of the cost and boundary length attributes of those spatially defined PUs could be incorporated into more complex reserve selection problems. This independent assignment of synthetic PUs to spatial PUs and their attributes could be done across many different landscapes or randomly repeated many times on the same landscape and then solve the spatially constrained version of that problem in each of the different variants.  In this way, problems of known underlying difficulty with respect to species abundance and co-occurrence patterns could be used to test the relative contribution of spatial compactness constraints to the difficulty of a problem.  

This combination of a compositional approach and the theoretically guided spread of difficulty in the underlying simple problems provides a foundation for asking questions about the relative contributions of any problem aspect to overall difficulty under uncertainty. For example, do spatial connectivity requirements make inherently difficult problems easier by constraining the solution space, or do they make them harder by adding another layer of complexity? Understanding these interactions could inform both method development and practical application decisions.

Unfortunately, composed problems would no longer have the automatically known correct solution provided by our solution planting method.  Consequently, we would need to apply an exact solver like ILP (with sufficient computational resources) to guarantee correct solutions to composed problems before adding uncertainty.  Fortunately, this potentially expensive computation would only need to be done once for each composed problem, no matter how many reserve selectors and/or uncertainties were to be tested.  

##  Related Work  {#relatedWorkDiscussionSection}  

###  Related ecological work    

We know of only two reserve selection studies that explore variation in species co-occurrence structure as a factor in reserve selector performance.  @presseyEffectsDataCharacteristics1999 explores efficiency and suboptimality of four methods based on variation in PU size, dataset size, and feature nestedness and rarity within variants of a single 1886 PU landscape with 248 features.  @viscontiBuildingRobustConservation2015 examines robustness of solutions to implementation changes based on varying a complementarity measure within many versions of four synthetic 20 PU landscapes with 5 species.  Case studies such as @warmanSensitivitySystematicReserve2004, @cheokSympathyDevilDetailing2016, and @rodewaldTradeoffsValueBiodiversity2019,  explore combinations of problem attributes such as PU size and shape, target values, thematic resolution, and costs on single landscapes.  While these studies do sample different input values for problems, they do not attempt to systematically control for a generalizable population of problem structures with theoretically guided variation in difficulty. The lack of theoretical underpinning matters because without it, varying the input data may still lead to many problems of similar difficulty even if the inputs varied widely.  

###  Related computational work

The relationship between problem structure and difficulty has long attracted attention in computational theory.  The proven difficulty of NP-hard problems is based on worst-case performance, but work such as @goldberg1982ipl suggested that average-case problems might be comparatively easy.  However, later work showed this was likely a consequence of choosing a test distribution containing primarily easy problems (@franco1983dam, @mitchell1992ptncaia).  Simultaneously, interest was sparked by work proposing the existence of abrupt phase transitions in the difficulty of NP-complete decision problems around threshold values of problem structural descriptors  (@huberman1987ai, @cheeseman1991ip1ijcai).  The proximity of parameter values to the threshold indicated the problem's relative difficulty (which is how our parameters for model RB were chosen).  The 1990s and 2000s saw numerous papers attempting to identify the threshold location and algorithm behavior in its neighborhood by using and improving random problem generators (@smithLocatingPhaseTransition1996, @achlioptas1997papocp, @gent2001c), including model RB (@xu2005ipnijcai) which we have adapted here.  Independent of phase transitions and following from @krishnamurthy1987itc,  Sanchis was simultaneously developing different generators for testing approximation algorithms for NP-complete problems, including vertex cover (@sanchis1989, @sanchisExperimentalTheoreticalResults1996a).  We have not explored her methods here but they may be of future interest.  

#  Conclusions  

Current evaluation approaches for SCP methods provide limited ability to predict or understand method accuracy under uncertainty on previously unseen individual problems whose structures differ from the initial method presentation or when method assumptions may not hold perfectly. This represents a significant challenge for both method developers seeking to understand their methods' performance characteristics and practitioners needing to assess the reliability of their conservation plans.

We have adapted and extended a theoretically grounded solution planting method to generate synthetic conservation planning problems with controllable difficulty, known optimal solutions, and statistically useful sample sizes for learning to predict method accuracy under uncertainty. We used this problem generator to create a publicly available benchmark set of thousands of test problems with known correct solutions, a range of input uncertainties, and systematic variation in problem difficulty.

Our evaluation demonstrated substantial output error and error magnification at nearly every level of input error for every reserve selector tested. This variation highlights the importance of being able to make reliable predictions about the amount and nature of likely reserve selection error on individual problems rather than relying solely on summary statistics. Without reliable per-problem understanding of difficulty, it is difficult to assess risks and determine whether additional actions such as gathering new information would be beneficial.

Our approach of representing individual reserve selection problems as bipartite graphs enabled computation of structural attributes that substantially improved prediction of individual problem difficulty for specific reserve selectors compared to using problem size alone. While our predictions were not perfect, the improvement suggests that this direction of research could provide valuable tools for both method developers and practitioners.

We believe that developing better predictive understanding of existing reserve selector behavior under uncertainty is an important complement to creating new reserve selectors. We acknowledge that rigorous, informative evaluation of methods under uncertainty requires significant computational resources and programming effort, but we believe the benefits justify this investment.

This work provides publicly available tools for improving reserve selector development and for identifying problems where optimization uncertainty may be concerning, suggesting a need for additional actions or method improvements. By making both the methods and benchmark data publicly available, we hope to support continued progress in understanding and improving systematic conservation planning approaches.

#  References  

<div id="refs"></div>

```{r includeOrExcludeAppendices, echo=FALSE}

#  Temporary exit to avoid building appendices every time
#  Remove this chunk when you're ready to build appendices too.

if (!params$build_appendices) knitr::knit_exit()
```

# Acknowledgements

**\textcolor{red}{Edited from version in bdpg\/Paper\_1\_method\/v11\_Paper\_1\_method\_\_body.Rmd:720}**

A.G. was supported by the Australian Research Council through Discovery Project DP150102472.  W.T.L. was supported in the early part of this work by a grant from Process Minerals International through Condition 13 of Australian EPBC Act approval 2010/5759. W.T.L. also thanks Chris Murphy of the Australian Department of Sustainability, Environment, Water, Population and Communities (SEWPAC) for his instigation of this funding. W.T.L. also thanks Simon Jones of RMIT University Geopatial Science Department for bridging funding and other forms of support.  The experiments were supported by use of the Nectar Research Cloud and by Nectar node operators Intersect and Monash University. The Nectar Research Cloud is a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC). The authors thank Iris Bergmann, Jeanne Panek, and Eric Berlow for useful comments on early drafts of this paper. W.T.L. also thanks Ke XU for rapid answers to questions about details of model RB, though any errors in the implementation of the model are the responsibility of W.T.L.  No conflict of interest is declared by either author.  
