---
params:
#--------------------  START of setting params variables  --------------------# 
  initialDate: "2022-12-20"
#-----  Rmd file and chunk control variables  -----# 
  chunk_echo: FALSE    #  Variable to allow turning echo of chunk code on/off for multiple, specified chunks.
  chunk_include: FALSE    #  Variable to allow turning echo of chunk output on/off for multiple, specified chunks.
  build_appendices: FALSE
#-----  predicted dominance variables  -----# 
  use_predicted_dominance_values: FALSE    ##  2024 01 21 ##FALSE
  use_predicted_FP_dominant: FALSE    ##  2024 01 21 ##FALSE
  use_predicted_FN_dominant: FALSE
  evaluate_dominance_prediction_model: FALSE    ##  2024 01 21 ##FALSE
#-----  input inclusion/exclusion and modification variables  -----# 
  exclude_ZL: TRUE
  bdpg_p_needs_fixing: TRUE
  exclude_imperfect_wraps: FALSE    #  2020 06 09 unfinished fails w/ both T & F
  gurobi_problem_filter: "all"    # "all" OR "completed" OR "unfinished"
  exclude_APP_0_inErr: TRUE    #  Necessary to avoid Inf and NaN magnifications
  remove_zero_output_errors: FALSE
  use_gurobi_optimal_runs_only: FALSE    #  Is this different from option "gurobi_problem_filter"?  2022 12 25 - BTL
  use_perfect_wraps_only: FALSE    #  Different from option "exclude_imperfect_wraps"?  2022 12 25 - BTL
  use_FN_dominant: TRUE
  use_FP_dominant: TRUE
  remove_probs_with_gt_10_pct_input_err: TRUE
  separate_by_redundancy: FALSE
#-----  file writing variables  -----# 
  write_tibs_to_csv: FALSE
  write_most_important_tibs_to_csv: TRUE
  file_type_to_write: "csv"    #"rds"
  add_gen_time_to_csv_name: FALSE
#-----  path variables  -----# 
  relative_path_to_input_data: "Data/Clean/All_batches/cln_exp."
  relative_path_to_data_out_loc: "Data/TempOutput"
  ggplot_save_path: "Paper_8_all_combined_for_Ecological_Monographs/Saved_plots"
#-----  plotting variables  -----# 
  exclude_greedy_rs_in_fit_plots: FALSE
  show_only_test_results: TRUE
  force_colors: TRUE
  display_train_as_final_pred_using_plot: FALSE    #  Whether prediction plots in pdf should show train or test data
#-----  prep for fitting variables  -----# 
  do_BoxCox: TRUE
#-----  fitting function variables  -----# 
  do_lm: TRUE    ## 2024 01 30 ##
  fitting_model_str: "lm"     ## 2024 01 30 ##
  show_lm_model_fit_coefficients_and_summary: FALSE
  echoExamplefitResultsCoefficient: FALSE    # <<<<<<<<<<==========
  do_rf: FALSE    ## 2024 01 30 ##
  ## 2024 01 30 ##fitting_model_str: "rf"    
  do_lm_cv: FALSE
  use_party_pkg_for_rf: FALSE    ## 2024 01 30 ##  This was FALSE already.  When I set it to TRUE, it failed with "Line 2019 Error in eval(predvars, data, env) : object 'train_y_vec' not found.
  do_glmnet_caret: FALSE
  do_glmnet_UC: FALSE    ## 2024 01 30 ##
  ## 2024 01 30 ## fitting_model_str: "glmnet_UC"        ## 2024 01 30 ## 
  SHOW_ALL_GLMNET_UC_PLOTS: FALSE
  VERBOSE_GLMNET_UC: FALSE        ## 2024 01 30 ## 
  VERBOSE_LM: FALSE
  VERBOSE_LM_CATS: FALSE
#-----  misc variables  -----# 
  mag_base_col_name_str: "max_TOT_FN_FP_rate"    #"max_TOT_FN_FP_rate"    #  "rsp_euc_realized_Ftot_and_cost_in_err_frac"
#-----  vestigial variables  -----# 
#  near_1_tol: 0.05
#  do_all_batches: TRUE  #FIRST ADDED FROM P6    #  Still used somewhere?  2024 01 23
#  create_p1_COR_data: TRUE
#  training_split_denom: 2
#--------------------  END of setting params variables  --------------------# 
#title: |  
#    | Learning to predict reserve selection optimization errors 
#    | under uncertainty - p8 v6
title: |  
    | Learning to predict reserve selection error under uncertainty 
    | (main paper only, no data preparation)
    | p8 v18
#title: "Learning to predict reserve selection error under uncertainty and varying problem difficulty (OR using solution planting) - p8 v6"
author: |  
    | William T. Langford^1,3^, Ascelin Gordon^2^
    | 
    | 1 Romsey VIC 3434, Australia
    | 2 School of Global, Urban and Social Studies, RMIT University, Melbourne, VIC 3000, Australia
    | 3 Corresponding author email: btlangford.work\@gmail.com
    | `r paste0 ('#-------------------------  RMD FILE AND CHUNK CONTROL VARIABLES   -------------------------#')`
    | `r paste0 ('[chunk_echo: ', params$chunk_echo, ']')`
    | `r paste0 ('[chunk_include: ', params$chunk_include, ']')`
    | `r paste0 ('[build_appendices: ', params$build_appendices, ']')`
    | `r paste0 ('#------------------------------  PREDICTED DOMINANCE VARIABLES   ------------------------------#')`
    | `r paste0 ('[use_predicted_dominance_values: ', params$use_predicted_dominance_values, ']')`
    | `r paste0 ('[use_predicted_FP_dominant: ', params$use_predicted_FP_dominant, ']')`
    | `r paste0 ('[use_predicted_FN_dominant: ', params$use_predicted_FN_dominant, ']')`
    | `r paste0 ('[evaluate_dominance_prediction_model: ', params$evaluate_dominance_prediction_model, ']')`
    | `r paste0 ('#--------------------  INPUT INCLUSION/EXCLUSION AND MODIFICATION VARIABLES   --------------------#')`
    | `r paste0 ('[exc ZL: ', params$exclude_ZL, ']')`
    | `r paste0 ('[bdpg_p_needs_fixing: ', params$bdpg_p_needs_fixing, ']')`
    | `r paste0 ('[exc imperfect: ', params$exclude_imperfect_wraps, ']')`
    | `r paste0 ('[gur probs: ', params$gurobi_problem_filter, ']')` 
    | `r paste0 ('[exc APP 0 inErr: ', params$exclude_APP_0_inErr, ']')` 
    | `r paste0 ('[remove_zero_output_errors: ', params$remove_zero_output_errors, ']')`
    | `r paste0 ('[use_gurobi_optimal_runs_only: ', params$use_gurobi_optimal_runs_only, ']')`
    | `r paste0 ('[use_perfect_wraps_only: ', params$use_perfect_wraps_only, ']')`
    | `r paste0 ('[use_FN_dominant: ', params$use_FN_dominant, ']')`
    | `r paste0 ('[use_FP_dominant: ', params$use_FP_dominant, ']')`
    | `r paste0 ('[remove_probs_with_gt_10_pct_input_err: ', params$remove_probs_with_gt_10_pct_input_err, ']')`
    | `r paste0 ('[separate_by_redundancy: ', params$separate_by_redundancy, ']')`
    | `r paste0 ('#-----------------------------------  FILE WRITING VARIABLES   -----------------------------------#')`
    | `r paste0 ('[write_tibs_to_csv: ', params$write_tibs_to_csv, ']')`
    | `r paste0 ('[write_most_important_tibs_to_csv: ', params$write_most_important_tibs_to_csv, ']')`
    | `r paste0 ('[file_type_to_write: ', params$file_type_to_write, ']')`
    | `r paste0 ('[add_gen_time_to_csv_name: ', params$add_gen_time_to_csv_name, ']')`
    | `r paste0 ('#----------------------------------------  PATH VARIABLES   ----------------------------------------#')`
    | `r paste0 ('[relative_path_to_input_data: ', params$relative_path_to_input_data, ']')`
    | `r paste0 ('[relative_path_to_data_out_loc: ', params$relative_path_to_data_out_loc, ']')`
    | `r paste0 ('[ggplot_save_path: ', params$ggplot_save_path, ']')`
    | `r paste0 ('#----------------------------------------  PLOTTING VARIABLES   ----------------------------------------#')`
    | `r paste0 ('[exclude_greedy_rs_in_fit_plots: ', params$exclude_greedy_rs_in_fit_plots, ']')`
    | `r paste0 ('[show_only_test_results: ', params$show_only_test_results, ']')`
    | `r paste0 ('[force_colors: ', params$force_colors, ']')`
    | `r paste0 ('[display_train_as_final_pred_using_plot: ', params$display_train_as_final_pred_using_plot, ']')`
    | `r paste0 ('#-----------------------------------  PREP FOR FITTING VARIABLES   -----------------------------------#')`
    | `r paste0 ('[do_BoxCox: ', params$do_BoxCox, ']')`
    | `r paste0 ('#-----------------------------------  FITTING FUNCTION VARIABLES   -----------------------------------#')`
    | `r paste0 ('[do_lm: ', params$do_lm, ']')`
    | `r paste0 ('[fitting_model_str: ', params$fitting_model_str, ']')`
    | `r paste0 ('[show_lm_model_fit_coefficients_and_summary: ', params$show_lm_model_fit_coefficients_and_summary, ']')`
    | `r paste0 ('[echoExamplefitResultsCoefficient: ', params$echoExamplefitResultsCoefficient, ']')`
    | `r paste0 ('[do_rf: ', params$do_rf, ']')`
    | `r paste0 ('[do_lm_cv: ', params$do_lm_cv, ']')`
    | `r paste0 ('[use_party_pkg_for_rf: ', params$use_party_pkg_for_rf, ']')`
    | `r paste0 ('[do_glmnet_caret: ', params$do_glmnet_caret, ']')`
    | `r paste0 ('[do_glmnet_UC: ', params$do_glmnet_UC, ']')`
    | `r paste0 ('[SHOW_ALL_GLMNET_UC_PLOTS: ', params$SHOW_ALL_GLMNET_UC_PLOTS, ']')`
    | `r paste0 ('[VERBOSE_GLMNET_UC: ', params$VERBOSE_GLMNET_UC, ']')`
    | `r paste0 ('[VERBOSE_LM: ', params$VERBOSE_LM, ']')`
    | `r paste0 ('[VERBOSE_LM_CATS: ', params$VERBOSE_LM_CATS, ']')` 
    | `r paste0 ('#----------------------------------------  MISC VARIABLES   ----------------------------------------#')`
    | `r paste0 ('[mag_base_col_name_str: ', params$mag_base_col_name_str, ']')`
date: "`r paste(params$initialDate,'thru', Sys.time())`"
header-includes: 
- \usepackage{longtable}
output:
  bookdown::pdf_document2: 
    keep_tex: true
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: '6'
    fig_caption: yes
    number_sections: yes
    extra_dependencies: ["float"]
  bookdown::word_document2: 
    reference_docx: ../rmarkdownWordTemplate.docx
    keep_tex: true
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: '6'
    fig_caption: yes
    number_sections: yes
#    extra_dependencies: ["booktabs"]
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '6'
    toc_float: yes
  html_notebook: 
    toc: yes
    toc_depth: 6
bibliography: ../btl_zotero_lib.bib
csl: ../Zotero/styles/ecological-monographs.csl
---

----------

\newpage

```{r version_history, eval=FALSE, echo=FALSE, cache=TRUE}

#  Version history

##  p8 v18 June 17, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v17_all_combined__body.Rmd.  

Changed all method name references in previous version.  Now need to make some changes to wording, etc that result from improvements made to wording in p9 v01 for the corresponding text in p8 v18.  

##  p8 v17 June 12, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v16_all_combined__body.Rmd.  

Need to change all references to Gurobi, Marxan_SA, and Marxan_SA_SS to ILP, SA, and SA_SS.  I've done a couple of these in the version 16, but I want to separate this action out as much as possible in case it causes any problems.  So, I'm going to do only this in version 17.  This could also be done using a fork for this name changing and another for many other changes, but merging the name change fork with all kinds of other changes made on another fork will be just as messy, if not more so.  I'm putting it in a separate version number as a compromise to make it easier to revert back to the end of v16 if there are unanticipated issues down the road after changing the names.

##  p8 v16 April 16, 2025  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v15_all_combined__body.Rmd.  

Made a fairly complete, somewhat reduced version of the fully body of the paper in v15, however, there are a fair number of small details that need to be repaired.  Creating v16 to make those repairs, such as entering the correct definitions of variables in the tables of variables.  

I'm also going to remove all uses of and references to the Latapy variables in both the text and the data loading, since they were of almost no use and greatly complicate the writing up.

##  p8 v15 August 4, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v14_all_combined__body.Rmd.  

Edited v14 with respect to Ascelin comments up through most of Discussion but stopped at beginning of Predictions section of Discussion.  I've decided to try to shrink the paper and as much as possible and get rid of as much controversial stuff as possible.  If I change my mind later about doing that, I should go back to the latest version of v14 and continue with the editing I had been doing there.  At the moment, continuing with those edits seems like a waste of time if I'm going to slash things, so I'm creating a new version 15 for slashing.  

##  p8 v14 June 6, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v13_all_combined__body.Rmd.  

Froze v13 at the point where I gave it to Ascelin for review as p8_v13_all_combined__body__2024-05-01-for-ascelin.docx.  Starting v14 to make changes resulting from Ascelin review.

##  p8 v13 April 22, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v12_all_combined__body.Rmd.  

In v12, I did a lot of editing to Discussion and Methods.  Discussion is down now to needing some moderate changes but is largely there in a long form.  I'm creating this version to freeze the things that need changing before I do those changes, in case I screw up the change and want to have an easy fallback point.

##  p8 v12 February 9, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v11_all_combined__body.Rmd.  

In v11, I stripped out nearly everything in the Results section to get down to something useful for a simplified final paper form, i.e., just a simple set of 4 input feature sets and no learning of dominance, etc.  I also finished writing a rough draft for the prediction section of the Results, so that there is now a full Results section, though it will no doubt need lots of editing later.  I'm creating a v12 so that there is a frozen, fallback v11 version of the finished Results and the many extraneous bits of Discussion that I'm going to strip down now in v12.

##  p8 v11 February 1, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v10_all_combined__body.Rmd.  

In v10, I reinstated learning separately for sets based on predicted FP/FN-dominance.  In the end, it showed very little difference from all other forms of learning (e.g., using rf or glm or lm with non-pred dominance).  So, in v11 I'm going to strip out nearly everything to get down to something useful for a simplified final paper form, i.e., just a simple set of roughly 4 input feature sets and no learning of dominance, etc.

##  p8 v10 January 29, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v09_all_combined__body.Rmd.  

In v9, I deleted many input feature sets that were fed to the prediction methods.  I also deleted the section about using logistic regression to learn to predict whether a problem was FP or FN-dominated.  I've decided to reinstate that section and do all the downstream learning of error prediction using data that is split based on the learned prediction of whether a problem is FP or FN-dominated.  In case this goes awry somehow, I'm starting a new v10 so that I can just fall back to the last v9 version if necessary.

##  p8 v09 January 27, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v08_all_combined__body.Rmd.  

In v8, I split the data preparation out from the main body of the paper and added code to the main body to read the files containing the prepared data.  That was a fairly major change, so I've decided to make a new version number.  

##  p8 v08 January 27, 2024  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v07_all_combined__body.Rmd.  

In v7, I cleaned up much of the code for learning to predict and the selection of bipartite measures for prediction.  The last version of the p8 v7 Rmd files and resulting pdf represent a verbose version of what will be in v8, i.e., I've left many notes in the file and I've run the lm code with reporting of fitting coefficients turned on (params$show_lm_model_fit_coefficients_and_summary set to TRUE).  In v8, I will remove or silence many of the prediction input feature sets that were tested in v7 to make the text less complicated and because there was very little performance difference among many of the feature sets.  The pdf for the end of p8 v7 should serve as a good reference though, if you need to look at more details of fitting and correlations, etc.  I'm hoping that v8 will be a fairly strict subset of v7 in terms of computation.  The text of Methods, Results, Discussion, and Conclusions will not be such a strict subset though.

Decided to split the data loading and preparation out into a separate Rmd file so that I can leave all kinds of headings and notes in the text without making a mess out of the body of the paper.  The intention is for the new file to be run first and generate one or more files that the main body can read in as the fully prepared data.  I will then remove all the prep code from the main p8 v8 body Rmd file and replace it with one or more file read statements.  The new prep file is called p8_v08_prep_data_for_p8_to_load_from_files.Rmd.

##  p8 v07 December 13, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v06_all_combined__body.Rmd.  

In v6, I condensed and rewrote many sections of the Discussion, but I'm still missing much of anything to say about the predictions and the bipartite graph measures.  So, in v7, I'm going to work on cleaning up the code for learning to predict and the selection of bipartite measures for prediction.  To make things more structured and defensible, I'm going to use the exploration and regression procedures from a couple of Zuur and Ieno papers.  

##  p8 v06 November 26, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v05_all_combined__body.Rmd.  

After adding Study Characteristics section in v5 Discussion to organize the many different topics, I'm now going to move many Discussion sections under those new organizing headings in the Discussion.  I'm making a new version number because this is likely to totally mess with the flow of various existing Discussion sections and I want to be able to fall back to v5 and recover that flow if this restructuring of the Discussion just makes a big mess of it.  

##  p8 v05 October 13, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v04_all_combined__body.Rmd.  

Preparing to add substantial changes to Intro and Discussion.  

##  p8 v04 January 7, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v03_all_combined__body.Rmd.  

v3 deleted all mag prediction plotting and hid all but 4 of the feature sets for predicting output errors.  It also added 2 facetted bar plots to summarize all the rmse and adj R2 results from the 4 feature sets.  It also pulled most of the TODO and notes out into a separate file so that body Rmd file for p8 is now ready to edit as a paper.  However, I think that I want to delete all of the feature set prediction chunks that are currently hidden so that the pdf will build faster.  I'm going to leave those hidden chunks as the last version of v3 so that if I need any of them, I can easily go back and get them after I delete them in v4.  

##  p8 v02 January 6, 2023  
##  p8 v03 January 6, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v02_all_combined__body.Rmd.  

Finished getting v02 working with all of the p1-p3/6 data loading and prediction code mixed with pasting in nearly all of the body text from p6.  In v03, I'm going to strip out many of the prediction plots and that sort of thing so that it gets down to a manageable size.  However, I wanted to freeze the v02 version at the point of working with every single plot plus all the p6 text.  That way, after I cut a bunch of things out in v03, I can still go back to v02 to grab anything that I shouldn't have cut out in v03.

##  p8 v02 January 6, 2023  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/p8_v01_all_combined.Rmd.  Also renaming to include "__body" in the file name since that had somehow been dropped in v01.  

Everything was working in v01 with unified data loading from p1-p3, 13 different input feature sets predicting rep shortfall and solution cost errors and error magnifications, and bar plotting the adjusted R2 and rmse for all 13 datasets and each predicted error variable.  So, I'm freezing that version and moving on to a new version to incorporate all the text from paper 6 (though probably using the model generator text from paper 1 instead of the reduced paper 6 version of that text).  I'll probably also get rid of all the predicting of magnifications because they're pretty similar to the raw error predictions (though a little worse) and don't really advance the story we're trying to tell.  They're more useful in the paper 2/6 story that's characterizing the errors made in optimization.  

##  p8 v01 December 20, 2022  

Cloned from bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/v01_Paper_8_all_combined__body.Rmd

Even after stripping out most of the p6 stuff to make the first cut at v01_Paper_8_all_combined__body.Rmd, there was still too much noise in the file for the early going where I'm trying to get code working instead of writing text.  So, I'm cutting out all the text, appendices, etc, so that I can visually focus on getting the basic code right before I mess with the text again.  This will mostly be about getting the code from paper 3 set up correctly and stripped down.

Last but not least, in all the bdpgtext directories, the file names have started with the version number and used the full word "Paper" in them.  The full word is unnecessary and having the version before the paper number in the file name is often confusing.  So, I'm switching the name to lead with "p8_v01" instead.  I've taken the file that this file is based on (the one beginning "v01_Paper_8" and put it in bdpgtext/Paper_8_all_combined_for_Ecological_Monographs/older_Rmd_versions.

##  v01 (of paper 8) December 17, 2022  

Cloned from bdpgtext/Paper_6_p1p2_combined/v13_Paper_6_p1p2_combined__body.Rmd

Combining papers 1, 2, and 3 by cannibalizing:
- paper 6, which combined papers 1 and 2, and 
-  paper 5, which was the first try at the long form of combining all 3 papers but has been had the first 2 parts of it superseded by paper 6. 

I've copied in the entire p6 v13 Rmd file, but I'm going to strip out nearly everything to make sure that I'm starting with a clean slate and getting rid of any cruft that has built up in p6. 

```

----------

```{r include=FALSE}
#  Header code chunks (option setting, history, etc.)

##  Set latex and knitr options
```

```{cat, engine.opts = list(file = "header.tex")}
%  This chunk and the pdf_document header lines: 
%
%    keep_tex: true
%    includes:
%      in_header: header.tex
%
%  are hacks to keep knitting to a pdf from blowing up with the 
%  following message due to a known bug in pandoc:
%
%  This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded %  format=pdflatex)
%   restricted \write18 enabled.
%  entering extended mode
%  ! Missing number, treated as zero.
%  <to be read again> 
%                     \protect 
%  l.514 ...nter}\rule{0.5\linewidth}{\linethickness}
%                                                    \end{center} 
%
%  I found this solution at:
%      Horizontal rule in R Markdown / Bookdown causing errors
%          https://stackoverflow.com/questions/58587918/horizontal-rule-in-r-markdown-bookdown-causing-errors 
%
%  On 2020 01 05, I tried replacing the current pandoc that knitr uses (version 2.3.1) 
%  with the latest version of pandoc (version 2.9.1).  While that does 
%  fix this problem, it blows up in a completely different way that I have 
%  not been able to find a way to fix, so I'm going with this hack.  
%  I will have to use this hack in EVERY file I want to knit that uses  
%  markdown's "---" to specify a horizontal line.
%
%  Note that the comment lines in this chunk must be marked with "%" instead 
%  of "#" because they're going to be included in the latex header.tex file 
%  and "%" is the latex comment marker.
%  Some documentation for the "cat engine" that drives this chunk 
%  can be found at:
%      https://bookdown.org/yihui/rmarkdown-cookbook/eng-cat.html

\renewcommand{\linethickness}{0.05em}
```

```{r global_options, include=FALSE}

    #  Global options are set here based on some examples in:
    #      https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
    #      https://kbroman.org/knitr_knutshell/pages/figs_tables.html

knitr::opts_chunk$set(#fig.width=12, 
                      #fig.height=8, 
                      fig.path='Figs/',  #  Save figures to indiv files in Figs/
                      echo=FALSE,        #  Don't echo code to output
                      
                      warning=FALSE,  
                      message=FALSE, 

                      fig.pos = "H", out.extra = ""    #  Keep latex from floating figures
                     )
```

```{r loadLibraries, echo=FALSE}

##  Load R libraries  

     #===========================================================================
    #  2022 12 20 - BTL
    #  THIS LONG COMMENT ABOUT THE message=FALSE CHUNK OPTION COMES FROM THE 
    #  LIBRARY LOADING SECTION OF P3 V6, HOWEVER THAT CHUNK IN THAT FILE 
    #  NO LONGER INCLUDES THAT CHUNK OPTION. IT JUST SAYS include=FALSE NOW.  
    #  SO, MAYBE THE message OPTION DOESN'T HAVE TO BE EXPLICITLY SET NOW 
    #  BECAUSE IT'S SET IN THE GLOBAL CHUNK OPTIONS IN THE BROMAN CHUNK ABOVE.
    #  -------------------------------------------------------------------------
    #  Note that "message=FALSE" is necessary for this chunk if you want to 
    #  generate pdfs.  When the tidyverse package is loaded, it writes puts 
    #  out a message that includes some unicode that the normal latex engine 
    #  (used to produce the pdf) can't handle and it crashes with the following 
    #  message:
    #       ! Package inputenc Error: Unicode character [sqrt symbol goes here] (U+221A)
    #       (inputenc)                not set up for use with LaTeX.
    #  Note that I've also had to remove the sqrt symbol from the error message 
    #  when embedding it in the comment here, because even inside the comment, 
    #  latex tried to render that and crashed.
    #  More information about this can be found at:
    #   - https://community.rstudio.com/t/tidyverse-1-2-1-knitting-to-pdf-issue/2880/4
    #   - https://community.rstudio.com/t/cant-render-tidyverse-1-2-startup-message-in-latex/2811/5
    #   - https://chrisbeeley.net/?p=1037
    #===========================================================================

   #===========================================================================
    #  Suppress startup message for tidyverse package because when it's loaded, 
    #  it puts out a message that includes some unicode that the normal latex  
    #  engine (used to produce the pdf) can't handle and it crashes with the  
    #  following message:
    #       ! Package inputenc Error: Unicode character [sqrt symbol goes here] (U+221A)
    #       (inputenc)                not set up for use with LaTeX.
    #  Note that I've also had to remove the sqrt symbol from the error message 
    #  when embedding it in the comment here, because even inside the comment, 
    #  latex tried to render that and crashed.
    #  More information about this can be found at:
    #   - https://community.rstudio.com/t/tidyverse-1-2-1-knitting-to-pdf-issue/2880/4
    #   - https://community.rstudio.com/t/cant-render-tidyverse-1-2-startup-message-in-latex/2811/5
    #   - https://chrisbeeley.net/?p=1037
    #===========================================================================

suppressPackageStartupMessages (library (tidyverse))
library("tidylog", warn.conflicts = FALSE)    #  Load AFTER tidyverse packages

library (here)
library (glue)

library (knitr)    #  For include_graphics(), kable()

library (ggplot2)
library (scales)    #  For percent in "scale_y_continuous (labels = percent, ..."
library(ggthemes)

library (patchwork)    #  To make combinations of ggplots.
library (cowplot)    #  For background_grid() function (at least).

#library(viridis)    #  For color scale.  Not sure if necessary in the end.

#library(GGally)  #  For ggpairs() function.  Probably won't need this in the end.  

#library (readr)

###  Libraries from p3 v6

#library (tidymodels)

library (corrplot)  #  For correlation plots
library (corrr)    #  For correlate() function

library (ranger)    #  For random forests

library (caret)  #  For data preprocessing functions, e.g., scaling and BoxCox

#library (DMwR)    #  For regr.eval()  #  2024 03 06 - No longer necessary.
                   #  Have copied and slightly modified regr.eval() source 
                   #  code into the plotting and evaluation code R file for 
                   #  this project since it was the only thing used from DMwR 
                   #  and the DMwR library has been updated to DMwR2, so it 
                   #  might be a problem for other people to get at some point.
##library (DMwR2)  #  DON'T LOAD THIS - SOME EXAMPLES FROM BOOK FAIL SINCE THEIR 
                  #  TYPES HAVE CHANGED.  FOR EXAMPLE, algae IS A TIBBLE IN THE 
                  #  REVISED LIBRARY AND A DATA FRAME IN THE ORIGINAL.
                  #  THIS CAUSED SOME OF MY CODE TO CHOKE.

#library (stringr)    #  For str_replace()

#library (e1071)
library (factoextra)    #  Related to principal components

library (glmnet)   #  implementing regularized regression approaches

#library (lindia)   #  diagnostic ggplots for linear regression

#library (cowplot)    #  To plot multiple plots together


#library (usethis)    #  For echo functions like ui_done().

library (bdpg)    #  For safe_sample().

library (broom)    #  For tidy() to apply to glm prediction results.

```

```{r setFilePaths, include=params$chunk_include}

##  Set file paths
 
proj_dir = here()
cat ("\n\nproj_dir = here() = ", proj_dir, "\n", sep='')
```

```{r setSeeds, include=params$chunk_include}

###  Set random number seeds for reproducibility

seed1 = 12345
seed2 = 8910
seed3 = 1230

# seed1 = 54321
# seed2 = 198
# seed3 = 321

# seed1 = 101
# seed2 = 17
# seed3 = 83

seed = set.seed (seed1)
```

```{r setBdpgOptionsThatAreHardToSetInParams, include=FALSE}

##  Set bdpg options  

    #  Two options are either used globally or are hard to set in 
    #  the header params list, so set them here.

    #  This is both hard to set in params header and used globally.
if (as.logical (params$exclude_ZL))
    {
    rs_method_names_list = c("ILP", "SA", "UR_Forward", 
                             "SA_SS")
    } else
    {
    rs_method_names_list = c("ILP", "SA", "UR_Forward", 
                             "ZL_Backward", "SA_SS")
    }

```

```{r checkParamsAndSetDefaults, include=FALSE}

    #  Check the params list for errors and missing default values.
    #  Either set values appropriately or crash if that's more appropriate.

cat ("VALIDATION CODE FOR PARAMS IS STILL MISSING HERE !!")

```

#  Option settings  

```{r echoOptions, include=TRUE}

#  Params should all be properly set at this point.  

#-------------------------------------------------------------------------------

    #  Echo the values of the main options.

cat ("\nMain options are:\n")

for (idx in 1:length(params))   
  cat ("\n", names(params)[idx], ": ", params[[idx]], sep="")

cat ("\nrs_method_names_list = ", rs_method_names_list, "\n")
```

```{r loadP1andP2FunctionDefns, include=FALSE}

#  Load R functions

#-------------------------------------------------------------------------------
#  2020 08 20 - BTL
#  "Rmarkdown Cookbook" section	"16.1 Source external R scripts" says to 
#  include the "local" argument.
#       https://bookdown.org/yihui/rmarkdown-cookbook/source-script.html
#       "We recommend that you use the argument local in source() or envir in 
#        sys.source() explicitly to make sure the code is evaluated in the 
#        correct environment, i.e., knitr::knit_global(). The default values 
#        for them may not be the appropriate environment: you may end up 
#        creating variables in the wrong environment, and being surprised 
#        that certain objects are not found in later code chunks."
#  I haven't noticed a problem with this, but maybe it's been there and 
#  I just haven't had it affect something important enough to notice.
#-------------------------------------------------------------------------------

# source (file.path (proj_dir,         #  For ggplot w/ magrays, etc
#                    "R/v3_Paper_2_bdpg_analysis_scripts_function_defns.paper_2.R"), 
#         local = knitr::knit_global())  
# 
# source (file.path (proj_dir, "R/v1_p5_unifiedDataLoading.R"), 
#         local = knitr::knit_global())

## 2022 12 17  ##source (file.path (proj_dir, 
## 2022 12 17  ##"R/v1_p6_load_libraries_and_R_source_code.R"), 
## 2022 12 17  ##        local = knitr::knit_global())



source (file.path (proj_dir, "/R_new/p8.unifiedDataLoading.v01.R"), 
         local = knitr::knit_global())

#--------------------

source (file.path (proj_dir, "/R_new/p8.preprocessingForLearning.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/p8.BoxCoxAndNormalizingFunctions.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_plotting_and_evaluation_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_utility_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v1_paper_3_fitting_functions.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/p8.matildaFunctions.v01.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R_new/v2_paper_2_func_defns_for_plotting.R"), 
         local = knitr::knit_global())

source (file.path (proj_dir, "/R/v2_paper_3_cv_test_train_splitting_functions.R"), 
         local = knitr::knit_global())
```

```{r loadPreppedDataFromFiles, include=FALSE}

#  Load prepped data from files
#  These are all data structures that are referenced in the paper but 
#  have been prepared elsewhere to simplify this file, e.g., in
#   p8_v??_prep_data_for_p8_to_load_from_files.Rmd.

#--------------------  

filtered_full_initial_exp_tib = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "filtered_full_initial_exp_tib.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p2_app_wrap_tib = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p2_app_wrap_tib.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p3_working_train_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_train_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

p3_working_test_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_test_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

p3_train_aux_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_train_aux_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

p3_test_aux_df = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_test_aux_df.gurobi__all.exclude_imperfect_wraps__FALSE.csv")

#--------------------  

```

```{r load_p3_working_train_data, include=FALSE}

p3_working_train_df__before_any_preprocessing = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_train_df__before_any_preprocessing.gurobi__all.exclude_imperfect_wraps__FALSE.csv")
```

```{r load_p3_working_test_data, include=FALSE}

p3_working_test_df__before_any_preprocessing = 
    load_file_into_tibble (
                base_path = file.path (proj_dir, 
                                       params$relative_path_to_data_out_loc), 
                "p3_working_test_df__before_any_preprocessing.gurobi__all.exclude_imperfect_wraps__FALSE.csv")
```
















```{r convertRSmethodNamesInLoadedData, include=FALSE}

#  Replace all occurrences of "Gurobi" in loaded data with "ILP" to satisfy 
#  reviewer.

#--------------------  

filtered_full_initial_exp_tib <- filtered_full_initial_exp_tib %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p2_app_wrap_tib <- p2_app_wrap_tib %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p3_working_train_df <- p3_working_train_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))



p3_working_test_df <- p3_working_test_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

p3_train_aux_df <- p3_train_aux_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))



p3_test_aux_df <- p3_test_aux_df %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

#--------------------  

```

```{r convertRSmethodNamesInp3_working_train_data, include=FALSE}

p3_working_train_df__before_any_preprocessing <- p3_working_train_df__before_any_preprocessing %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

```

```{r convertRSmethodNamesInp3_working_test_data, include=FALSE}

p3_working_test_df__before_any_preprocessing <- p3_working_test_df__before_any_preprocessing %>%
  mutate(rs_method_name = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Gurobi", "ILP", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA", "SA", rs_method_name)) %>%
  
  mutate(rs_method_name = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name)) %>%
  mutate(rs_method_name_fac = ifelse(rs_method_name == "Marxan_SA_SS", "SA_SS", rs_method_name))

```


















----------

\newpage

#  New, reduced and most current ToDo list  

2025 04 16 - There are various files around holding different versions of things that need or might need doing.  However, some of that stuff is out of date after reducing the scope of the Discussion etc in v15, so I'm putting a new version of the ToDo list here for things that I want to deal with immediately.  I'll go back to the other ToDo lists and see if there's anything useful or necessary there after I take care of these more immediate things.  

##  Still to do  

- Big jobs  
  - Data
    - Write code to upload/download/traverse/act on the data sets.
    - Trim and upload the data sets to Open Science Foundation site.  
  - Replace all references to Gurobi in text and figures with something about either ILP or NV (non-voting) group.  
    - This is a big job, especially in dealing with the figures because many labels and titles are taken directly from variable names rather than text arrays.  

- Medium jobs  
  - Remove all Latapy references in the text and in building the input data sets.  
  - Correct predictor variable definitions in APP SI that have XXX by them since they are at least partially incorrect. 

- Small jobs  

- Things that can only be done in the final version
  - Check all paired references throughout the text to make sure they match (e.g., Figure numbers and Appendix numbers).  

##  Done  

- DONE: get rid of little joke about tzar error help for bugs: "(not that there were ever *any* bugs)"  
  - Deleted the phrase.

- DONE: fix equation for rep shortfall error in Methods section  
  - Is the text wrong or is the code wrong and the text is just reflecting it?  I'm pretty sure the text is just wrong.  If it's not, then everything is in trouble everywhere.
    - Looked at the bdpg code and the code is correct.  The text in bdpgtext's statement of the equation was wrong.  Fixed it but also changed the variable for the number of species in that equation from being variants of "m" to being variants of "K", since that is the variable name used earlier in the Methods when stating the problem.  

- DONE:  Various small issues in the Methods section on Computing Environment.  

- DONE:  Should say somewhere that no doubt all of our reserve selector code can be improved by experts, but it doesn't really matter.  We're just showing how you can compare methods using the benchmark using fairly straightforward implementations.  If you don't like our implementation, you can run your own since the data is freely available.  

- DONE:  Fix tables of variable definitions so the columns are not dummy duplicates (e.g., the bipartite graph variables.  

- DONE:  The citations for R Core Team [(@rcoreteam)] are not rendering correctly, e.g., in the Methods section about simulated annealing.  
    - It just says "Team (n.d.)".  ["(n.d.)" means no date in zotero.]
    - One citation was just [Rlang] somewhere and I've changed it to the rcoreteam citation, but I don't know why Rlang was there.  Was there a different thing to be cited (e.g., the rlang package)?  Or, is it the proper citation link instead of rcoreteam?
    - I think that this may have to be done by hand in the final pdfs or doc files.  I haven't been able to find a way to enter the citation correctly in zotero because it expects a first and last name with a comma. 
    - I have decided to just skip the zotero citation for R and instead, just put "R Core Team, 2022" in wherever I want to cite it.  Even that is not what you're supposed to do because it lacks a version number, but I've used lots of versions of R and lots of versions of Mac OS X too.  

----------

\newpage

#  Abstract  

1. Mathematical optimization methods are commonly used for conservation reserve selection.  However, the true accuracy of these methods under input uncertainty is unknown for several reasons, including: i) methods are generally tested on small numbers of problems and uncertainties, ii) there is little variation in test problem structure and difficulty, and iii) the correct solutions for the uncertain test problems are often unknown, implying the true amount of reserve selection error is also unknown.  Consequently, it is difficult to predict a reserve selector's error on any specific problem.  This is important because  unexpectedly large error can drastically increase the already large risks to species covered by the plans.  Moreover, overly optimistic beliefs about reserve selector accuracy hinders development of better methods.   

2. We use graph theory and solution planting to build a three step sequence to learn to predict optimizer error on individual problems under input uncertainty.   First, we introduce and extend solution planting methods from computational complexity theory for generating reserve selection problems with known solutions and theoretically-guided variation in difficulty.  Second, we apply these generators to produce 18,000 problems of varying difficulty, known uncertainty, and known correct solutions, then compare the errors of four reserve selectors on all problems.  Third, we convert each problem into a bipartite graph and compute graph structure measures to use as inputs to statistical models that predict the amount of each reserve selector's error on each individual problem under uncertainty.  

3. The new generators provide publicly available, easily generated, statistically useful numbers of experiments with realistic size, known solutions, varying difficulty, and structural variety.  The method evaluations demonstrated large variation and large reserve selector output error even with small input errors (median output error ~5 times input error).  Learning statistical models from graph measures over this data led to a roughly 3-fold improvement over using problem size alone in predicting the amount and nature of optimizer error under uncertainty on individual reserve selection problems.  

4. This work provides a framework for improving reserve selector development and for identifying problems where optimizer risk may be unacceptably large, implying a need for other actions or method improvements.  

----------

#  Keywords    

systematic conservation planning, uncertainty, reserve selection method comparison, 
computational solution planting, synthetic species data, problem difficulty, graph theory


----------

\newpage

#  Introduction  

The natural world has been significantly altered by multiple anthropogenic drivers, resulting in growing loss of biodiversity (@diaz2019). Mathematical approaches to optimally allocating conservation actions represent a promising avenue for addressing this problem.  These methods are often referred to as systematic conservation planning (SCP) (@margules2000n). *Reserve selection* is one important subfamily of these SCP approaches where the action to be optimized is the selection of locations (referred to as "planning units") to implement new protected areas or "reserves".  An example of reserve selection is to combine maps of species distributions, land parcels (i.e., planning units), and costs as inputs to determine the most cost-effective set of parcels to reserve to protect a specified proportion of habitat for a range of species.  

These methods have shown great promise and have been widely adopted. However, the development and evaluation of reserve selection methods face several fundamental challenges that the research community continues to address. Even the simplest mathematical formulations of reserve selection fall into the class of provably difficult computational problems known as NP-hard problems (@garey1979).  While many problems in this class allow finding an optimal solution within a reasonable time, other problems are intractable using current methods.  Crucially, this variation in individual problem difficulty relates to the structure of the problem, not just its size and can be difficult to predict.  As Sarkar et al. noted twenty years ago regarding "place prioritization algorithms" (PPAs) (Sarkar et al. 2004):

```  
    We could not identify any set of features that predicted the 
    accuracy of an algorithm. Even size was not an unequivocal 
    predictor. However, the various PPAs, both optimal and heuristic, 
    tracked the problems in qualitatively similar ways.  All found 
    the same ones difficult or easy which suggests that there are 
    definite characteristics of the data sets that explain the 
    performance of various PPAs. This is a problem that will merit 
    further study.  
```  

In the intervening twenty years, improved SCP methods have made it possible to exactly solve most reserve selection problems when the input data is correct.  However, similar problems remain in developing tools to understand and predict method performance on specific individual problems under the uncertainty that is pervasive in real-world SCP inputs.  Many input values can be difficult to determine accurately and may also change over time, e.g., presence of rare species, land costs, land availability, as well as  large-scale effects such as climate, economics, and land-use change.  

The research community has made significant progress in developing methods that account for varying degrees of uncertainties, providing optimal or near-optimal solutions for specific problem reformulations (e.g., @beechStochasticApproachMarine2008, @andoOptimalPortfolioDesign2012,  @tullochIncorporatingUncertaintyAssociated2013, @billionnet2015ema, @haider2018em, @runtingReducingRiskReserve2018, @eatonSpatialConservationPlanning2019,  @sierra-altamiranda2020em, @ghasemisaghand2021e).  However, each approach necessarily makes assumptions about the inclusion and quantification of uncertainties (e.g., declaring upper bounds on the magnitude of input errors or on the number of input variables containing error).  Given the complexity of addressing many common uncertainties simultaneously, many SCP problem formulations have necessarily focused on subsets of uncertainties to maintain mathematical tractability (@moilanen2008bc).  For example, methods may assume that costs are known with certainty (@haider2018em) or that there is no overprediction of species presence in the input (@velazco2020bc).  Another  difficult issue is ensuring the correctness of uncertainty quantifications themselves, e.g., producing well-calibrated probabilities of species presence/absence - which is a difficult problem given the scarcity of data (@valavi2022em , @muscatello2021cb).  Together, these assumptions have the effect of shifting uncertainties from input variables to the variables characterizing their uncertainties.

Given these inherent challenges, understanding how reserve selectors are likely to perform on real-world problems with uncertainty remains an important open question.  In particular, the magnitudes of errors in the solution for any specific problem are difficult to predict. Currently, we have two primary approaches for demonstrating method reliability: theoretical guarantees and empirical evaluations.  

In the first case, methods such as robust integer linear programming (@billionnetSolvingProbabilisticReserve2011,  @billionnetMathematicalOptimizationIdeas2013,  @haider2018em) have theoretical guarantees to produce an optimal solution assuming that i) all of the problem uncertainties and problem characteristics can be and have been correctly captured in the problem definition and method assumptions, as well as ii) a feasible solution exists, and iii) sufficient computational resources are available.  These represent important theoretical advances, but meeting all of these theoretical criteria is demanding in practice and we currently lack the tools to know the consequences of failing to meet them for individual real-world problems.    

In the second case, where methods lack theoretical guarantees (e.g., heuristic methods or methods where assumptions may not hold perfectly), we must rely on empirical evaluations.  However, the field faces several well-recognized obstacles to providing strong evidence for generalizing empirical results to specific, previously unseen problems:

1)  **Sample size**: Comprehensive evaluation requires substantial time and computational resources, so evaluations usually rely on single real-world case studies or small ad-hoc synthetic data sets  that provide limited coverage of the full space of problems.  

2)  **Problem difficulty**:  We have few theoretical or empirical studies characterizing problem attributes that control problem difficulty under uncertainty other than problem size.  Consequently, most evaluations involving more than a single case study vary problems in the test set primarily based on problem size rather than structural diversity, though some notable exceptions explore specific aspects of problem structure (e.g., @presseyEffectsDataCharacteristics1999, @viscontiBuildingRobustConservation2015).

3)  **Data sharing**: Sharing of test problems between researchers is limited, which hampers comparative evaluation and the development of shared benchmarks for understanding problem attributes.  

4)  **Uncertainty**:  Evaluating methods under uncertainty requires careful experimental design to capture the range of uncertainty types and magnitudes found in practice.  Current evaluation problems often include no uncertainty or only include a single type and amount of uncertainty, e.g., studies where all uncertainties are 10% false negatives.

5)  **Known solution**: The true optimum is generally unknown in real-world problems with uncertain inputs, making it difficult to quantify solution error. 

These obstacles create a shared problem for the entire research community.  They can  make it difficult to give us clear direction about where to focus improvement efforts for methods whose true strengths and weaknesses are unknown.  Moreover, they make it difficult to provide practitioners with reliable guidance about how methods will perform on their specific problems and can increase the risks to species covered by plans with unknown amounts of error.  

Lacking reliable predictions of solution error, practitioners often can and do take  actions to address suspected uncertainties in their input data and problem formulation.  For example, they might improve the sampling and modelling of species or they might increase possibilities for redundancy in solutions by increasing representation targets.  However, determining the consequences and appropriate magnitudes of such actions remains difficult without better predictive evidence for solution error types and amounts resulting from the intrinsic difficulty of specific problem structures such as species co-occurrence and abundance patterns.  

To address these shared issues, we propose developing systematic approaches for making explicit, generalizable, and testable predictions of reserve selector performance based on individual problem characteristics.  This would enable practitioners to estimate how difficult their problem is likely to be for a specific reserve selector and to determine whether it would be useful and cost-effective to gather more data to reduce uncertainty, i.e., the value of information (@canessa2015mee, @raymond2020joaea).  It would also give us more information on which problem structures methods perform best and worst, facilitating targeted improvements to methods.  While such prediction is challenging, explicit attempts would help identify knowledge gaps in our understanding of both methods and problems.

Making predictions requires us to address the five difficulties listed earlier.  We need large sets of publicly available reserve selection problems with known optimal solutions as well as systematic variation in controlled uncertainties and structural difficulties.  In this paper, we contribute to this goal by adapting techniques from computational complexity theory and graph theory to present a sequence of interconnected actions addressing the five difficulties above:

1) We introduce and extend a *solution planting* method for generating test problems with known correct solutions, controllable problem difficulty as a function of problem structure, and ecologically relevant patterns of species abundance (Sections \ref{modelRBexplanation} and \ref{modelWrapExplanation}),  

2) We use this method to generate a publicly available benchmark set of 3,000 independent problems and 18,000 variants with wide variation in size and difficulty, including two types of input error across a range of error magnitudes. We demonstrate its use by evaluating four reserve selectors across this problem set (Section  \ref{comparisonResultsSection}),

3) Based on those results, we learn models to predict the amount of error different reserve selectors will make on individual problems under uncertainty.  We do this by reframing each problem as a bipartite graph and computing a set of predictive input features based on graph-theoretic measures characterizing structural attributes of that problem (Section  \ref{predictionResultsSection})

Our goal is to help method developers understand their methods' performance characteristics and to help practitioners make informed and reliable decisions about method selection. 

----------

#  Definitions  

In what follows, "AppSI", "CorSI" and "ShinySI" are supplemental information files with appendices containing more detail and further results.  

Throughout this paper, we use terminology from multiple fields that may be unfamiliar to some readers in other fields, so we first specify ecological and graph theory terms before giving detailed examples illustrating the problem generators.

**Reserve Selection Method**  The mathematical optimization techniques used in SCP and reserve selection begin from a clear statement of the decision problem, e.g., choosing which land could be obtained for the minimum cost to meet specified targets for  representing a group of species.  In some cases, the problem statement is then explicitly converted into a mathematical form where the optimum can be solved or searched for by an optimization method, where the specific form may depend on the type of optimization method  The method may be exact (e.g., integer linear programming) or approximate (e.g., simulated annealing).  In other cases, methods may not explicitly encode the problem statement into an objective function and instead, will use a simple step-wise heuristic that hopes to lead to a good, if not optimal, value with respect to the original problem statement (e.g., select parcels in order of the number of species occupying them.  Each algorithm's ability to find an optimal solution may also be affected by the values chosen for various parameters specific to the algorithm, e.g., the number of random restarts to do in a simulated annealing algorithm.  Similarly, assumptions made about uncertainties in the input data will also contribute to the accuracy of the result with respect to the true optimum (e.g., assumed bounds on the maximum amount of error in the input data).  For brevity, we will use the terms *reserve selector* or *reserve selection method*  to refer to the combination of i) the optimization algorithm, ii) the chosen parameter settings, iii) the chosen mathematical formulation of the problem, and iv) any assumptions made about the data used by the algorithm.

**Problem difficulty and/or Method performance**  The amount of error in the solution given a known correct optimum solution rather than the amount of time required to solve the problem.  

**Problem structure**  In this paper, problem structure is primarily the combination of species co-occurrence pattern and species rank abundance distribution.  In more complex problems, structure would also include other information, e.g., distributions of representation targets and PU costs.  

**Planning units (PUs) and Conservation features**  The conservation planning literature commonly refers to *planning units* (*PUs*) and *conservation features*. Without loss of generality, we use *PUs* and "*species*" because our examples use species distributions. Our results apply equally to any non-species features that can be associated with PUs.  We refer to the full set of PUs as the "*landscape*", but there is no spatial arrangement of PUs assumed. 

**Rank abundance distribution**  In ecology, species *abundance* refers to counts of organisms, but here we use the number of PUs where a species occurs as a surrogate for abundance. By *species rank abundance distributions*, we refer to the number of PUs each species occurs on, ranked in decreasing order from species occurring on the most PUs to those on the fewest (**Fig. 3 \textcolor{cyan}{\ref{fig:buildExampleWrapDistHist}}**).

**Graph theory terms**  The graph theory literature often uses *vertices* and *edges*, but here we use *nodes* and *links* since they are more common in ecology, except when we refer to the *minimum vertex cover* problem since that is its usual name in computational literature (@garey1979).  In what follows, *nodes correspond to PUs and links correspond to species*. Graph theory terms required to understand our implementation of model RB are:  

- **_clique_**: A set of nodes where every node is connected to every other node in that set.  There can be many cliques within a single graph.
- **_independent set_**: A subset of the graph's nodes where no pair of 
nodes inside that subset has a link connecting them, though each may connect to nodes outside the subset.
- **_maximum independent set_**: An independent set where no other independent set in the graph contains more nodes though others could be equal size.
- **_vertex cover_**: A subset of nodes in a graph where every link in the full graph has at least one end connected to a node in the cover.
- **_minimum vertex cover_**: A vertex cover such that no other cover contains fewer nodes though others could be equal size.  

See **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**b) for an example of both a maximum independent set and a minimum vertex cover.

#  Problem generators  

Our need to generate problems with a range of difficulty and known correct solutions is similar to the need in the computational complexity literature for building problems to evaluate approximations to solutions of NP-hard problems.  There, researchers have developed methods to generate difficult constraint satisfaction problems with known optimal solutions using a technique called *solution planting* (see @zhou2021acm for overview of planting literature).  Solution planting involves generating a structured set of elements that is designated to be the optimal answer to a problem (e.g., a set of planning units to reserve and the species occurrences on them), then adding misleading elements (such as other planning units and species occurrences) in a way that hides this correct answer but is guaranteed not to alter what is the optimal solution.  

In **Section 6.1.2 \textcolor{cyan}{\ref{modelRBexplanation} }**  below, we describe a solution planting method based on *model RB* borrowed from computational complexity theory (@xu2005ipnijcai) for quickly generating large sets of problems with known optimal solutions, whose difficulty varies widely even within problems of the same size.  An important advantage of the method is that it provides a theoretical basis for which input parameter combinations will generate problems with different levels of difficulty (**Section 6.1.2 \textcolor{cyan}{\ref{modelRBexplanation}}**).  

However, an ecological issue arising from the theoretical origins of model RB is that it uses a uniform distribution of problem elements (in our case, species) that is unlikely in real-world reserve selection. To make it useful in reserve selection evaluation, we have developed a new extension of the method to generate what we call "Wrap" problems with more ecologically relevant, user-specified distributions of species occurrence wrapped around the Base problem while preserving its planted solution (**Section 6.1.3 \textcolor{cyan}{\ref{modelWrapExplanation}}**). This enables the rigorous evaluation of the performance of target-based reserve selection algorithms across a wide range of problems with varying difficulties and levels of uncertainty where we know the correct solution.  

##  Base problem using model RB {#modelRBexplanation}

Using model RB to generate reserve selection problems relies on the fact that for some hard problems, the solution to one type of problem can be translated into the solution for a different type of problem. We can translate a vertex cover problem into a reserve selection problem by  designating (i) each node as a PU, and (ii) each link as a species that occupies the two PUs connected by the link and occupying no other PUs. With this correspondence, a minimum vertex cover then represents the smallest number of PUs that contain at least one occurrence of each species.  Next, as explained in @xuke2014, we rely on two facts: First, it is easy to generate a graph with a known maximum independent set and second, a maximum independent set is the complement of a minimum vertex cover.  For example, nodes above the dashed line in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**b) constitute a maximum independent set in that graph, while all nodes outside that set automatically make up a minimum vertex cover (nodes below dashed line in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**b).

@xuke2014 briefly describes how to use model RB to easily generate a graph and a maximum independent set for that graph (and therefore, a reserve selection problem with a known solution). Quoting from @xuke2014 :  

1. Generate n disjoint cliques, each of which has $n\alpha$ vertices (where $\alpha > 0$ is a constant);
2. Randomly select two different cliques and then generate without repetitions $pn2\alpha$ random edges between these two cliques (where $0 < p < 1$ is a constant);
3. Run Step 2 (with repetitions) for another $rn~ln(n)$ times (where $r>0$ is a constant).

###  Example of creating base problem using model RB {#modelRBexample}

To make this clearer in the context of reserve selection, we now provide a simple example with diagrams in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}** to illustrate the process of deriving a reserve selection problem with a known optimal solution.  The essence of the algorithm is the following four steps: (i) choose parameters; (ii) generate core of graph (cliques); (iii) designate independent set and reserve solution; and (iv) hide solution using multiple rounds of linking between cliques. Steps ii and iii plant a solution, i.e., they create a problem with a known solution.  Step iv hides the solution by adding extra links (species) that leave the original solution intact but add noise that the search must wade through. Pseudocode for the algorithm is given in **CorSI Appendix B \textcolor{cyan}{\ref{appendixPseudocodeModelRB}}** and we now step through a simple example illustrated in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**.

**Choose parameters.** Four parameters are required to specify each reserve selection problem.  The first two are the number of cliques and the number of nodes per clique used in planting the solution.  The second two parameters relate to hiding the solution, which is done in a series of rounds where in each round, a pair of cliques is randomly chosen and then several random links are made between those two cliques.  This means that the final two parameters we need are the number of rounds of linking and the number of links per round.  

We can designate those four variables directly, but if we do, we have no connection to the theoretical predictions made in the development of model RB.  It was derived in a theoretical setting of constraint satisfaction problems in general and requires the user to specify four different input parameters related to the computational theory behind predicting the problem's difficulty (@xu2005ipnijcai): 

- $n$: number of cliques
- $\alpha$: exponent driving number of nodes per clique
- $r$: constraint tightness (multiplier driving number of rounds of linking)
- $p$: constraint density (proportion driving number of links per round)

These variables are now used to derive variables with a direct interpretation for building reserve selection problems:  

- $n$: number of cliques = $n$
- $d$: number of nodes per clique = $n^\alpha$
- $m$: number of rounds of linking = $rnln(n)$
- $\theta$: number of links per round = $pd^2$

For our example, we arbitrarily choose parameter values that will lead to a graph that is small and simple to understand, i.e., $n=3,d=4,m=4,\theta=2$.

**Generate core of graph.**  Based on those parameter choices, we define 3 cliques containing 4 nodes in each clique, initially with no node linked to any node outside of its clique (**Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**a).  Since cliques are fully connected by definition, each 4-node clique has 6 links, giving 18 links in the graph.  This corresponds to 18 species spread across 12 PUs with each species occurring on exactly 2 PUs.  

**Designate independent set and reserve solution.**  Randomly choose one node from each clique to be part of the independent set, making a total of 3 nodes (nodes above dashed line in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**b). Because we have chosen one node from each clique and the cliques have been initialized with no connections between them, we know these 3 nodes have no connections between them and therefore, comprise an independent set. We also know that the independent set cannot be any larger than this because each of these independent nodes is connected to every other node in its clique, so none of the other nodes in those cliques are independent of the chosen nodes or independent of any other nodes in their clique.

Since the cover and the independent set are complements of each other, we now know that the minimum vertex cover consists of all nodes not in the maximum independent set (nodes below dashed line in **Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**b).  Thus, the 9 PUs in the minimum vertex cover set are known to be a correct solution: the smallest reserve selection that will provide at least one occurrence of all 18 species.  Note that no node in the cover set could be brought into the independent set because that node would violate independence by being connected to a node already in the independent set.

**Hide the solution.**  model RB says that we can make the problem more difficult by hiding this solution through adding more links in a specific way. We do 4 rounds of random linking (given by $m$).  In each round, we randomly choose 2 cliques and add 2 links (given by $\theta$), each between randomly chosen pairs of nodes having one node in each clique (**Fig. 1 \textcolor{cyan}{\ref{fig:BaseProbExampleDiagram}}**c,d). Note that there is no significance in there being 2 links.  We could have chosen any number of links between the 2 cliques, but chose 2 to keep the example simple.

There is one caveat in the linking; no link can violate the independent set condition by linking the independent node of one clique to the independent node of another clique.  We *could* connect the independent node of one clique to a non-independent node of another clique without violating the independent set, but for simplicity, we restrict all added links to go between nodes not in the independent set.  

After 4 rounds we now have 8 new links for a total of 26 links in this graph.  Since we have added no new nodes, the final problem still has 12 PUs but now has 26 species, each occurring on exactly 2 PUs.  Finally, the smallest possible set of PUs covering at least one instance of all species still must contain 9 PUs.  

```{r BaseFig, echo=FALSE, fig.align="center", out.height = "40%", fig.cap = "\\label{fig:BaseProbExampleDiagram} Diagram depicting the steps to generate a synthetic reserve selection problem using model RB. The dashed line indicates separation between maximum independent set nodes and minimum vertex cover nodes.  The three nodes above the line are maximum independent set PUs while the nine nodes below the line are minimum vertex cover PUs (an optimal reserve selection). Each link represents a species that occurs on the two connected PU nodes and nowhere else. The colors of links in c) and d) distinguish different rounds of linking."}

base_img_path = file.path (proj_dir, "Figures/Figure_RB.png")
include_graphics(base_img_path)
```

##  Wrapped problem {#modelWrapExplanation}

There are two issues with using model RB to generate useful synthetic reserve selection problems. First, model RB produces problems with ecologically unrealistic abundance distributions of exactly 2 occurrences of every species, thus we need to extend the method to generate more realistic distributions of species occurrences. Second, in preliminary testing we found that model RB generally generates its most difficult problems where the correct solution is a large proportion of the total number of PUs. Since realistic reserve selection problems generally require selecting a small proportion of the landscape, we need to be able to generate test problems with solutions requiring smaller proportions of the landscape.

We address these issues by developing a procedure for wrapping one rank abundance distribution around another in a way that guarantees that the original base problems solution is still the solution to the *wrapped* problem but allows the final problem to have an arbitrary rank abundance distribution. Moreover, we can specify the proportion of the wrapped problems landscape taken up by the correct solution. This may mean that we are unable to generate problems as difficult as the hardest benchmark problems, but our results show that we can still generate problems with a wide range of difficulty for some reserve selectors.

The key idea in wrapping is that any rank abundance distribution will have a section of the distribution specifying species that occur on exactly 2 PUs. To generate a wrapped problem, we simply build a rank abundance curve of a desired form that contains a part of the distribution corresponding to the set of species on exactly two PUs in the base problem.

Consider a new example Base problem to be wrapped (**Fig. 2 \textcolor{cyan}{\ref{fig:WrapProbExampleDiagram}}**a).  For simplicity, this example has only two base cliques, each containing three nodes and three links.  Suppose that additional rounds of linking between those cliques have added four more links, resulting in a total of ten species in this Base problem.  If we label those nodes with PU IDs A-F and number the links as species IDs 5-14 (the reason for not starting these IDs at 1 will become clear below), then we can move from a graph representation to a landscape representation of 6 PUs containing species occurrences (**Fig. 2 \textcolor{cyan}{\ref{fig:WrapProbExampleDiagram}}**b) with 10 species occurring on exactly 2 PUs each.  

```{r WrapFig, echo=FALSE, fig.align="center", out.height = "80%", fig.cap = "\\label{fig:WrapProbExampleDiagram}Diagram of the wrapping process for increasing the number of species and patches to allow arbitrary rank abundance distributions. Here, letters are PUs, numbers are species, red numbers are species added by the wrapping procedure, and black numbers are species generated by the original model RB generator. (a) Original model RB problem as nodes and links. Nodes are PUs and links are species that occupy the 2 PUs connected by the link. The dashed line separates the independent set from the solution set. (b) Same Base problem as a) but converted from a graph representation to a set representation, with each PU containing species IDs of all species occupying the PU. (c) Wrapped problem using set representation. The bottom dashed line separates PUs in the solution set from PUs that were added in wrapping."}
#{r Wrap, echo=FALSE, out.height = "80%"}
#{r Wrap, echo=FALSE, out.height = "80%", fig.cap = "\\label{fig:Wrap}Diagram for example wrapped distribution.  Letters are PUs.  Numbers are species."}

#  Had to:
#  - draw in powerpoint
#  - copy and paste each figure into Word using the text inline setting for the image once it's pasted in
#  - from word, "File/Save as Adobe PDF"
#  - open the resulting pdf in Preview and then Export it to PNG, using 600 dpi

#wrap_img_path = file.path (proj_dir, "Figures/figure wrap from powerpoint.png")
    #  2022 07 03
    #  Reviewer 3 requested that this diagram have the same styling as the Base 
    #  diagram, so I did that, but it has somehow it now has much more space 
    #  between it and the caption.
    #  Need to use a different program to rebuild it.  
wrap_img_path = file.path (proj_dir, "Figures/figure wrap from powerpoint v2 2022 07 03.png")
include_graphics(wrap_img_path)
```

Now consider the rank abundance distribution of species occurrence counts in **Fig. 3 \textcolor{cyan}{\ref{fig:buildExampleWrapDistHist}}**. Here, four new species have been added and we designate these as wrapping species, labelling them as species 1-4. Looking at the way they have been distributed on the PUs we can easily see species 1-4 occur on 5, 4, 4, and 3 PUs, respectively, and the base species (5-14) occur on exactly 2 PUs each. So, we have a rank abundance distribution that fully includes the species from the base problem and includes 4 new species. Note that the choice of the wrapping species and their occurrence counts could follow any distribution, as long as it contains the original set of species that occur on exactly 2 PUs. Note that the wrapping species could also contain more species that occur on exactly 2 PUs.

```{r buildExampleWrapDistHistogram, echo=FALSE, fig.align="center", out.width = "50%", fig.cap = "\\label{fig:buildExampleWrapDistHist}Species rank abundance distribution for the example Wrapped distribution in Fig. 2 \\textcolor{cyan}{\\ref{fig:WrapFig}}c with species sorted in decreasing order by number of PUs occupied by each species.  Grey bars indicate number of PUs occupied for species generated in the model RB Base problem.  Red bars indicate number of PUs occupied for species generated during wrapping."}

SPP_TYPE = as.factor (
            c ("wrap_spp","wrap_spp","wrap_spp","wrap_spp",
            "base_spp","base_spp","base_spp","base_spp","base_spp","base_spp",
            "base_spp","base_spp","base_spp","base_spp")
            )
bar_colors = c(wrap_spp="red", base_spp="grey")

spp_ID = 1:14
abundance = rep (2, 14)
abundance [1:4] = 0

df <- data.frame (spp_ID = spp_ID,
                  abundance = abundance)
#head(df)

# p<-ggplot(df, aes(x=spp_ID, y=abundance)) +
#   geom_bar(stat="identity", 
#             aes (fill = SPP_TYPE)
#             ) +
#   ylim(0, 5) + 
#   scale_fill_manual(values=bar_colors) + 
#   labs (x = "Species ID", y="Num PUs Occupied") +
#   theme(legend.position="right") + 
#   ggtitle("BASE Spp Rank Abundance Distribution    ") + 
#     theme_classic() + 
#     theme(plot.title = element_text(margin = margin(t = 10, b = -35)))     + 
#   theme(plot.title=element_text( hjust=1, vjust=0.5, face='bold'))
# p


abundance [1:4] = c(5,4,4,3)

df <- data.frame (spp_ID = spp_ID,
                  abundance = abundance)
#head(df)

p <- ggplot (data=df, aes(x=spp_ID, y=abundance)) +
  geom_bar(stat="identity", 
            aes (fill = SPP_TYPE)) + 
  
#  ylim(0, 5) + 
  coord_cartesian (ylim = c(0, 5)) + 
                  
  scale_fill_manual(values=bar_colors) + 
  labs (x = "Species ID", y="Num PUs Occupied") +
  theme(legend.position="right") + 
  ggtitle("WRAPPED Spp Rank Abundance Distribution    ") + 
    theme_classic() + 
    theme(plot.title = element_text(margin = margin(t = 10, b = -35))) + 
  theme(plot.title=element_text( hjust=1, vjust=0.5, face='bold')) 

p

#ggsave(plot=plot_1, filename="plot_1.png", height=5, width=5)
```
 
Next, we examine how to create a problem where the correct solution occupies a specified proportion of the wrapped problems landscape, e.g., 40% to keep this example small. In this case, we need to have 10 PUs in the wrapped problem landscape since there were 4 PUs in the base problems solution. So, we need to add 4 more PUs to the base problems 6 and decide how to spread the 16 total occurrences of the 4 new species across the new, larger landscape.

If we take one occurrence of each of the four new species and randomly assign each to any PU in the base solution, then we are guaranteed that the base solution is a solution to the full wrapped problem. If we then randomly distribute the remaining occurrences of each of the new species across the four newly added PUs (without replacement), then we have a full wrapped problem with the desired rank abundance distribution (**Fig. 2 \textcolor{cyan}{\ref{fig:WrapProbExampleDiagram}}**c). Finally, we know the optimal solution because there are no occurrences of the base problem species outside of the original base problem.

There are two things to note.  First, while the wrapping distribution could contain species that occur on only 1 PU, we do not allow that because any PU containing such a species would automatically have to be included in the solution and therefore, would reduce the problem to an equivalent, smaller problem.  Second, the wrapped problem loses the theoretical guarantees that applied to the original model RB since we have now violated some of the RB assumptions.  However, in practice we still find wide variation in problem difficulty for the wrapped problems.

Pseudocode for the wrapping algorithm is given in **CorSI Appendix C \textcolor{cyan}{\ref{appendixPseudocodeWrap}}**.

###  More realistic species distribution

We now have a method for generating wrapped problems, but that does not tell us what we should use as a more ecologically realistic wrapping abundance distribution.  As there are no rules governing what species are included in a real-world reserve selection, it is possible for the chosen species set to reflect almost any distribution.  We know of no studies that characterize the rank abundance distributions of species that have been included in real-world reserve selection problems. 

Here, we used lognormal distributions as an example wrapping distribution. While there is no consensus on what is a theoretically correct rank abundance distribution, we used lognormal distributions as our example distribution because they have at least been discussed by ecologists in relation to theoretical models for rank abundance distributions (@stevens2009). Note that while the lognormal distribution is undoubtedly a more realistic rank abundance distribution over the *full* set of species in a particular location than model RBs uniform distribution, the set of species considered in a reserve selection problem is never the full set of species occurring at that location.  Moreover, the practitioner can arbitrarily choose *any* subset of the species to include and therefore, the abundance distribution shape could be arbitrary. However, our wrapping method will accommodate any distribution shape that the practitioner thinks is appropriate, as long as it includes the base problems set of species that occur on exactly two PUs.

Once we have chosen the lognormal as our demonstration distribution, the question becomes which lognormal to use in each problem, i.e., what values are given to the lognormal's two parameters that control its shape? We address this choice by using an optimizer to search for the two parameters of a particular lognormal that i) doesnt exceed a user-specified upper bound on total number of species, and ii) includes the base problems given number of species that occur on exactly two PUs (though it can include more species that occur on exactly two PUs).  

We guide the parameter search by providing an evaluation function that penalizes parameter choices that yield a lognormal exceeding a given maximum number of all species (so that problems dont become unrealistically large) or that fall below the base problems number of species on two PUs. The choice of evaluation function to use is flexible in that it just needs to embody whatever constraints satisifies the user's beliefs about the appropriate function shape and somehow penalizes for violating them.  However, its form does have to differ from the squared error form used in regression because we only have data for the section of the curve containing species occurring on exactly two patches.  Everything else about the curve is underconstrained.  Though we use a lognormal as the target and our specific evaluation function to guide fitting, the same procedure works for choosing parameters of any wrapping distribution. Similarly, any desired evaluation function and optimization algorithm could be used. More details of the fitting procedure are given in **CorSI Appendix C.1 \textcolor{cyan}{\ref{appendixPseudocodeLognormal}}**.  

###  More realistic representation targets 

The wrapping method can be extended to allow a larger range of targets for any wrapping species as long as the target for the wrapping species is no larger than the size of the Base solution set and the Base species still have a target of 1.  We just need to drop at least the target number of occurrences of each wrap species on that number of PUs in the Base solution set instead of dropping just one occurrence as we had done previously.  More details are given in **CorSI Appendix C.3 \textcolor{cyan}{\ref{appendixPseudocodeMoreRealisticTargets}}**.

While this now allows known solutions for problems with a much greater range of targets, the disadvantage is that it moves even farther from the model RB theory that points to where the hard problems are.  We need more theory about this new kind of problem and how to avoid building only easy problems.  For example, the closer the target values are to their abundances and the closer those are to the size of the Base solution, it seems the easier the problem would be since those Base solution PUs will be forced to be part of the reserve selector's solution choice.  Other constraint relations may have similar effects.  

Since we know of no real-world data on what would be a reasonable distribution of representation targets and because it most closely matches the model RB theory, our method demonstration results below are only for problems with a target of 1 occurrence for all species.  
 
#  Methods  {#methodsSection}

Here, we describe the main elements of the methods used in the paper, however there are many more details required to fully describe the methods.  These details are given in the AppSI, and CorSI files, and in our R code for the R library `bdpg` at: https://github.com/langfob/bdpg.

##  Problem set  

We use the problem generators to create a large, publicly available dataset of reserve selection problems that all have known optimal solutions and different input uncertainties.  The goals of the data set are i) to show the existence of large variation in the difficulty of even comparatively simple reserve selection problems with uncertainty, ii) to have problem sizes small enough to be able for reserve selectors to finish, and iii) to have problems not be at the upper end of difficulty, e.g., as reflected in problems built as challenge problems for optimization contests.  Our goal is to generate a problem set that is focused on helping understand algorithm behavior as a function of problem structure more than focused on comparison of algorithms.  While we do use the generated problems to do a simple comparison of some reserve selection methods in **Section 8 \textcolor{cyan}{\ref{comparisonResultsSection}}**, the primary focus of generating the problem set is for learning to predict method behavior in **Section 9 \textcolor{cyan}{\ref{predictionResultsSection}}**.

We generated 3,000 sets of problems with 6 problems in each set.  First, a "correct" Base problem without uncertainty (designated "COR Base") is generated using model RB method and then a Wrap problem without uncertainty (designated "COR Wrap") is generated from the Base problem using a lognormal as the wrapping distribution.   Then, four "APP" versions of the COR Wrap problem are derived by adding four different kinds of error to it (**Section 6.3 \textcolor{cyan}{\ref{inputErrorGeneration}}**).  These are designated "APP Wrap" problems since they are the data that would be "apparent" to a reserve selector which would have no knowledge of the correct input values in a real-world setting.  Since we used Model RB to generate the problems, the representation target for all species was one occurrence and all patches have equal cost.  

COR problems were generated using model RB parameters drawing uniform random values in the following ranges:  

  - $\alpha \in [0.2,0.6]$
  - $n \in [8,20]$
  - $r \in [0.8,3.0]$ 
  - $p \in [0.14,0.36]$

We based these ranges on examples in @xu2005ipnijcai with an eye toward creating variation in problem structure and difficulty but backing away from parameter choices near the critical values that are predicted in the paper to lead to exceptionally difficult problems.

```{r}
cor_data_retvals = create_p1_COR_data (filtered_full_initial_exp_tib, params)

    #  Dimensions of spp and PUs to embed in text describing the bounds of 
    #  spp and PUs created in COR problems.
min_base_spp = cor_data_retvals$min_base_spp
max_base_spp = cor_data_retvals$max_base_spp

min_wrap_spp = cor_data_retvals$min_wrap_spp
max_wrap_spp = cor_data_retvals$max_wrap_spp

min_base_PUs = cor_data_retvals$min_base_PUs
max_base_PUs = cor_data_retvals$max_base_PUs

min_wrap_PUs = cor_data_retvals$min_wrap_PUs
max_wrap_PUs = cor_data_retvals$max_wrap_PUs

# min_base_spp = 18
# max_base_spp = 541
# 
# min_wrap_spp = 39
# max_wrap_spp = 1479
# 
# min_base_PUs = 14
# max_base_PUs = 120
# 
# min_wrap_PUs = 59
# max_wrap_PUs = 765
```

The resulting range of the number of species for Base problems was [`r min_base_spp`, `r max_base_spp`].  For Wrap problems, it was [`r min_wrap_spp`, `r max_wrap_spp`]. The range of the number of PUs was [`r min_base_PUs`, `r max_base_PUs`] for Base problems and [`r min_wrap_PUs`, `r max_wrap_PUs`] for Wrap problems.  Total Wrap solution sizes were in the range [7.48, 12.5]% of the Wrap landscape.   More details of input parameter values used in problem generation are in COR SI.  

##  Input error generation {#inputErrorGeneration}

In what follows, a *Positive* is defined to be an occurrence of a species on a PU and a *Negative* is the absence of a species on a PU, *FP* means False Positives, *FN* means False Negatives, *TP* means True Positives, and *TN* means True Negatives.  Appending *ct* to any of these indicates the integer count of the corresponding variable, e.g., *TPct* means the number of True Positives.

For each APP Wrap problem, we drew a uniform random value $v$ between 0 and 0.1 to simulate an input error rate of up to 10%.  Using this number, we generated four different types of input error, yielding four different APP Wrap problems corresponding to each COR Wrap problem: FP-only = $v$, FN-only = $v$, Unmatched (both FP = $v$ and FN = $v$), and Matched (both FP count and FN count matching the FN error *count* derived from FN error proportion  = $v$).  We used the same rate $v$ in all four cases so that comparisons can be made for the effects of the same error rate across different error types on the same underlying problem.  Both matched and unmatched error models are included because each species has far more PUs where it doesnt occur (True Negatives) than PUs where it does occur (True Positives).  That imbalance means that the same proportional error rate for FP as FN will produce a far larger number of False Positives occurrences than False Negatives. This turns out to make the reserve selector error on Unmatched problems be nearly the same as for the FP-only case.  Similarly, for the Matched case, making the number of False Positives match the number of False Negatives drastically reduces the number of False Positives and leads to behavior similar to the FN-only case.

In most results, we label the combined set of results from FN-only cases and Matched cases as **FN-dominated** for plotting and labeling convenience.  We say "dominated" because there are still a few False Positives in a Matched problem, but it is the existence of the False Negatives that dominate the behavior.  Similarly, we label the set of FP-only problems merged with  Unmatched problems as **FP-dominated** even though the Unmatched problems contain a relatively small number of False Negatives that generally have little effect on the outcome.  

```{r ConfusionMatrixFig, echo=FALSE, fig.align="center", out.height = "20%", fig.cap = "\\label{fig:ConfusionMatrixTableImg} Confusion matrix"}

cm_img_path = file.path (proj_dir, "Figures/confusionMatrixDiagram_PPscreenGrab.png")
include_graphics(cm_img_path)
```

Given the confusion matrix of counts in **Fig. 4 \textcolor{cyan}{\ref{fig:ConfusionMatrixTableImg}}**, we then define the **False Negative rate** as the proportion of cells that should correctly be labeled as Positive but are labeled as Negative:

\begin{equation}
  \label{eq:FNrateEqn}
  FNrate = \frac{FNct}{TPct + FNct}
\end{equation}

Similarly, we define the **False Positive rate** as the proportion of cells that should correctly be labeled as Negative but are labeled as Positive:

\begin{equation}
  \label{eq:FPrateEqn}
  FPrate = \frac{FPct}{FPct + TNct}
\end{equation}

Also from the confusion matrix, we define  ***total input error rate*** $\boldsymbol{E_{ti}}$ (the proportion of all cells that are incorrectly labelled) as:  

\begin{equation}
  \label{eq:totInputErrEqn}
  E_{ti} = \frac{FPct + FNct}{TPct + FPct + FNct + TNct}
\end{equation}

##  Problem formulation  {#problemFormulationSection}  

In our experiments, we formally define the optimization problem to be solved using a formulation that is identical to that used in @beyer2016em (equation 1).  

\begin{equation}
  \label{eq:ilpProblemFormulation}
  \begin{aligned}
    min &\sum_{i=1}^{N} c_i x_i  \\
    \text{subject to} &\sum_{i=1}^{N} r_{ik} x_i \geq T_k, \; \; k \in K  \\
    &x_i \in \{0,1\}, \; \; i \in N
  \end{aligned}
\end{equation}

where  

- $N$ is the number of planning units, 
- $K$ is the number of species in the problem, 
- $x_i$ is a binary decision variable indicating whether $PU_i$ is included in the solution ($x_i = 1$) or not included ($x_1 = 0$), 
- $c_i$ is the cost of $PU_i$, 
- $r_{ik}$ is the contribution of $PU_i$ to species $k$, and 
- $T_k$ is the minimum target value to be achieved for species $k$ across all PUs in the problem.  

In our experiments, all COR costs are identical (i.e., $c_i = 1$) and all targets are identical (i.e., $T_k = 1$).  Our source code allows APP costs to have positive values other than 1 and to not be identical (to allow cost error), but in the experiments shown here, they are all 1 for simplicity.  Note that there are also no spatial constraints in our problem formulation.  

##  Output error measures  {#outputErrorMeasuresMethodsSection}  

Unlike some other studies, our primary performance interest in the reserve selectors was not in their speed, but rather in the accuracy of their solution under input uncertainty and the degree to which they violated any specified constraints.  So, for each reserve selector run, we calculated three measures of output error with respect to the known optimal solution of the COR problem and with respect to violations of the solution constraints given in the problem formulation (Eq. (**5 \textcolor{cyan}{\ref{eq:ilpProblemFormulation}}**)): representation shortfall error, solution cost error, and total output error.  For each of those three, we were also interested in the degree to which optimizing over uncertain data amplified or corrected input error.  Consequently, we also calculated the corresponding amount of error magnification for each output error measure as the ratio of the output error to the total input error.  Magnification values greater than 1 imply error amplification, less than 1 imply error correction, and equal 1 implies no change to the total input error amount.  Magnification provides an intuitive way to standardize error measurement independent of the amount of input error.  

***Representation shortfall error*** $\boldsymbol{E_{rep}}$ is the proportion of the number of species that don't at least meet their representation target in the selector's solution:  

\begin{equation}
  \label{eq:repShortfallErrEqn}
  E_{rep} = \frac{K_f}{K}
\end{equation}

where $K_f$ is the number of species that fall short of their representation target, and $K$ is the total number of species.  

***Signed solution cost error*** $\boldsymbol{E_{sc}}$ is the error in the cost of the reserve selector's solution as a proportion of the known optimal cost, with negative values indicating underestimation of the optimal solution cost and positive values indicating overestimation:   

\begin{equation}
  \label{eq:solutionCostErrEqn}
  E_{sc} = \frac{C_{rs} - C_{opt}}{C_{opt}}
\end{equation}

where $C_{rs}$ is the reserve selector's solution cost, and $C_{opt}$ is the optimal solution cost.  

***Total output error*** $\boldsymbol{E_{to}}$ is just the root sum of squares combination of representation error and solution cost error:

\begin{equation}
  \label{eq:totalOutputErrEqn}
  E_{to} = \sqrt{E_{sc}^2 + E_{rep}^2}
\end{equation}  

***Error magnification*** $\boldsymbol{ErrMag_{out}}$ is the ratio of any of the errors above to the total input error rate on the same problem, which for any of the errors can be expressed as:

\begin{equation}
  \label{eq:errMagEqn}
  ErrMag_{out} = \frac{E_{out}}{E_{ti}}
\end{equation}  

where $E_{out} \in \{E_{rep}, E_{sc}, E_{to}\}$.  

##  Reserve selectors  {#reserveSelectorsMethodsSection}  

```{r computeGurobiUnfinishedProportion}

gur_temp_tib = filter (p2_app_wrap_tib, rs_method_name == "ILP")
gur_status_cts = count (gur_temp_tib, gurobi_status)
gur_unfinished_proportion = gur_status_cts$n[2] / sum (gur_status_cts$n)
```

We explore the consequences of the common practice of ignoring uncertainty in reserve selection by comparing the accuracy of four reserve selection methods that ignore uncertainty in their inputs (**Section 7  \textcolor{cyan}{\ref{resultsSection}}**).  No doubt our versions of all these reserve selectors can be improved by experts with knowledge of individual problems.  However, our goal here is to provide an example of using the generated benchmark set and to produce training and testing data for learning to predict reserve selection error on individual problems.   

Note that as stated earlier, when we refer to a "reserve selector" or "reserve selection method", we are using it as a shorthand for the combination of the algorithm, its software implementation, its problem formulation, and its parameterization.  Changing any of those elements would likely affect its performance and is therefore considered a different "reserve selector".  In this work, we are not trying to make any claims about which of these methods are better than the other in general.  We are only using them as examples for our proposed approach to method evaluation and error prediction.  

We ran four different reserve selectors on each problem in 3,000 independent groups of six problems each for a total of 72,000 reserve selector runs.  The four reserve selectors we tested were:  

- Integer linear programming (hereafter, ILP)
- Simulated annealing (hereafter, SA) 
- Simulated annealing summed solution (hereafter, SA_SS)
- Greedy search using unprotected richness (hereafter, UR_Forward) 

###  Integer linear programming (ILP)  

We used Gurobi version 8.00  (@gurobioptimizationllc2020) as our integer linear programming solver.  Gurobi is proprietary linear programming software that is guaranteed to find the exact, optimal solution when given enough resources (time and memory) and no violations of its assumptions (e.g., input data does not contain errors).  

Gurobi is run using its default settings derived from examples in (@beyer2016em, @gurobioptimizationllc2020).  An exception to the defaults is the time allowed for Gurobi to run.  Because ILP methods maintain an upper bound on the distance to the correct solution as they work and because they often make most of their progress early in the run, it is common to limit ILP run times by specifying a solution gap as the point to end a run (e.g., quit when within 5% of the guaranteed lower bound of the optimal solution).   In our early tests, difficult problems could cause Gurobi to take more than a week to finish or it would crash after a few days (possibly from not enough memory).  Consequently, we gave Gurobi a time limit equal to the amount of time SA required to finish the same problem. This seemed reasonable given that @beyer2016em, @schuster2020p, and @vanderkamHeuristicAlgorithmsVs2007 all state that ILP ran at least as fast as Marxan. 

In spite of that limit, Gurobi still crashed on some problems due to what appeared to be a lack of computer memory.  Whenever it did crash, we restarted that problem on the Nectar virtual machine with the next largest amount of memory.  If it crashed again, we repeated the process of stepping up in memory until Gurobi ran long enough to either finish or hit the specified Marxan-based time limit.  This early stopping resulted in Gurobi failing to find the final optimal solution (i.e., solution gap = 0%) in `r round (gur_unfinished_proportion, 2)*100`% of the problems.  

There is a question as to whether those problems should be included in the results.  We have chosen to include them for two reasons.  First, the original total error plots for ILP, SA, and UR_Forward look very similar, regardless of whether looking at finished or unfinished problems.  This suggests that even if Gurobi had finished, it would likely have gotten similar output errors to SA.  Second, the fact that when Gurobi failed to finish, SA and UR_Forward scored poorly on these same problems suggests that these were the hardest problems in the data set.  Removing those problems would have removed the more difficult problems from the datasets for all reserve selectors.  So, we include these problems and note that the results for Gurobi on those problems are not certain. We discuss this option choice further in **AppSI Appendices C.3 \textcolor{cyan}{\ref{GurobiTextFromP1}} and D \textcolor{cyan}{\ref{GurobiTimeLimitsAppendix}}**.  The Shiny app allows you to exclude them if you want to explore the consequences of only including easier problems.

###  Simulated annealing (SA)  

We used Marxan version 2.43 (@watts2009em&s) as our simulated annealing solver.  Marxan is a stochastic heuristic method that is not guaranteed to find the exact, correct solution, but often finds good or even exact solutions when the input data is correct.  

For each problem, Marxan was run for 1E6 iterations and this was independently repeated 100 times.  The lowest cost solution among the 100 replicates was designated the SA solution for that problem.   All other options except the species penalty factor (spf) were set to Marxan defaults.  See **CorSI Appendix F.2 \textcolor{cyan}{\ref{appendixMarxanParSettingsUsed}}** for details of spf option setting.  

###  Simulated annealing with summed solution (SA_SS)  

We used Marxan version 2.43 (@watts2009em&s) plus our own R source code (R Core Team, 2022)) in our ensemble (voting) method of solution.  SA_SS is a voting method based on greedy selection of PUs in order by the number of Marxan restart solutions that the PU appears in.  We implemented this method in R using the outputs of 100 Marxan random restarts.  

The Marxan software has many different ways of computing possible solutions.  Here we are interested in a simple greedy voting version of its irreplaceability measure.  There is probably a way to do this built into Marxan, but we get the job done using our own R source code.  

###  Greedy complementarity-based search (UR_Forward)  

UR_Forward is a greedy, complementarity-based heuristic implemented in R.  It selects PUs in order by the number of species on the patch that have not yet met their representation target (i.e., Unprotected Richness).  

Further details of parameter settings used for running each reserve selector are given in **CorSI Appendix F \textcolor{cyan}{\ref{appendixReserveSelectorParameters}}**.

##  Computing environment  

We developed the R software and preliminary experiments for this paper on a MacBook Pro under OS X High Sierra (10.13.6).  We ran all reserve selector experiments divided among elements of our 40 CPU allocation on the Australian Research Data Commons (ARDC) Nectar Research Cloud of Ubuntu linux (version 18.04 LTS) virtual machines.  We ran each experiment first on a single CPU virtual machine with 8 GB^[I think that all of these Nectar virtual machine memory and disk specifications in this section are right, but I have no way of checking now since nectar changed these configurations long ago.] of memory, however, some experiments ran out of memory and the reserve selector crashed.  If the experiment ran out of memory, we reran that experiment (using the same random seed) on successively larger virtual machines with more memory until we found a VM where it did not crash.  The largest machine used had 16 CPUs and 48 GB of memory.  The Nectar setup did not allow stepping up in memory without also stepping up in the number of CPUs.  This means that measured run times aren't necessarily equivalent across runs, especially since some of the reserve selectors could take advantage of multiple CPUs (e.g., Gurobi's ILP implementation) while others didn't.  Another issue with run-time measurements was that we were using virtual machines inside a much larger computing cluster and VM speed could be influenced by the load on the cluster as a whole.  These two issues mean that our use of the measured Marxan SA run-times as the time limit for the Gurobi ILP runs were only approximate.  However, given that we were increasing the resources available to Gurobi after each crash, this is unlikely to have put Gurobi at a disadvantage for the given time limit.  

With the exception of Gurobi, Marxan, and a few bash scripts used in aggregating data, all results for this paper were generated using programs written in R (R Core Team, 2022).  All of the source code is freely available under the MIT license as an R package called `bdpg` that can be downloaded from: https://github.com/langfob/bdpg

We also used a custom program called `tzar` to help manage the scheduling and reproducibility of experiments across many machines.  The input parameters for each model run are documented in a yaml file that is read by tzar and used to build an R list that is passed to the main R source code.  Because this is a standard R list structure, using `tzar` is not required; the user can build the list any way they like.  `tzar` and its source code are freely available for download from:  
https://tzar-framework.atlassian.net/wiki/spaces/TD/pages/524311/Introduction  
Source code for an R package that made it easier to debug R source code running under `tzar` is also freely available under the MIT license at:
https://github.com/langfob/tzar

##  Shiny app  {#shinyAppMethodsSection}  

Because there are so many possible option selections in our methods, we have also provided an online graphical interface to the data using R's Shiny package (@chang2021).  Readers interested in different option choices, data breakdowns, and plot types than those discussed below are directed to the Shiny app at [https://langfob.shinyapps.io/bdpgShiny/](https://langfob.shinyapps.io/bdpgShiny/), where choices can be varied via button selections (see ShinySI file for instructions).  

##  Learn functions to predict reserve selector accuracy on individual problems using bipartite graph  {#learnFunctionsToPredictMethodsSection}  

\textcolor{red}{This whole section to describe the graph features and the fitting procedures is incomplete and sketched out informally because I'm not sure how we want to present it in a way that feels familiar to ecologists.  Same goes for the corresponding Results and Discussion sections, though not to the degree of informality and incompleteness as here.}  

Given large variation in problem difficulty and input uncertainty, knowing the average output error across all problems has little utility for a single, specific conservation decision. Consequently, using the large amount of data gathered in the evaluation step above, our last step is to ask to what degree it is possible to reliably predict how much error a given reserve selector is likely to have on individual problems.  Doing this requires knowledge of informative structural characteristics of problems to use as input features/covariates in fitting a predictive model of error.  

With rare exceptions (e.g., @presseyEffectsDataCharacteristics1999), the primary explicit features used to characterize the likely difficulty of reserve selection problems have been measures of problem size, i.e., the  number of species and planning units.  However, these problem size measures are only mildly informative, as reported by Sarkar et al. in their method comparison (@sarkarPlacePrioritizationBiodiversity2004).

Here, we consider what other features might better capture characteristics of a problem that make it difficult for a reserve selector.  A different lens on problem features is to look at measures of structural complexity.   These are abundant in the graph theory literature, so we have converted each reserve selection problem from a list of species and planning units into a bipartite graph like those used in plant-pollinator studies (\textcolor{red}{vazquez?  dormann?}).  

A bipartite graph is a set of nodes and links where the nodes of the graph are divided into two separate classes of nodes and the links in the graph are only allowed between nodes that are not in the same class.  For example, in plant-pollinator bipartite graphs, there could be links between a plant and each creature that pollinates it.  In reserve selection, we can split nodes into planning units and species and draw links between each planning unit and each of the species that occupy it.  We can then compute any of the multitude of structural measures that describe the graph (e.g., connectivity and redundancy [insert references for graph libraries?])  to use as predictors. In **Section 6.9.7 \textcolor{cyan}{\ref{fittingProceduresMethodsSection} }** and **Section 9 \textcolor{cyan}{\ref{predictionResultsSection} }**, we show how we can compute these kinds of structural measures over the erroneous input data.  Note that  these predictions are made without recourse to quantities that can't be known in real-world problems, e.g., the true amount of input error.  

In choosing  variables to use for predicting performance, it is important to differentiate between ones whose values can and cannot be known for real-world problems.  For example, the true input error rates might be possible to estimate, but not to know exactly in a real-world problem.  In what follows, we fit predictive models of output error based on different subsets of all the variables that we have measured for each problem.  In all but the final model fitting, we use only variables that are knowable in a real-world problem.  In the final model fitting, we use both knowable and unknowable variables to help understand what might be an upper bound on our predictive ability as well as what would be the relative value of knowing or very reliably estimating things that we are unable to know in a real-world problem.  

###  Graph measures used as input features for learning to predict reserve selector output error  {#graphMeasuresMethodsSection}  

To enable the use of bipartite network measures for each reserve selection problem, we encode each problem as a bipartite network where one level of the network has a node for each species in the problem and the other level has a node for each PU in the problem.  Then, for each species in the problem, we add a link between the node for that species and each PU that it occupies in the data.  We represent this node and link data as an adjacency matrix where the columns are species and the rows are PUs.  If a given species is linked to a given PU (i.e., the species occupies that PU), then there is a 1 in the adjacency matrix at the corresponding species row PU column pair.  All PUs not occupied by a given species have a 0 at the corresponding location in the adjacency matrix.  

We then use the R package `bipartite` (@dormann2008rn, @dormannIndicesGraphsNull2009) to compute different measures of the structure of the bipartite network represented by the adjacency matrix.  The `bipartite` package can compute many different measures, some of which are quite time-consuming (e.g., nestedness).  To reduce total computation time for each problem, we have only computed measures that are relatively quick to compute.  

Because the bipartite package is more oriented toward computing measures on  plant-pollinator networks, the names of some of those measures are confusing when applied to our experiments.  For example, "number of species" in bipartite's naming refers to both the total number of plant species and pollinator species, which is essentially the total number of nodes in the bipartite network.  In our experiments, this is the total number of both species *and* PUs.  To reduce naming confusion,  in our outputs we have replaced these kinds of `bipartite` package names with names that are more meaningful for this paper.  The `bipartite` package names and their equivalent `bdpg` package names for bipartite network metrics used are given in **AppSI Appendix I \textcolor{cyan}{\ref{bipartiteVarNameConversionAppendix}}**

The list of all measured variables that can be used for prediction and their definitions is quite long, so it is shown in **AppSI Appendix J \textcolor{cyan}{\ref{predInputVarNamesAndDefnsAppendix}}**

###  Exploratory data split  {#exploratoryDataSplitMethodsSection}

We had many parameter and method and feature selection decisions to make, so we did an initial split of the full data set into an exploration set and a final experiment set.  The full set contained six different batches of 500 experiments each.  We did the experiments in batches because we didn't know how long it would take to do large numbers of experiments where problem size and structure varied a lot.  The batches differed only in the random seed used to start the batch.  We arbitrarily chose two of the batches to use as the exploratory set and reserved the other four batches to be used after our final fitting and feature selection decisions had been made.  

We used the exploratory set in several different ways.  We began by using one batch as the training data and the other batch as the test data.  We freely experimented with different combinations of variables and different subsets of input features.  This would be unacceptable if this was used to report the final measures of our predictions, but was acceptable here because we had sequestered the other four batches to use in final testing.  

While working with the exploratory set, we also did some other variations to give at least a little look at the robustness of the results we were getting from the basic setup.  For example, we occasionally reversed which batch was test and which was train.  More importantly, we also experimented with partitioning batches with respect to the independence of problems within them.  This is of interest because each COR Wrap problem has four APP Wrap problems derived from it, so these four problems are not independent of each other.  This lack of independence can affect the computation of some statistics related to model fitting, so we also created four independent subsets of each batch.  We created these four subsets by collecting the IDs of all of the COR Wrap problems in a batch and then randomly assigning each of the APP Wrap problems derived from each COR Wrap problem to a different one of the four subsets.  In this way, all APP Wrap problems within any of the four subsets were independent of all other problems within the same subset.  This also meant that the four subsets of a single batch each contained only 125 problems rather than the full 500.  However, it meant that statistics related to using the smaller subsets were not tainted by dependence among problems within the subset.  

###  Test and train data splits  {#testAndTrainSplitMethodsSection}

**\textcolor{red}{What to do in this section is still up in the air.  Everything that is presented in the predictions section of that paper right now is based only on the Exploratory results because I don't want to taint the results for data that was sequestered, i.e., the last four batches of problems.  Below, I'll explain various ways that the final results could be partitioned and computed.  I don't think the results that we get will depend much on which choice is taken.}**

All decisions about data preprocessing, feature selection, and fitting method choice were made using the exploratory data set only.  

There are a number of ways that the data can be split.  All of these need to be viewed in terms of whether you want to maintain independence of problems within sets (test or training sets) or between sets or both.  Here are five examples of possible ways to do the splitting **\textcolor{red}{(and I'm not sure which is best or even most normal-seeming for ecologists)}**:  

1)  The simplest way would be to just call the Exploratory set the final training data and call the other four batches the test data.   This is legal because the test data has never been used or seen in any of the exploration even though all of the training data has been seen during exploration.  

2)  Option 1) ignores the independence issue mentioned above.  While all problems in the training set would be independent of all problems in the test set, there would be dependence among problems within the test set and this might taint some statistics computed over that test set.  So, we could apply the same independent partitioning scheme mentioned above to the create as many independent test subsets as you want and measure performance on each of them to get a distribution of scores.  Since the test data contains four batches, each of the independent subsets would contain 500 independent problems.  

  - It's not at all clear what affect it has on the test scores to have all of the examples be independent of each other or not.  We could test the affect in a very simple way by training on the Exploratory data and then testing it in several different ways.  First, we could separately test on each of the 4 test batches without worrying about independence within the batches.  Second, we could test on the full set combining all 4 test sets, again without worrying about independence.  Then, we could split the full combined test set into 4 subsets of independent problems and test on each of them and compare with the previous non-independent test scores.  You could also do bootstrapped versions of the full test set without independence to see how those compared.  

3)  Once the learning scheme was chosen based on the Exploratory data set, we could ignore that Exploratory data and do some kind of cross-validation within the full test set.  

4)  Only using the Exploratory data set as the training data reduces the amount of training data by quite a bit compared to the full data set, so we might want to do some kind of combination of the Exploratory data with one or more of the test data batches to train and use the rest of the batches as test.  For example, we could combine the two Exploratory batches with three of the test batches as training data and then test on the remaining test batch.  These could also be done with the independent subset grouping procedures too.

5)  We could also use fewer in training and more in testing.  In fact, we could combine all test examples into one set and then random draw a subset of problems from that set to use for training data and use the rest as test data.  However, we would need to choose the problems to include in test or train based on their COR Wrap problem so that no APP Wrap problem in the test set was derived from the same COR Wrap problem as any APP Wrap problem in the training set.  

###  Data preparation and preprocessing  {#dataPrepAndPreprocessingMethodsSection}  

Dormann et al. reported that many bipartite network measures are strongly correlated with network size (@dormannCollinearityReviewMethods2013) and many of the other variables that we have measured as possible input features for prediction are correlated to varying degrees.  We have measured the correlations of all these input variables with each other and done exploratory prediction tests with removing highly variables based on their degree of correlation.  \textcolor{red}{[In the end, I ignored these correlation things because nothing mattered much to predictive power other than the total number of variables included in the set of predictors.]}

###  Measures of fit  {#measuresOfFitMethodsSection}

To measure the quality of each model fit, we computed both adjusted $R^2$ and root mean square error (RMSE) for predictions of representation shortfall and for predictions of solution cost error.  

###  Feature selection  {#featureSelectionMethodsSection}

\textcolor{red}{[I explored many different combinations of input features, but nothing seemed to matter much beyond how many variables were included, the more the better.  This result is expected if you're using $R^2$, but it held for both adjusted $R^2$ and RMSE.]}

###  Fitting procedures  {#fittingProceduresMethodsSection}

\textcolor{red}{[I tried several different fitting procedures (linear regression, random forests, glm (with elastic net regularization)) and they all performed similarly, so I chose the simplest, most conventional one to present in the paper, i.e., simple linear regression.]}

----------

#  Results {#resultsSection}

#  Results - Reserve selection comparisons {#comparisonResultsSection}

ILP solved all problems without uncertainty (COR Base and COR Wrap) without any errors, as did SA.  SA_SS and UR_Forward solved most COR problems without error.  Details of results on COR problems are given in **CorSI Appendix H \textcolor{cyan}{\ref{corResultsAppendix}}**.  

The rest of this section describes results for the reserve selectors on the APP Wrap problems, i.e., problems with uncertainty.  All reserve selectors showed both large errors and large amounts of variation at every level of input error greater than 1% (up to our maximum sampled input uncertainty of 10%).  Results for the methods that did not involve voting (ILP, SA, and UR_Forward) were visually almost indistinguishable on most plots, so we will generally refer to them together as the **Non-Voting (NV)** group.  Note that in nearly all plots shown here, positive y-axis values are truncated well below maximum error values to make the plots clearer.  

In **AppSI Appendix F \textcolor{cyan}{\ref{DiffsAppendix}}**, we show plots of the difference between error scores for each pair of reserve selectors on each problem to make it easier to compare selectors.  

## Total output error

```{r countMags, include=FALSE, echo=params$chunk_include}
### May want to move this to where it's used in the text.

#  I think that `mag_frac` is only used in one place in the document and that's 
#  to just embed its value in a sentence.  Having the calculation here is 
#  something of a non-sequitir, so it might be better to just computed it 
#  right before the line of text that uses it.

mag_base_col_name_str = params$mag_base_col_name_str

p2_app_wrap_mag_frac_retvals = compute_mag_frac (p2_app_wrap_tib, params)

#####mag_base_col_name_str = p2_app_wrap_mag_frac_retvals$p2_app_wrap_mag_frac_retvals
mag_col_name_str = p2_app_wrap_mag_frac_retvals$mag_col_name_str
shortfall_mag_str = p2_app_wrap_mag_frac_retvals$shortfall_mag_str
mag_frac = p2_app_wrap_mag_frac_retvals$mag_frac
cat ("mag_frac = ", mag_frac)
```

Total output error is shown in **Fig. 5 \textcolor{cyan}{\ref{fig:InVsTotOutErrLE125inResults}}** as a scatter plot where each dot represents one problem.  The total output error is shown as a function of the input error for that problem.  The large variation in output error at nearly every level of input error shows that there was considerable variation in problem difficulty that was independent of the size of the input error.  In `r mag_frac`% of all problems, total output error was greater than input error for that problem (i.e., most dots are above the *1x* line).  In other words, the  optimization process of selecting a set of locations to efficiently represent species almost always amplified the input error.  Box plots showing variation in total output error and magnification broken down by single percents can be viewed in the Shiny app.  

```{r plotInVsTotOutErrLE125inResults, fig.cap = "\\label{fig:InVsTotOutErrLE125inResults}Total output error as a function of total input error, showing only problems with input error up to 10\\% and total output errors truncated at 125\\%.  Problems are colored by their dominant error type (FP or FN).  The black lines indicate levels where output error is the same as the input error (1x), is 5 times the input error (5x), and is 10 times the input error (10x)."}
#  NOTE:  2021 11 10 - BTL
#         The double backslash before the %s in the chunk header figure caption  
#         seems to be necessary to have the captions produced correctly by latex 
#         only when the commands to stop figures from floating are used.  
#         Bizarre, but it seems to work.  Same thing is necessary if you have 
#         any underscores in your caption.
#         If you don't do this, then captions are rendered with:
#             \begin{figure}[H]
#             \caption{
#             ...
#             } \end{figure}
#         wrapped around them.
# inVsTotOutErrLE125inResultsPlot = 

ggplot_faceted_with_mag_rays (rs_method_names_list, 
        plot_title_str = paste0 ("Total output error"),
    filter (p2_app_wrap_tib, rsr_COR_euc_out_err_frac <= 1.25), 
        x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",    #"rsp_euc_realized_Ftot_and_cost_in_err_frac", 
        y_var     = "rsr_COR_euc_out_err_frac", 
        color_var = dom_err_type,    #"id",
        facet_var = "rs_method_name_fac",    #"rs_method_name" 

        show_x_as_percent = TRUE, 
        show_y_as_percent = TRUE, 
    
        x_axis_label = "Input Error", 
        y_axis_label = "Total Output Error"
                             ) + 
  
  background_grid (major = c("y"),
  minor = c("y"), size.major = 0.5,
  size.minor = 0.2, color.major = "grey85", color.minor = "grey85")

# inVsTotOutErrLE125inResultsPlot
# 
# ggsave (plot=inVsTotOutErrLE125inResultsPlot,
#         device="png",
#         filename="Figs/inVsTotOutErrLE125inResultsPlot.png",
#         height=5, width=5)


```

##  Signed solution cost error  

Signed solution cost errors are shown in **Fig. 6 \textcolor{cyan}{\ref{fig:InVsCostErrLE125inResults}}** and represent how much more or less cost the reserve selectors think they need to meet each species' target when compared to the optimal solution.  Plots for the absolute values of the same data can be viewed in the Shiny app.  The NV group underestimated the optimal cost on FP-dominated problems and overestimated on FN-dominated problems.  In FP-dominated problems, reserve selectors can choose areas where species appear present but are absent, resulting in an underestimation of the optimal cost. In FN-dominated problems, the cost was overestimated because the reserve selectors dont have information about some locations where species are present, which could allow more efficient solutions.  SA_SS overestimated cost on nearly all problems, with greater overestimation for FN-dominated problems. All selectors showed a wide range of error and error  magnification at all levels of input error.

```{r plotInVsCostErrLE125inResults, fig.cap = "\\label{fig:InVsCostErrLE125inResults}Signed solution cost error as a function of input error, showing only problems with input errors up to 10\\% and solution cost errors truncated at 125\\%. Problems are colored by their dominant error type.  Black lines above the x-axis indicate levels where solution cost error is the same as the input error (1x), 5 times the input error (5x), and 10 times the input error (10x).  Black lines below the x-axis are the negative equivalents of those above the axis."}

#  Extra text removed from caption:
#    FN-dominated problems are in blue.  FP-dominated problems are in red.  The 1x, 5x, and 10x rays indicate the level where output error is 1 times input error, 5 times input error, and 10 times input error (for both underestimation and overestimation).  The y-axis (output error) is truncated below at -100% because that's the maximum underestimation of cost possible.  It's truncated above at 125% because only ZL_Backward had many values above 125% (up to 500%).  Truncation at 125% avoids compressing resolution for the 4 other methods.  Full display of FN-dominated values for ZL_Backward can be seen in Appendix.

ggplot_faceted_with_mag_rays (rs_method_names_list,
                              plot_title_str = paste0 ("Signed Solution Cost Error"),
                              filter (p2_app_wrap_tib, rs_solution_cost_err_frac <= 1.25), 

                              x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",
                              y_var     = "rs_solution_cost_err_frac", 
                              
                              color_var = dom_err_type,    #"id",
                              facet_var = "rs_method_name_fac",    #"rs_method_name" 
                                  
                              y_min = -1,
                              y_max = 1.25,
                              
                              x_axis_label = "Input Error", 
                              y_axis_label = "Signed Solution Cost Error",
                  
                              plot_rep_shortfall_err_bound = FALSE, 
                              plot_rays = TRUE, 
                              plot_neg_rays_too = TRUE
                                                   ) + 
                        
                              background_grid (major = c("y"), 
                                               minor = c("y"), 
                                               size.major = 0.5, 
                                               size.minor = 0.2, 
                                               color.major = "grey85", 
                                               color.minor = "grey85") 

```

##  Representation shortfall

Representation shortfall errors are shown in **Fig. 7 \textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**, which represents the proportion of species missing from the solution due to the FPs and FNs in the input data. As expected, FN-dominated problems showed virtually no representation shortfalls for any reserve selector, as in most cases, this resulted in greater areas than necessary being selected. In FP-dominated problems, all reserve selectors showed a range of shortfalls due to selecting locations where species are not present.  For the NV group, shortfalls ranged from 0 to approximately 80% once input error was at least 5%.  Values for SA_SS had a smaller range (40-70%) once input error was at least 5% and tended to have a much larger concentration of values near 0.  

```{r plotInVsRepShortfallinResults, fig.cap = "\\label{fig:inVsRepShortfallinResults}Representation shortfall error showing only problems with input errors up to 10\\%. Problems are colored by their dominant error type.  The black lines indicate levels where output error is the same as the input error (1x), is 5 times the input error (5x), and is 10 times the input error (10x)."}

#  Extra text removed from caption:
#  FP-dominated problems are in red.  FN-dominated problems are in blue.  Since there is very little representation shortfall in FN-dominated problems,the blue FN dots on the graph are very localized and overplot each other.  The 1x, 5x, and 10x rays indicate the level where output error is 1 times input error, 5 times input error, and 10 times input error.


ggplot_faceted_with_mag_rays (rs_method_names_list, 
        plot_title_str = paste0 ("Representation Shortfall Error"),
    p2_app_wrap_tib, 
        x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",    #"rsp_euc_realized_Ftot_and_cost_in_err_frac", 
        y_var     = "rsr_COR_spp_rep_shortfall", 
#        color_var = "id",
      color_var = "dom_err_type",
        facet_var = "rs_method_name_fac",    #"rs_method_name"
    
                              y_min = 0,
                              y_max = 1,
                              
        x_axis_label = "Input Error", 
        y_axis_label = "Rep Shortfall",
                             ) + 
  
  background_grid (major = c("y"),
  minor = c("y"), size.major = 0.5,
  size.minor = 0.2, color.major = "grey85", color.minor = "grey85")

```

The error magnification for the representation shortfalls in FP-dominated problems are shown in **Fig. 8 \textcolor{cyan}{\ref{fig:inVsRepShortfallMagLE25inResults}}**.  Note that FN-dominated values are clustered along the x-axis and hard to see as they had no shortfall.  Because the upper bound on representation shortfall error is 100%, the upper bound on magnification is *100/inputErrorPercent*, and this bound is shown in the figure as the curved black line.  Maximum shortfalls for reserve selectors in the NV group approached this upper bound as input error increased.  In contrast, the maximum magnifications for SA_SS were lower and flatter across the input error range, since SA_SS tended to have lower representation shortfalls for a given input error (**Fig. 7 \textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**).

```{r plotInVsRepShortfallMagLE25inResults, fig.cap = "\\label{fig:inVsRepShortfallMagLE25inResults}Representation shortfall error magnification as a function of total input error, showing only problems with input error up to 10\\% and shortfall magnification up to 25x. The maximum possible shortfall magnification at any input error percentage is shown as a solid black curve.  Problems are colored by their dominant error type."}

#  Extra text removed from caption:
#    FP-dominated problems are in red.  The y-axis (output error) is truncated at 125% because only a tiny number of values for any reserve selector are above 125%.  FN-dominated problems are in blue.  Since there is very little representation shortfall in FN-dominated problems, the blue FN dots on the graph are very localized and overplot each other.  Also, because the upper bound on rep shortfall error is 100%, the upper bound on magnification is 1/inErrFrac.  That upper bound is plotted as the curved solid line.

ggplot_faceted_with_mag_rays (rs_method_names_list,
                              plot_title_str = paste0 ("Representation Shortfall Magnification"), 
                              filter (p2_app_wrap_tib, max_TOT_FN_FP_shortfall_mag <= 25),    #filtered(),

                              x_var     = !!mag_base_col_name_str,    #"max_TOT_FN_FP_rate",    #!!(sym(x_var_name_str)),    #"max_TOT_FN_FP_rate",    #"rsp_euc_realized_Ftot_and_cost_in_err_frac",
                              y_var     = !!shortfall_mag_str,        #"rep_shortfall_mag",    #!!(sym(y_var_name_str)),    #!!input$OutErrVarInput,    

                              color_var = dom_err_type,    #"id",
                              facet_var = "rs_method_name_fac",                 #"rs_method_name" 
                                  
                              y_min = 0,    #y_min,
                              y_max = 25,    #y_max,
                              
                              x_axis_label = "Input Error",                     #x_var_labelling_str,    #"Input Error", 
                              y_axis_label = "Rep Shortfall Mag",               #y_var_labelling_str, 
                  
                              plot_rep_shortfall_err_bound = TRUE,              #plot_rep_shortfall_mag_bound, 
                              plot_rays = FALSE,                                #plot_mag_rays, 
                              plot_neg_rays_too = FALSE                         #plot_neg_mag_rays
                                                   ) + 
                        
                              background_grid (major = c("y"), 
                                               minor = c("y"), 
                                               size.major = 0.5, 
                                               size.minor = 0.2, 
                                               color.major = "grey85", 
                                               color.minor = "grey85") 

```

## Representation shortfall vs. Signed solution cost error

The relationship between representation shortfall and signed solution cost error on each problem is shown in **Fig. 9 \textcolor{cyan}{\ref{fig:repShortfallVsCostErrLE125inResults}}**.  Each point there can be viewed as a vector representation of the two components that we have defined as the Total Output Error (as shown in **Fig. 5 \textcolor{cyan}{\ref{fig:InVsTotOutErrLE125inResults}}**).  

All four reserve selectors had similar behavior on FN-dominated problems, i.e., little or no representation shortfall and a wide range of optimal solution cost error.  On FP-dominated problems, the NV group of selectors all showed mostly underestimation of optimal solution cost and a wide range of representation shortfall.  The SA_SS selector showed less representation shortfall than the NV group and tended to overestimate optimal solution cost rather than underestimating it.  

The large black rectangles in **Fig. 9 \textcolor{cyan}{\ref{fig:repShortfallVsCostErrLE125inResults}}** identify  particularly difficult problems where both representation shortfall and cost underestimate were greater than or equal to 50% on the same problem.  The small rectangles near the origin identify the region where all output errors were less than 10%.  This is where all output errors would fall if output errors were no larger than any input error.  A large proportion of output errors fell outside this region, showing that across our simulated problems, input errors tended to be magnified by the optimization process.  

```{r plotRepShortfallVsCostErrLE125inResults, fig.cap = "\\label{fig:repShortfallVsCostErrLE125inResults}Representation shortfall vs. Signed solution cost error showing only problems where absolute value of cost error is less than 125\\%.  Problems are colored by their dominant error type.  The large rectangle in the lower right corner of the plot identifies problems where the reserve selector has more than 50\\% error in both cost and representation shortfall.  The small rectangle on the 0\\% line identifies values where all output errors are less than the maximum input error of 10\\%."}

#  Extra text removed from caption:
#    FP-dominated problems are in red.  FN-dominated problems are in blue.  

# repShortfallVsCostErrLE125inResultsPlot = 
  
ggplot_faceted_with_mag_rays (rs_method_names_list,
                              plot_title_str = paste0 ("Representation shortfall vs. Signed solution cost Error"), 
                              filter (p2_app_wrap_tib, rs_solution_cost_err_frac <= 1.25), 

                              x_var     = "rsr_COR_spp_rep_shortfall", 
                              y_var     = "rs_solution_cost_err_frac", 
                              
                              color_var = dom_err_type,    #"id",
                              facet_var = "rs_method_name_fac", 
                                  
                              x_min = 0,
                              x_max = 1,
                              
                              y_min = -1,
                              y_max = 1.25,
                              
                              x_axis_label = "Rep Shortfall", 
                              y_axis_label = "Solution Cost Error",
                  
                              plot_rep_shortfall_err_bound = FALSE, 
                              plot_rays = FALSE, 
                              plot_neg_rays_too = FALSE
                                                   ) + 
                        
                              background_grid (major = c("y"), 
                                               minor = c("y"), 
                                               size.major = 0.5, 
                                               size.minor = 0.2, 
                                               color.major = "grey85", 
                                               color.minor = "grey85") + 
  
    geom_rect (mapping=aes (xmin=0.5, xmax=1.0, ymin=-0.5, ymax=-1.0),
     color="black", alpha=0, size=0.05) + 

    geom_rect (mapping=aes (xmin=0, xmax=0.1, ymin=-0.1, ymax=0.1),
     color="black", alpha=0, size=0.1)
        
# repShortfallVsCostErrLE125inResultsPlot
# 
# ggsave (plot=repShortfallVsCostErrLE125inResultsPlot,
#         device="png",
#         filename="Figs/repShortfallVsCostErrLE125inResultsPlot.png",
#         height=5, width=5)

```

##  Summary of behavior of individual reserve selectors  

The NV group of reserve selectors all had similar behavior across all error measures and input errors.  All showed a strong bias toward underestimating cost of the optimal solution and overestimating the proportion of species reaching their targets.  They also often showed considerable magnification of input errors, with magnification on many problems approaching the theoretical upper limit of representation shortfall magnification for input errors above 4 or 5% (**Fig. 8 \textcolor{cyan}{\ref{fig:inVsRepShortfallMagLE25inResults}}**).  

While all four reserve selectors had large errors and widely varying behavior on FN-dominated problems, SA_SS had more moderated behavior on FP-dominated problems than the other selectors.  In particular, it had less extreme representation shortfall (Fig. **7 \textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**) and shortfall magnification (**Fig. 8  \textcolor{cyan}{\ref{fig:inVsRepShortfallMagLE25inResults}}**) , but at the expense of greater overestimation of optimal solution cost (**Fig. 6 \textcolor{cyan}{\ref{fig:InVsCostErrLE125inResults}}**).  However, SA_SS representation shortfalls still reached nearly 75% for some problems (**Fig. 7  \textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**).   

#  Results - Learning to predict output errors {#predictionResultsSection}

\textcolor{red}{This whole section to describe the graph features and the fitting procedures is incomplete because I'm not sure how we want to present it in a way that feels familiar to ecologists.  Same goes for the corresponding Methods and Discussion sections.}  

We now describe the results of using results shown in the previous section to learn to predict output errors for each reserve selector using four different sets of input features.  Here, we only report the results for learning to predict using multivariate linear regression.  We did also experiment with random forest and elastic net methods, but found miniscule differences in performance when compared with simple linear regression.  Much greater differences in performance were visible when we changed the set of input features given to the fitting function, so that is what we report here.  

For each set of input features, we show one grid of plots for representation shortfall and a second grid of plots for solution cost error.  In each of the grids, we show a subplot for each reserve selector.  In each subplot, the x-axis corresponds to the true value of the output error made by that reserve selector on each problem in the test set and the y-axis corresponds to the output error value predicted by the learned model for the given fitting method and reserve selector.  

As with the method comparison results, the results for learning to predict output error of the reserve selectors in the NV set were similar for both representation shortfall and solution cost error.  The prediction results for SA_SS were considerably worse than for predicting the results of any of the NV methods, especially for solution cost error prediction.  

```{r eval=FALSE}
This text may or may not be useful, but I'm going to leave it out for now.  If someone makes the same mistake that I did, then I can include it again.  

One thing to note in the plots that follow is that spread of x values differs between reserve selectors since each reserve selector had different output errors on the same problems.  This is in contrast to the previous Results section, where the x values usually represented input error, which was identical across reserve selectors for the same problem.  This means that in the previous section, the rightmost points for each reserve selector in each plot aligned with each other, while in the plots that follow, the rightmost points don't necessarily align.  For example, in **Fig. 10 \textcolor{cyan}{\ref{fig:predRepShortfallUsingPUsAndSppOnly}}** the representation shortfall true values for SA extend out beyond 0.8 while the values for SA_SS finish around 0.65.  
```

```{r settingsThatApplyToAllPredPlots}

    #  Settings that apply to all prediction plots

fitting_model_str = params$fitting_model_str    #  "rf"

display_train_as_final_pred_using_plot = 
                          params$display_train_as_final_pred_using_plot

include_median_redundancies = FALSE
```

```{r creacyanlFittingScoresDF}

#  The fitting summary bar charts for rmse and R2 need access to the 
#  rmse and R2 values that are buried in the fitting routines, so we need 
#  to create a data frame to hold all of them.  This data frame will be passed 
#  down into each call to do the fitting/predicting 
#  (fit_and_predict_output_error_using_feature_set()).  The rmse and R2 values 
#  for each input feature set rep shortfall and solution cost error are set 
#  in that routine and appended to the this all_fitting_scores_df.  Then, 
#  the data frame is returned to the calling code to be passed down in again 
#  for the next error type and input feature set.  
#  So, even though in this Rmd file it looks liek all_fitting_scores_df is not 
#  being used, it *is* being used lower down in the calling chain.  
 
all_fitting_scores_df = data.frame (test_or_train     = NULL, 
                                    fitting_model_str = NULL, 
                                    vars_used_str     = NULL, 
                                    measure_name_str  = NULL, 
                                    rs_method_name    = NULL, 
                                    rmse              = NULL, 
                                    R2                = NULL, 
                                    adj_R2            = NULL)
```

##  Predict using **PUs And Spp only** data (2 input variables)  

The primary structural attributes of problems that are reported in reserve selection studies are the number of planning units and the number of species.  Generally, no other problem attributes are reported, so we start by considering how well these two attributes allow us to predict output errors with a linear regression model.  (Input variables used are listed in **Table 3 \textcolor{cyan}{\ref{tab:makePUsAndSppOnlyVarsTable}}**.)  Results for predicting representation shortfall are shown in **Fig. 10 \textcolor{cyan}{\ref{fig:predRepShortfallUsingPUsAndSppOnly}}** and for solution cost error are shown in **Fig. 11 \textcolor{cyan}{\ref{fig:predSolCostErrorUsingPUsAndSppOnly}}**.    Prediction was poor for all reserve selectors on both reserve selection measures.  For both measures, there was large variation in predicted values at every level of true values and predictions did not even show an upward trend as true values increased. The primary difference between the reserve selectors was that predictions for the NV reserve selectors had much more variation at every level of true error value than predictions for SA_SS.  In short, problem size alone predicts very little about method performance on either representation shortfall or cost error.  

```{r setPUsAndSppOnlyParams}
vars_used_str = "PUsAndSppOnly" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

inVars = c(                                #  2 variables
            "rsp_num_spp", 
            "rsp_num_occupied_PUs")
```

&nbsp;  

```{r makePUsAndSppOnlyVarsTable}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                "number of species", 
                "number of occupied PUs" 
                )

invars_table_caption = "Input features for model based on number of PUs and species only."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingPUsAndSppOnlyFiles, include=FALSE}

###  Write Matilda files using PUs And Spp only features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildPUsAndSppOnlyTestAndTrain, include=FALSE}

###  Prep for PUsAndSppOnly learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoPUsAndSppOnlyNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Rep shortfall using PUsAndSppOnly data

```{r predictRepShortfallUsingPUsAndSppOnly, fig.cap = "\\label{fig:predRepShortfallUsingPUsAndSppOnly}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing only the number of planning units and the number of species.  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )
```

```{r testQuiet1, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
#### Example REP SHORTFALL lm model fit results using PUsAndSppOnly data

============start lm results for Gurobi ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.133340   0.004246   31.41   <2e-16 ***
rsp_num_spp          -0.212580   0.012587  -16.89   <2e-16 ***
rsp_num_occupied_PUs  0.301299   0.012587   23.94   <2e-16 ***
---
Residual standard error: 0.186 on 1916 degrees of freedom
Multiple R-squared:  0.308,  Adjusted R-squared:  0.3073 
F-statistic: 426.5 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.157388   0.004661   33.77   <2e-16 ***
rsp_num_spp          -0.247196   0.013818  -17.89   <2e-16 ***
rsp_num_occupied_PUs  0.344247   0.013818   24.91   <2e-16 ***
---
Residual standard error: 0.2042 on 1916 degrees of freedom
Multiple R-squared:  0.3178, Adjusted R-squared:  0.3171 
F-statistic: 446.2 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.143884   0.004291   33.53   <2e-16 ***
rsp_num_spp          -0.237715   0.012722  -18.68   <2e-16 ***
rsp_num_occupied_PUs  0.325865   0.012722   25.61   <2e-16 ***
---
Residual standard error: 0.188 on 1916 degrees of freedom
Multiple R-squared:  0.3232, Adjusted R-squared:  0.3224 
F-statistic: 457.4 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.054500   0.002161   25.23   <2e-16 ***
rsp_num_spp          -0.092749   0.006405  -14.48   <2e-16 ***
rsp_num_occupied_PUs  0.133111   0.006405   20.78   <2e-16 ***
---
Residual standard error: 0.09465 on 1916 degrees of freedom
Multiple R-squared:  0.2559, Adjusted R-squared:  0.2551 
F-statistic: 329.4 on 2 and 1916 DF,  p-value: < 2.2e-16

```

### Solution cost error using PUsAndSppOnly data 

```{r predictSolCostErrorUsingPUsAndSppOnly, fig.cap = "\\label{fig:predSolCostErrorUsingPUsAndSppOnly}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing only the number of planning units and the number of species.  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin"}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )
```

```{r testQuiet2, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
#### Example SOLUTION COST ERROR lm model fit results using PUsAndSppOnly data

============start lm results for Gurobi ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.078268   0.006052   12.93   <2e-16 ***
rsp_num_spp           0.482875   0.017942   26.91   <2e-16 ***
rsp_num_occupied_PUs -0.565191   0.017942  -31.50   <2e-16 ***
---
Residual standard error: 0.2651 on 1916 degrees of freedom
Multiple R-squared:  0.3558, Adjusted R-squared:  0.3552 
F-statistic: 529.2 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.096163   0.005744   16.74   <2e-16 ***
rsp_num_spp           0.462184   0.017030   27.14   <2e-16 ***
rsp_num_occupied_PUs -0.530818   0.017030  -31.17   <2e-16 ***
---
Residual standard error: 0.2516 on 1916 degrees of freedom
Multiple R-squared:  0.3461, Adjusted R-squared:  0.3454 
F-statistic: 507.1 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.139418   0.006031   23.12   <2e-16 ***
rsp_num_spp           0.459830   0.017879   25.72   <2e-16 ***
rsp_num_occupied_PUs -0.572686   0.017879  -32.03   <2e-16 ***
---
Residual standard error: 0.2642 on 1916 degrees of freedom
Multiple R-squared:  0.3848, Adjusted R-squared:  0.3842 
F-statistic: 599.3 on 2 and 1916 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)           0.337808   0.006951   48.60   <2e-16 ***
rsp_num_spp           0.321192   0.020607   15.59   <2e-16 ***
rsp_num_occupied_PUs -0.273832   0.020607  -13.29   <2e-16 ***
---
Residual standard error: 0.3045 on 1916 degrees of freedom
Multiple R-squared:  0.1194, Adjusted R-squared:  0.1185 
F-statistic: 129.9 on 2 and 1916 DF,  p-value: < 2.2e-16

```

##  Predict using **PROB SIZE and DENSITY** data (5 input variables)    

Measuring only the number of species and PUs ignores any information about the occurrence and co-occurrence patterns of the species on the PUs.  Perhaps the simplest next step is to compute two simple measures of the relative density of species occurrences based on the adjacency matrix that describes all species occurrences on PUs in the problem.  (Input variables used are listed in **Table 4 \textcolor{cyan}{\ref{tab:makeProbSizeAndDensityVarsTable}}**.)  The two measures are the problem's realized fraction of all edges possible were all species on all PUs, and the average number of links per node in the bipartite network representation of the problem.  Both use the total number of realized edges/occurrences in the problem, but the first computes the density with respect to the number of species times the number of PUs while the second computes the density with respect to the sum of the number of bipartite network nodes, where each species is a node and each PU is a node.  \textcolor{red}{[What was the Dormann et al name for the links per node variable and what was its effect in that paper?  Need to very briefly mention that here.]}  In addition to the two density measures, we also add a feature for the product of the number of species and number of PUs as a measure of total possible size of the problem.  We don't need to add the sum of number of PUs and species since that is a linear combination of the two already included size variables.  

The inclusion of the three extra size and density variables produces large gains in our ability to predict representation shortfall across all four reserve selectors (**Fig. 12 \textcolor{cyan}{\ref{fig:predRepShortfallUsingProbSizeAndDensity}}**).  The rmse for the NV reserve selectors drops by more than 60% and for rmse for SA_SS drops by 44%.  Predictions for all reserve selectors now show an upward trend that is much closer to the 1:1 correct prediction line instead of the generally flat predictions shown for number of species and PUs alone.  In spite of showing a better trend, there is a noticeable overestimation of representation shortfall NV reserve selectors for true values less than approximately 0.4 and underestimation for true values in the range of roughly 0.4 through 0.65.  For SA_SS, overestimation occurs up to approximately 0.15 and underestimation for true values greater than around 0.25.

The gains in predictive power for solution cost error  (**Fig. 13 \textcolor{cyan}{\ref{fig:predSolCostErrorUsingProbSizeAndDensity}}**) are noticeable but not as powerful as those for representation shortfall.  For the NV reserve selectors, the rmse values for the predictions drop by approximately 35%, but for SA_SS, there is essentially no change compared to predictions using only the number of species and PUs.  Also noticeable is that the improvements for the NV reserve selectors are only for FP-dominated problems.  Prediction on FN-dominated problems appears to be only changed in terms of reducing the variation at any point along the true-values axis.  

```{r setProbSizeAndDensityParams}

#  "All prob size" is probably not a good name for this.  
#  Most papers give the number of species and the number of PUs, which are 
#  commonly interpreted as problem size. 
#  However, there are other variables that can be computed just from arithmetic 
#  combinations of those 2 variables and they relate to things that are more 
#  like measures of density and potential for density.  
#  I'm lumping them all together here to represent everything that can be 
#  derived from just knowing the number of species and PUs and the marginal 
#  totals of the adjacency matrix.

vars_used_str = "ProbSizeAndDensity" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

inVars = c(                               #  5 variables
            "rsp_num_spp"
            , "rsp_num_occupied_PUs"
            
# #         #  Not sure whether to lump these in with problem size variables or not.
# #         #  It's more like they're density variables that can be computed from simple 
# #         #  counts without any notion of a graph being involved, but they 
# #         #  don't fit with the notion of problems get harder as they include 
# #         #  more spp and PUs, which is what most papers report.
            
            , "links_per_PUsAndSpp"
            , "edge_frac_of_possible"
            , "sppPUprod"
          )
```

&nbsp;  

```{r makeProbSizeAndDensityVarsTable}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                "number of species", 
                "number of occupied PUs", 
                "average number of links per node, where species and PUs are nodes", 
                "proportion of links made out of all possible links between species and PUs", 
                "product of number of species and number of PUs, i.e, size of adjacency matrix"
                )

invars_table_caption = "Input features for model based on all variables related to problem size and link density."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingProbSizeAndDensityFiles, include=FALSE}

###  Write Matilda files using ProbSizeAndDensity features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildProbSizeAndDensityTestAndTrain, include=FALSE}

###  Prep for ProbSizeAndDensity learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoProbSizeAndDensityNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Rep shortfall using ProbSizeAndDensity data

```{r predictRepShortfallUsingProbSizeAndDensity, fig.cap = "\\label{fig:predRepShortfallUsingProbSizeAndDensity}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing problem size and density derived variables (listed in Table 4 \\textcolor{cyan}{\\ref{tab:makeProbSizeAndDensityVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet3, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example REP SHORTFALL lm model fit using ProbSizeAndDensity data

============start lm results for Gurobi ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.133340   0.001658  80.442   <2e-16 ***
rsp_num_spp           -0.284296   0.019012 -14.954   <2e-16 ***
rsp_num_occupied_PUs   0.011496   0.015687   0.733    0.464    
links_per_PUsAndSpp   -0.409320   0.013785 -29.694   <2e-16 ***
edge_frac_of_possible  0.599878   0.012012  49.942   <2e-16 ***
sppPUprod              0.614418   0.028506  21.554   <2e-16 ***
---
Residual standard error: 0.07261 on 1913 degrees of freedom
Multiple R-squared:  0.8947, Adjusted R-squared:  0.8944 
F-statistic:  3251 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.157388   0.001627  96.736   <2e-16 ***
rsp_num_spp           -0.338030   0.018661 -18.115   <2e-16 ***
rsp_num_occupied_PUs  -0.036234   0.015398  -2.353   0.0187 *  
links_per_PUsAndSpp   -0.358845   0.013530 -26.522   <2e-16 ***
edge_frac_of_possible  0.587223   0.011790  49.808   <2e-16 ***
sppPUprod              0.704471   0.027980  25.178   <2e-16 ***
---
Residual standard error: 0.07127 on 1913 degrees of freedom
Multiple R-squared:  0.917,  Adjusted R-squared:  0.9168 
F-statistic:  4227 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.143884   0.001529  94.125   <2e-16 ***
rsp_num_spp           -0.304798   0.017533 -17.385   <2e-16 ***
rsp_num_occupied_PUs   0.006067   0.014467   0.419    0.675    
links_per_PUsAndSpp   -0.370859   0.012712 -29.173   <2e-16 ***
edge_frac_of_possible  0.573497   0.011077  51.773   <2e-16 ***
sppPUprod              0.621736   0.026289  23.650   <2e-16 ***
---
Residual standard error: 0.06696 on 1913 degrees of freedom
Multiple R-squared:  0.9143, Adjusted R-squared:  0.914 
F-statistic:  4079 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.054500   0.001042  52.306  < 2e-16 ***
rsp_num_spp           -0.064315   0.011951  -5.382 8.29e-08 ***
rsp_num_occupied_PUs   0.099782   0.009861  10.119  < 2e-16 ***
links_per_PUsAndSpp   -0.328511   0.008665 -37.912  < 2e-16 ***
edge_frac_of_possible  0.393645   0.007550  52.135  < 2e-16 ***
sppPUprod              0.191970   0.017919  10.713  < 2e-16 ***
---
Residual standard error: 0.04564 on 1913 degrees of freedom
Multiple R-squared:  0.8272, Adjusted R-squared:  0.8267 
F-statistic:  1832 on 5 and 1913 DF,  p-value: < 2.2e-16
```

### Solution cost error using ProbSizeAndDensity data 

```{r predictSolCostErrorUsingProbSizeAndDensity, fig.cap = "\\label{fig:predSolCostErrorUsingProbSizeAndDensity}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing problem size and density derived variables (listed in Table 4 \\textcolor{cyan}{\\ref{tab:makeProbSizeAndDensityVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet4, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example SOLUTION COST ERROR lm model fit using ProbSizeAndDensity data

============start lm results for Gurobi ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.078268   0.003754  20.849  < 2e-16 ***
rsp_num_spp            0.704441   0.043058  16.360  < 2e-16 ***
rsp_num_occupied_PUs   0.095495   0.035528   2.688  0.00725 ** 
links_per_PUsAndSpp   -0.010103   0.031220  -0.324  0.74628    
edge_frac_of_possible -0.293170   0.027204 -10.777  < 2e-16 ***
sppPUprod             -0.932602   0.064561 -14.445  < 2e-16 ***
---
Residual standard error: 0.1645 on 1913 degrees of freedom
Multiple R-squared:  0.7525, Adjusted R-squared:  0.7519 
F-statistic:  1163 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.096163   0.003784  25.411   <2e-16 ***
rsp_num_spp            0.626329   0.043404  14.430   <2e-16 ***
rsp_num_occupied_PUs   0.035207   0.035814   0.983    0.326    
links_per_PUsAndSpp    0.027446   0.031471   0.872    0.383    
edge_frac_of_possible -0.297238   0.027423 -10.839   <2e-16 ***
sppPUprod             -0.794643   0.065080 -12.210   <2e-16 ***
---
Residual standard error: 0.1658 on 1913 degrees of freedom
Multiple R-squared:  0.7167, Adjusted R-squared:  0.7159 
F-statistic: 967.7 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.139418   0.003921  35.560   <2e-16 ***
rsp_num_spp            0.659347   0.044967  14.663   <2e-16 ***
rsp_num_occupied_PUs   0.028600   0.037104   0.771   0.4409    
links_per_PUsAndSpp    0.059516   0.032604   1.825   0.0681 .  
edge_frac_of_possible -0.344632   0.028410 -12.131   <2e-16 ***
sppPUprod             -0.884847   0.067424 -13.124   <2e-16 ***
---
Residual standard error: 0.1717 on 1913 degrees of freedom
Multiple R-squared:  0.7404, Adjusted R-squared:  0.7398 
F-statistic:  1091 on 5 and 1913 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)            0.337808   0.006808  49.621  < 2e-16 ***
rsp_num_spp            0.397071   0.078081   5.085 4.03e-07 ***
rsp_num_occupied_PUs  -0.019592   0.064428  -0.304  0.76109    
links_per_PUsAndSpp   -0.148381   0.056614  -2.621  0.00884 ** 
edge_frac_of_possible  0.047910   0.049332   0.971  0.33158    
sppPUprod             -0.269822   0.117075  -2.305  0.02129 *  
---
Residual standard error: 0.2982 on 1913 degrees of freedom
Multiple R-squared:  0.1567, Adjusted R-squared:  0.1545 
F-statistic: 71.08 on 5 and 1913 DF,  p-value: < 2.2e-16
```

##  Predict using **nonLatapyGRAPH** data (27 input variables)

Extending the exploration of the structure of the reserve selection problem, we next compute many more complexity measures over the adjacency matrix of the bipartite graph representing the problems and use those as inputs in the prediction of reserve selection errors.  (Input variables used are listed in **Table 5 \textcolor{cyan}{\ref{tab:makeNonLatapyGraphVarsTable}}**.)  Here, there's continuing improvement in prediction accuracy compared to the ProbSizeAndDensity variable set, but not nearly the step up provided by ProbSizeAndDensity over using only the number of species and PUs.  

For representation shortfall, rmse values for all of the reserve selectors have improved by only 0.01 or 0.02 compared to ProbSizeAndDensity predictions (**Fig. 14 \textcolor{cyan}{\ref{fig:predRepShortfallUsingNonLatapyGRAPH}}**).  For the NV reserve selectors, there has also been a small reduction in the underestimation of shortfall in the middle range of true values.  

Rmse values for solution cost error predictions on NV reserve selectors have improved by 0.04 (25% reduction) while SA_SS predictions have improved from an rmse value of 0.31 to a new value of 0.25 (19% reduction) (**Fig. 15 \textcolor{cyan}{\ref{fig:predSolCostErrorUsingNonLatapyGRAPH}}**).  The most notable change for all reserve selectors is that predictions for FN-dominated problems are no longer effectively flat.  While they are far from a perfect fit to the true values, the predicted values now have an upward trend that is closer to the 1:1 line.  However, for NV reserve selectors, there is still considerable underestimation of solution cost error as true solution cost error increases.  While predictions for SA_SS have improved somewhat, they are still low quality, showing large variation at all true values, with overestimation for small true error values and understimation for large true error values.  

```{r setNonLatapyGraphParams}
vars_used_str = "nonLatapyGraph" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

include_median_redundancies=FALSE

#inVars = c("rsp_num_occupied_PUs", "rsp_num_spp")
inVars = c(                                                  #  27 variables                             
#                                         "ig_lcctop" ,  
#                                         "ig_lccbottom" ,  
#                                         "ig_distop" ,  
#                                         "ig_disbottom" ,  
#                                         "ig_cctop" ,  
#                                         "ig_ccbottom" ,  
#                                         "ig_cclowdottop" ,  
#                                         "ig_cclowdotbottom" ,  
#                                         "ig_cctopdottop" ,  
#                                         "ig_cctopdotbottom" ,  
#                                         
#                                         "ig_mean_bottom_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_bottom_bg_redundancy" ,  
#                                         "ig_mean_top_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_top_bg_redundancy" ,  
                    
                                    #  bipartite package metrics
#2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
                            
                                        "web_asymmetry" , 
                                        "links_per_PUsAndSpp" , 
                                        "cluster_coefficient" , 
# "weighted_NODF" , 
# "interaction_strength_asymmetry" , 
                                        "specialisation_asymmetry" , 
                                        "linkage_density" , 
                                        "weighted_connectance" , 
                                        "Shannon_diversity" , 
                                        "interaction_evenness" , 
                                        "Alatalo_interaction_evenness" , 
                        
                                        "mean.number.of.shared.partners.PUs" , 
                                        "mean.number.of.shared.partners.Spp" , 
                                        "cluster.coefficient.PUs" , 
                                        "cluster.coefficient.Spp" , 
                                        "niche.overlap.PUs" , 
                                        "niche.overlap.Spp" , 
                                        "togetherness.PUs" , 
                                        "togetherness.Spp" , 
                                        "C.score.PUs" , 
                                        "C.score.Spp" , 
                                        "V.ratio.PUs" , 
                                        "V.ratio.Spp" , 
                                        "functional.complementarity.PUs" , 
                                        "functional.complementarity.Spp" , 
                                        "partner.diversity.PUs" , 
                                        "partner.diversity.Spp" , 
                                        "generality.PUs" , 
                                        "vulnerability.Spp"
                         )
```

&nbsp;  

```{r makeNonLatapyGraphVarsTable}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

in_var_defns = c(
                # "number of species", 
                # "number of occupied PUs", 
                # "average number of links per node, where species and PUs are nodes", 
                # "proportion of links made out of all possible links between species and PUs", 
                # "product of number of species and number of PUs, i.e, size of adjacency matrix", 
                
#                                         "ig_lcctop" ,  
#                                         "ig_lccbottom" ,  
#                                         "ig_distop" ,  
#                                         "ig_disbottom" ,  
#                                         "ig_cctop" ,  
#                                         "ig_ccbottom" ,  
#                                         "ig_cclowdottop" ,  
#                                         "ig_cclowdotbottom" ,  
#                                         "ig_cctopdottop" ,  
#                                         "ig_cctopdotbottom" ,  
#                                         
#                                         "ig_mean_bottom_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_bottom_bg_redundancy" ,  
#                                         "ig_mean_top_bg_redundancy" ,  
# # 0 variance for FNs                                        "ig_median_top_bg_redundancy" ,  

  
  
                    #  bipartite package metrics

                                          #2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
                          #"connectance",   #  Shouldn't this be removed?  Same as edge_frac_of_possible?
                          #                    "SHOULD REMOVE FROM EVERYTHING SET?  bip_connectance:  Realised proportion of possible links (Dunne et al. 2002): num_links/(num_spp*num_PUs). In reserve selection, this is the proportion of the occupancy matrix (bpm) that contains 1s, i.e., 'positives'.  Same as ig_bidens (renamed to ig_realized_frac_of_all_possible_links?) and edge_frac_of_possible.  This is the standardised number of species combinations often used in co-occurrence analyses (Gotelli & Graves 1996).", 

                "Balance between numbers in the two levels: ...",    #  "positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2", 
                "average number of links per node, where species ...",    #  "and PUs are nodes", 

                        
                "Mean, across all nodes of the number of realized ...",    #  "links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
# "weighted_NODF" , 
# "interaction_strength_asymmetry" , 
                "Asymmetry (PU level vs. spp level) of specialisation ...",    #  "now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
                "Marginal totals-weighted diversity of interactions ...",    #  "per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
                "Linkage density divided by number of nodes in the n...",    #  "etwork (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
                "Shannon's diversity of interactions (i.e. network entries)." , 
                "Shannon's evenness for the web entries." , 
                "A different measure for web entry evenness, as ...",    #  "proposed by Muller et al. (1999)." , 

#"number.of.Spp",  #  Shouldn't this be removed?  Same as rsp_num_spp?
                    
                "Based on the distance matrix between PUs counting ...",    #  "the number of spp that both PUs interact with." , 
                "Based on the distance matrix between spp counting ...",    #  "the number of PUs that both spp interact with." , 
                "Mean, across all PUs of the number of realized links ...",    #  "divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
                "Mean, across all spp of the number of realized links ...",    #  "divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
                "Mean similarity in interaction pattern between PUs, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "Mean similarity in interaction pattern between spp, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level" , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level." , 
                "UNCLEAR - Functional complementarity? for a ...",    #  "given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "UNCLEAR - Functional complementarity? for a given ...",    #  "level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for PUs." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for spp." , 
                "Mean number of spp per PU." ,    #  REMOVE, since it duplicates rsp_num_spp_per_PU
                "Mean number of PUs per spp." 
      # 
      #   
      #                       
      #                                     #  bipartite package metrics
      # #2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
      #                             
      #                                         "Balance between numbers in the two levels: positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2" , 
      #                                         "[links_per_species in bipartite package].  Mean number of links per node (qualitative): sum of links divided by number of nodes." , 
      #                                         "Mean, across all nodes of the number of realized links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
      # # "weighted_NODF" , 
      # # "interaction_strength_asymmetry" , 
      #                                         "Asymmetry (PU level vs. spp level) of specialisation now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
      #                                         "Marginal totals-weighted diversity of interactions per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
      #                                         "Linkage density divided by number of nodes in the network (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
      #                                         "Shannon's diversity of interactions (i.e. network entries)." , 
      #                                         "Shannon's evenness for the web entries." , 
      #                                         "A different measure for web entry evenness, as proposed by Muller et al. (1999)." , 
      #                         
      #                                         "Based on the distance matrix between PUs counting the number of spp that both PUs interact with." , 
      #                                         "Based on the distance matrix between spp counting the number of PUs that both spp interact with." , 
      #                                         "Mean, across all PUs of the number of realized links divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
      #                                         "Mean, across all spp of the number of realized links divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
      #                                         "Mean similarity in interaction pattern between PUs, calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
      #                                         "Mean similarity in interaction pattern between spp, calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
      #                                         "UNCLEAR - Mean number of co-occupancies across all species-combinations." , 
      #                                         "UNCLEAR - Mean number of co-occupancies across all species-combinations." , 
      #                                         "UNCLEAR -  (Normalised) mean number of checkerboard combinations across all species of the level." , 
      #                                         "UNCLEAR -  (Normalised) mean number of checkerboard combinations across all species of the level." , 
      #                                         "UNCLEAR - Variance-ratio of species numbers to interaction numbers within species of a level" , 
      #                                         "UNCLEAR - Variance-ratio of species numbers to interaction numbers within species of a level." , 
      #                                         "UNCLEAR - Functional complementarity? for a given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
      #                                         "UNCLEAR - Functional complementarity? for a given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
      #                                         "Mean Shannon diversity of the number of interactions for PUs." , 
      #                                         "Mean Shannon diversity of the number of interactions for spp." , 
      #                                         "Mean number of spp per PU." , 
      #                                         "Mean number of PUs per spp."
                )

invars_table_caption = "Input features for model based on non-Latapy graph variables."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingNonLatapyGraphFiles, include=FALSE}

###  Write Matilda files using GRAPH features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildNonLatapyGraphTestAndTrain, include=FALSE}

###  Prep for GRAPH learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoNonLatapyGRAPHNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Rep shortfall using NonLatapyGRAPH data

```{r predictRepShortfallUsingNonLatapyGRAPH, fig.cap = "\\label{fig:predRepShortfallUsingNonLatapyGRAPH}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing all nonLatapy bipartite graph variables (listed in Table 5 \\textcolor{cyan}{\\ref{tab:makeNonLatapyGraphVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet5, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example REP SHORTFALL lm model fit using NonLatapyGRAPH data

============start lm results for Gurobi ===================
Coefficients:
                                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.133340   0.001294 103.026  < 2e-16 ***
web_asymmetry                      -0.261740   0.046196  -5.666 1.69e-08 ***
links_per_PUsAndSpp                -0.440172   0.140359  -3.136 0.001739 ** 
cluster_coefficient                 0.009499   0.011091   0.856 0.391851    
specialisation_asymmetry            0.031632   0.004756   6.650 3.82e-11 ***
linkage_density                    -0.049920   0.077078  -0.648 0.517288    
weighted_connectance                1.250464   0.265697   4.706 2.70e-06 ***
Shannon_diversity                   0.088706   0.048740   1.820 0.068920 .  
interaction_evenness               -0.094218   0.131580  -0.716 0.474049    
Alatalo_interaction_evenness        0.001890   0.001372   1.377 0.168586    
mean.number.of.shared.partners.PUs -4.821305   0.484923  -9.942  < 2e-16 ***
mean.number.of.shared.partners.Spp  0.550740   0.196189   2.807 0.005049 ** 
cluster.coefficient.PUs            -0.928727   0.222974  -4.165 3.25e-05 ***
cluster.coefficient.Spp             3.166402   0.345133   9.174  < 2e-16 ***
niche.overlap.PUs                   0.315174   0.072130   4.370 1.31e-05 ***
niche.overlap.Spp                  -0.249571   0.112388  -2.221 0.026495 *  
togetherness.PUs                    0.133586   0.031264   4.273 2.03e-05 ***
togetherness.Spp                    0.044563   0.015963   2.792 0.005296 ** 
C.score.PUs                         0.281229   0.047205   5.958 3.05e-09 ***
C.score.Spp                        -0.353427   0.089459  -3.951 8.08e-05 ***
V.ratio.PUs                        -0.117190   0.011546 -10.150  < 2e-16 ***
V.ratio.Spp                        -0.060867   0.010646  -5.718 1.25e-08 ***
functional.complementarity.PUs      0.958301   0.084803  11.300  < 2e-16 ***
functional.complementarity.Spp      0.770573   0.140464   5.486 4.67e-08 ***
partner.diversity.PUs              -0.174163   0.034774  -5.008 6.00e-07 ***
partner.diversity.Spp               0.867906   0.071034  12.218  < 2e-16 ***
generality.PUs                      0.329037   0.035200   9.348  < 2e-16 ***
vulnerability.Spp                  -0.605699   0.161106  -3.760 0.000175 ***
---
Residual standard error: 0.0567 on 1891 degrees of freedom
Multiple R-squared:  0.9365, Adjusted R-squared:  0.9356 
F-statistic:  1034 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.1573880  0.0012195 129.055  < 2e-16 ***
web_asymmetry                      -0.3337188  0.0435295  -7.667 2.81e-14 ***
links_per_PUsAndSpp                -0.2906577  0.1322580  -2.198 0.028094 *  
cluster_coefficient                 0.0186839  0.0104509   1.788 0.073972 .  
specialisation_asymmetry            0.0307600  0.0044819   6.863 9.10e-12 ***
linkage_density                    -0.1017984  0.0726291  -1.402 0.161193    
weighted_connectance                0.9607997  0.2503628   3.838 0.000128 ***
Shannon_diversity                  -0.0120148  0.0459269  -0.262 0.793652    
interaction_evenness               -0.0392968  0.1239863  -0.317 0.751320    
Alatalo_interaction_evenness        0.0001074  0.0012932   0.083 0.933843    
mean.number.of.shared.partners.PUs -4.4734810  0.4569362  -9.790  < 2e-16 ***
mean.number.of.shared.partners.Spp  1.0594179  0.1848659   5.731 1.16e-08 ***
cluster.coefficient.PUs            -1.0216120  0.2101051  -4.862 1.26e-06 ***
cluster.coefficient.Spp             2.4675613  0.3252137   7.588 5.09e-14 ***
niche.overlap.PUs                   0.2280435  0.0679673   3.355 0.000809 ***
niche.overlap.Spp                  -0.1495411  0.1059020  -1.412 0.158094    
togetherness.PUs                    0.1638675  0.0294593   5.563 3.04e-08 ***
togetherness.Spp                    0.0471188  0.0150415   3.133 0.001759 ** 
C.score.PUs                         0.2126580  0.0444809   4.781 1.88e-06 ***
C.score.Spp                        -0.3931188  0.0842963  -4.664 3.33e-06 ***
V.ratio.PUs                        -0.1544428  0.0108799 -14.195  < 2e-16 ***
V.ratio.Spp                        -0.0456878  0.0100312  -4.555 5.58e-06 ***
functional.complementarity.PUs      0.7914759  0.0799084   9.905  < 2e-16 ***
functional.complementarity.Spp      0.4962578  0.1323577   3.749 0.000183 ***
partner.diversity.PUs              -0.1875568  0.0327673  -5.724 1.21e-08 ***
partner.diversity.Spp               0.5546598  0.0669348   8.287  < 2e-16 ***
generality.PUs                      0.3172049  0.0331689   9.563  < 2e-16 ***
vulnerability.Spp                  -0.0955974  0.1518082  -0.630 0.528951    
---
Residual standard error: 0.05342 on 1891 degrees of freedom
Multiple R-squared:  0.9539, Adjusted R-squared:  0.9532 
F-statistic:  1449 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.143884   0.001121 128.325  < 2e-16 ***
web_asymmetry                      -0.302746   0.040021  -7.565 6.03e-14 ***
links_per_PUsAndSpp                -0.503819   0.121598  -4.143 3.57e-05 ***
cluster_coefficient                 0.011838   0.009609   1.232  0.21811    
specialisation_asymmetry            0.028230   0.004121   6.851 9.90e-12 ***
linkage_density                    -0.098135   0.066775  -1.470  0.14183    
weighted_connectance                1.234326   0.230184   5.362 9.22e-08 ***
Shannon_diversity                  -0.011435   0.042225  -0.271  0.78656    
interaction_evenness                0.073048   0.113993   0.641  0.52173    
Alatalo_interaction_evenness        0.001050   0.001189   0.883  0.37719    
mean.number.of.shared.partners.PUs -4.723468   0.420108 -11.243  < 2e-16 ***
mean.number.of.shared.partners.Spp  0.869191   0.169966   5.114 3.47e-07 ***
cluster.coefficient.PUs            -1.140048   0.193171  -5.902 4.25e-09 ***
cluster.coefficient.Spp             2.779794   0.299002   9.297  < 2e-16 ***
niche.overlap.PUs                   0.302316   0.062489   4.838 1.42e-06 ***
niche.overlap.Spp                  -0.196914   0.097367  -2.022  0.04328 *  
togetherness.PUs                    0.126107   0.027085   4.656 3.45e-06 ***
togetherness.Spp                    0.042807   0.013829   3.095  0.00199 ** 
C.score.PUs                         0.242666   0.040896   5.934 3.51e-09 ***
C.score.Spp                        -0.394101   0.077502  -5.085 4.04e-07 ***
V.ratio.PUs                        -0.123696   0.010003 -12.366  < 2e-16 ***
V.ratio.Spp                        -0.058717   0.009223  -6.367 2.42e-10 ***
functional.complementarity.PUs      0.901163   0.073468  12.266  < 2e-16 ***
functional.complementarity.Spp      0.628037   0.121690   5.161 2.71e-07 ***
partner.diversity.PUs              -0.139574   0.030126  -4.633 3.85e-06 ***
partner.diversity.Spp               0.717028   0.061540  11.651  < 2e-16 ***
generality.PUs                      0.313306   0.030496  10.274  < 2e-16 ***
vulnerability.Spp                  -0.297341   0.139573  -2.130  0.03327 *  
---
Residual standard error: 0.04912 on 1891 degrees of freedom
Multiple R-squared:  0.9544, Adjusted R-squared:  0.9537 
F-statistic:  1466 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.0545003  0.0008789  62.011  < 2e-16 ***
web_asymmetry                       0.0444831  0.0313704   1.418 0.156357    
links_per_PUsAndSpp                -0.5334270  0.0953144  -5.596 2.51e-08 ***
cluster_coefficient                 0.0011349  0.0075317   0.151 0.880240    
specialisation_asymmetry            0.0095595  0.0032300   2.960 0.003118 ** 
linkage_density                     0.2083017  0.0523417   3.980 7.16e-05 ***
weighted_connectance                0.4357991  0.1804291   2.415 0.015814 *  
Shannon_diversity                  -0.0207634  0.0330981  -0.627 0.530521    
interaction_evenness               -0.1439390  0.0893532  -1.611 0.107369    
Alatalo_interaction_evenness        0.0018343  0.0009320   1.968 0.049188 *  
mean.number.of.shared.partners.PUs -1.5012321  0.3293004  -4.559 5.47e-06 ***
mean.number.of.shared.partners.Spp -0.1665143  0.1332274  -1.250 0.211509    
cluster.coefficient.PUs            -0.2252040  0.1514166  -1.487 0.137099    
cluster.coefficient.Spp             1.6224132  0.2343719   6.922 6.06e-12 ***
niche.overlap.PUs                   0.2680558  0.0489820   5.473 5.03e-08 ***
niche.overlap.Spp                  -0.2827167  0.0763205  -3.704 0.000218 ***
togetherness.PUs                    0.0384308  0.0212305   1.810 0.070427 .  
togetherness.Spp                    0.0036306  0.0108400   0.335 0.737716    
C.score.PUs                         0.0832230  0.0320561   2.596 0.009500 ** 
C.score.Spp                        -0.1778363  0.0607498  -2.927 0.003459 ** 
V.ratio.PUs                         0.0109687  0.0078408   1.399 0.162003    
V.ratio.Spp                        -0.0390255  0.0072292  -5.398 7.58e-08 ***
functional.complementarity.PUs      0.5700088  0.0575876   9.898  < 2e-16 ***
functional.complementarity.Spp      0.5176640  0.0953862   5.427 6.47e-08 ***
partner.diversity.PUs               0.1056201  0.0236145   4.473 8.18e-06 ***
partner.diversity.Spp               0.7270548  0.0482380  15.072  < 2e-16 ***
generality.PUs                      0.0014604  0.0239039   0.061 0.951289    
vulnerability.Spp                  -1.0787276  0.1094037  -9.860  < 2e-16 ***
---
Residual standard error: 0.0385 on 1891 degrees of freedom
Multiple R-squared:  0.8785, Adjusted R-squared:  0.8767 
F-statistic: 506.3 on 27 and 1891 DF,  p-value: < 2.2e-16
```

### Solution cost error using NonLatapyGRAPH data 

```{r predictSolCostErrorUsingNonLatapyGRAPH, fig.cap = "\\label{fig:predSolCostErrorUsingNonLatapyGRAPH}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing all nonLatapy bipartite graph variables (listed in Table 5 \\textcolor{cyan}{\\ref{tab:makeNonLatapyGraphVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet6, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example SOLUTION COST ERROR lm model fit using NonLatapyGRAPH data

============start lm results for Gurobi ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.0782680  0.0029699  26.354  < 2e-16 ***
web_asymmetry                      -0.5456610  0.1060047  -5.148 2.91e-07 ***
links_per_PUsAndSpp                -2.6919161  0.3220798  -8.358  < 2e-16 ***
cluster_coefficient                 0.0783041  0.0254505   3.077  0.00212 ** 
specialisation_asymmetry           -0.0829379  0.0109145  -7.599 4.67e-14 ***
linkage_density                    -0.3321130  0.1768693  -1.878  0.06057 .  
weighted_connectance                3.3993465  0.6096932   5.576 2.82e-08 ***
Shannon_diversity                   0.4377701  0.1118429   3.914 9.39e-05 ***
interaction_evenness               -0.4686950  0.3019362  -1.552  0.12076    
Alatalo_interaction_evenness       -0.0007794  0.0031493  -0.247  0.80457    
mean.number.of.shared.partners.PUs -2.4519501  1.1127486  -2.204  0.02768 *  
mean.number.of.shared.partners.Spp  0.5242599  0.4501927   1.165  0.24436    
cluster.coefficient.PUs             1.2546801  0.5116561   2.452  0.01429 *  
cluster.coefficient.Spp             0.3204726  0.7919729   0.405  0.68578    
niche.overlap.PUs                   1.2551450  0.1655166   7.583 5.25e-14 ***
niche.overlap.Spp                  -5.0783466  0.2578967 -19.691  < 2e-16 ***
togetherness.PUs                    0.1029219  0.0717405   1.435  0.15155    
togetherness.Spp                   -0.0345511  0.0366298  -0.943  0.34567    
C.score.PUs                         0.7164908  0.1083217   6.614 4.84e-11 ***
C.score.Spp                        -1.3798843  0.2052815  -6.722 2.37e-11 ***
V.ratio.PUs                         0.0061341  0.0264952   0.232  0.81694    
V.ratio.Spp                        -0.0602540  0.0244285  -2.467  0.01373 *  
functional.complementarity.PUs      1.7688424  0.1945959   9.090  < 2e-16 ***
functional.complementarity.Spp     -1.6181598  0.3223225  -5.020 5.64e-07 ***
partner.diversity.PUs               0.0423661  0.0797963   0.531  0.59553    
partner.diversity.Spp               0.8822429  0.1630023   5.412 7.01e-08 ***
generality.PUs                      1.0981565  0.0807743  13.595  < 2e-16 ***
vulnerability.Spp                   0.9399844  0.3696893   2.543  0.01108 *  
---
Residual standard error: 0.1301 on 1891 degrees of freedom
Multiple R-squared:  0.8469, Adjusted R-squared:  0.8447 
F-statistic: 387.4 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.0961627  0.0029946  32.112  < 2e-16 ***
web_asymmetry                      -0.5864201  0.1068886  -5.486 4.66e-08 ***
links_per_PUsAndSpp                -2.5586089  0.3247656  -7.878 5.55e-15 ***
cluster_coefficient                 0.0846858  0.0256628   3.300 0.000985 ***
specialisation_asymmetry           -0.0824011  0.0110055  -7.487 1.07e-13 ***
linkage_density                    -0.3363219  0.1783441  -1.886 0.059475 .  
weighted_connectance                3.3235521  0.6147773   5.406 7.26e-08 ***
Shannon_diversity                   0.3934821  0.1127755   3.489 0.000496 ***
interaction_evenness               -0.4793666  0.3044540  -1.575 0.115536    
Alatalo_interaction_evenness       -0.0007695  0.0031755  -0.242 0.808552    
mean.number.of.shared.partners.PUs -2.4760888  1.1220276  -2.207 0.027448 *  
mean.number.of.shared.partners.Spp  0.7635050  0.4539467   1.682 0.092748 .  
cluster.coefficient.PUs             1.1440396  0.5159228   2.217 0.026710 *  
cluster.coefficient.Spp             0.1425583  0.7985770   0.179 0.858337    
niche.overlap.PUs                   1.2357066  0.1668968   7.404 1.98e-13 ***
niche.overlap.Spp                  -5.0537166  0.2600473 -19.434  < 2e-16 ***
togetherness.PUs                    0.1243337  0.0723387   1.719 0.085820 .  
togetherness.Spp                   -0.0316669  0.0369352  -0.857 0.391353    
C.score.PUs                         0.7414911  0.1092249   6.789 1.51e-11 ***
C.score.Spp                        -1.4514726  0.2069933  -7.012 3.25e-12 ***
V.ratio.PUs                        -0.0303906  0.0267161  -1.138 0.255459    
V.ratio.Spp                        -0.0552210  0.0246322  -2.242 0.025088 *  
functional.complementarity.PUs      1.7190265  0.1962186   8.761  < 2e-16 ***
functional.complementarity.Spp     -1.7123073  0.3250103  -5.268 1.53e-07 ***
partner.diversity.PUs              -0.0414516  0.0804617  -0.515 0.606493    
partner.diversity.Spp               0.7277690  0.1643615   4.428 1.01e-05 ***
generality.PUs                      1.1575503  0.0814478  14.212  < 2e-16 ***
vulnerability.Spp                   1.1587481  0.3727720   3.108 0.001909 ** 
---
Residual standard error: 0.1312 on 1891 degrees of freedom
Multiple R-squared:  0.8246, Adjusted R-squared:  0.8221 
F-statistic: 329.3 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for UR_Forward ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         1.394e-01  3.126e-03  44.597  < 2e-16 ***
web_asymmetry                      -5.079e-01  1.116e-01  -4.552 5.65e-06 ***
links_per_PUsAndSpp                -2.646e+00  3.390e-01  -7.805 9.80e-15 ***
cluster_coefficient                 7.593e-02  2.679e-02   2.834  0.00464 ** 
specialisation_asymmetry           -8.352e-02  1.149e-02  -7.270 5.25e-13 ***
linkage_density                    -3.282e-01  1.862e-01  -1.763  0.07806 .  
weighted_connectance                3.528e+00  6.418e-01   5.497 4.38e-08 ***
Shannon_diversity                   3.658e-01  1.177e-01   3.107  0.00192 ** 
interaction_evenness               -4.731e-01  3.178e-01  -1.489  0.13678    
Alatalo_interaction_evenness       -1.378e-03  3.315e-03  -0.416  0.67769    
mean.number.of.shared.partners.PUs -1.998e+00  1.171e+00  -1.706  0.08816 .  
mean.number.of.shared.partners.Spp  4.417e-01  4.739e-01   0.932  0.35141    
cluster.coefficient.PUs             1.361e+00  5.386e-01   2.527  0.01159 *  
cluster.coefficient.Spp            -9.445e-02  8.336e-01  -0.113  0.90981    
niche.overlap.PUs                   1.211e+00  1.742e-01   6.952 4.95e-12 ***
niche.overlap.Spp                  -5.256e+00  2.715e-01 -19.363  < 2e-16 ***
togetherness.PUs                    1.362e-01  7.552e-02   1.804  0.07140 .  
togetherness.Spp                   -3.376e-02  3.856e-02  -0.876  0.38138    
C.score.PUs                         7.550e-01  1.140e-01   6.622 4.61e-11 ***
C.score.Spp                        -1.421e+00  2.161e-01  -6.577 6.20e-11 ***
V.ratio.PUs                        -9.547e-05  2.789e-02  -0.003  0.99727    
V.ratio.Spp                        -5.475e-02  2.571e-02  -2.129  0.03337 *  
functional.complementarity.PUs      1.584e+00  2.048e-01   7.731 1.73e-14 ***
functional.complementarity.Spp     -1.611e+00  3.393e-01  -4.749 2.20e-06 ***
partner.diversity.PUs               1.752e-02  8.400e-02   0.209  0.83475    
partner.diversity.Spp               7.682e-01  1.716e-01   4.477 8.02e-06 ***
generality.PUs                      1.149e+00  8.502e-02  13.519  < 2e-16 ***
vulnerability.Spp                   1.109e+00  3.891e-01   2.850  0.00442 ** 
---
Residual standard error: 0.1369 on 1891 degrees of freedom
Multiple R-squared:  0.8369, Adjusted R-squared:  0.8345 
F-statistic: 359.3 on 27 and 1891 DF,  p-value: < 2.2e-16

============start lm results for Marxan_SA_SS ===================
Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                         0.3378081  0.0056338  59.961  < 2e-16 ***
web_asymmetry                      -1.4529112  0.2010884  -7.225 7.22e-13 ***
links_per_PUsAndSpp                -4.2785727  0.6109780  -7.003 3.47e-12 ***
cluster_coefficient                 0.1467585  0.0482791   3.040 0.002400 ** 
specialisation_asymmetry           -0.0656481  0.0207046  -3.171 0.001545 ** 
linkage_density                    -0.9523956  0.3355169  -2.839 0.004580 ** 
weighted_connectance                6.6780178  1.1565740   5.774 9.03e-09 ***
Shannon_diversity                   0.6492510  0.2121633   3.060 0.002243 ** 
interaction_evenness                0.3225439  0.5727659   0.563 0.573410    
Alatalo_interaction_evenness       -0.0002745  0.0059741  -0.046 0.963356    
mean.number.of.shared.partners.PUs -8.5441097  2.1108585  -4.048 5.38e-05 ***
mean.number.of.shared.partners.Spp  2.9866043  0.8540051   3.497 0.000481 ***
cluster.coefficient.PUs            -1.0423573  0.9705999  -1.074 0.282991    
cluster.coefficient.Spp             1.2670114  1.5023542   0.843 0.399139    
niche.overlap.PUs                   2.1162382  0.3139812   6.740 2.10e-11 ***
niche.overlap.Spp                  -6.3561586  0.4892242 -12.992  < 2e-16 ***
togetherness.PUs                    0.3332868  0.1360900   2.449 0.014415 *  
togetherness.Spp                    0.0222012  0.0694858   0.320 0.749378    
C.score.PUs                         1.5035321  0.2054837   7.317 3.73e-13 ***
C.score.Spp                        -2.4936134  0.3894144  -6.403 1.91e-10 ***
V.ratio.PUs                        -0.3833634  0.0502608  -7.627 3.77e-14 ***
V.ratio.Spp                        -0.0856273  0.0463403  -1.848 0.064788 .  
functional.complementarity.PUs      3.8110828  0.3691440  10.324  < 2e-16 ***
functional.complementarity.Spp     -3.1263036  0.6114383  -5.113 3.49e-07 ***
partner.diversity.PUs              -0.5865649  0.1513718  -3.875 0.000110 ***
partner.diversity.Spp               1.7116260  0.3092116   5.535 3.54e-08 ***
generality.PUs                      2.2542268  0.1532269  14.712  < 2e-16 ***
vulnerability.Spp                   2.0616775  0.7012920   2.940 0.003324 ** 
---
Residual standard error: 0.2468 on 1891 degrees of freedom
Multiple R-squared:  0.4291, Adjusted R-squared:  0.4209 
F-statistic: 52.64 on 27 and 1891 DF,  p-value: < 2.2e-16
```

##  Predict using **Everything** data (42 input variables)

The final set of input variables we consider here is cheating to a certain degree and is meant to suggest an upper bound on our ability to predict with variables that we are currently aware of.  (Input variables used are listed in **Table 6 \textcolor{cyan}{\ref{tab:makeEverythingVarsTable}}**.)  This set contains all variables that we have measured about each problem, which is cheating in the sense that some of these variables are unknowable at the time of prediction, particularly for non-synthetic, real-world problems.  In particular, we include the true realized input error rates (FP, FN, etc.), the correct solution's fraction of the landscape, and the model RB input parameters.  The point here is to ask, if we *were* able to know all of these things about each problem, how much better could we predict output errors than using just the variables already tested.  This does not represent a theoretical upper bound on predictive performance since there may be other more informative variables that we are not aware of or have not measured, however, it does give us a sense of how well our other variable sets are doing.  

For representation shortfall, using all of the input features provided only 0.01 improvement in rmse for each of the reserve selectors (**Fig. 16 \textcolor{cyan}{\ref{fig:predRepShortfallUsingEverything}}**).  However, by this point, rmse values are small enough that even 0.01 is roughly a 20% improvement over the graph measures alone.  Like the graph measures, it also improved the underprediction in the mid-range of true error values for the NV reserve selectors, though it provided little or no reduction in the spread of predictions at each true error value.  

As with the previous input feature sets, the bigger improvements are in predicting solution cost error (**Fig. 17 \textcolor{cyan}{\ref{fig:predSolCostErrorUsingEverything}}**).  Here, rmse for predictions on the NV reserve selectors dropped by about one third, i.e., from around 0.12 to around 0.08.  The rmse for SA_SS dropped by 20%, i.e., from 0.25 to 0.2.  Again, most improvements for all reserve selectors were in the FN-dominated problems, particularly with a reduction in the spread of predictions at any given true value.  This was most noticeable in the SA_SS FN-dominated problems.  

```{r setEverythingParams}
vars_used_str = "Everything" 
source_string = paste0 (vars_used_str, "_", fitting_model_str) 

include_median_redundancies=FALSE

inVars = c(                                           #  55 variables
                            #            "rsp_combined_num_occ_and_unocc_PUs",   #  USELESS?  SHOULD REMOVE?  SAME OR NEARLY THE SAME AS rsp_num_occupied_PUs.
            "rsp_num_occupied_PUs", 
            "rsp_num_spp", 
                            #            "rsp_num_spp_per_PU",    #  REMOVE since it duplicates generality.PUs
            "sppPUprod", 
            
            "rsp_alpha__", 
            "rsp_n__num_groups", 
            "rsp_p__prop_of_links_between_groups", 
            "rsp_r__density", 
            "rsp_d__number_of_nodes_per_group", 
                            #            "rsp_nominal_p__prop_of_links_between_groups",    #  REMOVE, since the actual p is what was used in the experiments. 
            "actual_sol_frac_of_landscape", 
            
                            #            "ig_top",    #  REMOVE, since it's the same as the number of spp. 
                            #            "ig_bottom",    #  REMOVE, since it's the same as the number of occupied PUs.
            "ig_num_edges_m", 
                                  #            "ig_ktop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                            #            "ig_kbottom",    #  REMOVE since same as rsp_num_spp_per_PU. 
                            #            "ig_bidens",    #  REMOVE since same as edge_frac_of_possible and bip_connectance. 
                                  #            "ig_lcctop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_lccbottom",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_distop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_disbottom",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_cctop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_ccbottom",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_cclowdottop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_cclowdotbottom",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_cctopdottop",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                                  #            "ig_cctopdotbottom",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                            #            "ig_mean_bottom_bg_redundancy",   #  2024 03 06 - BUG FIX to eliminate a -Inf value that blew up rmse calculations
                                  #            "ig_median_bottom_bg_redundancy",    #  REMOVE 2025 02 05, since it's a Latapy variable. 
                            #            "ig_mean_top_bg_redundancy",   #  2024 03 06 - BUG FIX to eliminate a -Inf value that blew up rmse calculations
                                #            "ig_median_top_bg_redundancy",    #  REMOVE 2025 02 05, since it's a Latapy variable. 

                            #"connectance",   #  Shouldn't this be removed?  Same as edge_frac_of_possible?
            "web_asymmetry", 
            "links_per_PUsAndSpp", 
            "cluster_coefficient", 
            "specialisation_asymmetry", 
            "linkage_density", 
            "weighted_connectance", 
            "Shannon_diversity", 
            "interaction_evenness", 
            "Alatalo_interaction_evenness", 
                            #"number.of.Spp",  #  Shouldn't this be removed?  Same as rsp_num_spp?
            "mean.number.of.shared.partners.PUs", 
            "mean.number.of.shared.partners.Spp", 
            "cluster.coefficient.PUs", 
            "cluster.coefficient.Spp", 
            "niche.overlap.PUs", 
            "niche.overlap.Spp", 
            "togetherness.PUs", 
            "togetherness.Spp", 
            "C.score.PUs", 
            "C.score.Spp", 
            "V.ratio.PUs", 
            "V.ratio.Spp", 
            "functional.complementarity.PUs", 
            "functional.complementarity.Spp", 
            "partner.diversity.PUs", 
            "partner.diversity.Spp", 
            "generality.PUs", 
            "vulnerability.Spp", 

            "rsp_realized_FP_rate", 
            "rsp_realized_FN_rate", 
            "rsp_realized_Ftot_rate", 
                            # "rsp_euc_realized_FP_and_cost_in_err_frac",  #  REMOVE:  In all experiments reported here, cost input error rate is 0, so this is the same as the realized FP rate.
                            # "rsp_euc_realized_FN_and_cost_in_err_frac",  #  REMOVE:  In all experiments reported here, cost input error rate is 0, so this is the same as the realized FN rate. 
                            # "rsp_euc_realized_Ftot_and_cost_in_err_frac",  #  REMOVE:  In all experiments reported here, cost input error rate is 0, so this is the same as the realized Ftot rate. 

            "gurobi_mipgap", 
            "edge_frac_of_possible"    #  Same as bip_connectance?
            )
```

&nbsp;  

```{r makeEverythingVarsTable}

#  NOTE that you need to have the "&nbsp;  " line just before this chunk if you 
#  want the table to have a blank line separating it from the preceding paragraph.  
#  Without it, the table butts right up to the previous paragrapha and looks bad.

#  NOTE:  2024 03 28 
#  I have temporarily truncated all long lines in these definitions with a "..." 
#  so that the table fits on the page.  When kable() runs it off of the page, 
#  it also shifts that caption and table number completely off the page and 
#  I don't want that.  I need to figure out how to get the tables to wrap 
#  long lines of text within a fixed column width.  
  
in_var_defns = c(

                                #"rsp_combined_num_occ_and_unocc_PUs:  USELESS?  SHOULD REMOVE?  SAME OR NEARLY THE SAME AS rsp_num_occupied_PUs.  Combined number of occupied and unoccupied PUs", 
                "number of occupied PUs", 
                "number of species", 
                                #   REMOVE since it duplicates generality.PUs             "average number of species per PU, i.e., ...",    #  "number of species / number of PUs", 
                "product of number of species and number ...",    #  "of PUs, i.e, size of adjacency matrix", 

                "Exponent driving number of nodes per clique ...",    #  "in model RB generator for current Base problem.", 
                "Number of cliques in model RB generator of ...",    #  "current Base problem.", 
                "Constraint density, i.e., proportion driving ...",    #  "number of links per round in model RB generator of current Base problem.  number of links per round = p*d^2, where d is number of nodes per clique.", 
                "Constraint tightness, i.e., multiplier driving ...",    #  "number of rounds of linking in model RB generator of current Base problem.  Number of rounds of linking = r*n*ln(n).", 

                "Number of nodes per clique.", 
                                #"rsp_nominal_p__prop_of_links_between_groups",    #  REMOVE, since the actual p is what was used in the experiments.
                "Fraction of the landscape contained in correct ...",    #  "solution, i.e., correct solution cost / rsp_num_occupied_PUs", 

                                #"ig_top",    #  REMOVE, since it's the same as the number of spp. 
                                #"ig_bottom",    #  REMOVE, since it's the same as the number of occupied PUs.
                "Total number of adjacency matrix cells containing ...",    #  "a 1, i.e., total number of links between spp and PUs.", 
                #Latapy removal 2025 02 05:  "Mean number of PUs occupied by each spp (i.e., ...",    #  "mean degree of top nodes).", 
                                #"ig_kbottom:  REMOVE since same as rsp_num_spp_per_PU.  Mean number of spp occupying each PU (i.e., mean degree of bottom nodes).", 
                                #"ig_bidens:  REMOVE since same as edge_frac_of_possible and bip_connectance.  Density of bipartite network, i.e., total number of realized links divided by total possible number of links.  For the adjacency matrix, this is the fraction of cells containing a 1.", 
                #Latapy removal 2025 02 05:"Size of largest connected component for spp.", 
                #Latapy removal 2025 02 05:"Size of largest connected component for PUs.", 
                #Latapy removal 2025 02 05:"Mean distance (shortest path) between spp.", 
                #Latapy removal 2025 02 05:"Mean distance (shortest path) between PUs", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for spp.  ...",    #  "Unsure of exact meaning from code.", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for PUs  ...",    #  "Unsure of exact meaning from code.", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for spp.  ...",    #  "Unsure of exact meaning from code.p", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for PUs  ...",    #  "Unsure of exact meaning from code.m", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for spp.  ...",    #  "Unsure of exact meaning from code.", 
                #Latapy removal 2025 02 05:"UNCLEAR - A clustering coefficient for PUs  ...",    #  "Unsure of exact meaning from code.", 
                                #            "ig_mean_bottom_bg_redundancy",   #  2024 03 06 - BUG FIX to eliminate a -Inf value that blew up rmse calculations
                #Latapy removal 2025 02 05:"Median of the fraction of pairs of neighbours of ...",    #  "a PU P linked to another node than P. In the projection, these nodes would be linked together even if P were not there.   If it is equal to 1 then the projection would be exactly the same without P; if it is 0 it means that none of its neighbours would be linked together in the projection.", 
                                #            "ig_mean_top_bg_redundancy",   #  2024 03 06 - BUG FIX to eliminate a -Inf value that blew up rmse calculations
                #Latapy removal 2025 02 05:"Median of the fraction of pairs of neighbours of ...",    #  "a species S linked to another node than S. In the projection, these nodes would be linked together even if S were not there.   If it is equal to 1 then the projection would be exactly the same without S; if it is 0 it means that none of its neighbours would be linked together in the projection.", 

                    #  bipartite package metrics

                                          #2022 03 25  "connectance" ,  #  Same things as ig_bidens and edge_frac_of_possible
                          #"connectance",   #  Shouldn't this be removed?  Same as edge_frac_of_possible?
                          #                    "SHOULD REMOVE FROM EVERYTHING SET?  bip_connectance:  Realised proportion of possible links (Dunne et al. 2002): num_links/(num_spp*num_PUs). In reserve selection, this is the proportion of the occupancy matrix (bpm) that contains 1s, i.e., 'positives'.  Same as ig_bidens (renamed to ig_realized_frac_of_all_possible_links?) and edge_frac_of_possible.  This is the standardised number of species combinations often used in co-occurrence analyses (Gotelli & Graves 1996).", 

                "Balance between numbers in the two levels: ...",    #  "positive values indicate more PUs negative more spp implemented as (num_PUs - num_spp)/2", 
                "average number of links per node, where species ...",    #  "and PUs are nodes", 

                        
                "Mean, across all nodes of the number of realized ...",    #  "links divided by the number of possible links for each node (i.e. average per-node connectance).  (Modified text from Dormann et al (2009))" , 
# "weighted_NODF" , 
# "interaction_strength_asymmetry" , 
                "Asymmetry (PU level vs. spp level) of specialisation ...",    #  "now based on d' (see dfun), which is insensitive to the dimensions of the web." , 
                "Marginal totals-weighted diversity of interactions ...",    #  "per species (quantitative). Actually, this is computed as the average of vulnerability and generality (Bersier et al. 2002)." , 
                "Linkage density divided by number of nodes in the n...",    #  "etwork (Bersier et al. 2002). This will respond to whether non-interacting nodes are included or not." , 
                "Shannon's diversity of interactions (i.e. network entries)." , 
                "Shannon's evenness for the web entries." , 
                "A different measure for web entry evenness, as ...",    #  "proposed by Muller et al. (1999)." , 

#"number.of.Spp",  #  Shouldn't this be removed?  Same as rsp_num_spp?
                    
                "Based on the distance matrix between PUs counting ...",    #  "the number of spp that both PUs interact with." , 
                "Based on the distance matrix between spp counting ...",    #  "the number of PUs that both spp interact with." , 
                "Mean, across all PUs of the number of realized links ...",    #  "divided by the number of possible links for each PU (i.e. average per-PU connectance)." , 
                "Mean, across all spp of the number of realized links ...",    #  "divided by the number of possible links for each spp (i.e. average per-spp connectance)." , 
                "Mean similarity in interaction pattern between PUs, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "Mean similarity in interaction pattern between spp, ...",    #  "calculated e.g. as Horn-Morisita similarity (Krebs 1989) or as Bray-Curtis similarity (Mouillot et al. 2008). Values near 0 indicate no common use of niches, 1 indicates perfect niche overlap." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR - Mean number of co-occupancies across all ...",    #  "species-combinations." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR -  (Normalised) mean number of checkerboard ...",    #  "combinations across all species of the level." , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level" , 
                "UNCLEAR - Variance-ratio of species numbers to ...",    #  "interaction numbers within species of a level." , 
                "UNCLEAR - Functional complementarity? for a ...",    #  "given level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "UNCLEAR - Functional complementarity? for a given ...",    #  "level. This measure of niche complementarity (as described by Devoto et al. 2012), is computed as the total branch length of a ?functional dendrogram? based on qualitative differences of interactions of one level with the other. Thus, the ?functional? aspect of functional complementarity refers to the function of sharing interactions. Should be highly correlated with niche overlap, only binary." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for PUs." , 
                "Mean Shannon diversity of the number of interactions ...",    #  "for spp." , 
                "Mean number of spp per PU.", 
                "Mean number of PUs per spp.", 

                "Proportion of false positives in the APP adjacency ...",    #  "matrix.", 
                "Proportion of false negatives in the APP adjacency ...",    #  "matrix.", 
                "Combined proportion of false positives and false ...",    #  "negatives in the APP adjacency matrix.", 

#"rsp_euc_realized_FP_and_cost_in_err_frac:  Root mean square of realized FP rate and cost input error rate (in all experiments reported here, cost input error rate is 0, so this is the same as the realized FP rate).", 
#"rsp_euc_realized_FN_and_cost_in_err_frac:  Root mean square of realized FN rate and cost input error rate (in all experiments reported here, cost input error rate is 0, so this is the same as the realized FN rate).", 
#"rsp_euc_realized_Ftot_and_cost_in_err_frac:  Root mean square of realized Ftot rate and cost input error rate (in all experiments reported here, cost input error rate is 0, so this is the same as the realized Ftot rate).", 

                "Relative measure of the upper bound on the distance ...",    #  "to the correct solution as a proportion of the correct solution.", 
                "proportion of links made out of all possible links ..."    #  "between species and PUs"
                )

invars_table_caption = "Input features for model based on ALL variables."

#-----  

invars_table = data.frame (inputFeatures = inVars, 
                           inputFeatureDefns = in_var_defns)
names (invars_table) = c("Feature Names", "Feature Definitions")
kable (invars_table, 
       #align = 'lcc', 
       #digits = digits_to_show,    #c(3, 3, 3, 4, 6), 
       caption = invars_table_caption, 
       format = "latex", 
       booktabs = TRUE, 
       longtable = T
       )
```

```{r genRSmatildaUsingEverythingFiles, include=FALSE}

###  Write Matilda files using Everything features and *RS optimization only* (before learning)

write_matilda_files_for_optimizers_only (
                                   p3_working_train_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TRAIN", 
                                   params)

write_matilda_files_for_optimizers_only (
                                   p3_working_test_df__before_any_preprocessing, 
                                   inVars, 
                                   vars_used_str,    #source_string, 
                                   "TEST", 
                                   params)
```

```{r buildEverythingTestAndTrain, include=TRUE}

###  Prep for Everything learning

working_x_df_train_test_pair = 
    build_feature_set_specific_test_and_train (working_train_df = p3_working_train_df, 
                                                      working_test_df  = p3_working_test_df, 
                                                      inVars, 
                                                      params, 
                                                      include_median_redundancies)

p3_train_x_df = working_x_df_train_test_pair$p3_train_x_df
p3_test_x_df  = working_x_df_train_test_pair$p3_test_x_df
```

```{r echoEverythingNames, eval=FALSE}
cat ("\nFeatures used:\n\n"); names (p3_train_x_df) [-1]    #  -1 to drop the rs_name variable
```

### Rep shortfall using Everything data

```{r predictRepShortfallUsingEverything, fig.cap = "\\label{fig:predRepShortfallUsingEverything}True representation shortfall vs. predicted representation shortfall on each problem in the test set using linear model trained for each reserve selector on input feature set containing all recorded variables both knowable and unknowable at run time (listed in Table 6 \\textcolor{cyan}{\\ref{tab:makeEverythingVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}


all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = -0.2, y_max_on_plot = 1.2,       
                               
          fitting_func = fit_rep_shortfall,
          perf_metric_name_for_file_name_str = "abs_rep_shortfall_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet7, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example REP SHORTFALL lm model fit using Everything data

============start lm results for Gurobi ===================   
Coefficients: (11 not defined because of singularities)   
                                             Estimate Std. Error t value   Pr(>|t|)       
(Intercept)                                  0.133340   0.001075 123.997    < 2e-16 ***   
rsp_combined_num_occ_and_unocc_PUs          -0.051659   0.059170  -0.873   0.382743       
rsp_num_occupied_PUs                        -0.065358   1.071463  -0.061   0.951367       
rsp_num_spp                                  0.818294   0.540220   1.515   0.130007       
rsp_num_spp_per_PU                                 NA         NA      NA         NA       
sppPUprod                                          NA         NA      NA         NA       
rsp_alpha__                                 -0.006763   0.003089  -2.189   0.028705 *     
rsp_n__num_groups                            0.016731   0.012391   1.350   0.177083       
rsp_p__prop_of_links_between_groups          0.021117   0.018657   1.132   0.257826       
rsp_r__density                              -0.017691   0.002336  -7.575   5.64e-14 ***   
rsp_d__number_of_nodes_per_group             0.083668   0.042166   1.984   0.047376 *     
rsp_nominal_p__prop_of_links_between_groups -0.015799   0.012857  -1.229   0.219299       
actual_sol_frac_of_landscape                 0.037467   0.028329   1.323   0.186137       
ig_top                                             NA         NA      NA         NA       
ig_bottom                                          NA         NA      NA         NA       
ig_num_edges_m                               0.882724   2.360823   0.374   0.708518       
ig_ktop                                      2.452393   1.283735   1.910   0.056241 .     
ig_kbottom                                   3.168493   0.797120   3.975   7.31e-05 ***   
ig_bidens                                          NA         NA      NA         NA       
ig_lcctop                                    0.109476   0.037149   2.947   0.003249 **    
ig_lccbottom                                -0.082611   0.036574  -2.259   0.024015 *     
ig_distop                                    0.013172   0.038157   0.345   0.729977       
ig_disbottom                                -0.046865   0.039414  -1.189   0.234573       
ig_cctop                                    -0.360278   0.262437  -1.373   0.169974       
ig_ccbottom                                 -0.198214   0.123573  -1.604   0.108879       
ig_cclowdottop                              -0.144452   0.113388  -1.274   0.202836       
ig_cclowdotbottom                            0.166470   0.075418   2.207   0.027415 *     
ig_cctopdottop                               0.060976   0.193189   0.316   0.752320       
ig_cctopdotbottom                            0.203106   0.091976   2.208   0.027349 *     
ig_mean_bottom_bg_redundancy                -0.011199   0.022710  -0.493   0.621984       
ig_median_bottom_bg_redundancy               0.044733   0.021602   2.071   0.038511 *     
ig_mean_top_bg_redundancy                    0.002803   0.022661   0.124   0.901573       
ig_median_top_bg_redundancy                 -0.002787   0.017908  -0.156   0.876359       
connectance                                        NA         NA      NA         NA       
web_asymmetry                               -0.277311   0.062634  -4.427   1.01e-05 ***   
links_per_PUsAndSpp                         -4.706931   1.677893  -2.805   0.005080 **    
cluster_coefficient                         -0.004561   0.009713  -0.470   0.638688       
specialisation_asymmetry                     0.015589   0.004560   3.419   0.000642 ***   
linkage_density                             -0.686776   0.128559  -5.342   1.03e-07 ***   
weighted_connectance                         1.280166   0.384394   3.330   0.000884 ***   
Shannon_diversity                           -0.011381   0.083500  -0.136   0.891599       
interaction_evenness                        -1.230221   0.306190  -4.018   6.11e-05 ***   
Alatalo_interaction_evenness                 0.001841   0.001145   1.607   0.108131       
number.of.Spp                                      NA         NA      NA         NA       
mean.number.of.shared.partners.PUs          -5.580276   2.709149  -2.060   0.039558 *     
mean.number.of.shared.partners.Spp           1.264839   0.309661   4.085   4.60e-05 ***   
cluster.coefficient.PUs                     -0.926555   0.308023  -3.008   0.002664 **    
cluster.coefficient.Spp                      3.332980   1.156309   2.882   0.003992 **    
niche.overlap.PUs                           -0.146267   0.111723  -1.309   0.190630       
niche.overlap.Spp                            0.061536   0.172157   0.357   0.720801       
togetherness.PUs                             0.063310   0.027429   2.308   0.021099 *     
togetherness.Spp                             0.022645   0.013540   1.672   0.094595 .     
C.score.PUs                                  0.157849   0.090440   1.745   0.081090 .     
C.score.Spp                                 -0.189484   0.110464  -1.715   0.086447 .     
V.ratio.PUs                                 -0.026164   0.016921  -1.546   0.122207       
V.ratio.Spp                                 -0.051827   0.014653  -3.537   0.000415 ***   
functional.complementarity.PUs               0.304340   0.180111   1.690   0.091246 .     
functional.complementarity.Spp              -0.477250   0.219505  -2.174   0.029815 *     
partner.diversity.PUs                       -0.252156   0.082050  -3.073   0.002149 **    
partner.diversity.Spp                        0.910722   0.322087   2.828   0.004741 **    
generality.PUs                               0.306291   0.064397   4.756   2.12e-06 ***   
vulnerability.Spp                            0.502574   0.601977   0.835   0.403897       
rsp_realized_FP_rate                         0.015976   0.039236   0.407   0.683924       
rsp_realized_FN_rate                         0.020733   0.001735  11.949    < 2e-16 ***   
rsp_realized_Ftot_rate                      -0.014116   0.006411  -2.202   0.027800 *     
rsp_euc_realized_FP_and_cost_in_err_frac           NA         NA      NA         NA       
rsp_euc_realized_FN_and_cost_in_err_frac           NA         NA      NA         NA       
rsp_euc_realized_Ftot_and_cost_in_err_frac         NA         NA      NA         NA       
gurobi_mipgap                                0.020980   0.004369   4.802   1.69e-06 ***   
edge_frac_of_possible                              NA         NA      NA         NA       
---   
Residual standard error: 0.04711 on 1860 degrees of freedom   
Multiple R-squared:  0.9569, Adjusted R-squared:  0.9556    
F-statistic: 712.1 on 58 and 1860 DF,  p-value: < 2.2e-16   
   
============start lm results for Marxan_SA ===================   
Coefficients: (11 not defined because of singularities)   
                                              Estimate Std. Error t value   Pr(>|t|)       
(Intercept)                                  1.574e-01  1.004e-03 156.753    < 2e-16 ***   
rsp_combined_num_occ_and_unocc_PUs          -1.798e-01  5.525e-02  -3.255   0.001155 **    
rsp_num_occupied_PUs                        -3.937e+00  1.000e+00  -3.935   8.62e-05 ***   
rsp_num_spp                                 -1.183e+00  5.044e-01  -2.345   0.019112 *     
rsp_num_spp_per_PU                                  NA         NA      NA         NA       
sppPUprod                                           NA         NA      NA         NA       
rsp_alpha__                                 -1.087e-03  2.884e-03  -0.377   0.706240       
rsp_n__num_groups                            2.917e-02  1.157e-02   2.521   0.011770 *     
rsp_p__prop_of_links_between_groups          3.799e-02  1.742e-02   2.181   0.029321 *     
rsp_r__density                              -1.154e-02  2.181e-03  -5.291   1.36e-07 ***   
rsp_d__number_of_nodes_per_group             5.636e-02  3.937e-02   1.432   0.152417       
rsp_nominal_p__prop_of_links_between_groups -2.762e-02  1.201e-02  -2.300   0.021538 *     
actual_sol_frac_of_landscape                 7.758e-02  2.645e-02   2.933   0.003396 **    
ig_top                                              NA         NA      NA         NA       
ig_bottom                                           NA         NA      NA         NA       
ig_num_edges_m                               9.934e+00  2.204e+00   4.507   6.99e-06 ***   
ig_ktop                                     -1.134e+00  1.199e+00  -0.946   0.344039       
ig_kbottom                                   2.022e+00  7.443e-01   2.717   0.006650 **    
ig_bidens                                           NA         NA      NA         NA       
ig_lcctop                                    6.848e-02  3.469e-02   1.974   0.048493 *     
ig_lccbottom                                -5.480e-02  3.415e-02  -1.605   0.108712       
ig_distop                                    6.034e-02  3.563e-02   1.694   0.090499 .     
ig_disbottom                                -7.851e-02  3.680e-02  -2.133   0.033024 *     
ig_cctop                                    -3.219e-01  2.450e-01  -1.314   0.189134       
ig_ccbottom                                 -3.494e-01  1.154e-01  -3.029   0.002490 **    
ig_cclowdottop                              -1.690e-01  1.059e-01  -1.596   0.110607       
ig_cclowdotbottom                            2.107e-01  7.042e-02   2.992   0.002804 **    
ig_cctopdottop                               1.795e-02  1.804e-01   0.100   0.920748       
ig_cctopdotbottom                            3.006e-01  8.588e-02   3.500   0.000476 ***   
ig_mean_bottom_bg_redundancy                 1.026e-02  2.120e-02   0.484   0.628477       
ig_median_bottom_bg_redundancy               2.602e-02  2.017e-02   1.290   0.197197       
ig_mean_top_bg_redundancy                    9.527e-04  2.116e-02   0.045   0.964089       
ig_median_top_bg_redundancy                  1.305e-02  1.672e-02   0.781   0.435072       
connectance                                         NA         NA      NA         NA       
web_asymmetry                               -3.099e-01  5.848e-02  -5.299   1.30e-07 ***   
links_per_PUsAndSpp                         -2.241e+00  1.567e+00  -1.430   0.152832       
cluster_coefficient                          1.035e-03  9.069e-03   0.114   0.909164       
specialisation_asymmetry                     1.306e-02  4.257e-03   3.068   0.002183 **    
linkage_density                             -4.506e-01  1.200e-01  -3.754   0.000180 ***   
weighted_connectance                         4.073e-01  3.589e-01   1.135   0.256597       
Shannon_diversity                           -2.384e-01  7.796e-02  -3.057   0.002265 **    
interaction_evenness                        -1.074e+00  2.859e-01  -3.756   0.000178 ***   
Alatalo_interaction_evenness                 3.648e-04  1.069e-03   0.341   0.733043       
number.of.Spp                                       NA         NA      NA         NA       
mean.number.of.shared.partners.PUs          -1.315e+01  2.530e+00  -5.199   2.23e-07 ***   
mean.number.of.shared.partners.Spp           1.328e+00  2.891e-01   4.594   4.65e-06 ***   
cluster.coefficient.PUs                     -6.304e-01  2.876e-01  -2.192   0.028508 *     
cluster.coefficient.Spp                      4.219e+00  1.080e+00   3.907   9.66e-05 ***   
niche.overlap.PUs                           -3.011e-01  1.043e-01  -2.886   0.003946 **    
niche.overlap.Spp                            2.599e-01  1.607e-01   1.617   0.106123       
togetherness.PUs                             8.675e-02  2.561e-02   3.387   0.000721 ***   
togetherness.Spp                             2.942e-02  1.264e-02   2.328   0.020045 *     
C.score.PUs                                  5.212e-02  8.444e-02   0.617   0.537150       
C.score.Spp                                 -1.271e-01  1.031e-01  -1.232   0.217980       
V.ratio.PUs                                 -6.241e-02  1.580e-02  -3.951   8.09e-05 ***   
V.ratio.Spp                                  5.430e-03  1.368e-02   0.397   0.691505       
functional.complementarity.PUs               2.545e-02  1.682e-01   0.151   0.879708       
functional.complementarity.Spp              -2.660e-01  2.050e-01  -1.298   0.194449       
partner.diversity.PUs                       -2.206e-01  7.661e-02  -2.879   0.004030 **    
partner.diversity.Spp                        1.628e+00  3.007e-01   5.412   7.04e-08 ***   
generality.PUs                               1.808e-01  6.013e-02   3.007   0.002673 **    
vulnerability.Spp                            2.963e+00  5.621e-01   5.272   1.51e-07 ***   
rsp_realized_FP_rate                         9.161e-02  3.663e-02   2.501   0.012486 *     
rsp_realized_FN_rate                         1.072e-02  1.620e-03   6.615   4.84e-11 ***   
rsp_realized_Ftot_rate                      -8.325e-03  5.986e-03  -1.391   0.164477       
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA         NA       
gurobi_mipgap                                2.841e-02  4.079e-03   6.965   4.53e-12 ***   
edge_frac_of_possible                               NA         NA      NA         NA       
---   
Residual standard error: 0.04398 on 1860 degrees of freedom   
Multiple R-squared:  0.9693, Adjusted R-squared:  0.9683    
F-statistic:  1011 on 58 and 1860 DF,  p-value: < 2.2e-16   
   
============start lm results for UR_Forward ===================   
Coefficients: (11 not defined because of singularities)   
                                              Estimate Std. Error t value   Pr(>|t|)       
(Intercept)                                  0.1438840  0.0009169 156.925    < 2e-16 ***   
rsp_combined_num_occ_and_unocc_PUs          -0.1047351  0.0504510  -2.076   0.038033 *     
rsp_num_occupied_PUs                        -2.2547409  0.9135825  -2.468   0.013676 *     
rsp_num_spp                                 -0.5585527  0.4606181  -1.213   0.225431       
rsp_num_spp_per_PU                                  NA         NA      NA         NA       
sppPUprod                                           NA         NA      NA         NA       
rsp_alpha__                                 -0.0023972  0.0026339  -0.910   0.362881       
rsp_n__num_groups                            0.0193732  0.0105650   1.834   0.066856 .     
rsp_p__prop_of_links_between_groups          0.0490894  0.0159077   3.086   0.002059 **    
rsp_r__density                              -0.0147916  0.0019914  -7.428   1.67e-13 ***   
rsp_d__number_of_nodes_per_group             0.0824443  0.0359531   2.293   0.021953 *     
rsp_nominal_p__prop_of_links_between_groups -0.0338140  0.0109629  -3.084   0.002070 **    
actual_sol_frac_of_landscape                 0.0553699  0.0241543   2.292   0.021997 *     
ig_top                                              NA         NA      NA         NA       
ig_bottom                                           NA         NA      NA         NA       
ig_num_edges_m                               6.0937461  2.0129543   3.027   0.002502 **    
ig_ktop                                     -0.3968898  1.0945756  -0.363   0.716947       
ig_kbottom                                   1.8224412  0.6796634   2.681   0.007397 **    
ig_bidens                                           NA         NA      NA         NA       
ig_lcctop                                    0.0671038  0.0316751   2.118   0.034265 *     
ig_lccbottom                                -0.0541132  0.0311847  -1.735   0.082862 .     
ig_distop                                   -0.0068156  0.0325348  -0.209   0.834092       
ig_disbottom                                -0.0127797  0.0336065  -0.380   0.703785       
ig_cctop                                    -0.4112317  0.2237667  -1.838   0.066256 .     
ig_ccbottom                                 -0.1656438  0.1053645  -1.572   0.116097       
ig_cclowdottop                              -0.0157952  0.0966804  -0.163   0.870241       
ig_cclowdotbottom                            0.0556020  0.0643048   0.865   0.387335       
ig_cctopdottop                               0.1070942  0.1647222   0.650   0.515675       
ig_cctopdotbottom                            0.1497691  0.0784235   1.910   0.056320 .     
ig_mean_bottom_bg_redundancy                 0.0241913  0.0193640   1.249   0.211715       
ig_median_bottom_bg_redundancy               0.0475919  0.0184185   2.584   0.009844 **    
ig_mean_top_bg_redundancy                   -0.0105314  0.0193217  -0.545   0.585782       
ig_median_top_bg_redundancy                  0.0042689  0.0152695   0.280   0.779835       
connectance                                         NA         NA      NA         NA       
web_asymmetry                               -0.3104955  0.0534051  -5.814   7.16e-09 ***   
links_per_PUsAndSpp                         -2.2809135  1.4306542  -1.594   0.111035       
cluster_coefficient                         -0.0011071  0.0082819  -0.134   0.893668       
specialisation_asymmetry                     0.0104825  0.0038878   2.696   0.007076 **    
linkage_density                             -0.4541884  0.1096161  -4.143   3.57e-05 ***   
weighted_connectance                         0.4997615  0.3277535   1.525   0.127477       
Shannon_diversity                           -0.0959170  0.0711962  -1.347   0.178073       
interaction_evenness                        -0.8272362  0.2610726  -3.169   0.001557 **    
Alatalo_interaction_evenness                 0.0012539  0.0009767   1.284   0.199367       
number.of.Spp                                       NA         NA      NA         NA       
mean.number.of.shared.partners.PUs          -9.6588270  2.3099540  -4.181   3.03e-05 ***   
mean.number.of.shared.partners.Spp           1.2675741  0.2640325   4.801   1.71e-06 ***   
cluster.coefficient.PUs                     -0.5243279  0.2626353  -1.996   0.046035 *     
cluster.coefficient.Spp                      3.5866724  0.9859264   3.638   0.000282 ***   
niche.overlap.PUs                           -0.2197698  0.0952604  -2.307   0.021162 *     
niche.overlap.Spp                            0.2484743  0.1467900   1.693   0.090676 .     
togetherness.PUs                             0.0488032  0.0233872   2.087   0.037046 *     
togetherness.Spp                             0.0231024  0.0115447   2.001   0.045525 *     
C.score.PUs                                  0.0167495  0.0771139   0.217   0.828072       
C.score.Spp                                 -0.0393385  0.0941867  -0.418   0.676240       
V.ratio.PUs                                 -0.0627754  0.0144273  -4.351   1.43e-05 ***   
V.ratio.Spp                                 -0.0157735  0.0124943  -1.262   0.206941       
functional.complementarity.PUs               0.1137614  0.1535718   0.741   0.458926       
functional.complementarity.Spp              -0.2112646  0.1871610  -1.129   0.259134       
partner.diversity.PUs                       -0.1646039  0.0699601  -2.353   0.018735 *     
partner.diversity.Spp                        1.2744381  0.2746275   4.641   3.72e-06 ***   
generality.PUs                               0.1429104  0.0549078   2.603   0.009322 **    
vulnerability.Spp                            1.9616810  0.5132754   3.822   0.000137 ***   
rsp_realized_FP_rate                         0.0118976  0.0334543   0.356   0.722152       
rsp_realized_FN_rate                         0.0064996  0.0014795   4.393   1.18e-05 ***   
rsp_realized_Ftot_rate                      -0.0025811  0.0054665  -0.472   0.636869       
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA         NA       
gurobi_mipgap                                0.0139139  0.0037250   3.735   0.000193 ***   
edge_frac_of_possible                               NA         NA      NA         NA       
---   
Residual standard error: 0.04017 on 1860 degrees of freedom   
Multiple R-squared:   0.97,  Adjusted R-squared:  0.9691    
F-statistic:  1037 on 58 and 1860 DF,  p-value: < 2.2e-16   
   
============start lm results for Marxan_SA_SS ===================   
Coefficients: (11 not defined because of singularities)   
                                              Estimate Std. Error t value   Pr(>|t|)       
(Intercept)                                  5.450e-02  7.649e-04  71.256    < 2e-16 ***   
rsp_combined_num_occ_and_unocc_PUs          -2.359e-03  4.209e-02  -0.056   0.955314       
rsp_num_occupied_PUs                         6.070e+00  7.621e-01   7.965   2.85e-15 ***   
rsp_num_spp                                  2.453e+00  3.842e-01   6.384   2.17e-10 ***   
rsp_num_spp_per_PU                                  NA         NA      NA         NA       
sppPUprod                                           NA         NA      NA         NA       
rsp_alpha__                                 -2.436e-03  2.197e-03  -1.108   0.267795       
rsp_n__num_groups                            1.071e-02  8.813e-03   1.216   0.224327       
rsp_p__prop_of_links_between_groups          7.867e-03  1.327e-02   0.593   0.553344       
rsp_r__density                              -1.157e-02  1.661e-03  -6.963   4.59e-12 ***   
rsp_d__number_of_nodes_per_group             5.121e-02  2.999e-02   1.707   0.087908 .     
rsp_nominal_p__prop_of_links_between_groups -5.650e-03  9.145e-03  -0.618   0.536783       
actual_sol_frac_of_landscape                 1.259e-02  2.015e-02   0.625   0.532283       
ig_top                                              NA         NA      NA         NA       
ig_bottom                                           NA         NA      NA         NA       
ig_num_edges_m                              -1.291e+01  1.679e+00  -7.691   2.34e-14 ***   
ig_ktop                                      3.267e+00  9.131e-01   3.578   0.000355 ***   
ig_kbottom                                   5.076e-01  5.670e-01   0.895   0.370724       
ig_bidens                                           NA         NA      NA         NA       
ig_lcctop                                    2.675e-02  2.642e-02   1.012   0.311468       
ig_lccbottom                                -3.703e-02  2.601e-02  -1.423   0.154783       
ig_distop                                   -2.348e-02  2.714e-02  -0.865   0.386974       
ig_disbottom                                 2.366e-02  2.803e-02   0.844   0.398870       
ig_cctop                                    -1.050e-01  1.867e-01  -0.563   0.573749       
ig_ccbottom                                  5.299e-02  8.789e-02   0.603   0.546651       
ig_cclowdottop                               2.243e-01  8.065e-02   2.781   0.005469 **    
ig_cclowdotbottom                           -1.240e-01  5.364e-02  -2.312   0.020889 *     
ig_cctopdottop                               1.098e-01  1.374e-01   0.799   0.424162       
ig_cctopdotbottom                           -4.991e-02  6.542e-02  -0.763   0.445627       
ig_mean_bottom_bg_redundancy                 3.547e-03  1.615e-02   0.220   0.826210       
ig_median_bottom_bg_redundancy               1.717e-02  1.536e-02   1.118   0.263882       
ig_mean_top_bg_redundancy                   -1.060e-02  1.612e-02  -0.658   0.510848       
ig_median_top_bg_redundancy                  1.382e-02  1.274e-02   1.085   0.278180       
connectance                                         NA         NA      NA         NA       
web_asymmetry                               -4.757e-02  4.455e-02  -1.068   0.285711       
links_per_PUsAndSpp                         -1.004e+00  1.193e+00  -0.842   0.400132       
cluster_coefficient                         -2.321e-03  6.909e-03  -0.336   0.736910       
specialisation_asymmetry                     2.347e-03  3.243e-03   0.724   0.469317       
linkage_density                             -1.432e-01  9.144e-02  -1.566   0.117598       
weighted_connectance                         2.209e-01  2.734e-01   0.808   0.419223       
Shannon_diversity                            8.803e-02  5.939e-02   1.482   0.138465       
interaction_evenness                         5.821e-02  2.178e-01   0.267   0.789294       
Alatalo_interaction_evenness                 1.804e-03  8.147e-04   2.214   0.026968 *     
number.of.Spp                                       NA         NA      NA         NA       
mean.number.of.shared.partners.PUs           1.079e+01  1.927e+00   5.599   2.47e-08 ***   
mean.number.of.shared.partners.Spp           4.313e-01  2.203e-01   1.958   0.050335 .     
cluster.coefficient.PUs                     -9.098e-02  2.191e-01  -0.415   0.677982       
cluster.coefficient.Spp                     -1.146e+00  8.224e-01  -1.393   0.163766       
niche.overlap.PUs                           -6.647e-02  7.946e-02  -0.836   0.402997       
niche.overlap.Spp                            8.093e-02  1.224e-01   0.661   0.508753       
togetherness.PUs                             2.901e-02  1.951e-02   1.487   0.137192       
togetherness.Spp                            -4.552e-03  9.630e-03  -0.473   0.636471       
C.score.PUs                                  7.890e-02  6.433e-02   1.227   0.220120       
C.score.Spp                                  4.465e-02  7.857e-02   0.568   0.569911       
V.ratio.PUs                                 -4.217e-03  1.203e-02  -0.350   0.726093       
V.ratio.Spp                                 -6.322e-02  1.042e-02  -6.066   1.59e-09 ***   
functional.complementarity.PUs              -5.730e-03  1.281e-01  -0.045   0.964327       
functional.complementarity.Spp               1.592e-01  1.561e-01   1.019   0.308143       
partner.diversity.PUs                       -3.480e-02  5.836e-02  -0.596   0.551094       
partner.diversity.Spp                       -1.044e+00  2.291e-01  -4.555   5.57e-06 ***   
generality.PUs                               4.046e-02  4.580e-02   0.883   0.377159       
vulnerability.Spp                           -3.346e+00  4.282e-01  -7.816   9.07e-15 ***   
rsp_realized_FP_rate                        -2.072e-03  2.791e-02  -0.074   0.940821       
rsp_realized_FN_rate                         8.640e-03  1.234e-03   7.001   3.54e-12 ***   
rsp_realized_Ftot_rate                      -9.355e-04  4.560e-03  -0.205   0.837475       
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA         NA       
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA         NA       
gurobi_mipgap                                8.582e-03  3.107e-03   2.762   0.005802 **    
edge_frac_of_possible                               NA         NA      NA         NA       
---   
Residual standard error: 0.03351 on 1860 degrees of freedom   
Multiple R-squared:  0.9095, Adjusted R-squared:  0.9066    
F-statistic: 322.2 on 58 and 1860 DF,  p-value: < 2.2e-16   
```

### Solution cost error using Everything data 

```{r predictSolCostErrorUsingEverything, fig.cap = "\\label{fig:predSolCostErrorUsingEverything}True solution cost error vs. predicted solution cost error on each problem in the test set using linear model trained for each reserve selector on input feature set containing all recorded variables both knowable and unknowable at run time (listed in Table 6 \\textcolor{cyan}{\\ref{tab:makeEverythingVarsTable}}).  Adjusted $R^2$ and rmse (on the test set) for each of the four learned prediction models are shown in the upper left corner of each plot.  Problems are colored by their dominant error type.  For reference, the line corresponding to a perfect fit of predicted to true values is shown diagonally upward from the origin."}

all_fitting_scores_df = fit_and_predict_output_error_using_feature_set (rs_method_names_list, 
          train_x_df = p3_train_x_df,    # p3_train_x_df_probSize
          test_x_df = p3_test_x_df,    # p3_test_x_df_probSize
          working_train_df = p3_working_train_df, 
          working_test_df = p3_working_test_df, 
          train_aux_df = p3_train_aux_df, 
          test_aux_df = p3_test_aux_df, 
          train_df__before_any_preprocessing = p3_working_train_df__before_any_preprocessing, 
          test_df__before_any_preprocessing  = p3_working_test_df__before_any_preprocessing, 
          params,  

          all_fitting_scores_df, x_min_on_plot = NA,      
          x_max_on_plot = NA, y_min_on_plot = NA, y_max_on_plot = NA,       
                               
          fitting_func = fit_cost_err_frac,
          perf_metric_name_for_file_name_str = "abs_sol_cost_err_resid", 
          
          vars_used_str,  
          inVars,
          source_string, 
          display_train_as_final_pred_using_plot
          )

```

```{r testQuiet8, eval=FALSE, echo=params$echoExamplefitResultsCoefficient}
####  Example SOLUTION COST ERROR lm model fit using Everything data

============start lm results for Gurobi ===================      
Coefficients: (11 not defined because of singularities)      
                                              Estimate Std. Error t value      Pr(>|t|)          
(Intercept)                                   0.078268   0.001822  42.963       < 2e-16 ***      
rsp_combined_num_occ_and_unocc_PUs            0.351724   0.100239   3.509      0.000461 ***      
rsp_num_occupied_PUs                        -14.534449   1.815166  -8.007      2.05e-15 ***      
rsp_num_spp                                  -5.670052   0.915187  -6.196      7.13e-10 ***      
rsp_num_spp_per_PU                                  NA         NA      NA            NA          
sppPUprod                                           NA         NA      NA            NA          
rsp_alpha__                                  -0.004934   0.005233  -0.943      0.345948          
rsp_n__num_groups                             0.067240   0.020991   3.203      0.001382 **       
rsp_p__prop_of_links_between_groups          -0.044518   0.031606  -1.409      0.159149          
rsp_r__density                                0.001196   0.003957   0.302      0.762446          
rsp_d__number_of_nodes_per_group              0.228947   0.071434   3.205      0.001373 **       
rsp_nominal_p__prop_of_links_between_groups   0.031920   0.021782   1.465      0.142973          
actual_sol_frac_of_landscape                 -0.360250   0.047991  -7.507      9.36e-14 ***      
ig_top                                              NA         NA      NA            NA          
ig_bottom                                           NA         NA      NA            NA          
ig_num_edges_m                               23.838628   3.999471   5.960      3.00e-09 ***      
ig_ktop                                     -11.622231   2.174775  -5.344      1.02e-07 ***      
ig_kbottom                                   -2.489358   1.350400  -1.843      0.065426 .        
ig_bidens                                           NA         NA      NA            NA          
ig_lcctop                                    -0.427087   0.062934  -6.786      1.54e-11 ***      
ig_lccbottom                                  0.217896   0.061960   3.517      0.000447 ***      
ig_distop                                    -0.446906   0.064642  -6.914      6.47e-12 ***      
ig_disbottom                                  0.616397   0.066772   9.231       < 2e-16 ***      
ig_cctop                                      0.413411   0.444594   0.930      0.352564          
ig_ccbottom                                   0.067012   0.209345   0.320      0.748927          
ig_cclowdottop                                0.456787   0.192091   2.378      0.017509 *        
ig_cclowdotbottom                            -0.047157   0.127765  -0.369      0.712104          
ig_cctopdottop                               -0.912614   0.327281  -2.788      0.005350 **       
ig_cctopdotbottom                             0.376267   0.155817   2.415      0.015840 *        
ig_mean_bottom_bg_redundancy                  0.163012   0.038474   4.237      2.38e-05 ***      
ig_median_bottom_bg_redundancy               -0.097482   0.036595  -2.664      0.007793 **       
ig_mean_top_bg_redundancy                    -0.181423   0.038390  -4.726      2.46e-06 ***      
ig_median_top_bg_redundancy                   0.159802   0.030338   5.267      1.54e-07 ***      
connectance                                         NA         NA      NA            NA          
web_asymmetry                                 0.496307   0.106109   4.677      3.12e-06 ***      
links_per_PUsAndSpp                           5.208261   2.842519   1.832      0.067071 .        
cluster_coefficient                           0.037830   0.016455   2.299      0.021615 *        
specialisation_asymmetry                     -0.026719   0.007725  -3.459      0.000554 ***      
linkage_density                               2.278653   0.217792  10.462       < 2e-16 ***      
weighted_connectance                         -5.135327   0.651202  -7.886      5.28e-15 ***      
Shannon_diversity                             0.060250   0.141457   0.426      0.670211          
interaction_evenness                          1.282829   0.518716   2.473      0.013484 *        
Alatalo_interaction_evenness                 -0.001951   0.001941  -1.006      0.314781          
number.of.Spp                                       NA         NA      NA            NA          
mean.number.of.shared.partners.PUs          -25.829826   4.589570  -5.628      2.10e-08 ***      
mean.number.of.shared.partners.Spp           -3.379705   0.524597  -6.442      1.49e-10 ***      
cluster.coefficient.PUs                       4.343964   0.521821   8.325       < 2e-16 ***      
cluster.coefficient.Spp                       8.534913   1.958904   4.357      1.39e-05 ***      
niche.overlap.PUs                            -0.508340   0.189270  -2.686      0.007300 **       
niche.overlap.Spp                            -0.106240   0.291652  -0.364      0.715697          
togetherness.PUs                              0.066902   0.046467   1.440      0.150100          
togetherness.Spp                              0.007608   0.022938   0.332      0.740172          
C.score.PUs                                  -1.038976   0.153215  -6.781      1.60e-11 ***      
C.score.Spp                                   1.045259   0.187136   5.586      2.67e-08 ***      
V.ratio.PUs                                  -0.022616   0.028665  -0.789      0.430231          
V.ratio.Spp                                   0.031320   0.024824   1.262      0.207223          
functional.complementarity.PUs                2.586342   0.305127   8.476       < 2e-16 ***      
functional.complementarity.Spp                5.044068   0.371864  13.564       < 2e-16 ***      
partner.diversity.PUs                        -0.071630   0.139001  -0.515      0.606392          
partner.diversity.Spp                         2.497913   0.545648   4.578      5.01e-06 ***      
generality.PUs                               -0.591629   0.109094  -5.423      6.62e-08 ***      
vulnerability.Spp                             6.453383   1.019810   6.328      3.10e-10 ***      
rsp_realized_FP_rate                         -0.147120   0.066469  -2.213      0.026994 *        
rsp_realized_FN_rate                          0.034886   0.002939  11.868       < 2e-16 ***      
rsp_realized_Ftot_rate                        0.109711   0.010861  10.101       < 2e-16 ***      
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA            NA          
gurobi_mipgap                                -0.011563   0.007401  -1.562      0.118386          
edge_frac_of_possible                               NA         NA      NA            NA          
---      
Residual standard error: 0.0798 on 1860 degrees of freedom      
Multiple R-squared:  0.9433, Adjusted R-squared:  0.9416       
F-statistic: 533.9 on 58 and 1860 DF,  p-value: < 2.2e-16      
      
============start lm results for Marxan_SA ===================      
Coefficients: (11 not defined because of singularities)      
                                              Estimate Std. Error t value     Pr(>|t|)        
(Intercept)                                   0.096163   0.001844  52.161      < 2e-16 ***    
rsp_combined_num_occ_and_unocc_PUs            0.307886   0.101441   3.035     0.002438 **     
rsp_num_occupied_PUs                        -17.142607   1.836922  -9.332      < 2e-16 ***    
rsp_num_spp                                  -7.138125   0.926155  -7.707     2.08e-14 ***    
rsp_num_spp_per_PU                                  NA         NA      NA           NA        
sppPUprod                                           NA         NA      NA           NA        
rsp_alpha__                                  -0.004939   0.005296  -0.933     0.351164        
rsp_n__num_groups                             0.073988   0.021243   3.483     0.000507 ***    
rsp_p__prop_of_links_between_groups          -0.038015   0.031985  -1.189     0.234787        
rsp_r__density                               -0.001546   0.004004  -0.386     0.699438        
rsp_d__number_of_nodes_per_group              0.231935   0.072290   3.208     0.001358 **     
rsp_nominal_p__prop_of_links_between_groups   0.026711   0.022043   1.212     0.225749        
actual_sol_frac_of_landscape                 -0.349745   0.048567  -7.201     8.62e-13 ***    
ig_top                                              NA         NA      NA           NA        
ig_bottom                                           NA         NA      NA           NA        
ig_num_edges_m                               30.041680   4.047406   7.422     1.74e-13 ***    
ig_ktop                                     -13.800732   2.200841  -6.271     4.46e-10 ***    
ig_kbottom                                   -2.967333   1.366585  -2.171     0.030031 *      
ig_bidens                                           NA         NA      NA           NA        
ig_lcctop                                    -0.461485   0.063689  -7.246     6.26e-13 ***    
ig_lccbottom                                  0.241982   0.062702   3.859     0.000118 ***    
ig_distop                                    -0.400558   0.065417  -6.123     1.12e-09 ***    
ig_disbottom                                  0.583708   0.067572   8.638      < 2e-16 ***    
ig_cctop                                      0.428514   0.449923   0.952     0.341010        
ig_ccbottom                                  -0.091854   0.211854  -0.434     0.664650        
ig_cclowdottop                                0.345992   0.194393   1.780     0.075263 .      
ig_cclowdotbottom                             0.135360   0.129296   1.047     0.295282        
ig_cctopdottop                               -0.923216   0.331204  -2.787     0.005366 **     
ig_cctopdotbottom                             0.519771   0.157685   3.296     0.000998 ***    
ig_mean_bottom_bg_redundancy                  0.154042   0.038935   3.956     7.89e-05 ***    
ig_median_bottom_bg_redundancy               -0.099817   0.037034  -2.695     0.007096 **     
ig_mean_top_bg_redundancy                    -0.183186   0.038850  -4.715     2.59e-06 ***    
ig_median_top_bg_redundancy                   0.155336   0.030702   5.059     4.62e-07 ***    
connectance                                         NA         NA      NA           NA        
web_asymmetry                                 0.412018   0.107381   3.837     0.000129 ***    
links_per_PUsAndSpp                           6.541984   2.876587   2.274     0.023066 *      
cluster_coefficient                           0.040122   0.016652   2.409     0.016074 *      
specialisation_asymmetry                     -0.027196   0.007817  -3.479     0.000515 ***    
linkage_density                               2.371644   0.220403  10.760      < 2e-16 ***    
weighted_connectance                         -5.220309   0.659007  -7.921     4.00e-15 ***    
Shannon_diversity                            -0.089226   0.143153  -0.623     0.533170        
interaction_evenness                          1.337941   0.524933   2.549     0.010890 *      
Alatalo_interaction_evenness                 -0.001907   0.001964  -0.971     0.331594        
number.of.Spp                                       NA         NA      NA           NA        
mean.number.of.shared.partners.PUs          -31.104227   4.644577  -6.697     2.81e-11 ***    
mean.number.of.shared.partners.Spp           -3.385690   0.530885  -6.377     2.27e-10 ***    
cluster.coefficient.PUs                       4.215334   0.528075   7.982     2.49e-15 ***    
cluster.coefficient.Spp                       9.198471   1.982382   4.640     3.73e-06 ***    
niche.overlap.PUs                            -0.539827   0.191538  -2.818     0.004878 **     
niche.overlap.Spp                            -0.166854   0.295148  -0.565     0.571922        
togetherness.PUs                              0.089489   0.047024   1.903     0.057188 .      
togetherness.Spp                              0.012503   0.023213   0.539     0.590214        
C.score.PUs                                  -1.008392   0.155051  -6.504     1.00e-10 ***    
C.score.Spp                                   0.927280   0.189379   4.896     1.06e-06 ***    
V.ratio.PUs                                  -0.034475   0.029009  -1.188     0.234808        
V.ratio.Spp                                   0.060022   0.025122   2.389     0.016983 *      
functional.complementarity.PUs                2.627444   0.308784   8.509      < 2e-16 ***    
functional.complementarity.Spp                4.965851   0.376321  13.196      < 2e-16 ***    
partner.diversity.PUs                        -0.208199   0.140667  -1.480     0.139022        
partner.diversity.Spp                         3.024917   0.552188   5.478     4.89e-08 ***    
generality.PUs                               -0.483488   0.110402  -4.379     1.26e-05 ***    
vulnerability.Spp                             8.080114   1.032032   7.829     8.17e-15 ***    
rsp_realized_FP_rate                         -0.135645   0.067266  -2.017     0.043886 *      
rsp_realized_FN_rate                          0.035120   0.002975  11.806      < 2e-16 ***    
rsp_realized_Ftot_rate                        0.110020   0.010991  10.010      < 2e-16 ***    
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA           NA        
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA           NA        
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA           NA        
gurobi_mipgap                                -0.013636   0.007490  -1.821     0.068822 .      
edge_frac_of_possible                               NA         NA      NA           NA        
---      
Residual standard error: 0.08076 on 1860 degrees of freedom      
Multiple R-squared:  0.9346, Adjusted R-squared:  0.9326       
F-statistic: 458.4 on 58 and 1860 DF,  p-value: < 2.2e-16      
      
============start lm results for UR_Forward ===================      
Coefficients: (11 not defined because of singularities)      
                                              Estimate Std. Error t value      Pr(>|t|)          
(Intercept)                                   0.139418   0.002023  68.909       < 2e-16 ***      
rsp_combined_num_occ_and_unocc_PUs            0.429779   0.111324   3.861      0.000117 ***      
rsp_num_occupied_PUs                        -16.742370   2.015896  -8.305       < 2e-16 ***      
rsp_num_spp                                  -5.846013   1.016392  -5.752      1.03e-08 ***      
rsp_num_spp_per_PU                                  NA         NA      NA            NA          
sppPUprod                                           NA         NA      NA            NA          
rsp_alpha__                                  -0.008568   0.005812  -1.474      0.140606          
rsp_n__num_groups                             0.068411   0.023313   2.935      0.003382 **       
rsp_p__prop_of_links_between_groups          -0.042947   0.035102  -1.223      0.221299          
rsp_r__density                               -0.001522   0.004394  -0.346      0.729188          
rsp_d__number_of_nodes_per_group              0.251633   0.079333   3.172      0.001539 **       
rsp_nominal_p__prop_of_links_between_groups   0.030770   0.024191   1.272      0.203544          
actual_sol_frac_of_landscape                 -0.384291   0.053298  -7.210      8.09e-13 ***      
ig_top                                              NA         NA      NA            NA          
ig_bottom                                           NA         NA      NA            NA          
ig_num_edges_m                               28.490251   4.441751   6.414      1.79e-10 ***      
ig_ktop                                     -12.666152   2.415272  -5.244      1.75e-07 ***      
ig_kbottom                                   -2.453165   1.499734  -1.636      0.102065          
ig_bidens                                           NA         NA      NA            NA          
ig_lcctop                                    -0.405649   0.069894  -5.804      7.60e-09 ***      
ig_lccbottom                                  0.217393   0.068812   3.159      0.001607 **       
ig_distop                                    -0.409531   0.071791  -5.705      1.35e-08 ***      
ig_disbottom                                  0.564543   0.074156   7.613      4.23e-14 ***      
ig_cctop                                      1.028662   0.493760   2.083      0.037358 *        
ig_ccbottom                                   0.034130   0.232496   0.147      0.883306          
ig_cclowdottop                               -0.012821   0.213333  -0.060      0.952085          
ig_cclowdotbottom                             0.055558   0.141894   0.392      0.695438          
ig_cctopdottop                               -1.345170   0.363473  -3.701      0.000221 ***      
ig_cctopdotbottom                             0.394058   0.173048   2.277      0.022890 *        
ig_mean_bottom_bg_redundancy                  0.109353   0.042728   2.559      0.010568 *        
ig_median_bottom_bg_redundancy               -0.072665   0.040642  -1.788      0.073949 .        
ig_mean_top_bg_redundancy                    -0.198614   0.042635  -4.658      3.41e-06 ***      
ig_median_top_bg_redundancy                   0.167682   0.033693   4.977      7.06e-07 ***      
connectance                                         NA         NA      NA            NA          
web_asymmetry                                 0.521329   0.117843   4.424      1.03e-05 ***      
links_per_PUsAndSpp                           5.244278   3.156857   1.661      0.096835 .        
cluster_coefficient                           0.039559   0.018275   2.165      0.030540 *        
specialisation_asymmetry                     -0.023184   0.008579  -2.702      0.006946 **       
linkage_density                               2.382488   0.241877   9.850       < 2e-16 ***      
weighted_connectance                         -5.019233   0.723215  -6.940      5.39e-12 ***      
Shannon_diversity                             0.067326   0.157100   0.429      0.668296          
interaction_evenness                          0.940035   0.576078   1.632      0.102895          
Alatalo_interaction_evenness                 -0.002639   0.002155  -1.224      0.220942          
number.of.Spp                                       NA         NA      NA            NA          
mean.number.of.shared.partners.PUs          -31.693724   5.097105  -6.218      6.20e-10 ***      
mean.number.of.shared.partners.Spp           -3.328453   0.582610  -5.713      1.29e-08 ***      
cluster.coefficient.PUs                       4.524515   0.579527   7.807      9.68e-15 ***      
cluster.coefficient.Spp                      10.886198   2.175528   5.004      6.15e-07 ***      
niche.overlap.PUs                            -0.306414   0.210200  -1.458      0.145085          
niche.overlap.Spp                            -0.659684   0.323904  -2.037      0.041825 *        
togetherness.PUs                              0.123762   0.051606   2.398      0.016573 *        
togetherness.Spp                              0.009001   0.025474   0.353      0.723878          
C.score.PUs                                  -1.028568   0.170158  -6.045      1.80e-09 ***      
C.score.Spp                                   0.775603   0.207831   3.732      0.000196 ***      
V.ratio.PUs                                  -0.019658   0.031835  -0.617      0.536987          
V.ratio.Spp                                   0.032959   0.027570   1.195      0.232051          
functional.complementarity.PUs                2.703107   0.338869   7.977      2.60e-15 ***      
functional.complementarity.Spp                4.893785   0.412986  11.850       < 2e-16 ***      
partner.diversity.PUs                        -0.114503   0.154373  -0.742      0.458345          
partner.diversity.Spp                         3.545143   0.605988   5.850      5.79e-09 ***      
generality.PUs                               -0.514604   0.121159  -4.247      2.27e-05 ***      
vulnerability.Spp                             6.824213   1.132585   6.025      2.03e-09 ***      
rsp_realized_FP_rate                         -0.180546   0.073820  -2.446      0.014546 *        
rsp_realized_FN_rate                          0.036122   0.003265  11.065       < 2e-16 ***      
rsp_realized_Ftot_rate                        0.109754   0.012062   9.099       < 2e-16 ***      
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA            NA          
gurobi_mipgap                                -0.032244   0.008219  -3.923      9.07e-05 ***      
edge_frac_of_possible                               NA         NA      NA            NA          
---      
Residual standard error: 0.08863 on 1860 degrees of freedom      
Multiple R-squared:  0.9328, Adjusted R-squared:  0.9307       
F-statistic: 445.1 on 58 and 1860 DF,  p-value: < 2.2e-16      
      
============start lm results for Marxan_SA_SS ===================      
Coefficients: (11 not defined because of singularities)      
                                              Estimate Std. Error t value      Pr(>|t|)          
(Intercept)                                   0.337808   0.004422  76.385       < 2e-16 ***      
rsp_combined_num_occ_and_unocc_PUs            0.100575   0.243339   0.413      0.679425          
rsp_num_occupied_PUs                        -31.472997   4.406448  -7.142      1.31e-12 ***      
rsp_num_spp                                  -9.950247   2.221682  -4.479      7.97e-06 ***      
rsp_num_spp_per_PU                                  NA         NA      NA            NA          
sppPUprod                                           NA         NA      NA            NA          
rsp_alpha__                                   0.003437   0.012704   0.271      0.786783          
rsp_n__num_groups                             0.132463   0.050958   2.599      0.009411 **       
rsp_p__prop_of_links_between_groups           0.013126   0.076727   0.171      0.864189          
rsp_r__density                               -0.013415   0.009605  -1.397      0.162681          
rsp_d__number_of_nodes_per_group              0.317221   0.173411   1.829      0.067515 .        
rsp_nominal_p__prop_of_links_between_groups  -0.013873   0.052877  -0.262      0.793077          
actual_sol_frac_of_landscape                 -0.341804   0.116502  -2.934      0.003389 **       
ig_top                                              NA         NA      NA            NA          
ig_bottom                                           NA         NA      NA            NA          
ig_num_edges_m                               63.183273   9.709007   6.508      9.79e-11 ***      
ig_ktop                                     -17.898318   5.279426  -3.390      0.000713 ***      
ig_kbottom                                    0.767226   3.278195   0.234      0.814980          
ig_bidens                                           NA         NA      NA            NA          
ig_lcctop                                    -0.582762   0.152778  -3.814      0.000141 ***      
ig_lccbottom                                  0.191542   0.150412   1.273      0.203019          
ig_distop                                    -0.701694   0.156924  -4.472      8.23e-06 ***      
ig_disbottom                                  0.999694   0.162093   6.167      8.49e-10 ***      
ig_cctop                                      1.639375   1.079285   1.519      0.128946          
ig_ccbottom                                  -0.630949   0.508201  -1.242      0.214564          
ig_cclowdottop                               -0.754884   0.466315  -1.619      0.105654          
ig_cclowdotbottom                             0.606495   0.310159   1.955      0.050682 .        
ig_cctopdottop                               -2.034106   0.794498  -2.560      0.010538 *        
ig_cctopdotbottom                             1.059046   0.378257   2.800      0.005166 **       
ig_mean_bottom_bg_redundancy                  0.187077   0.093398   2.003      0.045321 *        
ig_median_bottom_bg_redundancy               -0.029407   0.088837  -0.331      0.740666          
ig_mean_top_bg_redundancy                    -0.268148   0.093194  -2.877      0.004056 **       
ig_median_top_bg_redundancy                   0.248869   0.073649   3.379      0.000742 ***      
connectance                                         NA         NA      NA            NA          
web_asymmetry                                -0.040454   0.257587  -0.157      0.875224          
links_per_PUsAndSpp                           1.947810   6.900421   0.282      0.777765          
cluster_coefficient                           0.064892   0.039946   1.624      0.104439          
specialisation_asymmetry                      0.009596   0.018752   0.512      0.608897          
linkage_density                               2.368650   0.528707   4.480      7.92e-06 ***      
weighted_connectance                         -5.887253   1.580841  -3.724      0.000202 ***      
Shannon_diversity                            -0.418213   0.343398  -1.218      0.223429          
interaction_evenness                         -0.087543   1.259222  -0.070      0.944582          
Alatalo_interaction_evenness                 -0.002385   0.004711  -0.506      0.612685          
number.of.Spp                                       NA         NA      NA            NA          
mean.number.of.shared.partners.PUs          -74.363555  11.141515  -6.674      3.26e-11 ***      
mean.number.of.shared.partners.Spp           -3.783117   1.273498  -2.971      0.003010 **       
cluster.coefficient.PUs                       4.433649   1.266759   3.500      0.000476 ***      
cluster.coefficient.Spp                      26.296513   4.755382   5.530      3.66e-08 ***      
niche.overlap.PUs                            -0.518532   0.459466  -1.129      0.259232          
niche.overlap.Spp                             0.392187   0.708007   0.554      0.579693          
togetherness.PUs                              0.097650   0.112802   0.866      0.386780          
togetherness.Spp                              0.069296   0.055683   1.244      0.213483          
C.score.PUs                                  -1.276330   0.371941  -3.432      0.000613 ***      
C.score.Spp                                   1.038807   0.454287   2.287      0.022327 *        
V.ratio.PUs                                  -0.214494   0.069587  -3.082      0.002083 **       
V.ratio.Spp                                   0.048289   0.060263   0.801      0.423061          
functional.complementarity.PUs                3.725922   0.740717   5.030      5.37e-07 ***      
functional.complementarity.Spp                4.713016   0.902727   5.221      1.98e-07 ***      
partner.diversity.PUs                        -0.668926   0.337436  -1.982      0.047584 *        
partner.diversity.Spp                         9.103580   1.324600   6.873      8.57e-12 ***      
generality.PUs                               -0.209220   0.264835  -0.790      0.429628          
vulnerability.Spp                            14.005302   2.475662   5.657      1.78e-08 ***      
rsp_realized_FP_rate                         -0.213933   0.161359  -1.326      0.185061          
rsp_realized_FN_rate                          0.092462   0.007136  12.957       < 2e-16 ***      
rsp_realized_Ftot_rate                        0.156690   0.026366   5.943      3.34e-09 ***      
rsp_euc_realized_FP_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_FN_and_cost_in_err_frac            NA         NA      NA            NA          
rsp_euc_realized_Ftot_and_cost_in_err_frac          NA         NA      NA            NA          
gurobi_mipgap                                 0.029381   0.017967   1.635      0.102155          
edge_frac_of_possible                               NA         NA      NA            NA          
---      
Residual standard error: 0.1937 on 1860 degrees of freedom      
Multiple R-squared:  0.654,  Adjusted R-squared:  0.6432       
F-statistic: 60.61 on 58 and 1860 DF,  p-value: < 2.2e-16      
```

##  Results **Summary** for learning to predict output errors    

To make it easier to compare all of these predictive results, we show summary plots of rmse (**Fig. 18 \textcolor{cyan}{\ref{fig:barPlotRMSEvaluesForPreds}}**) and $R^2$ (**Fig. 19 \textcolor{cyan}{\ref{fig:barPlotAdjR2valuesForPreds}}**) for both output error types for SA and SA_SS.  We only show those two reserve selectors for visual simplicity and because all of the NV reserve selector results are so similar to the SA results.  We chose SA as the representative NV selector because it's the most like SA_SS and so, seems like the most informative comparison in relation to the effects of using a voting method over a non-voting method.    

The first result is that representation shortfall is much easier to predict than solution cost error for all of the reserve selectors.  The second result is that predicting solution cost error for SA_SS is much harder than predicting solution cost error for any of the other reserve selectors.  Even when cheating and using variables that would not be available for real predictive situations, we were unable to make very useful predictions for SA_SS.  Finally, the typically reported number of species and PUs were almost completely uninformative on their own.  However, the addition of density-related variables and graph complexity measures each radically improved prediction of output error, e.g., roughly tripling the $R^2$ values from using number of species and PUs alone.   

```{r filterToSmallSetOfFittingScores, include=FALSE}
all_fitting_scores_df %>% 
    filter (
            # (vars_used_str == "PCA1thru10") | 
            # (vars_used_str == "DormannProbSize") | 
        (vars_used_str == "PUsAndSppOnly") | 
            # (vars_used_str == "sppPUprod") | 
        ##2024 02 01##(vars_used_str == "LinksPerNodeOnly") | 
            # (vars_used_str == "PUs_Spp_SppPUprod") | 
        (vars_used_str == "ProbSizeAndDensity") | 
            # (vars_used_str == "FifthCut") | 
            # (vars_used_str == "SixthRemoval") | 
            # (vars_used_str == "FifthRemoval") | 
            # (vars_used_str == "FourthRemoval") | 
            # (vars_used_str == "SecondRemoval") | 
            # (vars_used_str == "FirstRemoval") | 
#            (vars_used_str == "spp_PUs_only") | 

        ##2024 02 01##(vars_used_str == "Glmnet7var") | 
            # (vars_used_str == "Glmnet9var") | 
        (vars_used_str == "nonLatapyGraph") | 
        ##2024 02 01##(vars_used_str == "graph")
        ##
        (vars_used_str == "Everything")  
             
             ) -> all_fitting_scores_df 

all_fitting_scores_df$ordered_vars_used = 
    factor (all_fitting_scores_df$vars_used, levels = c(

            # "sppPUprod",                     #  1 variable
      "PUsAndSppOnly",                 #  2 variables
            # "PUs_Spp_SppPUprod",             #  3 variables
      ##2024 02 01##"LinksPerNodeOnly",              #  1 variable
            # "DormannProbSize",               #  3 variables
            # 
            # "SixthRemoval",                  #  6 variables
      "ProbSizeAndDensity",                   #  5 variables
            # "FifthCut",                      #  8 variables
            # "PCA1thru10",                    #  10 variables
            # "FifthRemoval",                  #  9 variables
            
            
            
      ##2024 02 01##"Glmnet7var",                    #  7 variables
            # "Glmnet9var",                    #  9 variables
            # "FourthRemoval",                 #  20 variables 
            # "SecondRemoval",                 #  23 variables 
      "nonLatapyGraph",                #  27 variables
            
      ##2024 02 01##"graph",                         #  39 variables
            # "FirstRemoval",                  #  37 variables
            
      "Everything"                     #  69 variables 
            
             ))

rs_names_to_bar_plot = c("SA", "SA_SS")
```

```{r buildMarxanReducedFittingScoresSet, include=FALSE}

###  Filter the set of fitting scores down to include only the scores for SA and SA_SS on the test set.  

all_fitting_scores_df %>% 
    filter ((rs_method_name   == "SA") | 
              (rs_method_name == "SA_SS")) %>% 
    filter (train_or_test == "TEST") %>% 
    select (vars_used_str, measure_name_str, rs_method_name, rmse, adj_R2, ordered_vars_used) %>% 
    mutate (rmse = round (rmse, 2), 
            adj_R2 = round (adj_R2, 2)) -> 
  only_marxans_fitting_scores_df

only_marxans_fitting_scores_df %>% 
    mutate (label_measure_name_str = 
            ifelse (measure_name_str == "abs_rep_shortfall_resid", 
                    "Representation Shortfall", "Solution Cost Error")) -> 
    only_marxans_fitting_scores_df            
```

###  Summary of RMSE scores for predictions of rep shortfall and solution cost error for SA and SS

```{r barPlotRMSEvaluesForPredictions, fig.cap = "\\label{fig:barPlotRMSEvaluesForPreds}Summary of RMSE scores on test set for linear model predictions of rep shortfall and solution cost error for SA and and SA SS using four different input feature sets.  These are the RMSE values from the upper left corner of the SA and SA SS results in Figs. 10 \\textcolor{cyan}{ \\ref{fig:predRepShortfallUsingPUsAndSppOnly}} through 17 \\textcolor{cyan}{ \\ref{fig:predSolCostErrorUsingEverything}}.  Columns in the figure correspond to the output errors being predicted and rows correspond to the two reserve selectors whose performance is being summarized in the plot."}

  #  Based on facetting examples from 
  #  http://zevross.com/blog/2019/04/02/easy-multi-panel-plots-in-r-using-facet_wrap-and-facet_grid-from-ggplot2/

   #  Got some of this ggplot stuff from https://appsilon.com/ggplot2-bar-charts/
   #  
   #  Got factor ordering from https://sebastiansauer.github.io/ordering-bars/
   #  
   #  Also, some good sites for getting names of colors:
   #    - This one also has colorblind palettes:
   #      http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
   #    - Lots of colors
   #      https://www.datanovia.com/en/blog/awesome-list-of-657-r-color-names/

ggplot(data = only_marxans_fitting_scores_df, aes (x=ordered_vars_used, y=rmse)) +
                geom_col (position = position_dodge(), fill = "deepskyblue2") +  #"#3db5ff") +
                geom_text (aes(label = rmse), hjust = 1.4, size = 2.2) +
                coord_flip() + 
                labs (y = "RMSE", x = "Input Feature Set") + 
                ggtitle ("RMSE values for predicting output errors\nby Feature Set") + 
                theme(plot.title = element_text(hjust = 0.5)) + 
#    facet_grid (label_measure_name_str ~ rs_method_name) 
    facet_grid (rs_method_name ~ label_measure_name_str) 
```

###  Summary of Adjusted $R^2$ scores for predictions of rep shortfall and solution cost error for SA and SS

```{r barPlotAdjR2valuesForPredictions, fig.cap = "\\label{fig:barPlotAdjR2valuesForPreds}Summary of $R^2$ scores on test set for linear model predictions of rep shortfall and solution cost error for SA and and SA SS using four different input feature sets.  These are the $R^2$ values from the upper left corner of the SA and SA SS results in Figs. 10  \\textcolor{cyan}{\\ref{fig:predRepShortfallUsingPUsAndSppOnly}} through 17  \\textcolor{cyan}{\\ref{fig:predSolCostErrorUsingEverything}}.  Columns in the figure correspond to the output errors being predicted and rows correspond to the two reserve selectors whose performance is being summarized in the plot."}

ggplot(data = only_marxans_fitting_scores_df, aes (x=ordered_vars_used, y=adj_R2)) +
                geom_col (position = position_dodge(), fill = "goldenrod1") +  #"#0099f9") +
                geom_text (aes(label = adj_R2), hjust = 1.4, size = 2.2) +    # hjust = -0.2
                coord_flip() + 
                labs (y = "Adj R2", x = "Input Feature Set") + 
                ggtitle ("Adj R2 values for predicting output errors\nby Feature Set") + 
                theme(plot.title = element_text(hjust = 0.5)) + 
#    facet_grid (label_measure_name_str ~ rs_method_name) 
    facet_grid (rs_method_name ~ label_measure_name_str) 
```

----------

#  Discussion  

##  Discussion: Main points of paper  {#mainPointsDiscussionSection}   

The main goals of this paper are to demonstrate the importance of problem structure to method accuracy under uncertainty and to demonstrate the value of changing the focus of SCP evaluation away from case studies toward explicitly predicting method accuracy on individual problems under uncertainty.  

Our goal is not to definitively show which reserve selector is best.  Rather, we provide one approach for how problem structure and accuracy prediction under uncertainty can be incorporated into a sequence of steps that provides greater insight into an SCP method's strengths and weaknesses than current approaches to evaluation.  

We have used three simple, reusable ideas to address both prediction and understanding.  First, we have borrowed theoretically grounded **solution planting** methods from computational theory to explore synthetic reserve design problems with known correct solutions over a greater range of problem difficulty than has been currently examined in the SCP literature to date.  Second, we have shown how we can improve the ecological realism of these synthetic test problems by **embedding/wrapping** their species distribution inside whatever distribution a user considers more ecologically realistic to form a larger problem that is guaranteed to have the same optimal solution as the original problem. Third, we have shown how these reserve selection problems can be reframed as a **bipartite graph**.  This enables us to borrow measures from graph theory as inputs for fitting explicit predictive models of the accuracy of a given reserve selector on a given individual problem under uncertainty.  

Each of these three simple ideas could be profitably extended by other SCP researchers.  Here, we have combined them to given an integrated example of new problem generation techniques with source code a new set of shared benchmark data, and examples of learning to predict method behavior.  This study is important and unusual because it provides a large, shared sample of stochastically generated problems with the following attributes: i) controlled variation in difficulty beyond problem size, ii) known optimal solutions, iii) known and varying uncertainties of multiple types for each problem, and iv) learned predictions of reserve selector accuracy on individual problems.  

##  Discussion: Principal findings  {#principalFindingsDiscussionSection}  

Our results clearly show that variation in problem structure results in a wide range of problem difficulty for every tested level of uncertainty.  This was the case for all tested reserve selectors:  ILP, Simulated Annealing, Unprotected Richness, and Summed Solution.  Notably, while areas selected by ILP and Simulated Annealing had no errors on problems without uncertainty, adding even small amounts of input error ($\leq 10\%$) often led to both large amounts of output error and large amounts of variation in that output error at nearly every tested level of input error.  Importantly, optimization nearly always magnified input error in our data set (i.e., in `r mag_frac`% of all problems with input error had larger output error than input error).  

The fact that the tested reserve selectors had error in their outputs is unsurprising given that  none of them attempted to address uncertainty in the inputs (see **Section 10.3 \textcolor{cyan}{\ref{methodsTestedDiscussionSection} for discussion of testing methods that do address uncertainty)}**.  However, reserve selection without addressing uncertainty is still common and both the *amount* and *variation* of error magnification, was surprisingly large at nearly every level of input error (Figs. 5  **\textcolor{cyan}{\ref{fig:InVsTotOutErrLE125inResults}}**, 7  **\textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**, 8  **\textcolor{cyan}{\ref{fig:inVsRepShortfallMagLE25inResults}}**).  Furthermore, our results come from using only small amounts of input error compared to what is likely to be found in real-world data.  For example,  @tullochIncorporatingUncertaintyAssociated2013 tested their method for addressing uncertainty on real-world benthic habitat maps where error ranged from 0 to 72.5% error with a mean of 33% error, while all our input errors were no greater than 10% ^[ASCELIN: If you can find at least one other paper that has larger amounts of input error you could just make this sentence more generic and say input uncertainties have been considered up to xx\% while we only have errors up to 10\%.  I think this is also a very important point to strongly justify the relevance of your work.].  

In our tests, we measured two types of input error (False Positive-dominant and False Negative-dominant) and two types of output error (solution cost error and representation shortfall).  Unsurprisingly, the type of input error (FP-dominant or FN-dominant) was a reliable predictor of whether the output error was more in the form of a solution cost error (for FN-dominated) or representation shortfall (for FP-dominated).  This demonstrates the importance of testing on multiple types of input error and measuring multiple types of output error.   

In spite of these negative findings, there are two encouraging counterpoints in our results.  First, deriving our problem generators from computational theory on problem difficulty successfully created a benchmark set with demonstrable variety in individual problem difficulty, independent of problem size and uncertainty.  Second, computing and using attributes of problem structure as predictor inputs provided considerable gains in our ability to predict the amount of error in the reserve selection solutions beyond using problem size alone.  

The most important implication of this study is that evaluating conservation planning methods on either a single example, a small set of examples, or a larger set with identical statistical structure tells us little about the accuracy of the method beyond the test sample unless the method has theoretical guarantees.  If it has theoretical guarantees, the assumptions behind those guarantees must be matched by the data.  As our results show, violating our ILP solver's assumption of having no input errors seriously degraded its accuracy and eliminated its guaranteed quantification of solution quality.  

A valuable next step in testing is to explore the accuracy of robust ILP approaches that include input uncertainty, however, changing to robust forms of ILP does not eliminate assumption violations.  It does make the assumptions about input errors more transparent and is likely to be more robust than ignoring input uncertainty altogether, however, it still requires that all input uncertainties are included and accurately calibrated, which may be a significant challenge.   

##  Discussion: Methods tested  {#methodsTestedDiscussionSection}  

None of the reserve selectors that we evaluated in our method comparison (**Section 8 \textcolor{cyan}{\ref{comparisonResultsSection}}**) attempt to address uncertainty in the inputs.  In recent years, multiple authors have addressed uncertainty by showing how problems can be encoded with theoretical guarantees for ILP to take specific input uncertainties into account.  Consequently, some readers will feel that the problem of uncertainty has already been solved and our method comparisons are not useful.  We disagree for several reasons.  

First, we believe that our comparison of methods is useful because our main purpose is to investigate how we might better learn to predict the amount of output error and the problem characteristics that drive that error.  The type of problem generator we use and the basic sequence of actions for evaluating and learning to predict error that we use apply equally to both methods that do and do not attempt to account for uncertainties in their inputs.  Here, we chose to use reserve selectors that do not address uncertainty to simplify explanation and testing of the methods.  

Second, it is still important to evaluate the accuracy of methods that ignore uncertainty because it is not uncommon to still see real-world and published reserve selections that do continue to ignore some or all input uncertainties.  For example, the commonly used `prioritizr` package in R that helps conservation planners use ILP methods  currently does not provide methods for users to incorporate uncertainty in their reserve selection solutions.  ^[ASCELIN: I suggest just a bunch of citations here with or two high profile papers that completely ignore uncertainty and then one or two that just look at some limited uncertainty.]  More generally, @velazco2020bc, they found that 90% of papers in their systematic survey of reserve selection papers between 2007 and 2019 ignored overprediction of species presence (i.e., false positives).  By characterizing the consequences of ignoring small amounts of even just one type of input uncertainty, our study demonstrates why it is important to move to methods that *do* address uncertainty of all forms.  

A fair evaluation of methods that address uncertainty requires additional experimental controls to add uncertainty to the extra parameters and values introduced in those methods.  This is because current methods that address uncertainty (e.g., @billionnet2015ema, @haider2018em, @beechStochasticApproachMarine2008, @tullochIncorporatingUncertaintyAssociated2013, @runtingReducingRiskReserve2018, @sierra-altamiranda2020em) have pushed the uncertainty back into uncertainties in the estimates of other variables such as variances or probabilities (and these are often not well-calibrated (@valavi2022em)).  It is also important to remember that methods that incorporate uncertainty are unlikely to assign uncertainty values to all of the many variables affecting real problems, e.g., PU costs, which are also often uncertain or arbitrary or lacking evidence (@armsworth2014aotnyaos).  Other framing omissions may be error forms that are more difficult to measure and/or represent in a typical problem formulation, e.g., spatial correlation in risks to species survival after being reserved (e.g., fire or climate change (@albers2016po) and other examples such as those discussed in @drira2019, @robillardAssessingShelfLife2017, and @viscontiBuildingRobustConservation2015).  

##  Discussion:  Synthetic vs. Real-world data  

A common criticism of synthetic studies such as ours is that synthetic problems don't necessarily reflect real-world problems.  Real-world case studies are thought to be "a more realistic representation of outcomes expected in real-world applications" (@cheokSympathyDevilDetailing2016).  We see two important issues with this view.  First, since the uncertainties in real-world data are unknown, then the degree to which the real-world data correctly represents the true underlying data is unknown.  Second, even if the data for a real-world problem had no error, we are not aware of any study that explicitly demonstrates how the structural attributes of any  real-world problem is representative of those attributes across real-world problems *in general*.  This is because there is currently no library of diverse real-world reserve selector input datasets to compare against.  We know of no studies characterizing the distributions of real-world reserve selection problem traits such as problem difficulty, representation targets, abundance distributions, species co-occurrences, or PU costs so that one could determine whether or not the data in a given study is representative of those distributions. Problem attributes and the distributions of those attributes across real problems, constitute a completely missing area of shared knowledge in SCP method design and testing.  

Multiple authors in other fields have already demonstrated methods for generating new synthetic problems that have structural attributes that are the same as or similar to given real-world problems.  For example, You and Wu use deep learning to learn to generate bipartite networks whose bipartite graph properties match the training set graph properties (@you2019anips3acnips2n2d82vbc).  Smith-Miles et al. use genetic algorithms to \textcolor{red}{generate problems??? (citation?)}, and Maci et al. \textcolor{red}{@macia2010p1acgec too ?}.  These kinds of techniques could be employed in SCP.  

##  Discussion: Benchmark problem sets  {#setsOfProblemsDiscussionSection}  

We have provided a single synthetic benchmark set whose goal is to help understand problem structures that cause difficulties in optimization under uncertainty.  However, our set is just a start and other sets that serve different purposes and address the weaknesses of our benchmark are needed.  Careful design of benchmark sets is crucial since ad-hoc composition can skew derived conclusions about tested methods and for more than 30 years, authors in other fields have been highlighting issues and solutions in benchmark design (see @crowder1979atms, @hooker1994or, @hooker1995jh, @macia2010p1acgec, @maciaUCIMindfulRepository2014, @beiranvand2017oea, and **AppSI Appendix ??? \textcolor{cyan}{\ref{setsOfProblemsDiscussionAppendix}}** for more details and recommentdations in benchmark design). ^[\textcolor{red}{This list should also include Smith-Miles and Munoz and possibly Eugster and others.  Wasn't there also a fairly recent paper by a Brazilian woman, Loreno(?) or something like that?}]  ^[See particularly, section 5 on p. 6 of Paper_8_all_combined_for_Ecological_Monographs/DiscussionWork/09_benchmarking.v01.pdf.  That text is much of what needs to go in the benchmarking appendix.  Also see the following directory for many pdfs about benchmarking that should be referenced or at least listed in the appendix:  Projects/ProblemDifficulty/ProbDiff_Notes/PapersAndPdfsforProbDiff/AAA Benchmarking and algorithm selection.] 

One important goal of problem test sets is to provide a sample that would provide support for generalizing beyond the problem sample, toward the real-world.  For both real-world and synthetic SCP problem sets, we would like to know how much of the possible range of problem attributes the set covers in the input space.  Is any given case study an outlier or representative of many real-world problems?  Do the problems all land close together in the input space or are they spread across attribute values?  For synthetic problems, we would like to know similar things but we would also like to know to what degree the spread of our synthetic problems overlaps the set of real-world problems in the input space.  For both real-world and synthetic problems, we would also like to know whether the output space is smooth, i.e., do problems that are close together in the input space lead to similar performance in outputs.

While there are hundreds of individual SCP studies, there are currently no shared collections of SCP problems to support an answer to whether a given problem has the characteristics of real-world problems in general, or not.  For a single reserve selection problem, we would like to know the shape of the distribution of costs across all its PUs and the values of the parameters characterizing that distribution (e.g., the mean and standard deviation if it was a normal distribution).  Beyond this, we would like to know whether this metadata on attribute distributions is similar across all real-world problems or whether there are different types of distributions for each predictive attribute.  

Without a collection of real-world problems whose attribute distributions can be measured, there is no way to support any claim about how representative any test problem, real or synthetic, is of the set of all real-world problems.  Moreover, we cannot have any idea of whether any problem set covers or even intersects the set of real-world problems.  **\textcolor{red}{See Smith-Miles/Munoz refs?, Macia refs?, etc for more discussion of this.}

Determining the statistical distributions of structural attributes that describe real-world reserve selection problems is arguably one of the most useful projects that could help improve the accuracy of reserve selection methods. Without this, there is little support for being able to generalize claims about results of any experiments to real-world problems, with or without uncertainty.   A good starting point for this approach could be the large pool of problems that have been addressed with Marxan and its variants, since these all share a simple input file format that could be easily anonymized to protect sensitive data.  Meanwhile, we can at least partially explore the space of attributes that control problem difficulty using solution planting methods for synthetic problems like the ones we have introduced here.  

##  Discussion: Problem definition  {#externalProbDefDiscussionSection}  

The reserve selection problem tested here was to choose a set of PUs that minimizes the total cost of guaranteeing that every species occurs on at least one PU in the chosen reserve set (Eq. (**5 \textcolor{cyan}{\ref{eq:ilpProblemFormulation}}**)).  This can be criticized for being a simpler formulation of the reserve selection problems than is typically used in real-world problems.  For example, here there are no spatial compactness goals or constraints, there are restrictions on what representation targets are allowed, and every PU has the same cost. However, we still found large variation in problem difficulty and input error magnification in even these comparatively simple problems (as measured by solution cost error and representation shortfall).  The question is, are these comparatively simple problems predictive of the accuracy of the same reserve selection methods on more complex real-world problem framings or are more complex problems actually easier because they put greater constraints on the solution and in the process, reduce the search space of solutions?  At the moment, we have no good evidence either way.  

We can't answer this question without having similar problems with more complex framings, but there is a straightforward path to creating some of these more complex framings by composing elements of different problems.  For example, even though we have no spatial structure in our model RB problems, any of our problems can be laid over any number of different landscapes in the following way:  Every synthetic species in our problems is assigned to a particular synthetic PU.  Each of these PUs could in turn be renamed to any PU in a spatial landscape (real or synthetic) and all of the cost and boundary length attributes of those spatially defined PUs could then be used in a more complex reserve selection problem.  This independent assignment of synthetic PUs to spatial PUs and their attributes could be done across many different landscapes or randomly repeated many times on the same landscape and then solve the spatially constrained version of that problem in each of the different variants.  In this way, problems of known underlying difficulty with respect to species co-occurrence could be used to test the relative contribution of spatial compactness constraints to the difficulty of a problem.  

This composing of independent elements of SCP problems is reminiscent of the composition of elements of a plot in the grammar of graphics embodied in the R computer language's ggplot functions (@wickham2010jocags).  Other independent elements of problem definition could also be added here, e.g., error models, to create a kind of grammar of reserve selection problems.  These compositional paths of independent elements would provide a path for getting at the relative contribution of different problem elements to problem difficulty, e.g., the relative contribution of spatial configuration to the difficulty of the compound problem.  It could also be used as we have done, to try to learn to predict the solution error on individual problems.  

Unfortunately, the new chains of problem elements mean that the original model RB problem is now missing a known correct solution to the composed problem since we only know the solution for the simple problem.  At this point, we are unaware of any theoretical solution like model RB for the more complex problems.  This means that we would have to apply an exact solver like ILP (with abundant memory, processing speed, and time resources) to guarantee a correct solution to the composed problem before any uncertainty is added.  Fortunately, this potentially expensive computation would only need to be done once for each composed problem.  

One might wonder about the utility of our original model RB and Wrap problems in this scenario where their solutions are no longer known.  However, their theoretically guided spread of difficulty in the underlying simple problem provides a background for asking questions about whether an easy or hard simple problem is made more or less difficult by different kinds of variation in added attributes in the more complex problems.   

One other thing to note about the model RB and Wrapping problem generators is that there are modifications that can be made to those methods to allow increased (but not unbounded) flexibility in specifying representation targets.  More details on  modifications to allow somewhat more realistic representation targets are given in **CorSI Appendix section C.3 \textcolor{cyan}{\ref{appendixPseudocodeMoreRealisticTargets}}**.

##  Discussion: External evaluation criteria for solutions  {#externalEvaluationCriteriaDiscussionSection}  

Independent of which problems are chosen for a benchmark set, there is the question of how to measure outcomes to compare on those problems.  Here, we used representation shortfall and cost error as our external evaluation criteria, but there are other possibilities, depending on the purpose of the reserve selector and its evaluation.  For example, a common refrain in reserve selection is that reserve selectors are decision *aids*, not decision makers ^[ASCELIN: Important to add some references that make this point - @gameSixCommonMistakes2013 ?].  But the question is: what constitutes a good aid and how do we measure its quality as an aid?  

For the data in this study, we found that the quality of solution was often poor even if it were to be used as the exact reserve plan without any modification.  This raises the question of how useful are these tools as aids to decision-making?  Or, at what point do errors in a method's solution mean it is no longer useful, even as a decision aid?  If it is acceptable to contain large amounts of error, then what are the real criteria for utility of a reserve selection output and how should they be measured?  

We have found little discussion of what are the attributes of a good decision aid, much less encoding those attributes in an external evaluation function or in a method's internal objective function (though the decision theory literature does have some discussion of decision aiding *processes*, e.g., @tsoukias2007aor, @tsoukias2008ejoor, @meinard2019ejoor).  One exception is the view that being a good decision aid is the ability to suggest multiple possible solutions of similar quality rather than just one best solution.  For example, methods like Marxan already generate multiple solutions as a result of doing multiple random restarts of the simulated annealing process.  However, these solutions are generated independently and there is nothing in the evaluation function that enforces any criteria across the *set* of generated solutions to indicate that the set is a good aid.  For example, nothing stops the set from containing all identical or nearly identical elements.  

If an important attribute of a decision aid is to produce diverse solutions, it could be useful to apply some diversity measure to the sets of solutions.  For example, Brunel et al. provide a diversity measure and a method for using it to produce alternative solutions of a given quality with an ILP method (@brunel2023ema).  One problem with using sets of solutions is that much of reserve selection makes use of the complementarity of selected PUs within one solution to achieve coverage of species with lower cost.  Combining different subsets of diverse solutions may shatter that complementarity and degrade solution quality.    

However, even that still begs the question of what is it that is measured about each of the solutions.  None of what we have discussed here (cost, representation, robustness to error, etc.) addresses the more fundamental and important issues raised about whether any of these measures reflect things like community effects and persistence (@salomonPopulationViabilityEcological2006, @langfordWhenConservationPlanning2009, @cabeza2001tie), i.e., the analytical evidence for what Pressey et al. call the actual conservation impact, "the future difference made by our future actions" (@pressey2017bc).  In fact, little has changed since Moilanen's "Two paths to a suboptimal solution" (@moilanen2008bc) raised the issue of how SCP methods can produce apparently optimal results by simply choosing restricted problem definitions that mean little in the real world.  

##  Discussion: Prediction and graph measures^[This whole section to describe the graph features and the fitting procedures is incomplete because I'm not sure how we want to present it in a way that feels familiar to ecologists.  Same goes for the corresponding Methods and Results sections.] ^[\textcolor{red}{I have no idea what to do about plotting something like a prediction interval for lm output since the errors are nowhere near meeting prediction interval assumptions and prediction intervals are supposed to be very sensitive to those assumptions.  Bootstrapping?}]

In our study, we found that rephrasing problem structures as bipartite graphs and then computing various measures over these graphs generally allowed us to cut prediction error by close to 2/3 when compared to using problem size alone.  The amount of representation shortfall was roughly twice as easy to predict with almost every predictor set (other than problem size) as solution cost error was.  

One noticeable anomaly was that solution cost error was much more difficult to predict correctly for SA_SS, the only reserve selector that used a voting method.  We have no explanation for this.  

While our predictions were not perfect, the large jump in predictive power beyond relying on problem size alone indicates that it is worth pursuing better understanding of attributes of problem structure that influence accuracy under uncertainty.  Moreover, the simple act of explicitly trying to predict output error on individual problems provides a vehicle for analyzing what we do and don't know about the behavior of reserve selection methods.  The predictions may also be a way of providing practitioners with quantitative estimates of the value of gathering new information (@canessa2015mee) or of other actions such as modifying the values of representation targets for their own individual problems.    

###  Specific prediction features  ^[This section needs lots of work that should be based on the coefficients of the linear models, but that's something that it would be useful to have someone else do so that this section reads much more like a typical ecology paper's analysis of a linear model.  Meanwhile, here are some subsections that are either stubbed in to remember them or have some basic title and/or text to start from.]  

####  Biggest accuracy jump came from just adding 3 size and link density measures  

One useful result of our prediction experiments was the large improvement in  prediction accuracy that we found for adding three very simple input features to the usually reported number of species and PUs (**Fig. 18 \textcolor{cyan}{\ref{fig:barPlotRMSEvaluesForPreds}}**).   These features were the  (number of possible species/PU occupancy pairs (sppPUprod, ie, size of adjacency matrix), the average number of links per bipartite network node (links_per_PUsAndSpp), and the achieved proportion of all possible bipartite links per node (edge_frac_of_possible)).  All three are computed very simply from the bipartite graph's adjacency matrix.   

####  No additional graph measures stood out individually ^[Would it be better to use glmnet instead of lm?  Though, doesn't that just boil down to "selectFeatures + lm", and I've done the select in my input variable choices?  Then again, does it make that process of feature selection justifiably automatic rather than relying on my subjective or semi-subjective choices?]  

In the data exploration phase of this research, we tried several different methods for fitting predictive models, including multiple linear regression, regression trees, and generalized linear models.  The quality of the resulting predictions varied little between the models, so we have only presented results for the simple linear regression models.  

Similarly, we tried many variations of input features via things like removing correlated variables, using principle components analysis, etc.  However, none of these actions improved the results.  The best prediction came from using all of the graph variables.  Accuracy of predictions fell off fairly linearly with the proportion of graph variables removed from the input feature set as we tried removing highly correlated input features.  Similarly, using feature processing and selection techniques like principal components analysis or elastic net ^[need citationHastie(?)] led to decreased prediction performance with little gain in understanding.  

####  Other possible predictive features  

Our choice of predictive features and graph metrics can and should be expanded.  Our initial choice of graph metrics was constrained by run time and we excluded any metrics that took a long time to compute in preliminary tests.  For example, in @presseyEffectsDataCharacteristics1999, they used nestedness as a problem descriptor.  We would like to do the same, but when we did our experiments, we found that nestedness metrics took much longer to run than the other metrics.  In the time since our experiments were done, more computing power and time have become available and it may be more practical to compute those metrics now and add those results to the input variables in our publicly available data sets.  Another predictive feature that would be of interest to compute is the $X$ measure discussed in @viscontiBuildingRobustConservation2015.    

Another class of possible input features would be estimates of the error rates in inputs.  Intuitively, it seems like knowing the amount of input error should be helpful in making output error predictions, but when we included the correct values for input errors in the Everything predictions, there was little or no improvement over prediction using the full set of graph metric predictors without any unknowable predictor features (**Figs. 18 \textcolor{cyan}{\ref{fig:barPlotRMSEvaluesForPreds}} and 19 \textcolor{cyan}{\ref{fig:barPlotAdjR2valuesForPreds}}**).  There, we were even using the correct values for the input errors, which we can only estimate in real-world problems.  Consequently, adding input error estimates to the predictor variables seems not worth pursuing, however, that may only be the case for this data set.  It may still be worth trying on different data sets, particularly if there are other kinds of input errors beyond our FN and FP errors, e.g., the implementation losses discussed in @viscontiBuildingRobustConservation2015.  

###  Limits to prediction  

One important thing to note about our learned prediction models is that our predictions are made based on incorrect, apparent data and the same prediction would be made even if the given data was perfectly correct.  For example, consider a very simple problem with just one species that occurs on just one PU out of 100 PUs on the correct map.  Suppose that on the corresponding apparent map for the species, its presence is overestimated and it shows on all 100 PUs.  Similarly, consider another reserve selection problem that also has an apparent map that has 1 species and 100 PUs and the species is again shown on every PU.  However,  in this case, the species really does exist on every PU.  Because whatever output error prediction model we learn is always shown only the apparent map, it cannot distinguish between these two problems.  Consequently, it will necessarily return the same output error prediction for both problems.  This shows that there is an inherent error in our predictions that cannot be removed because the predictors are only ever able to see the apparent data.  It's not clear how important this is because we don't know how real-world problem structures are distributed so that we can know how common these kinds of overlaps are among very different apparent problems.  

##  Discussion: Future work  {#futureWorkDiscussionSection}  

As discussed in **Section 10.5 \textcolor{cyan}{\ref{setsOfProblemsDiscussionSection} }**, one of the most useful things that could be done next would be to collect a broad set of real-world reserve selection problems into a single, publicly available data set that could be shared for testing and developing reserve selection methods.  Structural characteristics of real conservation projects could then also be leveraged to generate more useful synthetic problems.  Multiple authors in other fields have already demonstrated methods for generating new synthetic problems that have structural attributes that are the same as or similar to given real-world problems.  For example, You and Wu use deep learning to learn to generate bipartite networks whose bipartite graph properties match the training set graph properties (@you2019anips3acnips2n2d82vbc).  Smith-Miles et al. use genetic algorithms to \textcolor{red}{generate problems??? (citation?)}, and Maci et al. \textcolor{red}{@macia2010p1acgec too ?}.  These kinds of techniques could be employed in SCP.  

Another important future project is to move on to more complex problem definitions that go beyond our simple problem definition.  This will likely require an experimental rather than theoretical method for finding the correct solution to the versions of the problems that don't contain any error.  ILP methods are currently powerful enough to solve the correct versions of most of these problems now if given enough time and memory.  Each of these would only need to be solved once to guarantee a Correct base for computing error under uncertainties added to the problems.  However, this doesn't give a theoretical guarantee for a spread of problem difficulty over a set of problems, so that would probably have to be pursued experimentally, perhaps through composition of problem elements as mentioned in **Section 10.7 \textcolor{cyan}{\ref{externalProbDefDiscussionSection}}**.  Increasing the complexity of problems by introducing non-binary problem elements may cause error prediction difficulties for our current bipartite graph measures, however, there are bipartite graph measures that allow for weighted links in the graph and those may be worth investigating.  

A final future work would be to test other reserve selection methods than we have tested here, even on the same data set that we have used here.  An obvious first choice would be robust ILP methods such as discussed in @haider2018em.  However, this requires careful design to reflect multiple errors in the input error/variance/bound estimates as well as effects of omissions of different types of error, e.g., the common practice of ignoring false positive errors.  We have also neither tested nor discussed significantly different methods like Zonation ^[Zonation citation?], where the result is not claimed to be optimal but instead, addresses much larger problems than optimal methods can currently solve.  These methods raise many of the same questions regarding relative problem difficulty and robustness to uncertainty.  Those questions might be addressed there through measures of error with respect to the solution found on Correct input data rather than with respect to a known optimal solution.  

##  Discussion: Related Work ^[Copied from **CorSI Appendix J \ref{appendixp1RelatedWork} - Related Work) in langford\_gordon-solution\_planting\_\_COR\_\_supplemental\_information.pdf]  {#relatedWorkDiscussionSection}  

###  Related ecological work    

To our knowledge, no reserve selection studies consider synthetic data generators to explicitly provide both known solutions and a range of problem structure/difficulty and size beyond small, fully enumerable sets such as those in @viscontiBuildingRobustConservation2015.  We know of only two reserve selection studies that explore variation in species co-occurrence structure as a factor in reserve selector performance.  @presseyEffectsDataCharacteristics1999 explores efficiency and suboptimality of four methods based on size variation of PUs, size of data set, and feature nestedness and rarity within variants of a single 1886 PU landscape with 248 features.  @viscontiBuildingRobustConservation2015 examines robustness of solutions to changes in implementation based on varying a complementarity measure within many versions of four synthetic 20 PU landscapes with 5 species.  Other case studies, such as @warmanSensitivitySystematicReserve2004, @cheokSympathyDevilDetailing2016, and @rodewaldTradeoffsValueBiodiversity2019,  explore combinations of problem attributes such as PU size and shape, targets, thematic resolution, and costs on a single landscape ^[\textcolor{red}{(Do they also ignore input uncertainty?)}].  While these studies do sample different input values for problems, none of them attempt to theoretically control for a generalizable population of problem structures with a range of difficulty. The theoretical guarantee matters because without it, varying the input data may still lead to many problems of similar difficulty even if the inputs varied widely.  

###  Related computational work

The relationship between problem structure and difficulty has long attracted attention in theory of computation.  The proven difficulty of NP-hard problems is based on worst-case performance, but work such as @goldberg1982ipl suggested that average-case problems might be comparatively easy.  However, later work showed this was likely a consequence of choosing a test distribution containing primarily easy problems (@franco1983dam, @mitchell1992ptncaia).  Simultaneously, interest was sparked by work proposing the existence of abrupt phase transitions in the difficulty of NP-complete decision problems around threshold values of problem structural descriptors  (@huberman1987ai, @cheeseman1991ip1ijcai).  The proximity of parameter values to the threshold indicated the problem's relative difficulty (which is how our parameters for model RB were chosen).  The 1990s and 2000s saw numerous papers attempting to identify the threshold location and algorithm behavior in its neighborhood by using and improving random problem generators (@smithLocatingPhaseTransition1996, @achlioptas1997papocp, @gent2001c), including model RB (@xu2005ipnijcai).  Independent of phase transitions and following from @krishnamurthy1987itc,  Sanchis was simultaneously developing different generators for testing approximation algorithms for NP-complete problems, including vertex cover (@sanchis1989, @sanchisExperimentalTheoreticalResults1996a).  We have not explored her methods here but they may be of future interest.  

--------------------  

#  Conclusions  

Little of the current evaluation of SCP methods allows us to either predict or understand method accuracy under uncertainty on previously unseen *individual* problems whose structures differ from the initial method presentation or when method assumptions are violated.  We have imported and extended a theory-guided solution planting method to generate synthetic conservation planning problems with controllable difficulty, known optimal solutions, and statistically useful sample sizes for learning to predict method accuracy under uncertainty.  We used this problem generator to create a publicly available benchmark set of thousands of test problems with known correct solutions as well as a range of input uncertainties and problem difficulty. 

We showed there was large variation in reserve selector accuracy at every level of input error and every reserve selector tested.  Consequently, it is important to be able to make reliable predictions about the amount and nature of likely reserve selection error on *individual* problems rather than estimating summary values such as the mean or median error.  Without reliable per-problem understanding of difficulty, we can't know what the risks are and whether further action is necessary, e.g., whether getting new information will be useful.  

Finally, our rephrasing of individual reserve selection problems as bipartite graphs enabled us to compute measures of structural attributes of problems that radically improved prediction of individual problem difficulty for specific reserve selectors over using problem size alone.

This study is important in comparison to other SCP studies because to our knowledge, there are no other SCP papers that i) explicitly target generating large problem sets under uncertainty with known correct solutions that vary in problem difficulty beyond problem size, ii) generate a large, publicly available benchmark set, or iii) explicitly attempt to learn to predict the amount of reserve selection error (both cost error and shortfall error) for particular reserve selectors on *individual* reserve selection problems.  

Even though this is a large study, it is only a first step and leaves many questions unanswered.  We see three primary issues for future work.  The most obvious issue is that the SCP community has no large, publicly available collection of real-world problems to characterize the universe of problems that SCP methods should generalize to.  A second issue with our study is that we only used reserve selectors that ignored input uncertainties.  While this is still common in reserve selection, recent years have seen the rise of methods that do address input uncertainty such as robust ILP methods.  Testing these methods on the problems generated here would be a useful next step.  A third issue with our tests is the simplicity of our problem definition.  It would be simple and useful to create more complex and ecologically realistic problem structures with known solutions via composition of independent problem elements.   

We believe that better predictive understanding of the behavior of existing reserve selectors is more important than creating new reserve selectors whose true accuracy on previously unseen problems continues to be poorly understood.  We acknowledge that rigorous, informative evaluation of methods under uncertainty requires significant amounts of time, computational resources, and programming. We believe that this burden of proof of an SCP methods safety and efficacy should be on method developers, not on the conservation practitioners who try to use these methods.  

--------------------  

#  References  

<div id="refs"></div>

```{r includeOrExcludeAppendices, echo=FALSE}

#  Temporary exit to avoid building appendices every time
#  Remove this chunk when you're ready to build appendices too.

if (!params$build_appendices) knitr::knit_exit()
```

-----

# Acknowledgements

**\textcolor{red}{Edited from version in bdpg\/Paper\_1\_method\/v11\_Paper\_1\_method\_\_body.Rmd:720}**

A.G. was supported by the Australian Research Council through Discovery Project DP150102472.  W.T.L. was supported in the early part of this work by a grant from Process Minerals International through Condition 13 of Australian EPBC Act approval 2010/5759. W.T.L. also thanks Chris Murphy of the Australian Department of Sustainability, Environment, Water, Population and Communities (SEWPAC) for his instigation of this funding. W.T.L. also thanks Simon Jones of RMIT University Geopatial Science Department for bridging funding and other forms of support.  The experiments were supported by use of the Nectar Research Cloud and by Nectar node operators Intersect and Monash University. The Nectar Research Cloud is a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC). The authors thank Iris Bergmann, Jeanne Panek, and Eric Berlow for useful comments on early drafts of this paper. W.T.L. also thanks Ke XU for rapid answers to questions about details of model RB, though any errors in the implementation of the model are the responsibility of W.T.L.  No conflict of interest is declared by either author.  

-----  

\newpage

# (APPENDIX) Appendix {-} 

#  Appendix - Dummy COR appendices to generate cross references in text above. {#appendixDummyCORlist}

```{r eval=FALSE}
#```{r include=FALSE}  #  Commenting out until ready to build these dummy appendices
# NOTE:  These dummy appendices are derived by running the following commands in the p6 directory (bdpgtext/Paper_6_p1p2_combined):

# > grep    -i    "{#"    v09_Paper_6_p1p2_combined____COR__supplemental_information.Rmd

#  Appendix - Full problem generators overview {#appendixP1ProblemGeneratorsExplanation}
##  Base problem using model RB {#CorModelRBexplanation}
##  Wrapped problem using new method {#CorModelWrapExplanation}
###  More realistic species distribution {#appendixMoreRealisticSppDistribution}
##  More realistic representation targets {#appendixMoreRealisticRepTgts}
#  Appendix - Algorithm for model RB generating Base reserve selection problem  {#appendixPseudocodeModelRB}
#  Appendix - Algorithm for wrapping Base model RB reserve selection problem {#appendixPseudocodeWrap}
##  Search for Wrap distribution parameters and build distribution {#appendixPseudocodeLognormal}
###  Compute controlling variables for wrapping process given a Base problem {#appendixComputeControllingVarsForWrap}
###  Search for wrapping distribution given Wrap control variables and Base problem {#appendixSearchForWrapDist} 
##  Wrapping a given distribution around a given Base problem {#appendixPseudocodeAssignNewSppAndPUsForWrap}  
##  More realistic representation targets {#appendixPseudocodeMoreRealisticTargets}
#  Appendix - Implementation parameters {#appendixImplementationParameters}
##  Model RB parameters  {#appendixModelRBparams}
##  Wrap parameters  {#appendixWrapParams}
###  Solution fraction of landscape {#appendixSolFracOfLandscape}  
#  Appendix - Distributions of problem structure characteristics {#probCharacteristicsAppendix}
##  Distributions of resulting model RB control values {#distModelRBcontrols}
###  Distributions of model RB variables driving PU counts {#appendixDistModelRBvarsDrivingPUcts}
###  Distributions of model RB variables driving species counts {#appendixDistModelRBvarsDrivingSppCts}
##  Distributions of resulting problem size variables {#appendixDistResultingProbSizeVars}
#  Appendix - Reserve selector parameters {#appendixReserveSelectorParameters}
##  Gurobi parameter settings used in `bdpg` parameters list {#appendixGurParSettingsUsed}
##  Marxan and Marxan_SA_SS parameter settings used in `bdpg` parameters list {#appendixMarxanParSettingsUsed}
##  UR_Forward parameter settings used in `bdpg` parameters list {#appendixURforwardParSettingsUsed}
#  Appendix - Are COR Wrap problems harder than the COR Base problem they wrap? {#appendixBaseWrapCompare}
#  Appendix - Detailed solution cost error results {#corResultsAppendix}
##  Solution cost error summary tables {#appendixSolCostErrSummaryTables}
###  Base and Wrap combined {#appendixBaseAndWrapCombined}  
###  Base only {#appendixBaseOnly}  
###  Wrap only {#appendixWrapOnly}  
##  Distributions of solution cost errors {#appendixDistSolCostErrors}
###  Distributions of all solution cost errors (zero and non-zero) {#appendixDistAllSolCostErrors}
###  Distributions of only *non-zero* solution cost errors {#appendixDistNonZeroSolCostErrors} 
##  Plots of problem size variables vs. Solution cost error {#appendixPlotsOfProbSizeVsSolCostErr}
###  Number of species {#appendixNumSpp}  
###  Number of PUs {#appendixNumPUs}  
###  Number of species per PU {#appendixNumSppPerPU}  
###  Sum of number of species and PUs {#appendixSumSppPU}  
###  log10 (sum of number of species and PUs) {#appendixLogSumSppPU}  
###  Product of number of species and PUs {#appendixProductSppPU}  
###  log10 (product of number of species and PUs) {#appendixLogProductSppPU}  
###  Correct solution cost {#appendixCorSolCost}
###  Correct solution fraction of total landscape {#appendixCorSolFrac}  
#  Appendix - Implementation issues {#appendixIssueNotes}
##  Nominal $p$ vs. effective $p'$ {#appendixNominalVsEffectiveP}
##  Actual solution fraction of landscape vs. desired fraction {#appendixActualSolutionFractions}
#  Appendix - Related Work {#appendixp1RelatedWork}  
##  Related ecological work {#appendixRelatedEcoWork}    
##  Related computational work {#appendixRelatedCompWork}
#  Appendix - COR SI Supplemental References {#appendixCorSuppRefs}  
```

#  Appendix - Dummy APP appendices to generate cross references in text above. {#appendixDummyAPPlist}

```{r eval=FALSE}
#```{r include=FALSE}  #  Commenting out until ready to build these dummy appendices
# NOTE:  These dummy appendices are derived by running the following commands in the p6 directory (bdpgtext/Paper_6_p1p2_combined):

# > grep    -i    "{#"    v09_Paper_6_p1p2_combined____APP__supplemental_information.Rmd

#  Appendix - Reproducibility and Availability {#appendixReproducibilityAndAvailability}  
##  Software availability {#appendixSoftwareAvailability}
##  Data availability {#appendixDataAvailability}
## Hardware {#appendixHardware}
#  Appendix - Conventions for naming, display, etc. {#ConventionsAppendix}  
##  Definitions {#appendixDefns}
##  Naming conventions {#appendixNamingConventions}  
##  Tables {#appendixTableConventions}  
###  Confidence intervals on medians in tables {#appendixTableCIs} 
###  Most common table format in these Appendices and in Shiny app  {#appendixMostCommonTableFormat}
# Appendix - Notes on parameter and equation choices {#MethodsParAndEqNotesAppendix}  
##  Error magnification base  {#ErrorMagnificationAppendix}
##  Incommensurate bounds of solution cost error and rep shortfall error  {#incommensurateBoundsAppendix}  
##  Unfinished Gurobi runs {#GurobiTextFromP1}  
##  Imperfect wraps {#appendixImperfectWraps}  
#  Appendix - Inclusion and relative difficulty of problems that Gurobi didn't finish {#GurobiTimeLimitsAppendix}
##  Overview of unfinished Gurobi problems section {#appendixOverviewGurUnfinishedSection}  
###  Definition of mipgap {#appendixMipgapDefn}  
###  Inability to provably determine cause of unfinished problems {#appendixInabilityToProve}  
##  FN-dominated values {#appendixTimeLimitsFNdom}  
###  FN-dominated raw error values {#appendixTimeLimitsFNdomRaw}
###  FN-dominated error magnifications {#appendixTimeLimitsFNdomMag}
##  FP-dominated values {#appendixTimeLimitsFPdom}
###  FP-dominated raw error values {#appendixTimeLimitsFPdomRaw}
####  FP-dominated total output error (finished & unfinished) {#appendixTimeLimitsFPdomTotOutErr}  
####  FP-dominated rep shortfall  (finished & unfinished) {#appendixTimeLimitsFPdomRepShortfall}  
###  FP-dominated error magnifications {#appendixTimeLimitsFPdomMags}
####  FP-dominated total output error MAGNIFICATION (finished & unfinished) {#appendixTimeLimitsFPdomTotOutErrMag}  
####  FP-dominated representation shortfall MAGNIFICATION (finished & unfinished) {#appendixTimeLimitsFPdomRepShortfallMag}  
##  Distributions of mipgap values {#appendixMipgapDists} 
###  Density of Gurobi mipgaps {#appendixMipgapDensity}  
###  Gurobi mipgap as a function of problem size {#appendixMipgapAsFuncOfProbSize} 
##  Gurobi mipgaps vs. error measures across all reserve selectors {#gurMipgapsVsErrorsAppendix}
###  Gurobi mipgap vs. Abs solution cost error {#appendixMipgapVsAbsSolCostErr} 
####  Gurobi mipgap vs. Abs solution cost error MAGNIFICATION {#appendixMipgapVsAbsSolCostMag} 
###  Gurobi mipgap vs. Signed solution cost error {#appendixMipgapVsSignedSolCostErr} 
####  Gurobi mipgap vs. Signed solution cost error MAGNIFICATION {#appendixMipgapVsSignedSolCostMag} 
###  Gurobi mipgap vs. Total output error {#appendixMipgapVsTotOutErr} 
####  Gurobi mipgap vs. Total output error MAGNIFICATION {#appendixMipgapVsTotOutMag} 
###  Gurobi mipgap vs. representation shortfall {#appendixMipgapVsRepShortfall} 
####  Gurobi mipgap vs. representation shortfall MAGNIFICATION {#appendixMipgapVsRepShortfallMag} 
#  Appendix - Inclusion and relative difficulty of imperfect Wrap problems  {#ImperfectWrapsAppendix}
##  All {#appendixImperfectWrapsAll}  
###  All raw {#appendixImperfectWrapsAllRaw}  
###  All mag {#appendixImperfectWrapsAllMag}
##  FN-dominated {#appendixImperfectWrapsFNdom}
###  FN-dominated raw {#appendixImperfectWrapsFNdomRaw}  
###  FN-dominated mag {#appendixImperfectWrapsFNdomMag}
##  FP-dominated {#appendixImperfectWrapsFPdom}
###  FP-dominated raw {#appendixImperfectWrapsFPdomRaw}
###  FP-dominated mag {#appendixImperfectWrapsFPdomMag}
#  Appendix - Differences between pairs of reserve selectors on individual problems {#DiffsAppendix}
##  Structure of each section {#appendixDiffsSecStruct}  
##  Marxan_SA minus Gurobi {#appendixDiffsMsaGur}
###  Marxan_SA minus Gurobi - Total error diff tables {#appendixDiffsMsaGurTotErr}
###  Marxan_SA minus Gurobi - Rep shortfall diff tables {#appendixDiffsMsaGurRepShortfall}  
##  Marxan_SA_SS minus Gurobi {#appendixDiffsMsassGur}
###  Marxan_SA_SS minus Gurobi - Total error diff tables {#appendixDiffsMsassGurTotErr}
###  Marxan_SA_SS minus Gurobi - Rep shortfall diff tables {#appendixDiffsMsassGurRepShortfall}
##  UR_Forward minus Gurobi {#appendixDiffsUrfGur}
###  UR_Forward minus Gurobi - Total error diff tables {#appendixDiffsUrfGurTotErr}
###  UR_Forward minus Gurobi - Rep shortfall diff tables {#appendixDiffsUrfGurRepShortfall}
##  Marxan_SA_SS minus Marxan_SA {#appendixDiffsMsassMsa}  
###  Marxan_SA_SS minus Marxan_SA - Total error diff tables {#appendixDiffsMsassMsaTotErr}
###  Marxan_SA_SS minus Marxan_SA - Rep shortfall diff tables {#appendixDiffsMsassMsaRepShortfall}
##  UR_Forward minus Marxan_SA {#appendixDiffsUrfMsa}
###  UR_Forward minus Marxan_SA - Total error diff tables {#appendixDiffsUrfMsaTotErr}
###  UR_Forward minus Marxan_SA - Rep shortfall diff tables {#appendixDiffsUrfMsaRepShortfall}
##  UR_Forward minus Marxan_SA_SS {#appendixDiffsUrfMsass}
###  UR_Forward minus Marxan_SA_SS - Total error diff tables {#appendixDiffsUrfMsassTotErr}
###  UR_Forward minus Marxan_SA_SS - Rep shortfall diff tables {#appendixDiffsUrfMsassRepShortfall}
#  Appendix - APP SI Supplemental References {#appendixAppSuppRefs}  
```

#  Appendix - Procedure for building dummy appendices to generate cross references in text above. {#appendixProcForBuildingDummyAppendices}

##  New instructions for dummy appendices and cross-references  

The "Old instructions for dummy appendices" section below didn't really explain everything necessary to do this, so I've written up a full explanation in the daily log file in a long entry for "2024-04-14 Sunday".  I'm still going to leave the old instructions in here below in case they have anything else of use at some point.

##  Old instructions for dummy appendices  

In the daily log file on March 13, 2022, there is a section called **\textcolor{red}{"Fixing problems with what was resubmitted"}**.  It explains the procedure for building and using the dummy appendices for cross referencing.  Here is the text from there:  

2022-03-13 Sunday

The resubmission of p1 as p6 was done on February 26, 2022.  In the following couple of weeks (up through March 13, 2022), I've been fixing some things that had to do with cross-referencing errors in the resubmitted SI files.  

- I also fixed a line or two in the main body file's Methods section on reserve selectors.  It's main problem was that it said Marxan_SA_SS in one place where it should have said Marxan_SA.  
- I also got rid of a bunch of file paths that were hard-coded for my machine.  I made them relative to the bdpgtext root directory instead.  
- I've renamed the SI files to be v10 instead of the v09 they were submitted as.
- However, I just looked at the MEE site and I don't see a way to update the SI files.  It just tells me that it's all out for review.  I may write to them to see if there's a way to update or I may ignore the whole thing.

I had a file called "to do for submission.docx" that I was working from in making the resubmission.  I'm going to paste the text from there into here so that it's easier to find.

- That file was in
  - /Users/bill/D/Projects/ProblemDifficulty/RnotInPkgs/bdpgtext/Paper_6_p1p2_combined/MEE_resubmission_preparation/to do for submission.docx
- The p6 directory now has 3 different subdirectories related to the resubmission:
  - MEE_resubmission_preparation
  - MEE_resubmission
  - MEE_resubmission_fixes
- The first two were used in building the resubmission on February 26th.  The 3rd one is where I've put the fixes to the SI files that I've made after the February resubmission.  

Here is the text from the to do for submission file:  

- [\textcolor{red}{There was nothing under this heading.  Not sure if something happened to it or maybe the stuff below is supposed to have been indented under it.}]

**Things I've done in final preparation of resubmission**

- In the SI files, I've created a "langford_gordon-solution_planting\_\_\_\_..." version of each of the SI files (though not for the code examples SI because I haven't submitted that SI this time, maybe later).  
- In each of the langford_gordon... SI files:
  - Removed the "p6 v09 - " lead-in text before "Supplemental" in title.
  - Removed all of the version history from the Supplemental files.
  - Removed all of the echoing of options and the date in the header of the files:
```
    | 
    | `r paste0 ('[gur probs: ', params$gurobi_problem_filter, ']')` 
    | `r paste0 ('[exc imperfect: ', params$exclude_imperfect_wraps, ']')`
    | `r paste0 ('[exc ZL: ', params$exclude_ZL, ']')`
    | `r paste0 ('[do all batches: ', params$do_all_batches, ']')`
    | `r paste0 ('[exc 0 APP inErr: ', params$exclude_APP_0_inErr, ']')`
    | `r paste0 ('[exc > 10% inErr: ', params$remove_probs_with_gt_10_pct_input_err, ']')`
    | `r paste0 ('[near_1_tol: ', params$near_1_tol, ']')`
    | `r paste0 ('[write_tibs_to_csv: ', params$write_tibs_to_csv, ']')`
    | `r paste0 ('[write_most_important_tibs_to_csv: ', params$write_most_important_tibs_to_csv, ']')`
    | `r paste0 ('[add_gen_time_to_csv_name: ', params$add_gen_time_to_csv_name, ']')`
    | `r paste0 ('[relative_path_to_input_data: ', params$relative_path_to_input_data, ']')`
    | `r paste0 ('[data_out_loc: ', params$data_out_loc, ']')`
    | `r paste0 ('[build_appendices: ', params$build_appendices, ']')`
    | `r paste0 ('[bdpg_p_needs_fixing: ', params$bdpg_p_needs_fixing, ']')`
    | `r paste0 ('[file_type_to_write: ', params$file_type_to_write, ']')`
    | `r paste0 ('[show_excess_tables: ', params$show_excess_tables, ']')`
    | `r paste0 ('[mag_base_col_name_str: ', params$mag_base_col_name_str, ']')`
date: "`r paste(params$initialDate,'thru', Sys.time())`"
```

- **\textcolor{red}{Things I've done both to those langford_gordon SI files and to the original SI files.}**

  - Removed "\*\*\\textcolor{red}{  ... }\*\* and any hard-coded figure/appendix/section numbers that reference something inside the current file.
    - I had made these red and hard-coded the figure numbers in the main body because any latex colors dissappear when you generate a Word file, so xrefs alone are invisible in the Word output.  That meant I had to guess at what the final figure/appendix/section number would be in the submitted Word version.  By putting it right next to what the xref output, I could verify that the correct references were being used in the Word version by just looking for red bits in the pdf version.  By putting them in red, I could easily find them.
    - In the appendices (SI files), the xrefs both show up and work correctly since I submit the pdfs, not a Word version.  So, that means I didn't need to bother with doing this red hard-coding in the SI files, with one exception.  In the AppSI file, there are some references to equations (and maybe figures) that are in the main body and I don't know how to xref in latex to a different file.  So, I need a red flag in the SI only for those kinds of out-of-file xrefs so that I can hard-code them correctly in the final SI file.
  - Changed the pointer to the zotero style file for MEE to point to a subdirectory of the bdpgtext directory, which meant that I had to copy that file in from its Zotero home in my D directory.  This isn't great, because it means that the style file has to be updated in more than one place if it changes, but it does make the whole thing more self-contained and doesn't make things about my own machine public.
    - The old path that I've replaced was /Users/bill/D/Zotero/styles/methods-in-ecology-and-evolution.csl
  - Trying to remove any reference to any file that isn't a relative reference within the bdpgtext tree, so that things are more portable and don't make anything about my machine, logins, etc public.






#  Appendix - \textcolor{red}{All archived text appendices have been moved to separate file} {#appendixTextToDropPossibilities}  

The archive file is in dir bdpgtext/Paper_8_all_combined_for_Ecological_Monographs and is called p8_v12_all_combined__body__ARCHIVED_MATERIAL_FROM_DUMMY_APPENDICES.Rmd.

The dummy appendices list sections below were copied from v13_Paper_6_p1p2_combined__body.Rmd in the paper 6 directory.  

#  Appendix - \textcolor{red}{Temp copy of Principal Findings AND Conclusions {#appendixPrincipalFindingsAndConclusions}  

There's a lot of overlap between the Discussion: Principal Findings and the long form of the Conclusions, but I'm having trouble figuring out how to capture the best of both but not have so much overlap.  In particular, I'd like to have a shorter Conclusion, but not lose the good flow of the long version.  In trying to do this, I broke up lots of the text in the Principal Findings section into subsections with headings that might help figure out where the overlaps with the Conclusions were.  So far, however, I haven't found a good way to solve the problem.  I'm going to ignore it for the moment, but I want to put the old Principal Findings back and save the decomposed version with headers in here in case I want to look at it again.  

##  Discussion: Principal findings  

###  Wide variation in difficulty under even small amounts of uncertainty  

Our results clearly show there is a wide variation in problem difficulty at every tested level of uncertainty when there is variation in problem structure.  

  - Notably, while ILP and Marxan_SA had no errors on problems without uncertainty, adding even small amounts of input error ($\leq 10\%$) often led to both large amounts of output error and large amounts of variation in that output error at nearly every tested level of input error.  
  - Importantly, optimization nearly always magnified input error in our data set (in agreement with @smith2006ms and @ignizio1999eo).  
    - \textcolor{red}{Check or remove that sentence, particularly the ignizio part.}

###  Large variation in error magnification  

Getting error in the outputs is unsurprising given that none of the tested reserve selectors attempted to address uncertainty in the inputs.  However, the *variation* in error magnification, was surprisingly large at nearly every level of input error (see Figs. 5  **\textcolor{cyan}{\ref{fig:InVsTotOutErrLE125inResults}}**, 7  **\textcolor{cyan}{\ref{fig:inVsRepShortfallinResults}}**, 8  **\textcolor{cyan}{\ref{fig:inVsRepShortfallMagLE25inResults}}**).    \textcolor{blue}{[Add something here about still needing to do tests like this on reserve selectors that address uncertainty because a) they don't address all uncertainties (e.g., some are hard to phrase quantitatively (non-procedurally?)), and b) they generally assume that they are given correct estimates of input error and this is seldom likely to be true.]}

###  Our amounts of input error were very small compared to real-world  

Furthermore, our results come from using only very small amounts of input error compared to what is likely to be found in real-world data.  For example,  @tullochIncorporatingUncertaintyAssociated2013 tested their method for addressing uncertainty on real-world benthic habitat maps where error ranged from 0 to 72.5% error with a mean of 33% error, while all of our input errors were no greater than 10%.  Our use of small input errors is especially important because one thing that commonly comes up in robust ILP papers is the possibility of infeasible solutions when errors \textcolor{red}{[bounds?]} are too large **\textcolor{red}{[Haider talks about getting infeasible solutions for large uncertainties on p. 294 of @haider2018em.  May also talk about them in other places.]}**.  

- \textcolor{red}{Also, are these robust ILP results *sufficient*, but not *necessary*?}
- \textcolor{red}{Also, some robust ILP papers mention average behavior (and behavior with smaller input error bounds?) vs. absolute, guaranteed worst-case results.  How should we be both *defining* and *measuring* robust performance?}
- \textcolor{red}{Also, what about PAC kinds of a definition of robustness?}  
  - \textcolor{red}{Doesn't Tulloch mention something that sound PAC-ish?}

###  Importance of testing on multiple types of input error instead of just one  

Unsurprisingly, the type of input error (FP-dominant or FN-dominant) was a good predictor of whether the output error was more in the form of a solution cost error (for FN-dominated) or representation shortfall (for FP-dominated).  

- This demonstrates the importance of testing on multiple types of input error and measuring multiple types of output error.  
- This is rarely done, probably because  the primary focus of reserve selection has been on optimizing cost while *assuming* that constraints such as representation targets are met.  
- This focus and assumption have meant that when uncertainties *are* included in tests, the uncertainties have often only included false negatives (e.g., **\textcolor{red}{Beyer? Carwardine? others?}**), perhaps because the primary interest in much SCP research has been in solution cost error premised on other constraints being met.   

###  Two positive findings  

In spite of these negative findings, there are two encouraging counterpoints in our results.  

- First, deriving our problem generators from computational theory on problem difficulty was successful in creating a benchmark set that had demonstrable variety in individual problem difficulty, independent of problem size and uncertainty.  
- Second, computing and using attributes of problem structure as predictor inputs provided considerable gains in our ability to predict the amount of error in the reserve selection solutions beyond the same use of problem size.  
- This is a useful step given what Sarkar et al. said in their method comparison (@sarkarPlacePrioritizationBiodiversity2004) **\textcolor{red}{[or was it @sarkarBiodiversityConservationPlanning2006 ?]}** :  

```  
    We could not identify any set of features that predicted the accuracy 
    of an algorithm. Even size was not an unequivocal predictor. However, 
    the various PPAs, both optimal and heuristic, tracked the problems in 
    qualitatively similar ways. All found the same ones difficult or easy 
    which suggests that there are definite characteristics of the data sets 
    that explain the performance of various PPAs. This is a problem that 
    will merit further study.  
```  

###  Implication that case studies are useless  

The most obvious and important implication of this study is that the nearly ubiquitous practice of evaluating conservation planning methods on a single example, a small set of examples, or a larger set with identical statistical structure tells little about the accuracy of the method unless the method has theoretical guarantees.  

- If it has theoretical guarantees, the assumptions behind those guarantees must be matched by the data.  
- For example, the ILP methods tested here are guaranteed to find the optimal solution to a properly encoded problem assuming: 
  - a) sufficient time and 
  - b) no errors in the inputs.  
- As shown in our results, violating our ILP solver's assumption of having no input errors seriously degraded its accuracy and eliminated its guaranteed quantification of solution quality.  
- Changing to robust forms of ILP neither guarantees nor eliminates assumption violations.  
- While it does make the assumptions about input errors more robust than ignoring all input uncertainty, it does not guarantee that all input uncertainties are covered or accurately represented.  

##  Conclusions  

**\textcolor{red}{NOTE: Need to check this to make sure that the 2 or 3 most important things come through the strongest (I might have gotten carried away with mentioning too many other things), i.e., i) variation in problem structure shows large variation in problem difficulty and therefore a need for more varied testing, and ii) using bipartite graph measures, we can greatly improve estimates of probable optimization error on individual problems based on problem structure. iii) Also, the need for shared set of real problems inform/derive target distributional information.}**  

Little of the current evaluation of SCP methods allows us to either predict or understand method behavior on previously unseen individual problems where problem structures differ from the initial method presentation or when method assumptions are violated, e.g., by uncertainties in their inputs.  

To address this problem,  

- we have imported and extended a theory-guided solution planting method to  generate synthetic conservation planning problems with controllable difficulty, known optimal solutions, and statistically useful sample sizes. 
- We then used this method to generate a publicly available benchmark set of thousands of test problems with known correct solutions and a range of input uncertainties.  
- We showed that problem structure needs to be taken into account in SCP evaluation as there was large variation in accuracy at every level of input error and every reserve selector tested.  
- We showed that basing accuracy estimates on a single case study or small numbers of problems with the same structure, only one amount and one type of uncertainty, and unknown correct solution does not capture this variation and is likely to be misleading.  
- We also found that optimization without consideration for one or more input uncertainties nearly always led to magnification of the input errors.  
- Moreover, exact solution of problems without input error was not a good predictor of accuracy on problems that did have input error.

Most importantly,  

- we showed that given the wide range of error found at nearly every level of input error, it is important to be able to make reliable predictions about the amount and nature of likely reserve selection error on individual problems rather than estimating summary values such as the mean or median error.  
- Without reliable per-problem understanding of difficulty, we can't know what the risks are and whether further action is necessary, e.g., whether getting new information will be useful.  
- Our rephrasing of individual reserve selection problems as bipartite graphs enabled us to compute measures of structural attributes of problems that radically improved prediction accuracy over using problem size alone.

This study is important in comparison to other SCP studies because to our knowledge, there are no other SCP papers that i) explicitly target generating large problem sets with known correct solutions that vary in problem difficulty beyond problem size, ii) generate a large, publicly available benchmark set, or iii) explicitly attempt to learn to predict the amount of reserve selection error (both cost error and shortfall error) for particular reserve selectors on *individual* reserve selection problems.  

Even though this is a large study, it is only a first step and leaves many questions unanswered.  We see three primary issues for future work.  

- i) The most obvious issue is that the SCP community has no publicly available collection of real-world problems to characterize the universe of problems that SCP methods should generalize to.  
  - Consequently, we lack evidence for estimating the distributional parameters of problem attributes such as distributions of costs or the shape of the rank abundance curve found in species selected for inclusion in real-world SCP problems.  
- ii) A second issue with our study is that we only used reserve selectors that ignored input uncertainties.  
  - While this is still common in reserve selection, recent years have seen the rise of methods that do address input uncertainty such as robust ILP methods.  
  - Testing these methods on the problems generated here would be a useful next step.  
  - However, these newer methods each bring their own violated assumptions about the nature, number, and magnitude of input uncertainties and effectively just push the uncertainties back into estimates of their nature and magnitude.  
  - Moreover, these methods only address a subset of the uncertainties.  
    - While these methods are no doubt an improvement on uncertainty-free methods, they will require similar carefully structured tests that incorporate errors in the estimates of and assumptions about input uncertainties.  
- iii) A third issue with our tests is the simplicity of our problem definition, e.g., all costs are equal, all representation targets are equal, no spatial constraints, etc.  
  - It would be simple and useful to create more complex and ecologically realistic problem structures with known solutions via composition of independent problem elements (e.g., species co-occurrences, cost distributions, allocation of species occurrences to landscape locations, etc.).  
  - Exact solution methods such as ILP could be applied to the base, correct problems to produce known correct solutions before addition of uncertainties.  

We believe that better predictive understanding of the behavior of existing reserve selectors is more important than creating new reserve selectors whose true accuracy on previously unseen problems continues to be poorly understood.  We acknowledge that rigorous, informative evaluation of methods under uncertainty requires significant amounts of time, computational resources, and programming. We believe that this burden of proof of an SCP methods safety and efficacy should be on method developers, not on the conservation practitioners who try to use these methods.  We hope this paper contributes to that end. 



