
@misc{centerforhistoryandnewmediaZoteroQuickStart,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}},
	annote = {Welcome to Zotero!
View the Quick Start Guide to learn how to begin collecting, managing, citing, and sharing your research sources.
Thanks for installing Zotero.},
}

@article{austinSpeciesDistributionModels2007,
	title = {Species distribution models and ecological theory: {A} critical assessment and some possible new approaches},
	volume = {200},
	issn = {03043800},
	shorttitle = {Species distribution models and ecological theory},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380006003140},
	doi = {10.1016/j.ecolmodel.2006.07.005},
	language = {en},
	number = {1-2},
	urldate = {2015-04-19},
	journal = {Ecological Modelling},
	author = {Austin, Mike},
	month = jan,
	year = {2007},
	pages = {1--19},
	file = {Austin_2007_Species distribution models and ecological theory.pdf:/Users/bill/D/Zotero/storage/52A8JVJZ/Austin_2007_Species distribution models and ecological theory.pdf:application/pdf},
}

@article{millerIncorporatingSpatialDependence2007,
	title = {Incorporating spatial dependence in predictive vegetation models},
	volume = {202},
	issn = {03043800},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380006006247},
	doi = {10.1016/j.ecolmodel.2006.12.012},
	language = {en},
	number = {3-4},
	urldate = {2015-04-19},
	journal = {Ecological Modelling},
	author = {Miller, Jennifer and Franklin, Janet and Aspinall, Richard},
	month = apr,
	year = {2007},
	pages = {225--242},
	file = {Miller et al_2007_Incorporating spatial dependence in predictive vegetation models.pdf:/Users/bill/D/Zotero/storage/2EMQ9JU4/Miller et al_2007_Incorporating spatial dependence in predictive vegetation models.pdf:application/pdf},
}

@incollection{kanevskiSupportVectorMachines2009,
	address = {Lausanne},
	edition = {First},
	title = {Support {Vector} {Machines} and {Kernel} {Methods}},
	isbn = {978-0-8493-8237-6},
	abstract = {This book discusses machine learning algorithms, such as artificial neural networks of different architectures, statistical learning theory, and Support Vector Machines used for the classification and mapping of spatially distributed data.  It presents basic geostatistical algorithms as well. The authors describe new trends in machine learning and their application to spatial data. The text also includes real case studies based on environmental and pollution data. It includes a CD-ROM with software that will allow both students and researchers to put the concepts to practice.},
	language = {English},
	booktitle = {Machine {Learning} for {Spatial} {Environmental} {Data}: {Theory}, {Applications}, and {Software}},
	publisher = {EPFL Press},
	collaborator = {Kanevski, Mikhail and Timonin, Vadim and Pozdnukhov, Alexi},
	month = jun,
	year = {2009},
	pages = {247--346},
}

@article{embrechtsStatisticsQuantitativeRisk2014,
	title = {Statistics and {Quantitative} {Risk} {Management} for {Banking} and {Insurance}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-022513-115631},
	doi = {10.1146/annurev-statistics-022513-115631},
	language = {en},
	number = {1},
	urldate = {2015-04-19},
	journal = {Annual Review of Statistics and Its Application},
	author = {Embrechts, Paul and Hofert, Marius},
	month = jan,
	year = {2014},
	pages = {493--514},
	file = {Embrechts_Hofert_2014_Statistics and Quantitative Risk Management for Banking and Insurance.pdf:/Users/bill/D/Zotero/storage/32GXZ2MF/Embrechts_Hofert_2014_Statistics and Quantitative Risk Management for Banking and Insurance.pdf:application/pdf},
}

@article{rougierClimateSimulatorsClimate2014,
	title = {Climate {Simulators} and {Climate} {Projections}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-022513-115652},
	doi = {10.1146/annurev-statistics-022513-115652},
	language = {en},
	number = {1},
	urldate = {2015-04-19},
	journal = {Annual Review of Statistics and Its Application},
	author = {Rougier, Jonathan and Goldstein, Michael},
	month = jan,
	year = {2014},
	pages = {103--123},
	file = {Rougier_Goldstein_2014_Climate Simulators and Climate Projections.pdf:/Users/bill/D/Zotero/storage/4G7GWRBP/Rougier_Goldstein_2014_Climate Simulators and Climate Projections.pdf:application/pdf},
}

@article{guttorpStatisticsClimate2014,
	title = {Statistics and {Climate}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-022513-115648},
	doi = {10.1146/annurev-statistics-022513-115648},
	language = {en},
	number = {1},
	urldate = {2015-04-19},
	journal = {Annual Review of Statistics and Its Application},
	author = {Guttorp, Peter},
	month = jan,
	year = {2014},
	pages = {87--101},
	file = {Guttorp_2014_Statistics and Climate.pdf:/Users/bill/D/Zotero/storage/58MP8XBN/Guttorp_2014_Statistics and Climate.pdf:application/pdf},
}

@article{elith2009arees,
	title = {Species distribution models: ecological explanation and prediction across space and time},
	volume = {40},
	shorttitle = {Species distribution models},
	url = {http://www.annualreviews.org/eprint/HWR4cusJrXYCSPZ9sUDj/full},
	number = {1},
	urldate = {2015-05-25},
	journal = {Annual Review of Ecology, Evolution, and Systematics},
	author = {Elith, Jane and Leathwick, John R.},
	year = {2009},
	pages = {677},
	file = {Elith_Leathwick_2009_Species distribution models.pdf:/Users/bill/D/Zotero/storage/VJ5QGGKS/Elith_Leathwick_2009_Species distribution models.pdf:application/pdf},
}

@book{avrielNonlinearProgrammingAnalysis2003,
	address = {Mineola, NY},
	title = {Nonlinear programming: analysis and methods},
	isbn = {0-486-43227-0},
	shorttitle = {Nonlinear programming},
	publisher = {Dover Publications},
	author = {Avriel, M.},
	year = {2003},
	keywords = {Nonlinear programming},
	annote = {Originally published: Englewood Cliffs, N.J. : Prentice-Hall, c1976},
	file = {Avriel_2003_Nonlinear programming.pdf:/Users/bill/D/Zotero/storage/9DMSGPP9/Avriel_2003_Nonlinear programming.pdf:application/pdf},
}

@article{elithStatisticalExplanationMaxEnt2011,
	title = {A statistical explanation of {MaxEnt} for ecologists},
	volume = {17},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1472-4642.2010.00725.x/full},
	number = {1},
	urldate = {2015-05-25},
	journal = {Diversity and Distributions},
	author = {Elith, Jane and Phillips, Steven J. and Hastie, Trevor and Dudík, Miroslav and Chee, Yung En and Yates, Colin J.},
	year = {2011},
	pages = {43--57},
	file = {Elith et al_2011_A statistical explanation of MaxEnt for ecologists.pdf:/Users/bill/D/Zotero/storage/CQG3Z3C5/Elith et al_2011_A statistical explanation of MaxEnt for ecologists.pdf:application/pdf},
}

@article{zhangWhatGoingGeoprocess2015,
	title = {What's going on about geo-process modeling in virtual geographic environments ({VGEs})},
	issn = {03043800},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S030438001500174X},
	doi = {10.1016/j.ecolmodel.2015.04.023},
	language = {en},
	urldate = {2015-05-25},
	journal = {Ecological Modelling},
	author = {Zhang, Chunxiao and Chen, Min and Li, Rongrong and Fang, Chaoyang and Lin, Hui},
	month = may,
	year = {2015},
	keywords = {guppy, virtual},
	file = {zhang et al 2015 - Whats going on about geo-process modeling in virtual geographic environments (VGEs) - VIRTUAL.pdf:/Users/bill/D/Zotero/storage/WI8TUXBK/zhang et al 2015 - Whats going on about geo-process modeling in virtual geographic environments (VGEs) - VIRTUAL.pdf:application/pdf},
}

@incollection{mouretNoveltybasedMultiobjectivization2011,
	title = {Novelty-based multiobjectivization},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-18272-3_10},
	urldate = {2015-05-25},
	booktitle = {New horizons in evolutionary robotics},
	publisher = {Springer},
	author = {Mouret, Jean-Baptiste},
	year = {2011},
	pages = {139--154},
	file = {mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/7EDWRSB3/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:application/pdf},
}

@incollection{akrourPreferencebasedPolicyLearning2011,
	title = {Preference-based policy learning},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23780-5_11},
	urldate = {2015-05-25},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
	year = {2011},
	pages = {12--27},
	file = {akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:/Users/bill/D/Zotero/storage/XHCVGFJI/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:application/pdf},
}

@inproceedings{karimzadehganExplorationexploitationTradeoffInteractive2010,
	title = {Exploration-exploitation tradeoff in interactive relevance feedback},
	url = {http://dl.acm.org/citation.cfm?id=1871631},
	urldate = {2015-05-25},
	booktitle = {Proceedings of the 19th {ACM} international conference on {Information} and knowledge management},
	publisher = {ACM},
	author = {Karimzadehgan, Maryam and Zhai, ChengXiang},
	year = {2010},
	pages = {1397--1400},
	file = {karimzadehgan zhai - exploration-exploitation tradeoff in interactive relevance feedback.pdf:/Users/bill/D/Zotero/storage/N66MWF6S/karimzadehgan zhai - exploration-exploitation tradeoff in interactive relevance feedback.pdf:application/pdf},
}

@inproceedings{krcahCombinationNoveltySearch2010,
	title = {Combination of novelty search and fitness-based search applied to robot body-brain co-evolution},
	url = {http://artax.karlin.mff.cuni.cz/~krcap1am/ero/doc/krcah-cjs10.pdf},
	urldate = {2015-05-25},
	booktitle = {Czech-{Japan} {Seminar} on {Data} {Analysis} and {Decision} {Making} in {Service} {Science}},
	author = {Krcah, Peter and Toropila, Daniel},
	year = {2010},
	pages = {1--6},
	file = {krcah toropila - combination of novelty search and fitness-based search applied to robot body-brain co-evolution.pdf:/Users/bill/D/Zotero/storage/F47T6AVQ/krcah toropila - combination of novelty search and fitness-based search applied to robot body-brain co-evolution.pdf:application/pdf},
}

@article{stillmanDerivingSimplePredictions2015,
	title = {Deriving simple predictions from complex models to support environmental decision-making},
	issn = {03043800},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380015001659},
	doi = {10.1016/j.ecolmodel.2015.04.014},
	language = {en},
	urldate = {2015-05-25},
	journal = {Ecological Modelling},
	author = {Stillman, Richard A. and Wood, Kevin A. and Goss-Custard, John D.},
	month = may,
	year = {2015},
	file = {stillman et al 2015 - Deriving simple predictions from complex models to support environmental decision-making - GUPPY - THRESHOLDS.pdf:/Users/bill/D/Zotero/storage/TCJ26JZC/stillman et al 2015 - Deriving simple predictions from complex models to support environmental decision-making - GUPPY - THRESHOLDS.pdf:application/pdf},
}

@article{phillipsSampleSelectionBias2009,
	title = {Sample selection bias and presence-only distribution models: implications for background and pseudo-absence data},
	volume = {19},
	shorttitle = {Sample selection bias and presence-only distribution models},
	url = {http://www.esajournals.org/doi/abs/10.1890/07-2153.1},
	number = {1},
	urldate = {2015-05-25},
	journal = {Ecological Applications},
	author = {Phillips, Steven J. and Dudík, Miroslav and Elith, Jane and Graham, Catherine H. and Lehmann, Anthony and Leathwick, John and Ferrier, Simon},
	year = {2009},
	pages = {181--197},
	file = {Phillips et al_2009_Sample selection bias and presence-only distribution models.pdf:/Users/bill/D/Zotero/storage/3HTESF67/Phillips et al_2009_Sample selection bias and presence-only distribution models.pdf:application/pdf},
}

@article{elithArtModellingRangeshifting2010,
	title = {The art of modelling range-shifting species},
	volume = {1},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00036.x/full},
	number = {4},
	urldate = {2015-05-25},
	journal = {Methods in ecology and evolution},
	author = {Elith, Jane and Kearney, Michael and Phillips, Steven},
	year = {2010},
	pages = {330--342},
}

@article{elithTheyHowThey2009,
	title = {Do they? {How} do they? {WHY} do they differ? {On} finding reasons for differing performances of species distribution models},
	volume = {32},
	shorttitle = {Do they?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0587.2008.05505.x/full},
	number = {1},
	urldate = {2015-05-25},
	journal = {Ecography},
	author = {Elith, Jane and Graham, Catherine H.},
	year = {2009},
	pages = {66--77},
	file = {Elith_Graham_2009_Do they.pdf:/Users/bill/D/Zotero/storage/X9AEXHBG/Elith_Graham_2009_Do they.pdf:application/pdf},
}

@article{barryErrorUncertaintyHabitat2006,
	title = {Error and uncertainty in habitat models},
	volume = {43},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2664.2006.01136.x/full},
	number = {3},
	urldate = {2015-05-25},
	journal = {Journal of Applied Ecology},
	author = {Barry, Simon and Elith, Jane},
	year = {2006},
	pages = {413--423},
	file = {Barry_Elith_2006_Error and uncertainty in habitat models.pdf:/Users/bill/D/Zotero/storage/ZXYZK86G/Barry_Elith_2006_Error and uncertainty in habitat models.pdf:application/pdf},
}

@article{guisanSensitivityPredictiveSpecies2007,
	title = {Sensitivity of predictive species distribution models to change in grain size},
	volume = {13},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1472-4642.2007.00342.x/full},
	number = {3},
	urldate = {2015-05-25},
	journal = {Diversity and Distributions},
	author = {Guisan, Antoine and Graham, Catherine H. and Elith, Jane and Huettmann, Falk},
	year = {2007},
	pages = {332--340},
}

@article{dormannCollinearityReviewMethods2013,
	title = {Collinearity: a review of methods to deal with it and a simulation study evaluating their performance},
	volume = {36},
	shorttitle = {Collinearity},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0587.2012.07348.x/full},
	number = {1},
	urldate = {2015-05-25},
	journal = {Ecography},
	author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carré, Gabriel and Marquéz, Jaime R. García and Gruber, Bernd and Lafourcade, Bruno and Leitão, Pedro J. and {others}},
	year = {2013},
	pages = {27--46},
	file = {Dormann et al_2013_Collinearity.pdf:/Users/bill/D/Zotero/storage/MJAZDF2H/Dormann et al_2013_Collinearity.pdf:application/pdf},
}

@article{ferrierUsingGeneralizedDissimilarity2007,
	title = {Using generalized dissimilarity modelling to analyse and predict patterns of beta diversity in regional biodiversity assessment},
	volume = {13},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1472-4642.2007.00341.x/full},
	number = {3},
	urldate = {2015-05-25},
	journal = {Diversity and distributions},
	author = {Ferrier, Simon and Manion, Glenn and Elith, Jane and Richardson, Karen},
	year = {2007},
	pages = {252--264},
	file = {Ferrier et al_2007_Using generalized dissimilarity modelling to analyse and predict patterns of.pdf:/Users/bill/D/Zotero/storage/VF2INNX9/Ferrier et al_2007_Using generalized dissimilarity modelling to analyse and predict patterns of.pdf:application/pdf},
}

@article{elithWorkingGuideBoosted2008,
	title = {A working guide to boosted regression trees},
	volume = {77},
	issn = {0021-8790, 1365-2656},
	url = {http://doi.wiley.com/10.1111/j.1365-2656.2008.01390.x},
	doi = {10.1111/j.1365-2656.2008.01390.x},
	language = {en},
	number = {4},
	urldate = {2015-05-25},
	journal = {Journal of Animal Ecology},
	author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
	month = jul,
	year = {2008},
	pages = {802--813},
	file = {Elith_et_al-2008-Journal_of_Animal_Ecology.pdf:/Users/bill/D/Zotero/storage/CHWAKAAZ/Elith_et_al-2008-Journal_of_Animal_Ecology.pdf:application/pdf},
}

@article{lagunaMathematicalModelLivestock2015,
	title = {Mathematical model of livestock and wildlife: {Predation} and competition under environmental disturbances},
	volume = {309-310},
	issn = {03043800},
	shorttitle = {Mathematical model of livestock and wildlife},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380015001714},
	doi = {10.1016/j.ecolmodel.2015.04.020},
	language = {en},
	urldate = {2015-05-25},
	journal = {Ecological Modelling},
	author = {Laguna, M.F. and Abramson, G. and Kuperman, M.N. and Lanata, J.L. and Monjeau, J.A.},
	month = aug,
	year = {2015},
	keywords = {voiceless},
	pages = {110--117},
	file = {Laguna et al. - 2015 - Mathematical model of livestock and wildlife Pred.pdf:/Users/bill/D/Zotero/storage/UB8ISUUR/Laguna et al. - 2015 - Mathematical model of livestock and wildlife Pred.pdf:application/pdf},
}

@inproceedings{simonciniCentricSelectionWay2009,
	title = {Centric selection: a way to tune the exploration/exploitation trade-off},
	shorttitle = {Centric selection},
	url = {http://dl.acm.org/citation.cfm?id=1570023},
	urldate = {2015-05-25},
	booktitle = {Proceedings of the 11th {Annual} conference on {Genetic} and evolutionary computation},
	publisher = {ACM},
	author = {Simoncini, David and Verel, Sébastien and Collard, Philippe and Clergue, Manuel},
	year = {2009},
	keywords = {exploration/exploitation tradeoff, genetic algorithms, GA},
	pages = {891--898},
	file = {Simoncini et al. - 2009 - Centric selection a way to tune the exploratione.pdf:/Users/bill/D/Zotero/storage/7TINSXNV/Simoncini et al. - 2009 - Centric selection a way to tune the exploratione.pdf:application/pdf},
}

@article{rungeDetectingFailureClimate2016,
	title = {Detecting failure of climate predictions},
	volume = {advance online publication},
	issn = {1758-6798},
	url = {http://dx.doi.org/10.1038/nclimate3041},
	journal = {Nature Clim. Change},
	author = {Runge, Michael C. and Stroeve, Julienne C. and Barrett, Andrew P. and McDonald-Madden, Eve},
	month = may,
	year = {2016},
}

@article{rungeDetectingFailureClimate2016a,
	title = {Detecting failure of climate predictions},
	volume = {advance online publication},
	issn = {1758-6798},
	url = {http://dx.doi.org/10.1038/nclimate3041},
	journal = {Nature Clim. Change},
	author = {Runge, Michael C. and Stroeve, Julienne C. and Barrett, Andrew P. and McDonald-Madden, Eve},
	month = may,
	year = {2016},
}

@article{loiselleAvoidingPitfallsUsing2003,
	title = {Avoiding {Pitfalls} of {Using} {Species} {Distribution} {Models} in {Conservation} {Planning}},
	volume = {17},
	issn = {08888892, 15231739},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2003.00233.x},
	doi = {10.1111/j.1523-1739.2003.00233.x},
	abstract = {Museum records have great potential to provide valuable insights into the vulnerability, historic distribution, and conservation of species, especially when coupled with species-distribution models used to predict species’ ranges. Yet, the increasing dependence on species-distribution models in identifying conservation priorities calls for a more critical evaluation of model robustness. We used 11 bird species of conservation concern in Brazil’s highly fragmented Atlantic Forest and data on environmental conditions in the region to predict species distributions. These predictions were repeated for five different model types for each of the 11 bird species. We then combined these species distributions for each model separately and applied a reserveselection algorithm to identify priority sites. We compared the potential outcomes from the reserve selection among the models. Although similarity in identification of conservation reserve networks occurred among models, models differed markedly in geographic scope and flexibility of reserve networks. It is essential for planners to evaluate the conservation implications of false-positive and false-negative errors for their specific management scenario before beginning the modeling process. Reserve networks selected by models that minimized false-positive errors provided a better match with priority areas identified by specialists. Thus, we urge caution in the use of models that overestimate species’ occurrences because they may misdirect conservation action. Our approach further demonstrates the great potential value of museum records to biodiversity studies and the utility of species-distribution models to conservation decision-making. Our results also demonstrate, however, that these models must be applied critically and cautiously.},
	language = {en},
	number = {6},
	urldate = {2020-01-21},
	journal = {Conservation Biology},
	author = {Loiselle, Bette A. and Howell, Christine A. and Graham, Catherine H. and Goerck, Jaqueline M. and Brooks, Thomas and Smith, Kimberly G. and Williams, Paul H.},
	month = dec,
	year = {2003},
	keywords = {sdm, reserve selection, bdpg, uncertainty},
	pages = {1591--1600},
	file = {loiselle et al 2003 - avoiding pitfalls of using species distribution models in conservation planning - consbio.pdf:/Users/bill/D/Zotero/storage/VVZA64LI/loiselle et al 2003 - avoiding pitfalls of using species distribution models in conservation planning - consbio.pdf:application/pdf},
}

@article{lepczykOntologyLandscapes2008,
	title = {An ontology for landscapes},
	volume = {5},
	issn = {1476945X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1476945X08000135},
	doi = {10.1016/j.ecocom.2008.04.001},
	abstract = {As ecological data increases in breadth, depth, and complexity, the discipline of ecology is increasingly inﬂuenced by information science. While this inﬂuence provides many opportunities for ecologists, it also necessitates a change in how we manage and share data, and perhaps more fundamentally, deﬁne concepts in ecology. Speciﬁcally, the information technology process of automated data integration entirely depends upon consistent concept deﬁnition. A common tool used in computer science and engineering to specify meanings, which is both novel and offers signiﬁcant potential to ecology, is an ontology. An ontology is a formal representation of knowledge in which concepts are described by their meaning and their relationship to each other. Ontologies are a tool that can be used to ‘explicitly specify a concept’ (Gruber, 1993) and this approach is uncommon in ecology. In this paper, we develop an ontology for the concept of ‘landscape’ that captures the most general deﬁnitions and usages of this term. We selected the concept of landscape because it is often used in very different ways by investigators and hence generates linguistic uncertainty. A graphic theoretic (i.e., visual) model is provided which describes the set of structuring rules we used to deﬁne the relationships between ‘landscape’ and appropriately related terms. Based upon these rules, a landscape necessarily contains a spatial component (i.e., area), structure and function (i.e., ecosystems), and is scale independent. This approach provides the set of necessary conditions for landscape studies to reduce linguistic uncertainty, and facilitate interoperability of data, i.e., in a manner that promotes data linkages and quantitative synthesis particularly by automatic data synthesis programs that are likely to become an important part of ecology in the future. Simply put, we use an ontology, a technique novel to ecology but not other disciplines, to deﬁne ‘landscape,’ thereby clearly delineating one subset of its potential general usage. As such this ontology can serve as both a checklist for landscape studies and a blueprint for additional ecological ontologies.},
	language = {en},
	number = {3},
	urldate = {2020-01-21},
	journal = {Ecological Complexity},
	author = {Lepczyk, Christopher A. and Lortie, Christopher J. and Anderson, Laurel J.},
	month = sep,
	year = {2008},
	keywords = {problem difficulty, bdpg, landscape ecology},
	pages = {272--279},
	file = {ontology for landscapes.pdf:/Users/bill/D/Zotero/storage/C7N8L4B5/ontology for landscapes.pdf:application/pdf},
}

@article{newChallengesUsingProbabilistic2007,
	title = {Challenges in using probabilistic climate change information for impact assessments: an example from the water sector},
	volume = {365},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Challenges in using probabilistic climate change information for impact assessments},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2007.2080},
	doi = {10.1098/rsta.2007.2080},
	language = {en},
	number = {1857},
	urldate = {2020-01-21},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {New, Mark and Lopez, Ana and Dessai, Suraje and Wilby, Rob},
	month = aug,
	year = {2007},
	pages = {2117--2131},
	file = {NEWETA~1.PDF:/Users/bill/D/Zotero/storage/KJAVL8EX/NEWETA~1.PDF:application/pdf},
}

@article{collinsEnsemblesProbabilitiesNew2007,
	title = {Ensembles and probabilities: a new era in the prediction of climate change},
	volume = {365},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Ensembles and probabilities},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2007.2068},
	doi = {10.1098/rsta.2007.2068},
	language = {en},
	number = {1857},
	urldate = {2020-01-21},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Collins, Mat},
	month = aug,
	year = {2007},
	keywords = {ensembles, climate change},
	pages = {1957--1970},
	file = {COLLIN~1.PDF:/Users/bill/D/Zotero/storage/U44ILK2B/COLLIN~1.PDF:application/pdf},
}

@article{stainforthConfidenceUncertaintyDecisionsupport2007,
	title = {Confidence, uncertainty and decision-support relevance in climate predictions},
	volume = {365},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2007.2074},
	doi = {10.1098/rsta.2007.2074},
	language = {en},
	number = {1857},
	urldate = {2020-01-21},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Stainforth, D.A and Allen, M.R and Tredger, E.R and Smith, L.A},
	month = aug,
	year = {2007},
	pages = {2145--2161},
	file = {STAINF~1.PDF:/Users/bill/D/Zotero/storage/WFBCH6QE/STAINF~1.PDF:application/pdf},
}

@article{lerouxAccountingSystemDynamics2007,
	title = {Accounting for {System} {Dynamics} in {Reserve} {Design}},
	volume = {17},
	url = {http://www.jstor.org/stable/40062090},
	abstract = {Systematic conservation plans have only recently considered the dynamic nature of ecosystems. Methods have been developed to incorporate climate change, population dynamics, and uncertainty in reserve design, but few studies have examined how to account for natural disturbance. Considering natural disturbance in reserve design may be especially important for the world's remaining intact areas, which still experience active natural disturbance regimes. We developed a spatially explicit, dynamic simulation model, CONSERV, which simulates patch dynamics and fire, and used it to evaluate the efficacy of hypothetical reserve networks in northern Canada. We designed six networks based on conventional reserve design methods, with different conservation targets for woodland caribou habitat, high-quality wetlands, vegetation, water bodies, and relative connectedness. We input the six reservenetworks into CONSERV and tracked the ability of each to maintain initial conservation targets through time under an active natural disturbance regime. None of the reservenetworks maintained all initial targets, and some over-representedcertain features, suggesting that both effectiveness and efficiency of reserve design could be improved through use of spatially explicit dynamic simulation during the planning process. Spatial simulation models of landscape dynamics are commonly used in natural resource management, but we provide the first illustration of their potential use for reservedesign. Spatial simulation models could be used iteratively to evaluate competing reserve designs and select targets that have a higher likelihood of being maintained through time. Such models could be combined with dynamic planning techniques to develop a general theory for reserve design in an uncertain world.},
	language = {en},
	number = {7},
	journal = {Ecological Applications},
	author = {Leroux, Shawn J. and Schmiegelow, Fiona K. A. and Cumming, Steve G. and Lessard, Robert B. and Nagy, John},
	year = {2007},
	keywords = {reserve selection, bdpg},
	pages = {1954--1966},
	file = {leroux et al 2007 - accounting for system dynamics in reserve design - ecoapps.pdf:/Users/bill/D/Zotero/storage/U3VVTBJS/leroux et al 2007 - accounting for system dynamics in reserve design - ecoapps.pdf:application/pdf},
}

@article{polhillWhatEveryAgentbased2006,
	title = {What every agent-based modeller should know about floating point arithmetic},
	volume = {21},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815204002749},
	doi = {10.1016/j.envsoft.2004.10.011},
	abstract = {Floating point arithmetic is a subject all too often ignored, yet, for agent-based models in particular, it has the potential to create misleading results, and even to inﬂuence emergent outcomes of the model. Using a simple demonstration model, this paper illustrates the problems that accumulated ﬂoating point errors can cause, and compares a number of techniques that might be used to address them. We show that inexact representation of parameter values, imprecision in calculation results, and diﬀering implementations of mathematical expressions can signiﬁcantly inﬂuence the behaviour of the model, and create issues for replicating results, though they do not necessarily do so. None of the techniques oﬀer a failsafe approach that can be applied in any situation, though interval arithmetic is the most promising.},
	language = {en},
	number = {3},
	urldate = {2020-01-21},
	journal = {Environmental Modelling \& Software},
	author = {Polhill, J. Gary and Izquierdo, Luis R. and Gotts, Nicholas M.},
	month = mar,
	year = {2006},
	keywords = {guppy, agent-based, floating point},
	pages = {283--309},
	file = {polhill izquierdo gotts 2006 - what every agent-based modeller should know about floating point arithmetic - envmodsoft.pdf:/Users/bill/D/Zotero/storage/I9B2EG2H/polhill izquierdo gotts 2006 - what every agent-based modeller should know about floating point arithmetic - envmodsoft.pdf:application/pdf},
}

@article{vazeImpactAccuracyResolution2010,
	title = {Impact of {DEM} accuracy and resolution on topographic indices},
	volume = {25},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815210000733},
	doi = {10.1016/j.envsoft.2010.03.014},
	abstract = {Topography is an important land-surface characteristic that affects most aspects of the water balance in a catchment, including the generation of surface and sub-surface runoff; the ﬂow paths followed by water as it moves down and through hillslopes and the rate of water movement. All of the spatially explicit fully distributed hydraulic and hydrological models use topography (represented by the DEM of the area modelled) to derive bathymetry. DEM is also used to derive some other key information critical in fully distributed hydraulic and hydrological models.},
	language = {en},
	number = {10},
	urldate = {2020-01-21},
	journal = {Environmental Modelling \& Software},
	author = {Vaze, Jai and Teng, Jin and Spencer, Georgina},
	month = oct,
	year = {2010},
	keywords = {uncertainty, DEM, koala},
	pages = {1086--1098},
	file = {vaze et al 2010 - impact of dem accuracy and resolution on topographic indices - envmodsoft.pdf:/Users/bill/D/Zotero/storage/VLA2ZWNZ/vaze et al 2010 - impact of dem accuracy and resolution on topographic indices - envmodsoft.pdf:application/pdf},
}

@article{wilsonSettingConservationPriorities2009,
	title = {Setting {Conservation} {Priorities}},
	volume = {1162},
	issn = {00778923},
	url = {http://doi.wiley.com/10.1111/j.1749-6632.2009.04149.x},
	doi = {10.1111/j.1749-6632.2009.04149.x},
	language = {en},
	number = {1},
	urldate = {2020-01-21},
	journal = {Annals of the New York Academy of Sciences},
	author = {Wilson, Kerrie A. and Carwardine, Josie and Possingham, Hugh P.},
	month = apr,
	year = {2009},
	pages = {237--264},
	file = {wilson et al 2009 - setting conservation priorities - annnyacadsci.pdf:/Users/bill/D/Zotero/storage/F9FKQJKP/wilson et al 2009 - setting conservation priorities - annnyacadsci.pdf:application/pdf},
}

@article{schmolkeEcologicalModelsSupporting2010,
	title = {Ecological models supporting environmental decision making: a strategy for the future},
	volume = {25},
	issn = {01695347},
	shorttitle = {Ecological models supporting environmental decision making},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016953471000100X},
	doi = {10.1016/j.tree.2010.05.001},
	language = {en},
	number = {8},
	urldate = {2020-01-21},
	journal = {Trends in Ecology \& Evolution},
	author = {Schmolke, Amelie and Thorbek, Pernille and DeAngelis, Donald L. and Grimm, Volker},
	month = aug,
	year = {2010},
	pages = {479--486},
	file = {schmolke et al 2010 - ecological models supporting environmental decision making - a strategy for the future - tree.pdf:/Users/bill/D/Zotero/storage/Q2ZD9M2K/schmolke et al 2010 - ecological models supporting environmental decision making - a strategy for the future - tree.pdf:application/pdf},
}

@article{erosNetworkThinkingRiverscape2011,
	title = {Network thinking in riverscape conservation – {A} graph-based approach},
	volume = {144},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320710003708},
	doi = {10.1016/j.biocon.2010.08.013},
	abstract = {Graph theoretic approaches have received increased interest recently in landscape planning and conservation in the terrestrial realm, because these approaches facilitate the effective modelling of connectivity among habitats. We examined whether basic principles of graph theory can be extended to other ecosystems. Speciﬁcally, we demonstrate how a network-based context can be used for enhancing the more effective conservation of riverine systems. We ﬁrst show how to use graph theoretic techniques to model riverscapes at the segment level. Then we use a real stream network (Zagyva river basin, Hungary) to examine the topological importance of segments in maintaining riverscape connectivity, using betweenness centrality, a commonly used network measure. Using the undirected graph model of this riverscape, we then prioritize segments for conservation purpose. We examine the value of each of the 93 segments present in the Zagyva river basin by considering the conservation value of local ﬁsh assemblages, connectivity and the size of the habitat patches. For this purpose we use the ‘integral index of connectivity’, a recently advocated habitat availability index. Based on the results the selection of the most valuable habitat segments can be optimized depending on conservation resources. Because of their inherent advantage in the consideration of connectivity relationships, we suggest that network analyses offer a simple, yet effective tool for searching for key segments (or junctions) in riverscapes for conservation and environmental management. Further, although the joint consideration of aquatic and terrestrial networks is challenging, the extension of network analyses to freshwater systems may facilitate the more effective selection of priority areas for conservation in continental areas.},
	language = {en},
	number = {1},
	urldate = {2020-01-21},
	journal = {Biological Conservation},
	author = {Erős, Tibor and Schmera, Dénes and Schick, Robert S.},
	month = jan,
	year = {2011},
	keywords = {stef, rivers, graph theory, networks},
	pages = {184--192},
	file = {eros et al 2010 - network thinking in riverscape conservation - a graph-based approach - biocons.pdf:/Users/bill/D/Zotero/storage/D6HKFZ44/eros et al 2010 - network thinking in riverscape conservation - a graph-based approach - biocons.pdf:application/pdf},
}

@article{schotterDECISIONMAKINGNAIVE,
	title = {{DECISION} {MAKING} {WITH} {NAÏVE} {ADVICE}},
	abstract = {In many of the decisions we make we rely on the advice of others who have preceded us. For example, before we buy a car, choose a dentist, choose a spouse, ﬁnd a school for our children, sign on to a retirement plan, etc. we usually ask the advice of others who have experience with such decisions. The same is true when we make major ﬁnancial decisions. Here people easily take advice from their fellow workers or relatives as to how to choose stock, balance a portfolio, or save for their child’s education. Although some advice we get is from experts, most of the time we make our decisions relying only on the rather uninformed word-of-mouth advice we get from our friends or neighbors. We call this ?aive advice? In this paper I will outline a set of experimental results that indicate that word-of-mouth advice is a very powerful force in shaping the decisions that people make and tends to push those decisions in the direction of the predictions of the rational theory.},
	language = {en},
	author = {Schotter, Andrew},
	pages = {26},
	file = {schotter - decision making with naive advice.pdf:/Users/bill/D/Zotero/storage/39HLDWC4/schotter - decision making with naive advice.pdf:application/pdf},
}

@article{ScenariomodelparameterNewMethod2003,
	title = {Scenario-model-parameter: a new method of cumulative risk uncertainty analysis},
	volume = {44},
	issn = {01406701},
	shorttitle = {Scenario-model-parameter},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140670103820167},
	doi = {10.1016/S0140-6701(03)82016-7},
	abstract = {The recently developed concepts of aggregate risk and cumulative risk rectify two limitations associated with the classical risk assessment paradigm established in the early 1980s. Aggregate exposure denotes the amount of one pollutant available at the biological exchange boundaries from multiple routes of exposure. Cumulative risk assessment is defined as an assessment of risk from the accumulation of a common toxic effect from all routes of exposure to multiple chemicals sharing a common mechanism of toxicity. Thus, cumulative risk constitutes an improvement over the classical risk paradigm, which treats exposures from multiple routes as independent events associated with each specific route. Risk assessors formulate complex models and identify many realistic scenarios of exposure that enable them to estimate risks from exposures to multiple pollutants and multiple routes. The increase in complexity of the risk assessment process is likely to increase risk uncertainty. Despite evidence that scenario and model uncertainty contribute to the overall uncertainty of cumulative risk estimates, present uncertainty analysis of risk estimates accounts only for parameter uncertainty and excludes model and scenario uncertainties. This paper provides a synopsis of the risk assessment evolution and associated uncertainty analysis methods. This evolution leads to the concept of the scenario – model – parameter (SMP) cumulative risk uncertainty analysis method. The SMP uncertainty analysis is a multiple step procedure that assesses uncertainty associated with the use of judiciously selected scenarios and models of exposure and risk. Ultimately, the SMP uncertainty analysis method compares risk uncertainty estimates determined using all three sources of uncertainty with conventional risk uncertainty estimates obtained using only the parameter source. An example of applying the SMP uncertainty analysis to cumulative risk estimates from exposures to two pesticides indicates that inclusion of scenario and model sources increases uncertainty of risk estimates relative to those estimated using only the parameter source. Changes in uncertainty magnitude may affect decisions made by risk managers.},
	language = {en},
	number = {3},
	urldate = {2020-01-21},
	journal = {Fuel and Energy Abstracts},
	month = may,
	year = {2003},
	pages = {184},
	file = {moschandreas karuchit - scenario-model-parameter -  a new method of cumulative risk uncertainty analysis.pdf:/Users/bill/D/Zotero/storage/5235J5WN/moschandreas karuchit - scenario-model-parameter -  a new method of cumulative risk uncertainty analysis.pdf:application/pdf},
}

@inproceedings{harmanMetricsAreFitness2004,
	address = {Chicago, IL, USA},
	title = {Metrics are fitness functions too},
	isbn = {978-0-7695-2129-9},
	url = {http://ieeexplore.ieee.org/document/1357891/},
	doi = {10.1109/METRIC.2004.1357891},
	abstract = {Metrics, whether collected statically or dynamically, and whether constructed from source code, systems or processes, are largely regarded as a means of evaluating some property of interest. This viewpoint has been very successful in developing a body of knowledge, theory and experience in the application of metrics to estimation, predication, assessment, diagnosis, analysis and improvement. This paper shows that there is an alternative, complementary, view of a metric: as a ﬁtness function, used to guide a search for optimal or near optimal individuals in a search space of possible solutions.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {10th {International} {Symposium} on {Software} {Metrics}, 2004. {Proceedings}.},
	publisher = {IEEE},
	author = {Harman, M. and Clark, J.},
	year = {2004},
	keywords = {EFs, metrics, optimization, fitness functions},
	pages = {58--69},
	file = {harman clark 2004 - metrics are fitness functions too.pdf:/Users/bill/D/Zotero/storage/9U55U845/harman clark 2004 - metrics are fitness functions too.pdf:application/pdf},
}

@inproceedings{insaniSelectingSuitableSolution2013,
	address = {Yogyakarta, Indonesia},
	title = {Selecting suitable solution strategies for {Classes} of graph coloring instances using data mining},
	isbn = {978-1-4799-0425-9 978-1-4799-0423-5 978-1-4799-0424-2},
	url = {http://ieeexplore.ieee.org/document/6676240/},
	doi = {10.1109/ICITEED.2013.6676240},
	abstract = {The Maximal Independent Set (MIS) formulation tackles the graph coloring problem (GCP) as the partitioning of vertices of a graph into a minimum number of maximal independent sets as each MIS can be assigned a unique color. Mehrotra and Trick [5] solved the MIS formulation with an exact IP approach, but they were restricted to solving smaller or easier instances. For harder instances, it might be impossible to get the optimal solution within a reasonable computation time. We develop a heuristic algorithm, hoping that we can solve these problems in more reasonable time. However, though heuristics can find a near-optimal solution extremely fast compared to the exact approaches, there is still significant variations in performance that can only be explained by the fact that certain structures or properties in graphs may be better suited to some heuristics more than others. Selecting the best algorithm on average across all instances does not help us pick the best one for a particular instance. The need to understand how the best heuristic for a particular class of instance depends on these graph properties is an important issue. In this research, we use data mining to select the best solution strategies for classes of graph coloring instances.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {2013 {International} {Conference} on {Information} {Technology} and {Electrical} {Engineering} ({ICITEE})},
	publisher = {IEEE},
	author = {Insani, Nur and Smith-Miles, Kate and Baatar, Davaatseren},
	month = oct,
	year = {2013},
	keywords = {problem difficulty, bdpg},
	pages = {208--215},
	file = {insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/WKL3Y57W/insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = dec,
	year = {2008},
	keywords = {problem difficulty, bdpg},
	pages = {1--25},
	file = {a6-smith-miles.pdf:/Users/bill/D/Zotero/storage/EH3M2958/a6-smith-miles.pdf:application/pdf;performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf:/Users/bill/D/Zotero/storage/7DWGI3PN/performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008a,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = dec,
	year = {2008},
	pages = {1--25},
	file = {a6-smith-miles.pdf:/Users/bill/D/Zotero/storage/EZ874QSU/a6-smith-miles.pdf:application/pdf},
}

@article{bowlyGenerationTechniquesLinear2019,
	title = {Generation techniques for linear programming instances with controllable properties},
	issn = {1867-2949, 1867-2957},
	url = {http://link.springer.com/10.1007/s12532-019-00170-6},
	doi = {10.1007/s12532-019-00170-6},
	abstract = {This paper addresses the problem of generating synthetic test cases for experimentation in linear programming. We propose a method which maps instance generation and instance space search to an alternative encoded space. This allows us to develop a generator for feasible bounded linear programming instances with controllable properties. We show that this method is capable of generating any feasible bounded linear program, and that parameterised generators and search algorithms using this approach generate only feasible bounded instances. Our results demonstrate that controlled generation and instance space search using this method achieves feature diversity more eﬀectively than using a direct representation.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Mathematical Programming Computation},
	author = {Bowly, Simon and Smith-Miles, Kate and Baatar, Davaatseren and Mittelmann, Hans},
	month = aug,
	year = {2019},
	file = {bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:/Users/bill/D/Zotero/storage/EFQD2YAQ/bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:application/pdf},
}

@incollection{corneOptimisationGeneralisationFootprints2010,
	address = {Berlin, Heidelberg},
	title = {Optimisation and {Generalisation}: {Footprints} in {Instance} {Space}},
	isbn = {978-3-642-15843-8 978-3-642-15844-5},
	shorttitle = {Optimisation and {Generalisation}},
	url = {http://link.springer.com/10.1007/978-3-642-15844-5_3},
	abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for speciﬁc problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its ‘seen’ instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm ‘footprints’, indicating how performance generalises in instance space. We ﬁnd much evidence that typical ways of choosing the ‘best’ algorithm, via tests over a distribution of instances, are seriously ﬂawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Parallel {Problem} {Solving} from {Nature}, {PPSN} {XI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Corne, David W. and Reynolds, Alan P.},
	editor = {Schaefer, Robert and Cotta, Carlos and Kołodziej, Joanna and Rudolph, Günter},
	year = {2010},
	doi = {10.1007/978-3-642-15844-5_3},
	pages = {22--31},
	file = {corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/RD2J5PZR/corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{munozPerformanceAnalysisContinuous2017,
	title = {Performance {Analysis} of {Continuous} {Black}-{Box} {Optimization} {Algorithms} via {Footprints} in {Instance} {Space}},
	volume = {25},
	issn = {1063-6560, 1530-9304},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/evco_a_00194},
	doi = {10.1162/evco_a_00194},
	abstract = {This article presents a method for the objective assessment of an algorithm’s strengths and weaknesses. Instead of examining the performance of only one or more algorithms on a benchmark set, or generating custom problems that maximize the performance difference between two algorithms, our method quantiﬁes both the nature of the test instances and the algorithm performance. Our aim is to gather information about possible phase transitions in performance, that is, the points in which a small change in problem structure produces algorithm failure. The method is based on the accurate estimation and characterization of the algorithm footprints, that is, the regions of instance space in which good or exceptional performance is expected from an algorithm. A footprint can be estimated for each algorithm and for the overall portfolio. Therefore, we select a set of features to generate a common instance space, which we validate by constructing a sufﬁciently accurate prediction model. We characterize the footprints by their area and density. Our method identiﬁes complementary performance between algorithms, quantiﬁes the common features of hard problems, and locates regions where a phase transition may lie.},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Evolutionary Computation},
	author = {Muñoz, Mario A. and Smith-Miles, Kate A.},
	month = dec,
	year = {2017},
	pages = {529--554},
	file = {evco_a_00194.pdf:/Users/bill/D/Zotero/storage/6A45ZAER/evco_a_00194.pdf:application/pdf},
}

@article{munozGeneratingNewSpaceFilling2019,
	title = {Generating {New} {Space}-{Filling} {Test} {Instances} for {Continuous} {Black}-{Box} {Optimization}},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco_a_00262},
	doi = {10.1162/evco_a_00262},
	abstract = {This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a twodimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Evolutionary Computation},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = jul,
	year = {2019},
	pages = {1--26},
	file = {evco_a_00262.pdf:/Users/bill/D/Zotero/storage/AQAKKYTJ/evco_a_00262.pdf:application/pdf},
}

@incollection{corneOptimisationGeneralisationFootprints2010a,
	address = {Berlin, Heidelberg},
	title = {Optimisation and {Generalisation}: {Footprints} in {Instance} {Space}},
	isbn = {978-3-642-15843-8 978-3-642-15844-5},
	shorttitle = {Optimisation and {Generalisation}},
	url = {http://link.springer.com/10.1007/978-3-642-15844-5_3},
	abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for speciﬁc problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its ‘seen’ instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm ‘footprints’, indicating how performance generalises in instance space. We ﬁnd much evidence that typical ways of choosing the ‘best’ algorithm, via tests over a distribution of instances, are seriously ﬂawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Parallel {Problem} {Solving} from {Nature}, {PPSN} {XI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Corne, David W. and Reynolds, Alan P.},
	editor = {Schaefer, Robert and Cotta, Carlos and Kołodziej, Joanna and Rudolph, Günter},
	year = {2010},
	doi = {10.1007/978-3-642-15844-5_3},
	pages = {22--31},
	file = {footppsn10.pdf:/Users/bill/D/Zotero/storage/6LNTBBCW/footppsn10.pdf:application/pdf},
}

@article{xingengFaceImageModeling2011,
	title = {Face {Image} {Modeling} by {Multilinear} {Subspace} {Analysis} {With} {Missing} {Values}},
	volume = {41},
	issn = {1083-4419, 1941-0492},
	url = {http://ieeexplore.ieee.org/document/5678656/},
	doi = {10.1109/TSMCB.2010.2097588},
	abstract = {Multilinear subspace analysis (MSA) is a promising methodology for pattern-recognition problems due to its ability in decomposing the data formed from the interaction of multiple factors. The MSA requires a large training set, which is well organized in a single tensor, which consists of data samples with all possible combinations of the contributory factors. However, such a “complete” training set is difﬁcult (or impossible) to obtain in many real applications. The missing-value problem is therefore crucial to the practicality of the MSA but has been hardly investigated up to present. To solve the problem, this paper proposes an algorithm named M2SA, which is advantageous in real applications due to the following: 1) it inherits the ability of the MSA to decompose the interlaced semantic factors; 2) it does not depend on any assumptions on the data distribution; and 3) it can deal with a high percentage of missing values. M2SA is evaluated by face image modeling on two typical multifactorial applications, i.e., face recognition and facial age estimation. Experimental results show the effectiveness of M2SA even when the majority of the values in the training tensor are missing.},
	language = {en},
	number = {3},
	urldate = {2020-01-22},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	author = {{Xin Geng} and Smith-Miles, K and {Zhi-Hua Zhou} and {Liang Wang}},
	month = jun,
	year = {2011},
	pages = {881--892},
	file = {geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf:/Users/bill/D/Zotero/storage/NTPEIPIT/geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf:application/pdf},
}

@article{munozReliabilityExploratoryLandscape,
	title = {Reliability of {Exploratory} {Landscape} {Analysis}},
	abstract = {The inherent difﬁculty of solving a black-box optimization problem depends on the characteristics of the problem’s ﬁtness landscape and the algorithm being used. Exploratory Landscape Analysis (ELA) methods can be used to describe the complexities of the problem using numerical features generated via a sampling process of the search space. Despite their success in a number of applications, ELA methods have signiﬁcant limitations typically related with the computational costs associated with generating accurate features. Consequently, only approximate features are available in practice which may be unreliable, leading to systemic errors. The overarching aim of this paper, is to evaluate the reliability of landscape features generated by well-known ELA methods. We describe a comprehensive evaluative framework combining exploratory and statistical validation stages. The results show that particular landscape features are highly volatile. In addition, instances of the same function can have feature values that are signiﬁcantly different. This implies that the ﬁtness landscapes may be statistically anisotropic. Finally, the results show evidence of a curse of the modality, meaning that the sample size should increase with the number of local optima.},
	language = {en},
	author = {Munoz, Mario A and Kirley, Michael and Smith-Miles, Kate},
	pages = {21},
	file = {J_RobustnessAnalysisELA_R5_preprint.pdf:/Users/bill/D/Zotero/storage/Y7N6Z4WR/J_RobustnessAnalysisELA_R5_preprint.pdf:application/pdf},
}

@inproceedings{gunnersenSpecVCMVImprovingCluster2011,
	address = {Melbourne, Vic, Australia},
	title = {{SpecVCMV}: {Improving} cluster visualisation},
	isbn = {978-1-61284-972-0 978-1-61284-969-0 978-1-61284-971-3},
	shorttitle = {{SpecVCMV}},
	url = {http://ieeexplore.ieee.org/document/6119660/},
	doi = {10.1109/IECON.2011.6119660},
	abstract = {This paper proposes a new approach to validating and visualising cluster structure by combining fuzzy membership functions and spectral clustering. By modifying the Visual Cluster Validity algorithm (VCV) to use an external fuzzy membership function as the distance measure and using sum of cluster membership as the sorting function, computational experiments on both the Zelnik-Manor synthetic and UCI real datasets show the proposed method, SpecVCMV, more clearly identifies the underlying cluster structure in the data.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {{IECON} 2011 - 37th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Gunnersen, Sverre and Smith-Miles, Kate and Lee, Vincent},
	month = nov,
	year = {2011},
	keywords = {clustering},
	pages = {2255--2260},
	file = {06119660.pdf:/Users/bill/D/Zotero/storage/CA2YDQYW/06119660.pdf:application/pdf},
}

@inproceedings{wuApproximateBayesianComputation2013,
	address = {Shanghai, China},
	title = {Approximate {Bayesian} computation for estimating rate constants in biochemical reaction systems},
	isbn = {978-1-4799-1309-1},
	url = {http://ieeexplore.ieee.org/document/6732528/},
	doi = {10.1109/BIBM.2013.6732528},
	abstract = {To study the dynamic properties of complex biological systems, mathematical modeling has been used widely in systems biology. Apart from the well-established knowledge for modeling techniques, there are still some difﬁculties while understanding the dynamics in system biology. One of the major challenges is how to infer unknown parameters in mathematical models based on the experimentally observed data sets. This is extremely difﬁcult when the experimental data are sparse and the biological systems are stochastic. To tackle this problem, in this work we revised one computation method for inference called approximate Bayesian computation (ABC) and conducted extensive computing tests to examine the inﬂuence of a number of factors on the performance of ABC. Based on simulation results, we found that the number of stochastic simulations and step size of the observation data have substantial inﬂuence on the estimation accuracy. We applied the ABC method to two stochastic systems to test the efﬁciency and effectiveness of the ABC and obtained promising approximation for the unknown parameters in the systems. This work raised a number of important issues for designing effective inference methods for estimating rate constants in biochemical reaction systems.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {2013 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine}},
	publisher = {IEEE},
	author = {Wu, Qianqian and Smith-Miles, Kate and Tian, Tianhai},
	month = dec,
	year = {2013},
	pages = {416--421},
	file = {06732528.pdf:/Users/bill/D/Zotero/storage/UI3PRRP4/06732528.pdf:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008b,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = dec,
	year = {2008},
	pages = {1--25},
	file = {a6-smith-miles.pdf:/Users/bill/D/Zotero/storage/74UUCAPD/a6-smith-miles.pdf:application/pdf},
}

@inproceedings{phuaAdaptiveCommunalDetection2007,
	address = {San Jose, California},
	title = {Adaptive communal detection in search of adversarial identity crime},
	isbn = {978-1-59593-846-6},
	url = {http://portal.acm.org/citation.cfm?doid=1288552.1288553},
	doi = {10.1145/1288552.1288553},
	abstract = {This paper is on adaptive real-time searching of credit application data streams for identity crime with many search parameters. Specifically, we concentrated on handling our domain-specific adversarial activity problem with the adaptive Communal Analysis Suspicion Scoring (CASS) algorithm. CASS’s main novel theoretical contribution is in the formulation of State-ofAlert (SoA) which sets the condition of reduced, same, or heightened watchfulness; and Parameter-of-Change (PoC) which improves detection ability with pre-defined parameter values for each SoA. With pre-configured SoA policy and PoC strategy, CASS determines when, what, and how much to adapt its search parameters to ongoing adversarial activity. The above approach is validated with three sets of experiments, where each experiment is conducted on several million real credit applications and measured with three appropriate performance metrics. Significant improvements are achieved over previous work, with the discovery of some practical insights of adaptivity into our domain.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceedings of the 2007 international workshop on {Domain} driven data mining  - {DDDM} '07},
	publisher = {ACM Press},
	author = {Phua, Clifton and Lee, Vincent and Smith-Miles, Kate and Gayler, Ross},
	year = {2007},
	pages = {1--10},
	file = {p1-phua.pdf:/Users/bill/D/Zotero/storage/98959G55/p1-phua.pdf:application/pdf},
}

@inproceedings{munozNonparametricModelSpace2017,
	address = {Berlin, Germany},
	title = {Non-parametric model of the space of continuous black-box optimization problems},
	isbn = {978-1-4503-4939-0},
	url = {http://dl.acm.org/citation.cfm?doid=3067695.3075971},
	doi = {10.1145/3067695.3075971},
	abstract = {Exploratory Landscape Analysis are data driven methods used for automated algorithm selection in continuous black-box optimization. Most of these methods follow strong assumptions that limit their characterization power, or loose information by compressing the data into a few scalar features. A more exible approach is to avoid explicit measuring and comparing of speci c structures. In this paper we present a proof-of-concept for a more general method, which produces non-parametric models of the space of problems. Using non-metric multidimensional scaling, we generate synthetic features for each problem, which could replace or complement the existing ones. We demonstrate approaches to produce algorithm recommendations and visual representations of the space. To validate the model, we compare our results with those obtained through existing methods, which show that our models have competitive performance.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion} on   - {GECCO} '17},
	publisher = {ACM Press},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	year = {2017},
	pages = {175--176},
	file = {p175-munoz.pdf:/Users/bill/D/Zotero/storage/AM2E7F7X/p175-munoz.pdf:application/pdf},
}

@article{gengFaceImageModeling,
	title = {Face image modeling by multilinear subspace analysis with missing values},
	abstract = {The main difﬁculty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-n product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difﬁcult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M2SA, which can work on the training tensor with massive missing values. Thus M2SA can be used to model face images even when there are only a small number of face images with limited variations (which will cause missing values in the training tensor). Experiments on face recognition show that M2SA can work reasonably well with up to 70\% missing values in the training tensor.},
	language = {en},
	author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua and Wang, Liang},
	pages = {4},
	file = {p629-geng.pdf:/Users/bill/D/Zotero/storage/8IDFN4PE/p629-geng.pdf:application/pdf},
}

@inproceedings{gengFacialAgeEstimation2008,
	address = {Vancouver, British Columbia, Canada},
	title = {Facial age estimation by nonlinear aging pattern subspace},
	isbn = {978-1-60558-303-7},
	url = {http://portal.acm.org/citation.cfm?doid=1459359.1459469},
	doi = {10.1145/1459359.1459469},
	abstract = {Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are deﬁned as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identiﬁed.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceeding of the 16th {ACM} international conference on {Multimedia} - {MM} '08},
	publisher = {ACM Press},
	author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua},
	year = {2008},
	pages = {721},
	file = {p721-geng.pdf:/Users/bill/D/Zotero/storage/852PU8S6/p721-geng.pdf:application/pdf},
}

@inproceedings{chanEvolvingStellarModels2019,
	address = {Prague, Czech Republic},
	title = {Evolving stellar models to find the origins of our galaxy},
	isbn = {978-1-4503-6111-8},
	url = {http://dl.acm.org/citation.cfm?doid=3321707.3321714},
	doi = {10.1145/3321707.3321714},
	abstract = {A er the Big Bang, it took about 200 million years before the very rst stars would form – now more than 13 billion years ago. Unfortunately, we will not be able to observe these stars directly. Instead, we can observe the ’fossil’ records that these stars have le behind, preserved in the oldest stars of our own galaxy. When the rst stars exploded as supernovae, their ashes were dispersed and the next generation of stars formed, incorporating some of the debris. We can now measure the chemical abundances in those old stars, which is similar to a genetic ngerprint that allows us to identify the parents.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} on   - {GECCO} '19},
	publisher = {ACM Press},
	author = {Chan, Conrad and Aleti, Aldeida and Heger, Alexander and Smith-Miles, Kate},
	year = {2019},
	pages = {1129--1137},
	file = {p1129-chan.pdf:/Users/bill/D/Zotero/storage/4Y3V2JCF/p1129-chan.pdf:application/pdf},
}

@inproceedings{munozGeneratingCustomClassification2017,
	address = {Berlin, Germany},
	title = {Generating custom classification datasets by targeting the instance space},
	isbn = {978-1-4503-4939-0},
	url = {http://dl.acm.org/citation.cfm?doid=3067695.3082532},
	doi = {10.1145/3067695.3082532},
	abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion} on - {GECCO} '17},
	publisher = {ACM Press},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	year = {2017},
	pages = {1582--1588},
	file = {p1582-munoz.pdf:/Users/bill/D/Zotero/storage/LBGZDNQM/p1582-munoz.pdf:application/pdf},
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011,
	address = {Berlin, Heidelberg},
	title = {Generalising {Algorithm} {Performance} in {Instance} {Space}: {A} {Timetabling} {Case} {Study}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	shorttitle = {Generalising {Algorithm} {Performance} in {Instance} {Space}},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_41},
	abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a “footprint” in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the diﬀerences between the instance generation methods, and the suitability of diﬀerent algorithms.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smith-Miles, Kate and Lopes, Leo},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_41},
	pages = {524--538},
	file = {smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/5DRPSJ6D/smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@inproceedings{munozGeneratingCustomClassification2017a,
	address = {Berlin, Germany},
	title = {Generating custom classification datasets by targeting the instance space},
	isbn = {978-1-4503-4939-0},
	url = {http://dl.acm.org/citation.cfm?doid=3067695.3082532},
	doi = {10.1145/3067695.3082532},
	abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion} on - {GECCO} '17},
	publisher = {ACM Press},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	year = {2017},
	pages = {1582--1588},
	file = {W_ClassProblemGenerator.pdf:/Users/bill/D/Zotero/storage/GMQX32QB/W_ClassProblemGenerator.pdf:application/pdf},
}

@article{neumannEvolutionaryDiversityOptimization2018,
	title = {Evolutionary {Diversity} {Optimization} {Using} {Multi}-{Objective} {Indicators}},
	url = {http://arxiv.org/abs/1811.06804},
	abstract = {Evolutionary diversity optimization aims to compute a diverse set of solutions where all solutions meet a given quality criterion. With this paper, we bridge the areas of evolutionary diversity optimization and evolutionary multi-objective optimization. We show how popular indicators frequently used in the area of multi-objective optimization can be used for evolutionary diversity optimization. Our experimental investigations for evolving diverse sets of TSP instances and images according to various features show that two of the most prominent multi-objective indicators, namely the hypervolume indicator and the inverted generational distance, provide excellent results in terms of visualization and various diversity indicators.},
	language = {en},
	urldate = {2020-01-22},
	journal = {arXiv:1811.06804 [cs]},
	author = {Neumann, Aneta and Gao, Wanru and Wagner, Markus and Neumann, Frank},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06804},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {1811.06804.pdf:/Users/bill/D/Zotero/storage/Q5XJ66N7/1811.06804.pdf:application/pdf},
}

@article{kangGRATISGeneRAtingTIme,
	title = {{GRATIS}: {GeneRAting} {TIme} {Series} with diverse and controllable characteristics},
	abstract = {The explosion of time series data in recent years has brought a ﬂourish of new time series analysis methods, for forecasting, clustering, classiﬁcation and other tasks. The evaluation of these new methods requires a diverse collection of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We generate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to eﬃciently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classiﬁcation. We illustrate the usefulness of our time series generation process through a time series forecasting application.},
	language = {en},
	author = {Kang, Yanfei and Hyndman, Rob J and Li, Feng},
	pages = {33},
	file = {1903.02787.pdf:/Users/bill/D/Zotero/storage/7U9P4USC/1903.02787.pdf:application/pdf},
}

@article{lorenaHowComplexYour2019,
	title = {How {Complex} {Is} {Your} {Classification} {Problem}?: {A} {Survey} on {Measuring} {Classification} {Complexity}},
	volume = {52},
	issn = {03600300},
	shorttitle = {How {Complex} {Is} {Your} {Classification} {Problem}?},
	url = {http://dl.acm.org/citation.cfm?doid=3362097.3347711},
	doi = {10.1145/3347711},
	language = {en},
	number = {5},
	urldate = {2020-01-22},
	journal = {ACM Computing Surveys},
	author = {Lorena, Ana C. and Garcia, Luís P. F. and Lehmann, Jens and Souto, Marcilio C. P. and Ho, Tin Kam},
	month = sep,
	year = {2019},
	pages = {1--34},
	file = {a107-lorena.pdf:/Users/bill/D/Zotero/storage/IRP6CHIS/a107-lorena.pdf:application/pdf},
}

@incollection{akgunInstanceGenerationGenerator2019,
	address = {Cham},
	title = {Instance {Generation} via {Generator} {Instances}},
	volume = {11802},
	isbn = {978-3-030-30047-0 978-3-030-30048-7},
	url = {http://link.springer.com/10.1007/978-3-030-30048-7_1},
	abstract = {Access to good benchmark instances is always desirable when developing new algorithms, new constraint models, or when comparing existing ones. Hand-written instances are of limited utility and are timeconsuming to produce. A common method for generating instances is constructing special purpose programs for each class of problems. This can be better than manually producing instances, but developing such instance generators also has drawbacks. In this paper, we present a method for generating graded instances completely automatically starting from a class-level problem speciﬁcation. A graded instance in our present setting is one which is neither too easy nor too diﬃcult for a given solver. We start from an abstract problem speciﬁcation written in the Essence language and provide a system to transform the problem speciﬁcation, via automated type-speciﬁc rewriting rules, into a new abstract speciﬁcation which we call a generator speciﬁcation. The generator speciﬁcation is itself parameterised by a number of integer parameters; these are used to characterise a certain region of the parameter space. The solutions of each such generator instance form valid problem instances. We use the parameter tuner irace to explore the space of possible generator parameters, aiming to ﬁnd parameter values that yield graded instances. We perform an empirical evaluation of our system for ﬁve problem classes from CSPlib, demonstrating promising results.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {Akgün, Özgür and Dang, Nguyen and Miguel, Ian and Salamon, András Z. and Stone, Christopher},
	editor = {Schiex, Thomas and de Givry, Simon},
	year = {2019},
	doi = {10.1007/978-3-030-30048-7_1},
	pages = {3--19},
	file = {akgun et al 2019 - Instance Generation via Generator Instances - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:/Users/bill/D/Zotero/storage/94DVN6ES/akgun et al 2019 - Instance Generation via Generator Instances - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:application/pdf},
}

@article{zabashtaNDSEMethodClassification,
	title = {{NDSE}: {Method} for {Classiﬁcation} {Instance} {Generation} {Given} {Meta}-{Feature} {Description}},
	abstract = {This paper addresses the problem of instance generation. We propose a method called NDSE that generates an instance of binary classiﬁcation problem given its vector description. We solve this problem by reducing it to an optimization task and applying genetic search. We use operators of addition and removal of objects and features to handle crossover and mutation operators over classiﬁcations datasets for genetic algorithm. We conducted several experiments, the results of which show that our method outperforms baseline methods.},
	language = {en},
	author = {Zabashta, Alexey and Filchenkov, Andrey},
	pages = {8},
	file = {e1d129149723eb77faec0f1d60dcf18d18b3.pdf:/Users/bill/D/Zotero/storage/XSC8QPQW/e1d129149723eb77faec0f1d60dcf18d18b3.pdf:application/pdf},
}

@article{eggenspergerEfficientBenchmarkingAlgorithm2018,
	title = {Efficient benchmarking of algorithm configurators via model-based surrogates},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-017-5683-z},
	doi = {10.1007/s10994-017-5683-z},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Machine Learning},
	author = {Eggensperger, Katharina and Lindauer, Marius and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
	month = jan,
	year = {2018},
	keywords = {Leyton-Brown},
	pages = {15--41},
	file = {Eggensperger2018_Article_EfficientBenchmarkingOfAlgorit.pdf:/Users/bill/D/Zotero/storage/ACUTAN5A/Eggensperger2018_Article_EfficientBenchmarkingOfAlgorit.pdf:application/pdf},
}

@article{kordikDiscoveringPredictiveEnsembles2018,
	title = {Discovering predictive ensembles for transfer learning and meta-learning},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-017-5682-0},
	doi = {10.1007/s10994-017-5682-0},
	abstract = {Recent meta-learning approaches are oriented towards algorithm selection, optimization or recommendation of existing algorithms. In this article we show how data-tailored algorithms can be constructed from building blocks on small data sub-samples. Building blocks, typically weak learners, are optimized and evolved into data-tailored hierarchical ensembles. Good-performing algorithms discovered by evolutionary algorithm can be reused on data sets of comparable complexity. Furthermore, these algorithms can be scaled up to model large data sets. We demonstrate how one particular template (simple ensemble of fast sigmoidal regression models) outperforms state-of-the-art approaches on the Airline data set. Evolved hierarchical ensembles can therefore be beneﬁcial as algorithmic building blocks in meta-learning, including meta-learning at scale.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Machine Learning},
	author = {Kordík, Pavel and Černý, Jan and Frýda, Tomáš},
	month = jan,
	year = {2018},
	pages = {177--207},
	file = {kordik et al 2018 - discovering predictive ensembles for transfer learning and meta-learning - KOALA - ENSEMBLE - TRANSFER LEARNING.pdf:/Users/bill/D/Zotero/storage/YPCM7YHJ/kordik et al 2018 - discovering predictive ensembles for transfer learning and meta-learning - KOALA - ENSEMBLE - TRANSFER LEARNING.pdf:application/pdf},
}

@article{lorenaDataComplexityMetafeatures2018,
	title = {Data complexity meta-features for regression problems},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-017-5681-1},
	doi = {10.1007/s10994-017-5681-1},
	abstract = {In meta-learning, classiﬁcation problems can be described by a variety of features, including complexity measures. These measures allow capturing the complexity of the frontier that separates the classes. For regression problems, on the other hand, there is a lack of such type of measures. This paper presents and analyses measures devoted to estimate the complexity of the function that should ﬁtted to the data in regression problems. As case studies, they are employed as meta-features in three meta-learning setups: (i) the ﬁrst one predicts the regression function type of some synthetic datasets; (ii) the second one is designed to tune the parameter values of support vector regressors; and (iii) the third one aims to predict the performance of various regressors for a given dataset. The results show the suitability of the new measures to describe the regression datasets and their utility in the meta-learning tasks considered. In cases (ii) and (iii) the achieved results are also similar or better than those obtained by the use of classical meta-features in meta-learning.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Machine Learning},
	author = {Lorena, Ana C. and Maciel, Aron I. and de Miranda, Péricles B. C. and Costa, Ivan G. and Prudêncio, Ricardo B. C.},
	month = jan,
	year = {2018},
	pages = {209--246},
	file = {lorena et al 2018 - Data complexity meta-features for regression problems - BDPG.pdf:/Users/bill/D/Zotero/storage/JIJPSZU3/lorena et al 2018 - Data complexity meta-features for regression problems - BDPG.pdf:application/pdf},
}

@article{fariaCellularFrustrationAlgorithms2019,
	title = {Cellular frustration algorithms for anomaly detection applications},
	volume = {14},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0218930},
	doi = {10.1371/journal.pone.0218930},
	abstract = {Cellular frustrated models have been developed to describe how the adaptive immune system works. They are composed by independent agents that continuously pair and unpair depending on the information that one sub-set of these agents display. The emergent dynamics is sensitive to changes in the displayed information and can be used to detect anomalies, which can be important to accomplish the immune system main function of protecting the host. Therefore, it has been hypothesized that these models could be adequate to model the immune system activation. Likewise it has been hypothesized that these models could provide inspiration to develop new artificial intelligence algorithms for data mining applications. However, computational algorithms do not need to follow strictly the immunological reality. Here, we investigate efficient implementation strategies of these immune inspired ideas for anomaly detection applications and use real data to compare the performance of cellular frustration algorithms with standard implementations of one-class support vector machines and deep autoencoders. Our results demonstrate that more efficient implementations of cellular frustration algorithms are possible and also that cellular frustration algorithms can be advantageous for semi-supervised anomaly detection applications given their robustness and accuracy.},
	language = {en},
	number = {7},
	urldate = {2020-01-22},
	journal = {PLOS ONE},
	author = {Faria, Bruno and Vistulo de Abreu, Fernao},
	editor = {Zhang, Le},
	month = jul,
	year = {2019},
	pages = {e0218930},
	file = {pone.0218930.pdf:/Users/bill/D/Zotero/storage/FXI9MEFV/pone.0218930.pdf:application/pdf},
}

@article{vanrijnOnlinePerformanceEstimation2018,
	title = {The online performance estimation framework: heterogeneous ensemble learning for data streams},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	shorttitle = {The online performance estimation framework},
	url = {http://link.springer.com/10.1007/s10994-017-5686-9},
	doi = {10.1007/s10994-017-5686-9},
	abstract = {Ensembles of classiﬁers are among the best performing classiﬁers available in many data mining applications, including the mining of data streams. Rather than training one classiﬁer, multiple classiﬁers are trained, and their predictions are combined according to a given voting schedule. An important prerequisite for ensembles to be successful is that the individual models are diverse. One way to vastly increase the diversity among the models is to build an heterogeneous ensemble, comprised of fundamentally different model types. However, most ensembles developed speciﬁcally for the dynamic data stream setting rely on only one type of base-level classiﬁer, most often Hoeffding Trees. We study the use of heterogeneous ensembles for data streams. We introduce the Online Performance Estimation framework, which dynamically weights the votes of individual classiﬁers in an ensemble. Using an internal evaluation on recent training data, it measures how well ensemble members performed on this and dynamically updates their weights. Experiments over a wide range of data streams show performance that is competitive with state of the art ensemble techniques, including Online Bagging and Leveraging Bagging, while being signiﬁcantly faster. All experimental results from this work are easily reproducible and publicly available online.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Machine Learning},
	author = {van Rijn, Jan N. and Holmes, Geoffrey and Pfahringer, Bernhard and Vanschoren, Joaquin},
	month = jan,
	year = {2018},
	pages = {149--176},
	file = {Rijn2018_Article_TheOnlinePerformanceEstimation.pdf:/Users/bill/D/Zotero/storage/7PFJELAA/Rijn2018_Article_TheOnlinePerformanceEstimation.pdf:application/pdf},
}

@article{talagalaFFORMPPFeaturebasedForecast2019,
	title = {{FFORMPP}: {Feature}-based forecast model performance prediction},
	shorttitle = {{FFORMPP}},
	url = {http://arxiv.org/abs/1908.11500},
	abstract = {This paper introduces a novel meta-learning algorithm for time series forecasting. The efﬁcient Bayesian multivariate surface regression approach is used to model forecast error as a function of features calculated from the time series. The minimum predicted forecast error is then used to identify an individual model or combination of models to produce forecasts. In general, the performance of any meta-learner strongly depends on the reference dataset used to train the model. We further examine the feasibility of using GRATIS (a feature-based time series simulation approach) in generating a realistic time series collection to obtain a diverse collection of time series for our reference set. The proposed framework is tested using the M4 competition data and is compared against several benchmarks and other commonly used forecasting approaches. The new approach obtains performance comparable to the second and the third rankings of the M4 competition.},
	language = {en},
	urldate = {2020-01-22},
	journal = {arXiv:1908.11500 [stat]},
	author = {Talagala, Thiyanga S. and Li, Feng and Kang, Yanfei},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.11500},
	keywords = {Statistics - Applications},
	annote = {Comment: 29 pages},
	file = {talagala et al 2019 - FFORMPP - Feature-based forecast model performance prediction.pdf:/Users/bill/D/Zotero/storage/8XW8IVU8/talagala et al 2019 - FFORMPP - Feature-based forecast model performance prediction.pdf:application/pdf},
}

@article{xuAverageAnalysisBacktracking,
	title = {An {Average} {Analysis} of {Backtracking} on {Random} {Constraint} {Satisfaction} {Problems}},
	abstract = {In this paper we propose a random CSP model, called Model GB, which is a natural generalization of standard Model B. This paper considers Model GB in the case where each constraint is easy to satisfy. In this case Model GB exhibits non-trivial behaviour (not trivially satisfiable or unsatisfiable) as the number of variables approaches infinity. A detailed analysis to obtain an asymptotic estimate (good to 1 + o(1) ) of the average number of nodes in a search tree used by the backtracking algorithm on Model GB is also presented. It is shown that the average number of nodes required for finding all solutions or proving that no solution exists grows exponentially with the number of variables. So this model might be an interesting distribution for studying the nature of hard instances and evaluating the performance of CSP algorithms. In addition, we further investigate the behaviour of the average number of nodes as r (the ratio of constraints to variables) varies. The results indicate that as r increases, random CSP instances get easier and easier to solve, and the base for the average number of nodes that is exponential in n tends to 1 as r approaches infinity. Therefore, although the average number of nodes used by the backtracking algorithm on random CSP is exponential, many CSP instances will be very easy to solve when r is sufficiently large.},
	language = {en},
	author = {Xu, Ke and Li, Wei},
	pages = {20},
	file = {amai-final.pdf:/Users/bill/D/Zotero/storage/GE6S7DQ8/amai-final.pdf:application/pdf},
}

@article{xuAverageSimilarityDegree2004,
	title = {On the average similarity degree between solutions of random k-{SAT} and random {CSPs}},
	volume = {136},
	issn = {0166218X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166218X0300204X},
	doi = {10.1016/S0166-218X(03)00204-X},
	abstract = {To study the structure of solutions for random k-SAT and random CSPs, this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other. It is proved that under certain conditions, as r (i.e. the ratio of constraints to variables) increases, the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point, shifting from a smaller value to a larger value abruptly. For random k-SAT this phenomenon will occur when k ≥ 5 . It is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions. Finally, we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Discrete Applied Mathematics},
	author = {Xu, Ke and Li, Wei},
	month = jan,
	year = {2004},
	pages = {125--149},
	file = {dam-final.pdf:/Users/bill/D/Zotero/storage/ERJMQWNP/dam-final.pdf:application/pdf},
}

@inproceedings{zouLogicDistanceBasedMethod2009,
	address = {Honolulu, Hawaii},
	title = {A {Logic} {Distance}-{Based} {Method} for {Deploying} {Probing} {Sources} in the {Topology} {Discovery}},
	isbn = {978-1-4244-4148-8},
	url = {http://ieeexplore.ieee.org/document/5426143/},
	doi = {10.1109/GLOCOM.2009.5426143},
	abstract = {Internet topology plays a vital role in studying network’s internal structure and properties. Currently traceroutebased topology discovery is the main approach to map the network. However, the deployments of probing sources are usually quite costly and complex. Even if the total numbers of sources are the same, the overall coverage of the sampled network may vary signiﬁcantly for different sources. As a result, it is of great importance for a topology discovery project to select a limited set of probing sources to detect more nodes and links. The aim of this paper is to investigate how to select a ﬁxed set of probing sources to maximize the coverage of the sampled network. We propose a novel logic distance-based method to make source placement decisions. Also we evaluate our approach and compare it with other known methods on real network topology and generated topologies.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {{GLOBECOM} 2009 - 2009 {IEEE} {Global} {Telecommunications} {Conference}},
	publisher = {IEEE},
	author = {Zou, Xin and Qiao, Zhongliang and Zhou, Gang and Xu, Ke},
	month = nov,
	year = {2009},
	pages = {1--6},
	file = {GLO09.pdf:/Users/bill/D/Zotero/storage/RVE68D5W/GLO09.pdf:application/pdf},
}

@inproceedings{qiaoTakingTooMany2010,
	address = {Zurich, Switzerland},
	title = {Taking {Too} {Many} {Destinations} {Can} {Be} {Bad} for {Traceroute} {Sampling}},
	isbn = {978-1-4244-7114-0},
	url = {http://ieeexplore.ieee.org/document/5560162/},
	doi = {10.1109/ICCCN.2010.5560162},
	abstract = {Considerable effort has been spent on collecting all the information of routers by using traceroute-like probes in the router-level topology measurements. This method has been argued to introduce uncontrolled sampling biases on statistical properties of the sample graph and heavy load to the network being measured. In order to improve the quality of the maps induced by the method, researchers are starting to investigate the deployment of large-scale distributed systems. But the lack of sources, the additional load introduced to the network and the potential uncontrolled scale in the IPv6 network cast a shadow over this direction. In this paper, we study traceroute sampling to represent the topology. Instead of finding a general strategy that would match all the graph properties, we focus on testing the impact of the proportion of destinations and sources on a single or several properties of the graph. We argue that, in order to obtain a more accurate sample graph, the general method of taking a small set of sources to perform traceroute-like probes to a large set of destinations is very unreasonable. Our results obtained from simulated experiments show that as the proportion of destinations and sources increases, the resulting properties of the sample graph can differ sharply from the underlying graph. The results also show that there is no single perfect proportion answer to meet all the graph properties but the small often perform better than the large overall. When we do the same measurement on several real-world networks, we find strong evidence for sampling bias because of taking so many destinations.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {2010 {Proceedings} of 19th {International} {Conference} on {Computer} {Communications} and {Networks}},
	publisher = {IEEE},
	author = {Qiao, Zhongliang and Chen, Mingming and Xu, Ke},
	month = aug,
	year = {2010},
	pages = {1--6},
	file = {ICCCN2010.pdf:/Users/bill/D/Zotero/storage/WMBEM6K3/ICCCN2010.pdf:application/pdf},
}

@article{zhaoEnhancingRobustnessScalefree2009,
	title = {Enhancing the robustness of scale-free networks},
	volume = {42},
	issn = {1751-8113, 1751-8121},
	url = {http://stacks.iop.org/1751-8121/42/i=19/a=195003?key=crossref.8e4b425d94d786c5c3c4c66b26bdbf94},
	doi = {10.1088/1751-8113/42/19/195003},
	abstract = {Error tolerance and attack vulnerability are two common and important properties of complex networks, which are usually used to evaluate the robustness of a network. Recently, much work has been devoted to determining the network design with optimal robustness. However, little attention has been paid to the problem of how to improve the robustness of existing networks. In this paper, we present a new parameter α, called enforcing parameter, to guide the process of enhancing the robustness of scale-free networks by gradually adding new links. Intuitively, α {\textless} 0 means the nodes with lower degrees are selected preferentially while the nodes with higher degrees will be more probably selected when α {\textgreater} 0. It is shown both theoretically and experimentally that when α {\textless} 0 the attack survivability of the network can be enforced apparently. Then we propose new strategies to enhance the network robustness. Through extensive experiments and comparisons, we conclude that establishing new links between nodes with low degrees can drastically enforce the attack survivability of scale-free networks while having little impact on the error tolerance.},
	language = {en},
	number = {19},
	urldate = {2020-01-22},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Zhao, Jichang and Xu, Ke},
	month = may,
	year = {2009},
	pages = {195003},
	file = {JPA09.pdf:/Users/bill/D/Zotero/storage/P3RUS44Y/JPA09.pdf:application/pdf},
}

@article{zhaoWeakTiesSubtle2010,
	title = {Weak ties: {Subtle} role of information diffusion in online social networks},
	volume = {82},
	issn = {1539-3755, 1550-2376},
	shorttitle = {Weak ties},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.82.016105},
	doi = {10.1103/PhysRevE.82.016105},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Physical Review E},
	author = {Zhao, Jichang and Wu, Junjie and Xu, Ke},
	month = jul,
	year = {2010},
	pages = {016105},
	file = {PRE2010.pdf:/Users/bill/D/Zotero/storage/F3YAGQ59/PRE2010.pdf:application/pdf},
}

@article{xuSATPhaseTransition1999,
	title = {The {SAT} {Phase} {Transition}},
	volume = {42},
	abstract = {Phase transition is an important feature of SAT problem. In this paper, for random k SAT model, we prove that as r (ratio of clauses to variables) increases, the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point ( r = rcr ). This phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other.},
	language = {en},
	number = {5},
	journal = {SCIENCE IN CHINA},
	author = {Xu, Ke and Li, Wei},
	year = {1999},
	pages = {13},
	file = {scichina99.pdf:/Users/bill/D/Zotero/storage/Q4S7XEB6/scichina99.pdf:application/pdf},
}

@article{schapaughMaximizingNewQuantity2014,
	title = {Maximizing a new quantity in sequential reserve selection},
	volume = {41},
	issn = {0376-8929, 1469-4387},
	url = {https://www.cambridge.org/core/product/identifier/S0376892913000544/type/journal_article},
	doi = {10.1017/S0376892913000544},
	abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. Numerous empirical studies support the notion that deﬁning and measuring objectives in terms of species richness (where the value of a site is equal to the number of species it contains, or contributes to an existing reserve network) can be inadequate for maintaining biodiversity in the longterm. An existing site-assessment framework that implicitly maximized the persistence probability of multiple species was integrated with a dynamic optimization model. The problem of sequential reserve selection as a Markov decision process was combined with stochastic dynamic programming to ﬁnd the optimal solution. The approach represents a compromise between representation-based approaches (maximizing occurrences) and more complex tools, like spatially-explicit population models. The method, the inherent problems and interesting conclusions are illustrated with a land acquisition case study on the central Platte River.},
	language = {en},
	number = {2},
	urldate = {2020-01-22},
	journal = {Environmental Conservation},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = jun,
	year = {2014},
	pages = {198--205},
	file = {schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf:/Users/bill/D/Zotero/storage/W7ZNKCNE/schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf:application/pdf},
}

@article{tyreINFERRINGPROCESSPATTERN2001,
	title = {{INFERRING} {PROCESS} {FROM} {PATTERN}: {CAN} {TERRITORY} {OCCUPANCY} {PROVIDE} {INFORMATION} {ABOUT} {LIFE} {HISTORY} {PARAMETERS}?},
	volume = {11},
	abstract = {A signiﬁcant problem in wildlife management is identifying ‘‘good’’ habitat for species within the short time frames demanded by policy makers. Statistical models of the response of species presence/absence to predictor variables are one solution, widely known as habitat modeling. We use a ‘‘virtual ecologist’’ to test logistic regression as a means of developing habitat models within a spatially explicit, individual-based simulation that allows habitat quality to inﬂuence either fecundity or survival with a continuous scale. The basic question is how good are logistic regression models of habitat quality at identifying habitat where birth rates are high and death rates low (i.e., ‘‘source’’ habitat)? We ﬁnd that, even when all the important variables are perfectly measured, and there is no error in surveying the species of interest, demographic stochasticity and the limiting effect of localized dispersal generally prevent an explanation of much more than half of the variation in territory occupancy as a function of habitat quality. This is true regardless of whether fecundity or survival is inﬂuenced by habitat quality. In addition, habitat models only detect a signiﬁcant effect of habitat on territory occupancy when habitat quality is spatially autocorrelated. We ﬁnd that habitat models based on logistic regression really measure the ability of the species to reach and colonize areas, not birth or death rates.},
	language = {en},
	number = {6},
	journal = {Ecological Applications},
	author = {Tyre, Andrew J and Possingham, Hugh P and Lindenmayer, David B},
	year = {2001},
	pages = {16},
	file = {tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf:/Users/bill/D/Zotero/storage/SA4S74X8/tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf:application/pdf},
}

@article{schapaughBayesianNetworksQuest2012,
	title = {Bayesian networks and the quest for reserve adequacy},
	volume = {152},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320712001577},
	doi = {10.1016/j.biocon.2012.03.014},
	abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. We describe a method that integrates correlates of persistence for multiple species into a single currency – site quality. Site quality is, in turn, an explicit measure of performance used in optimization. We develop a Bayesian network to assess site quality, which assigns an expected value to a property based on criteria arrayed into a causal diagram. We then use stochastic dynamic programming to determine whether an organization should acquire or reject a site placed on the public market. Our framework for assessing sites and making land acquisition decisions represents a compromise between the use of generic spatial design criteria and more intensive computational tools, like spatially-explicit population models. There is certainly a loss of precision by using site quality as a surrogate for more direct measures of persistence. However, we believe this simpliﬁcation is defensible when sufﬁcient data, expertise, or other resources are lacking.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Biological Conservation},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = aug,
	year = {2012},
	pages = {178--186},
	file = {Schapaugh and Tyre - 2012 - Bayesian networks and the quest for reserve adequa.pdf:/Users/bill/D/Zotero/storage/9DKID3LU/Schapaugh and Tyre - 2012 - Bayesian networks and the quest for reserve adequa.pdf:application/pdf},
}

@article{schapaughAccountingParametricUncertainty2013,
	title = {Accounting for parametric uncertainty in {Markov} decision processes},
	volume = {254},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380013000306},
	doi = {10.1016/j.ecolmodel.2013.01.003},
	abstract = {Markov decision processes have become the standard tool for modeling sequential decision-making problems in conservation. In many real-world applications, however, it is practically infeasible to accurately parameterize the state transition function. In this study, we introduce a new way of dealing with ambiguity in the state transition function. In contrast to existing methods, we explore the effects of uncertainty at the level of the policy, rather than at the level of decisions within states. We use information-gap decision theory to ask the question of how much uncertainty in the state transition function can be tolerated while still delivering a speciﬁed expected value given by the objective function. Accordingly, the goal of the optimization problem is no longer to maximize expected value, but to maximize local robustness to uncertainty (while still meeting the desired level of performance). We analyze a simple land acquisition problem, using info-gap decision theory to propagate uncertainties and rank alternative policies. Rather than requiring information about the extent of parameter uncertainty at the outset, info-gap addresses the question of how much uncertainty is permissible in the state transition function before the optimal policy would change.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = apr,
	year = {2013},
	pages = {15--21},
	file = {Schapaugh and Tyre - 2013 - Accounting for parametric uncertainty in Markov de.pdf:/Users/bill/D/Zotero/storage/GHSFK8Q8/Schapaugh and Tyre - 2013 - Accounting for parametric uncertainty in Markov de.pdf:application/pdf},
}

@article{burgFindingSmoothestPath2010,
	title = {Finding the {Smoothest} {Path} to {Success}: {Model} {Complexity} and the {Consideration} of {Nonlinear} {Patterns} in {Nest}-{Survival} {Data}},
	volume = {112},
	issn = {0010-5422, 1938-5129},
	shorttitle = {Finding the {Smoothest} {Path} to {Success}},
	url = {https://academic.oup.com/condor/article/112/3/421-431/5152584},
	doi = {10.1525/cond.2010.090053},
	abstract = {Quantifying patterns of nest survival is a ﬁrst step toward understanding why birds decide when and where to breed. Most studies of nest survival have relied on generalized linear models (GLM) to explore these patterns. However, GLMs require assumptions about the models’ structure that might preclude ﬁnding nonlinear patterns in survival data. Generalized additive models (GAM) provide a ﬂexible alternative to GLMs for estimating linear and nonlinear patterns in data. Here we present a comparison of GLMs and GAMs for explaining variation in nest-survival data. We used two different model-selection criteria, the Bayes (BIC) and Akaike (AIC) information criteria, to select among simple and complex models. Our study was focused on the analysis of Redwinged Blackbird (Agelaius phoeniceus) nests in the Rainwater Basin wetlands of south-central Nebraska. Under BIC, our quadratic model of nest age had the most support, and the model predicted a concave pattern of daily nest survival. We found more model-selection uncertainty under AIC and found support for additive models with ordinal effects of both day and age. These models predicted much more temporal variation than did the linear models. Following our analysis, we discuss some of the advantages and disadvantages of GAMs. Despite the possible limitations of GAMs, our results suggest that they provide an efﬁcient and ﬂexible way to demonstrate nonlinear patterns in nest-survival data.},
	language = {en},
	number = {3},
	urldate = {2020-01-22},
	journal = {The Condor},
	author = {Burg, Max Post van der and Powell, Larkin A. and Tyre, Andrew J.},
	month = aug,
	year = {2010},
	pages = {421--431},
	file = {Burg et al. - 2010 - Finding the Smoothest Path to Success Model Compl.pdf:/Users/bill/D/Zotero/storage/6JN4MDV2/Burg et al. - 2010 - Finding the Smoothest Path to Success Model Compl.pdf:application/pdf},
}

@article{hoffmanUseSimulatedData2010,
	title = {Use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence},
	volume = {33},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/j.1600-0587.2009.05495.x},
	doi = {10.1111/j.1600-0587.2009.05495.x},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Ecography},
	author = {Hoffman, Justin D. and Aguilar-Amuchastegui, Naikoa and Tyre, Andrew J.},
	month = apr,
	year = {2010},
	pages = {656--666},
	file = {Hoffman et al. - 2010 - Use of simulated data from a process-based habitat.pdf:/Users/bill/D/Zotero/storage/V4HHETRR/Hoffman et al. - 2010 - Use of simulated data from a process-based habitat.pdf:application/pdf},
}

@article{hefleyFavorableTeamScores2011,
	title = {Favorable {Team} {Scores} {Under} the {Team}-{Based} {Learning} {Paradigm}: {A} {Statistical} {Artifact}?},
	language = {en},
	author = {Hefley, Trevor},
	year = {2011},
	pages = {11},
	file = {Hefley - 2011 - Favorable Team Scores Under the Team-Based Learnin.pdf:/Users/bill/D/Zotero/storage/Z2RKRZT7/Hefley - 2011 - Favorable Team Scores Under the Team-Based Learnin.pdf:application/pdf},
}

@article{postvanderburgRoleBudgetSufficiency2014,
	title = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management: {Robust} {Species} {Management}},
	volume = {78},
	issn = {0022541X},
	shorttitle = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management},
	url = {http://doi.wiley.com/10.1002/jwmg.638},
	doi = {10.1002/jwmg.638},
	abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used ﬁeld data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efﬁcient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufﬁcient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufﬁcient budget. Below this budget, the costefﬁcient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufﬁcient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one’s budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. Ó 2013 The Wildlife Society.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {The Journal of Wildlife Management},
	author = {Post van der Burg, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
	month = jan,
	year = {2014},
	pages = {153--163},
	file = {Post van der Burg et al. - 2014 - On the role of budget sufficiency, cost efficiency.pdf:/Users/bill/D/Zotero/storage/66BU9ZN6/Post van der Burg et al. - 2014 - On the role of budget sufficiency, cost efficiency.pdf:application/pdf},
}

@article{powellTurningStudentsProblem,
	title = {Turning {Students} into {Problem} {Solvers}},
	language = {en},
	author = {Powell, Larkin A and Tyre, Andrew J and Conroy, Michael J and Peterson, James T and Williams, B Ken},
	pages = {4},
	file = {Powell et al. - Turning Students into Problem Solvers.pdf:/Users/bill/D/Zotero/storage/2K3QMFWV/Powell et al. - Turning Students into Problem Solvers.pdf:application/pdf},
}

@article{baaschEvaluationThreeStatistical2010,
	title = {An evaluation of three statistical methods used to model resource selection},
	volume = {221},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380009007194},
	doi = {10.1016/j.ecolmodel.2009.10.033},
	abstract = {The performance of statistical methods for modeling resource selection by animals is difﬁcult to evaluate with ﬁeld data because true selection patterns are unknown. Simulated data based on a known probability distribution, though, can be used to evaluate statistical methods. Models should estimate true selection patterns if they are to be useful in analyzing and interpreting ﬁeld data. We used simulation techniques to evaluate the effectiveness of three statistical methods used in modeling resource selection. We generated 25 use locations per animal and included 10, 20, 40, or 80 animals in samples of use locations. To simulate species of different mobility, we generated use locations at four levels according to a known probability distribution across DeSoto National Wildlife Refuge (DNWR) in eastern Nebraska and western Iowa, USA. We either generated 5 random locations per use location or 10,000 random locations (total) within 4 predetermined areas around use locations to determine how the deﬁnition of availability and the number of random locations affected results. We analyzed simulated data using discrete choice, logisticregression, and a maximum entropy method (Maxent). We used a simple linear regression of estimated and known probability distributions and area under receiver operating characteristic curves (AUC) to evaluate the performance of each method. Each statistical method was affected differently by number of animals and random locations used in analyses, level at which selection of resources occurred, and area considered available. Discrete-choice modeling resulted in precise and accurate estimates of the true probability distribution when the area in which use locations were generated was ≥ the area deﬁned to be available. Logistic-regression models were unbiased and precise when the area in which use locations were generated and the area deﬁned to be available were the same size; the ﬁt of these models improved with increased numbers of random locations. Maxent resulted in unbiased and precise estimates of the known probability distribution when the area in which use locations were generated was small (homerange level) and the area deﬁned to be available was large (study area). Based on AUC analyses, all models estimated the selection distribution better than random chance. Results from AUC analyses, however, often contradicted results of the linear regression method used to evaluate model performance. Discretechoice modeling was best able to estimate the known selection distribution in our study area regardless of sample size or number of random locations used in the analyses, but we recommend further studies using simulated data over different landscapes and different resource metrics to conﬁrm our results. Our study offers an approach and guidance for others interested in assessing the utility of techniques for modeling resource selection in their study area.},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Baasch, David M. and Tyre, Andrew J. and Millspaugh, Joshua J. and Hygnstrom, Scott E. and Vercauteren, Kurt C.},
	month = feb,
	year = {2010},
	pages = {565--574},
	file = {Baasch et al. - 2010 - An evaluation of three statistical methods used to.pdf:/Users/bill/D/Zotero/storage/X9WTIA8I/Baasch et al. - 2010 - An evaluation of three statistical methods used to.pdf:application/pdf},
}

@article{moilanenPlanningRobustReserve2006,
	title = {Planning for robust reserve networks using uncertainty analysis},
	volume = {199},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380006003127},
	doi = {10.1016/j.ecolmodel.2006.07.004},
	abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence–absence in sites, or on species-speciﬁc distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data—erroneous species presence–absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modiﬁcations to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and Ben-Haim, Yakov},
	month = nov,
	year = {2006},
	pages = {115--124},
	file = {Moilanen et al. - 2006 - Planning for robust reserve networks using uncerta.pdf:/Users/bill/D/Zotero/storage/TVWFHK9M/Moilanen et al. - 2006 - Planning for robust reserve networks using uncerta.pdf:application/pdf},
}

@article{hefleyNondetectionSamplingBias2013,
	title = {Nondetection sampling bias in marked presence-only data},
	volume = {3},
	issn = {20457758},
	url = {http://doi.wiley.com/10.1002/ece3.887},
	doi = {10.1002/ece3.887},
	language = {en},
	number = {16},
	urldate = {2020-01-22},
	journal = {Ecology and Evolution},
	author = {Hefley, Trevor J. and Tyre, Andrew J. and Baasch, David M. and Blankenship, Erin E.},
	month = dec,
	year = {2013},
	pages = {5225--5236},
	file = {Hefley et al. - 2013 - Nondetection sampling bias in marked presence-only.pdf:/Users/bill/D/Zotero/storage/JRX5UDBA/Hefley et al. - 2013 - Nondetection sampling bias in marked presence-only.pdf:application/pdf},
}

@article{hefley2014mee,
	title = {Correction of location errors for presence-only species distribution models},
	volume = {5},
	issn = {2041210X},
	url = {http://doi.wiley.com/10.1111/2041-210X.12144},
	doi = {10.1111/2041-210X.12144},
	language = {en},
	number = {3},
	urldate = {2020-01-22},
	journal = {Methods in Ecology and Evolution},
	author = {Hefley, Trevor J. and Baasch, David M. and Tyre, Andrew J. and Blankenship, Erin E.},
	editor = {Warton, David},
	month = mar,
	year = {2014},
	pages = {207--214},
	file = {Hefley et al. - 2014 - Correction of location errors for presence-only sp.pdf:/Users/bill/D/Zotero/storage/FWETV289/Hefley et al. - 2014 - Correction of location errors for presence-only sp.pdf:application/pdf},
}

@article{hefleyFittingPopulationGrowth2013,
	title = {Fitting population growth models in the presence of measurement and detection error},
	volume = {263},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380013002536},
	doi = {10.1016/j.ecolmodel.2013.05.003},
	abstract = {Population time series data from ﬁeld studies are complex and statistical analysis requires models that describe nonlinear population dynamics and observational errors. State-space formulations of stochastic population growth models have been used to account for measurement error caused by the data collection process. Parameter estimation, inference, and prediction are all sensitive to measurement error. The observational process may also result in detection errors and if unaccounted for will result in biased parameter estimates. We developed an N-mixture state-space modeling framework to estimate and correct for errors in detection while estimating population model parameters. We tested our methods using simulated data sets and compared the results to those obtained with state-space models when detection is perfect and when detection is ignored. Our N-mixture state-space model yielded parameter estimates of similar quality to a state-space model when detection is perfect. Our results show that ignoring detection errors can lead to biased parameter estimates including an overestimated growth rate, underestimated equilibrium population size and estimated population state that is misleading. We recommend that researchers consider the possibility of detection errors when collecting and analyzing population time series data.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Hefley, Trevor J. and Tyre, Andrew J. and Blankenship, Erin E.},
	month = aug,
	year = {2013},
	pages = {244--250},
	file = {Hefley et al. - 2013 - Fitting population growth models in the presence o.pdf:/Users/bill/D/Zotero/storage/HJXFCIH5/Hefley et al. - 2013 - Fitting population growth models in the presence o.pdf:application/pdf},
}

@article{tyreIdentifyingLandscapeScale2006,
	title = {Identifying landscape scale patterns from individual scale processes},
	volume = {199},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380006002882},
	doi = {10.1016/j.ecolmodel.2005.12.001},
	abstract = {Extrapolating across scales is a critical problem in ecology. Explicit mechanistic models of ecological systems provide a bridge from measurements of processes at small and short scales to larger scales; spatial patterns at large scales can be used to test the outcomes of these models. However, it is necessary to identify patterns that are not dependent on initial conditions, because small scale initial conditions will not normally be measured at large scales. We examined one possible pattern that could meet these conditions, the relationship between mean and variance in abundance of a parasitic tick in an individual based model of a lizard tick interaction. We scaled discrepancies between the observed and simulated patterns with a transformation of the variance–covariance matrix of the observed pattern to objectively identify patterns that are “close”.},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Michael Bull, C.},
	month = dec,
	year = {2006},
	pages = {442--450},
	file = {Tyre et al. - 2006 - Identifying landscape scale patterns from individu.pdf:/Users/bill/D/Zotero/storage/HB4NDJF8/Tyre et al. - 2006 - Identifying landscape scale patterns from individu.pdf:application/pdf},
}

@article{tyreIdentifyingMechanisticModels2007,
	title = {Identifying mechanistic models of spatial behaviour using pattern-based modelling: {An} example from lizard home ranges},
	volume = {208},
	issn = {03043800},
	shorttitle = {Identifying mechanistic models of spatial behaviour using pattern-based modelling},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380007003109},
	doi = {10.1016/j.ecolmodel.2007.06.004},
	abstract = {Landscape and population level patterns form through the aggregation of responses of individual organisms to heterogeneity. Spatial organization within a population can range from random overlap of individual home ranges, to completely exclusive territories, with most populations falling somewhere between these two extremes. A fundamental question in behavioral ecology concerns the factors that influence the degree of spatial overlap of home ranges, and the processes that determine how likely it is that an individual will access resources over its home range. However, traditional experimental methods are not always practical or possible. Pattern-based modeling is an alternative, non-intrusive technique for explaining observed patterns. We explored behavioral mechanisms for home range overlap in a Scincid lizard, Tiliqua rugosa, by constructing a spatially explicit individual based model. We tested two mechanisms, one that used refuge sites randomly and one that included a behavioral component. The random use model, the fixed total range model, incorporated all refuge sites within a circle of radius h. The behavioral model, the variable total range model, probabilistically incorporated refuge sites based on nearest neighbor distances and use by conspecifics. Comparisons between the simulated patterns and the observed patterns of range overlap provided evidence that the variable total range model was a better approximation of lizard space use than the fixed total range model. Pattern-based modeling showed substantial promise as a means for identifying behavioral mechanisms underlying observed patterns.},
	language = {en},
	number = {2-4},
	urldate = {2020-01-22},
	journal = {Ecological Modelling},
	author = {Tyre, Andrew and Kerr, Gregory D. and Tenhumberg, Brigitte and Bull, C. Michael},
	month = nov,
	year = {2007},
	pages = {307--316},
	file = {Tyre et al. - 2007 - Identifying mechanistic models of spatial behaviou.pdf:/Users/bill/D/Zotero/storage/C3M2G4GB/Tyre et al. - 2007 - Identifying mechanistic models of spatial behaviou.pdf:application/pdf},
}

@article{tyreIMPROVINGPRECISIONREDUCING2003,
	title = {{IMPROVING} {PRECISION} {AND} {REDUCING} {BIAS} {IN} {BIOLOGICAL} {SURVEYS}: {ESTIMATING} {FALSE}-{NEGATIVE} {ERROR} {RATES}},
	volume = {13},
	issn = {1051-0761},
	shorttitle = {{IMPROVING} {PRECISION} {AND} {REDUCING} {BIAS} {IN} {BIOLOGICAL} {SURVEYS}},
	url = {http://doi.wiley.com/10.1890/02-5078},
	doi = {10.1890/02-5078},
	abstract = {The use of presence/absence data in wildlife management and biological surveys is widespread. There is a growing interest in quantifying the sources of error associated with these data. We show that false-negative errors (failure to record a species when in fact it is present) can have a signiﬁcant impact on statistical estimation of habitat models using simulated data. Then we introduce an extension of logistic modeling, the zero-inﬂated binomial (ZIB) model that permits the estimation of the rate of false-negative errors and the correction of estimates of the probability of occurrence for false-negative errors by using repeated visits to the same site. Our simulations show that even relatively low rates of false negatives bias statistical estimates of habitat effects. The method with three repeated visits eliminates the bias, but estimates are relatively imprecise. Six repeated visits improve precision of estimates to levels comparable to that achieved with conventional statistics in the absence of false-negative errors. In general, when error rates are Յ50\% greater efﬁciency is gained by adding more sites, whereas when error rates are Ͼ50\% it is better to increase the number of repeated visits. We highlight the ﬂexibility of the method with three case studies, clearly demonstrating the effect of false-negative errors for a range of commonly used survey methods.},
	language = {en},
	number = {6},
	urldate = {2020-01-22},
	journal = {Ecological Applications},
	author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Field, Scott A. and Niejalke, Darren and Parris, Kirsten and Possingham, Hugh P.},
	month = dec,
	year = {2003},
	pages = {1790--1801},
	file = {Tyre et al. - 2003 - IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICA.pdf:/Users/bill/D/Zotero/storage/IWSTYW2D/Tyre et al. - 2003 - IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICA.pdf:application/pdf},
}

@article{toonenIfLarvaeWere2007,
	title = {If larvae were smart: a simple model for optimal settlement behavior of competent larvae},
	volume = {349},
	issn = {0171-8630, 1616-1599},
	shorttitle = {If larvae were smart},
	url = {http://www.int-res.com/abstracts/meps/v349/p43-61/},
	doi = {10.3354/meps06963},
	abstract = {Much research has been done on larval settlement cues. Rather than having simple fixed responses to constant environmental stimuli, it seems likely that settlement decisions made by individual larvae should vary depending on the individual and the conditions under which it encounters that cue. Here, we present a simple stochastic dynamic programming model that explores the conditions under which larvae may maximize their lifetime fitness by accepting lower quality habitat rather than continuing to search for superior habitat. Our model predicts that there is a relatively narrow range of parameter values over which larval selectivity among habitat types changes dramatically from 1 (larvae accept only optimal substrata) to 0 (indiscriminant settlement). This narrow range coincides with our best estimate of parameter values gleaned from empirical studies, and the model output matches data for the polychaete worm Hydroides dianthus remarkably well. The relative availability of habitats and the total time available to search for high quality habitat (i.e. the ability to delay metamorphosis) had the greatest effects on larval selectivity. In contrast, intuitive factors, including larval energetics and mortality, showed little effect on larval habitat preference, but could still alter the proportion of larvae settling in different habitats by reducing search time. Our model predicts that a given larva may behave differently depending on where it falls in the optimality decision matrix at the instant in which it locates substrata. This model provides a conceptual framework in which to conduct future studies involving variability in settlement decisions among individual larvae, and in which to consider the selective forces driving the evolution of specific larval settlement cues. Our results suggest that a combination of the maximum search period and the relative frequency and quality of optimal habitat likely exert the greatest influence on the evolution of larval selectivity in the field.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Marine Ecology Progress Series},
	author = {Toonen, Rj and Tyre, Aj},
	month = nov,
	year = {2007},
	pages = {43--61},
	file = {Toonen and Tyre - 2007 - If larvae were smart a simple model for optimal s.pdf:/Users/bill/D/Zotero/storage/7S9CG6C8/Toonen and Tyre - 2007 - If larvae were smart a simple model for optimal s.pdf:application/pdf},
}

@article{michaelsHowIndeterminismShapes2012,
	title = {How indeterminism shapes ecologists’ contributions to managing socio-ecological systems: {Indeterminism} in {SES}},
	volume = {5},
	issn = {1755263X},
	shorttitle = {How indeterminism shapes ecologists’ contributions to managing socio-ecological systems},
	url = {http://doi.wiley.com/10.1111/j.1755-263X.2012.00241.x},
	doi = {10.1111/j.1755-263X.2012.00241.x},
	abstract = {To make a difference in policy making about socio-ecological systems, ecologists must grasp when decision makers are amenable to acting on ecological expertise and when they are not. To enable them to do so we present a matrix for classifying a socio-ecological system by the extent of what we don’t know about its natural components and the social interactions that affects them. We use four examples, Midcontinent Mallards, Laysan Ducks, Pallid Sturgeon, and Rocky Mountain Grey Wolves to illustrate how the combination of natural and social source of indeterminism matters. Where social indeterminism is high, ecologists can expand the range of possible science-based options decision makers might consider even while recognizing societal-based concerns rather than science will dominate decision making. In contrast, where natural indeterminism is low, ecologists can offer reasonably accurate predictions that may well serve as inputs into decision making. Depending on the combination of natural and social indeterminism characterizing a particular circumstance, ecologists have different roles to play in informing socio-ecological system management.},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Conservation Letters},
	author = {Michaels, Sarah and Tyre, Andrew J.},
	month = aug,
	year = {2012},
	pages = {289--295},
	file = {Michaels and Tyre - 2012 - How indeterminism shapes ecologists’ contributions.pdf:/Users/bill/D/Zotero/storage/9PA8JWRK/Michaels and Tyre - 2012 - How indeterminism shapes ecologists’ contributions.pdf:application/pdf},
}

@article{fieldMinimizingCostEnvironmental2004,
	title = {Minimizing the cost of environmental management decisions by optimizing statistical thresholds},
	volume = {7},
	issn = {1461-023X, 1461-0248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2004.00625.x},
	doi = {10.1111/j.1461-0248.2004.00625.x},
	abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate (a) of 0.05. We derive optimal a-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on a ¼ 0.05 carries an expected cost over \$5 million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the speciesÕ value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional a-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical Ôburden of proofÕ in environmental decision-making when the cost of Type II errors is relatively high.},
	language = {en},
	number = {8},
	urldate = {2020-01-22},
	journal = {Ecology Letters},
	author = {Field, Scott A. and Tyre, Andrew J. and Jonzen, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
	month = aug,
	year = {2004},
	pages = {669--675},
	file = {Field et al. - 2004 - Minimizing the cost of environmental management de.pdf:/Users/bill/D/Zotero/storage/GMPE7CK5/Field et al. - 2004 - Minimizing the cost of environmental management de.pdf:application/pdf},
}

@article{quinnApplicationDetectabilityUse2011,
	title = {Application of detectability in the use of indicator species: {A} case study with birds},
	volume = {11},
	issn = {1470160X},
	shorttitle = {Application of detectability in the use of indicator species},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1470160X11000562},
	doi = {10.1016/j.ecolind.2011.03.003},
	abstract = {The use of indicator species is popular in ecological monitoring and management. In recent years, new methods to improve the quality and application of indicator data have been proposed and developed. Here we propose the use of detection probability in the selection and application of indicator species. We evaluated environmental and observer factors believed to affect detection of potential species. Observer effects were the most evident factor and may necessitate the greatest consideration in the use of indicator species. Our results call attention to the fact that raw counts are far from accurate and that the use of detection probability can and should be incorporated into sampling protocols, species selection, and the allocation of effort for projects that use indicator species as part of monitoring and management programs.},
	language = {en},
	number = {5},
	urldate = {2020-01-22},
	journal = {Ecological Indicators},
	author = {Quinn, John E. and Brandle, James R. and Johnson, Ron J. and Tyre, Andrew J.},
	month = sep,
	year = {2011},
	pages = {1413--1418},
	file = {Quinn et al. - 2011 - Application of detectability in the use of indicat.pdf:/Users/bill/D/Zotero/storage/3CUY68WZ/Quinn et al. - 2011 - Application of detectability in the use of indicat.pdf:application/pdf},
}

@article{schapaughSimpleMethodDealing2012,
	title = {A simple method for dealing with large state spaces},
	volume = {3},
	issn = {2041210X},
	url = {http://doi.wiley.com/10.1111/j.2041-210X.2012.00242.x},
	doi = {10.1111/j.2041-210X.2012.00242.x},
	abstract = {STATE SPACE Constructing an abstract MDP requires that we identify the state variables that must be retained in the problem. We ﬁrst identify a set of immediately relevant (IR) state variables. This set is formed by examining the reward structure of the problem and selecting the state variables that have the greatest impact on the reward for each state. The larger this set is, the more accurate the abstraction will be. Thus, by varying the size of the IR set, we can examine the balance between the quality of the abstraction and the feasibility of the speciﬁcation. Speciﬁcally, we examine each state variable that appears in the reward function and calculate the maximum range of the reward function for each of its values. In general, state variables with smaller ranges have greater overall eﬀect on reward than variables with larger ranges; these should be retained ﬁrst.},
	language = {en},
	number = {6},
	urldate = {2020-01-22},
	journal = {Methods in Ecology and Evolution},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	editor = {Freckleton, Robert},
	month = dec,
	year = {2012},
	pages = {949--957},
	file = {Schapaugh and Tyre - 2012 - A simple method for dealing with large state space.pdf:/Users/bill/D/Zotero/storage/WYQH3X2S/Schapaugh and Tyre - 2012 - A simple method for dealing with large state space.pdf:application/pdf},
}

@article{beyerem,
	title = {Solving conservation planning problems with integer linear programming (appendices)},
	volume = {328},
	language = {en},
	journal = {Ecological Modelling},
	author = {Beyer, Hawthorne L and Dujardin, Yann and Watts, Matt},
}

@article{runtingReducingRiskReserve2018,
	title = {Reducing risk in reserve selection using {Modern} {Portfolio} {Theory}: {Coastal} planning under sea-level rise},
	volume = {55},
	issn = {00218901},
	shorttitle = {Reducing risk in reserve selection using {Modern} {Portfolio} {Theory}},
	url = {http://doi.wiley.com/10.1111/1365-2664.13190},
	doi = {10.1111/1365-2664.13190},
	language = {en},
	number = {5},
	urldate = {2020-01-22},
	journal = {Journal of Applied Ecology},
	author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
	editor = {Magrach, Ainhoa},
	month = sep,
	year = {2018},
	pages = {2193--2203},
	file = {runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise.pdf:/Users/bill/D/Zotero/storage/PDRS9AGV/runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise.pdf:application/pdf},
}

@article{collenConservationPrioritizationContext2015,
	title = {Conservation prioritization in the context of uncertainty: {Conservation} prioritization in the context of uncertainty},
	volume = {18},
	issn = {13679430},
	shorttitle = {Conservation prioritization in the context of uncertainty},
	url = {http://doi.wiley.com/10.1111/acv.12222},
	doi = {10.1111/acv.12222},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Animal Conservation},
	author = {Collen, B.},
	month = aug,
	year = {2015},
	keywords = {reserve selection, bdpg, uncertainty},
	pages = {315--317},
	file = {collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf:/Users/bill/D/Zotero/storage/GZMWMXTA/collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf:application/pdf},
}

@article{troupinConservationPlanningUncertainty2018,
	title = {Conservation planning under uncertainty in urban development and vegetation dynamics},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0195429},
	doi = {10.1371/journal.pone.0195429},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {PLOS ONE},
	author = {Troupin, David and Carmel, Yohay},
	editor = {Zipp, Katherine},
	month = apr,
	year = {2018},
	pages = {e0195429},
	file = {248Lechner.doc:/Users/bill/D/Zotero/storage/8Z7NY39M/248Lechner.doc:application/msword;file.pdf:/Users/bill/D/Zotero/storage/944RSCEZ/file.pdf:application/pdf},
}

@article{runtingReducingRiskReserve2018a,
	title = {Reducing risk in reserve selection using modern portfolio theory: coastal planning under sea-level rise},
	volume = {55},
	copyright = {2018, The Authors, British Ecological Society},
	issn = {0021-8901, 1365-2664},
	shorttitle = {Reducing risk in reserve selection using modern portfolio theory},
	url = {http://dro.deakin.edu.au/view/DU:30113992},
	doi = {10.1111/1365-2664.13190},
	language = {eng},
	number = {5},
	urldate = {2020-01-22},
	journal = {Journal of applied ecology},
	author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
	month = sep,
	year = {2018},
	pages = {2193--2203},
	file = {Submitted Version:/Users/bill/D/Zotero/storage/KRMFAQGW/Runting et al. - 2018 - Reducing risk in reserve selection using modern po.pdf:application/pdf},
}

@article{meirDoesConservationPlanning2004,
	title = {Does conservation planning matter in a dynamic and uncertain world?},
	volume = {7},
	issn = {1461-023X, 1461-0248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2004.00624.x},
	doi = {10.1111/j.1461-0248.2004.00624.x},
	abstract = {Loss of biodiversity is one of the world’s overriding environmental challenges. Reducing those losses by creating reserve networks is a cornerstone of global conservation and resource management. Historically, assembly of reserve networks has been ad hoc, but recently the focus has shifted to identifying optimal reserve networks. We show that while comprehensive reserve network design is best when the entire network can be implemented immediately, when conservation investments must be staged over years, such solutions actually may be sub-optimal in the context of biodiversity loss and uncertainty. Simple decision rules, such as protecting the available site with the highest irreplaceability or with the highest species richness, may be more effective when implementation occurs over many years.},
	language = {en},
	number = {8},
	urldate = {2020-01-22},
	journal = {Ecology Letters},
	author = {Meir, Eli and Andelman, Sandy and Possingham, Hugh P.},
	month = aug,
	year = {2004},
	pages = {615--622},
	file = {ele6241.pdf:/Users/bill/D/Zotero/storage/9XHKRMNB/ele6241.pdf:application/pdf},
}

@article{hermosoUncertaintyCoarseConservation2012,
	title = {Uncertainty in coarse conservation assessments hinders the efficient achievement of conservation goals},
	volume = {147},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632071200033X},
	doi = {10.1016/j.biocon.2012.01.020},
	abstract = {Conservation planning is sensitive to a number of scale-related issues, such as the spatial extent of the planning area, or the size of units of planning. An extensive literature has reported a decline in efficiency of conservation outputs when planning at small spatial scales or when using large planning units. However, other key issues remain, such as the grain size used to represent the spatial distribution of conservation features. Here, we evaluate the effect of grain size of species distribution data vs. size of planning units on a set of performance measures describing efficiency (ratio of area where species are represented/ total area needed), rate of commission errors (species erroneously expected to occur), representativeness (proportion of species achieving the target) and a novel measure of overall conservation uncertainty (integrating commission errors and uncertainty in the actual locations where species occur). We compared priority areas for the conservation of freshwater fish in the Daly River basin (northern Australia). Our study demonstrates that the effect of grain size of species distribution data was more important than planning unit size on conservation planning performance, with an increase in commission errors up to 80\% and conservation uncertainty over 90\% when coarse data were used. This was more pronounced for rare than common species, where the mismatch between coarse representations of biodiversity patterns and the smaller areas of actual occupancy of species was more evident. Special attention should be paid to the high risk of misallocation of limited budgets when planning in heterogeneous or disturbed environments, where biodiversity is patchily distributed, or when planning for conservation of rare species.},
	language = {en},
	number = {1},
	urldate = {2020-01-22},
	journal = {Biological Conservation},
	author = {Hermoso, Virgilio and Kennard, Mark J.},
	month = mar,
	year = {2012},
	pages = {52--59},
	file = {3.3.2_hermoso_kennard_apr_2012_bc_pre-print_version.pdf:/Users/bill/D/Zotero/storage/HNDFX6I9/3.3.2_hermoso_kennard_apr_2012_bc_pre-print_version.pdf:application/pdf},
}

@article{fourcadePaintingsPredictDistribution2018,
	title = {Paintings predict the distribution of species, or the challenge of selecting environmental predictors and evaluation statistics},
	volume = {27},
	issn = {1466822X},
	url = {http://doi.wiley.com/10.1111/geb.12684},
	doi = {10.1111/geb.12684},
	abstract = {Aim: Species distribution modelling, a family of statistical methods that predicts species distributions from a set of occurrences and environmental predictors, is now routinely applied in many macroecological studies. However, the reliability of evaluation metrics usually employed to validate these models remains questioned. Moreover, the emergence of online databases of environmental variables with global coverage, especially climatic, has favoured the use of the same set of standard predictors. Unfortunately, the selection of variables is too rarely based on a careful examination of the species’ ecology. In this context, our aim was to highlight the importance of selecting ad hoc variables in species distribution models, and to assess the ability of classical evaluation statistics to identify models with no biological realism.},
	language = {en},
	number = {2},
	urldate = {2020-01-22},
	journal = {Global Ecology and Biogeography},
	author = {Fourcade, Yoan and Besnard, Aurélien G. and Secondi, Jean},
	month = feb,
	year = {2018},
	pages = {245--256},
	file = {fourcade et al 2017 - Paintings predict the distribution of species, or the challenge of selecting environmental predictors and evaluation statistics - MATT WHITE - SDM.pdf:/Users/bill/D/Zotero/storage/Z3KYAX9K/fourcade et al 2017 - Paintings predict the distribution of species, or the challenge of selecting environmental predictors and evaluation statistics - MATT WHITE - SDM.pdf:application/pdf},
}

@article{gerberBiodiversityMeasuresBased2011,
	title = {Biodiversity measures based on species-level dissimilarities: {A} methodology for assessment},
	volume = {70},
	issn = {09218009},
	shorttitle = {Biodiversity measures based on species-level dissimilarities},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921800911003466},
	doi = {10.1016/j.ecolecon.2011.08.013},
	abstract = {Biodiversity is widely recognized as a valuable natural asset to conserve. Yet biodiversity is often reported to be declining worldwide. Biodiversity measures can help evaluating it and conserving it, but need to be clearly deﬁned and assessed. In this paper, I review several biodiversity measures and develop a new one, all based on a matrix of species-level dissimilarity data. The data can be used in its raw form, regardless of its origin (e.g. studies of morphological traits, DNA hybridization experiments…) or of any graphical representation. Then, I propose a two-step assessment of the measures. First, I assess them in terms of their deviation from a strict additive law determining the contribution of each species to the diversity of the set in an ideal setting. This setting refers to a case where the data exactly determines the hierarchical ordering of the species. Second, I assess the measures based on their compliance with a list of axioms. These axioms reﬂect basic mathematical properties regarded as desirable for diversity measures, such as their monotonicity in species and dissimilarities. Finally, I show the importance of applying the new quantitative assessment and the axiomatic approach together when selecting a dissimilarity-based diversity measure.},
	language = {en},
	number = {12},
	urldate = {2020-01-24},
	journal = {Ecological Economics},
	author = {Gerber, Nicolas},
	month = oct,
	year = {2011},
	pages = {2275--2281},
	file = {gerber 2011 - Biodiversity measures based on species-level dissimilarities - A methodology for assessment.pdf:/Users/bill/D/Zotero/storage/IPB3H8WM/gerber 2011 - Biodiversity measures based on species-level dissimilarities - A methodology for assessment.pdf:application/pdf},
}

@article{liouiOptimalBenchmarkingActive2013,
	title = {Optimal benchmarking for active portfolio managers},
	volume = {226},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221712008089},
	doi = {10.1016/j.ejor.2012.10.043},
	abstract = {Within an agency theoretic framework adapted to the portfolio delegation issue, we show how to construct optimal benchmarks. In accordance with US regulations, the benchmark-adjusted compensation scheme is taken to be symmetric. The investor’s control consists in forcing the manager to adopt the appropriate benchmark so that his ﬁrst-best optimum is attained. Solving simultaneously the manager’s and the investor’s dynamic optimization programs in a fairly general framework, we characterize the optimal benchmark. We then provide completely explicit solutions when the investor’s and the manager’s utility functions exhibit different CRRA parameters. We ﬁnd that, even under optimal benchmarking, it is never optimal for the manager, and therefore for the investor, to follow exactly the benchmark, except in a very restrictive case. We ﬁnally assess by simulation the practical importance, in particular in terms of the investor’s welfare, of selecting a sub-optimal benchmark.},
	language = {en},
	number = {2},
	urldate = {2020-01-24},
	journal = {European Journal of Operational Research},
	author = {Lioui, Abraham and Poncet, Patrice},
	month = apr,
	year = {2013},
	pages = {268--276},
	file = {lioui poncet 2013 - Optimal benchmarking for active portfolio managers - FINANCE - BENCHMARKING.pdf:/Users/bill/D/Zotero/storage/KFLKPSPW/lioui poncet 2013 - Optimal benchmarking for active portfolio managers - FINANCE - BENCHMARKING.pdf:application/pdf},
}

@article{billionnetPhylogeneticConservationPrioritization2018,
	title = {Phylogenetic conservation prioritization with uncertainty},
	volume = {27},
	issn = {0960-3115, 1572-9710},
	url = {http://link.springer.com/10.1007/s10531-018-1593-z},
	doi = {10.1007/s10531-018-1593-z},
	abstract = {We consider a set of species S and are interested in the assessment of the subsets of S from a phylogenetic diversity viewpoint. Several measures can be used for this assessment. Here we have retained phylogenetic diversity (PD) in the sense of Faith, a measure widely used to reflect the evolutionary history accumulated by a group of species. The PD of a group of species X included in S is easy to calculate when the phylogenetic tree associated with S is perfectly known but this situation is rarely verified. We are interested here in cases where uncertainty regarding the length of branches and the topology of the tree is reflected in the fact that several phylogenetic trees are considered to be plausible for the set S. We propose several measures of the phylogenetic diversity to take account of the uncertainty arising from this situation. A natural problem in the field of biological conservation is to select the best subset of species to protect from a group of threatened species. Here, the best subset is the one that optimizes the proposed measures. We show how to solve these optimal selection problems by integer linear programming. The approach is illustrated by several examples.},
	language = {en},
	number = {12},
	urldate = {2020-01-24},
	journal = {Biodiversity and Conservation},
	author = {Billionnet, Alain},
	month = oct,
	year = {2018},
	pages = {3137--3153},
	file = {Billionnet 2018_Article_PhylogeneticConservationPriori.pdf:/Users/bill/D/Zotero/storage/SVCZLNA2/Billionnet 2018_Article_PhylogeneticConservationPriori.pdf:application/pdf},
}

@article{billionnetMathematicalOptimizationIdeas2013,
	title = {Mathematical optimization ideas for biodiversity conservation},
	volume = {231},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221713002531},
	doi = {10.1016/j.ejor.2013.03.025},
	abstract = {Several major environmental issues like biodiversity loss and climate change currently concern the international community. These topics that are related to the development of human societies have become increasingly important since the United Nations Conference on Environment and Development (UNCED) or Earth Summit in Rio de Janeiro in 1992. In this article, we are interested in the ﬁrst issue. We present here many examples of the help that using mathematical programming can provide to decision-makers in the protection of biodiversity. The examples we have chosen concern the selection of nature reserves, the control of adverse effects caused by landscape fragmentation, including the creation or restoration of biological corridors, the ecological exploitation of forests, the control of invasive species, and the maintenance of genetic diversity. Most of the presented models are – or can be approximated with – linear-, quadratic- or fractional-integer formulations and emphasize spatial aspects of conservation planning. Many of them represent decisions taken in a static context but temporal dimension is also considered. The problems presented are generally difﬁcult combinatorial optimization problems, some are well solved and others less well. Research is still needed to progress in solving them in order to deal with real instances satisfactorily. Moreover, relations between researchers and practitioners have to be strengthened. Furthermore, many recent achievements in the ﬁeld of robust optimization could probably be successfully used for biodiversity protection, a domain in which many data are uncertain.},
	language = {en},
	number = {3},
	urldate = {2020-01-24},
	journal = {European Journal of Operational Research},
	author = {Billionnet, Alain},
	month = dec,
	year = {2013},
	keywords = {reserve selection, bdpg, ILP},
	pages = {514--534},
	file = {billionnet 2013 - Mathematical optimization ideas for biodiversity conservation - BDPG.pdf:/Users/bill/D/Zotero/storage/PRZKSG8U/billionnet 2013 - Mathematical optimization ideas for biodiversity conservation - BDPG.pdf:application/pdf},
}

@book{corryModernAlgebraRise2004,
	address = {Basel ; Boston},
	edition = {2nd rev. ed},
	title = {Modern algebra and the rise of mathematical structures},
	isbn = {978-3-7643-7002-2},
	language = {en},
	publisher = {Birkhäuser Verlag},
	author = {Corry, Leo},
	year = {2004},
	keywords = {Algebra, Categories (Mathematics), History},
	file = {corry - Modern Algebra and the Rise of Mathematical Structures - 2nd ed - CREATIVITY - ABSTRACT STRATEGIES.pdf:/Users/bill/D/Zotero/storage/A54TSTYT/corry - Modern Algebra and the Rise of Mathematical Structures - 2nd ed - CREATIVITY - ABSTRACT STRATEGIES.pdf:application/pdf},
}

@article{dochertyCaseStructuringDiscussion1999,
	title = {The case for structuring the discussion of scientific papers},
	volume = {318},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.318.7193.1224},
	doi = {10.1136/bmj.318.7193.1224},
	language = {en},
	number = {7193},
	urldate = {2020-01-24},
	journal = {BMJ},
	author = {Docherty, M. and Smith, R.},
	month = may,
	year = {1999},
	pages = {1224--1225},
	file = {The_case_for_structuring_discussion_of_scientific_.pdf:/Users/bill/D/Zotero/storage/DXT6S9G8/The_case_for_structuring_discussion_of_scientific_.pdf:application/pdf},
}

@article{blandSTATISTICALMETHODSASSESSING,
	title = {{STATISTICAL} {METHODS} {FOR} {ASSESSING} {AGREEMENT} {BETWEEN} {TWO} {METHODS} {OF} {CLINICAL} {MEASUREMENT}},
	abstract = {In clinical measurement comparison of a new measurement technique with an established one is often needed to see whether they agree sufficiently for the new to replace the old. Such investigations are often analysed inappropriately, notably by using correlation coefficients. The use of correlation is misleading. An alternative approach, based on graphical techniques and simple calculations, is described, together with the relation between this analysis and the assessment of repeatability.},
	language = {en},
	author = {Bland, J Martin and Altman, Douglas G},
	pages = {9},
	file = {Statistical methods for assessing agreement.pdf:/Users/bill/D/Zotero/storage/WFNPVMS8/Statistical methods for assessing agreement.pdf:application/pdf},
}

@article{eatonSpatialConservationPlanning2019,
	title = {Spatial conservation planning under uncertainty: adapting to climate change risks using modern portfolio theory},
	volume = {29},
	issn = {1051-0761, 1939-5582},
	shorttitle = {Spatial conservation planning under uncertainty},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/eap.1962},
	doi = {10.1002/eap.1962},
	abstract = {Climate change and urban growth impact habitats, species, and ecosystem services. To buffer against global change, an established adaptation strategy is designing protected areas to increase representation and complementarity of biodiversity features. Uncertainty regarding the scale and magnitude of landscape change complicates reserve planning and exposes decision makers to the risk of failing to meet conservation goals. Conservation planning tends to treat risk as an absolute measure, ignoring the context of the management problem and risk preferences of stakeholders. Application of risk management theory to conservation emphasizes the diversification of a portfolio of assets, with the goal of reducing the impact of system volatility on investment return. We use principles of Modern Portfolio Theory (MPT), which quantifies risk as the variance and correlation among assets, to formalize diversification as an explicit strategy for managing risk in climate-driven reserve design. We extend MPT to specify a framework that evaluates multiple conservation objectives, allows decision makers to balance management benefits and risk when preferences are contested or unknown, and includes additional decision options such as parcel divestment when evaluating candidate reserve designs. We apply an efficient search algorithm that optimizes portfolio design for large conservation problems and a game theoretic approach to evaluate portfolio trade-offs that satisfy decision makers with divergent benefit and risk tolerances, or when a single decision maker cannot resolve their own preferences. Evaluating several risk profiles for a case study in South Carolina, our results suggest that a reserve design may be somewhat robust to differences in risk attitude but that budgets will likely be important determinants of conservation planning strategies, particularly when divestment is considered a viable alternative. We identify a possible fiscal threshold where adequate resources allow protecting a sufficiently diverse portfolio of habitats such that the risk of failing to achieve conservation objectives is considerably lower. For a range of sea-level rise projections, conversion of habitat to open water (14–180\%) and wetland loss (1–7\%) are unable to be compensated under the current protected network. In contrast, optimal reserve design outcomes are predicted to ameliorate expected losses relative to current and future habitat protected under the existing conservation estate.},
	language = {en},
	number = {7},
	urldate = {2020-01-24},
	journal = {Ecological Applications},
	author = {Eaton, Mitchell J. and Yurek, Simeon and Haider, Zulqarnain and Martin, Julien and Johnson, Fred A. and Udell, Bradley J. and Charkhgard, Hadi and Kwon, Changhyun},
	month = oct,
	year = {2019},
	file = {eap1962-sup-0001-appendixs1.pdf:/Users/bill/D/Zotero/storage/E8LS9LPE/eap1962-sup-0001-appendixs1.pdf:application/pdf;eap1962-sup-0002-datas1.zip:/Users/bill/D/Zotero/storage/CBJM2RG5/eap1962-sup-0002-datas1.zip:application/zip;eap1962-sup-0003-metadatas1.pdf:/Users/bill/D/Zotero/storage/WYNGGNF7/eap1962-sup-0003-metadatas1.pdf:application/pdf;eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG.pdf:/Users/bill/D/Zotero/storage/T37IVXAS/eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG.pdf:application/pdf},
}

@article{zhangConsensusForecastingSpecies2015,
	title = {Consensus {Forecasting} of {Species} {Distributions}: {The} {Effects} of {Niche} {Model} {Performance} and {Niche} {Properties}},
	volume = {10},
	issn = {1932-6203},
	shorttitle = {Consensus {Forecasting} of {Species} {Distributions}},
	url = {https://dx.plos.org/10.1371/journal.pone.0120056},
	doi = {10.1371/journal.pone.0120056},
	abstract = {Ensemble forecasting is advocated as a way of reducing uncertainty in species distribution modeling (SDM). This is because it is expected to balance accuracy and robustness of SDM models. However, there are little available data regarding the spatial similarity of the combined distribution maps generated by different consensus approaches. Here, using eight niche-based models, nine split-sample calibration bouts (or nine random model-training subsets), and nine climate change scenarios, the distributions of 32 forest tree species in China were simulated under current and future climate conditions. The forecasting ensembles were combined to determine final consensual prediction maps for target species using three simple consensus approaches (average, frequency, and median [PCA]). Species’ geographic ranges changed (area change and shifting distance) in response to climate change, but the three consensual projections did not differ significantly with respect to how much or in which direction, but they did differ with respect to the spatial similarity of the three consensual predictions. Incongruent areas were observed primarily at the edges of species’ ranges. Multiple stepwise regression models showed the three factors (niche marginality and specialization, and niche model accuracy) to be related to the observed variations in consensual prediction maps among consensus approaches. Spatial correspondence among prediction maps was the highest when niche model accuracy was high and marginality and specialization were low. The difference in spatial predictions suggested that more attention should be paid to the range of spatial uncertainty before any decisions regarding specialist species can be made based on map outputs. The niche properties and singlemodel predictive performance provide promising insights that may further understanding of uncertainties in SDM.},
	language = {en},
	number = {3},
	urldate = {2020-01-24},
	journal = {PLOS ONE},
	author = {Zhang, Lei and Liu, Shirong and Sun, Pengsen and Wang, Tongli and Wang, Guangyu and Zhang, Xudong and Wang, Linlin},
	editor = {Lötters, Stefan},
	month = mar,
	year = {2015},
	keywords = {sdm, uncertainty, ensembles, spatial similarity},
	pages = {e0120056},
	file = {zhang et al 2015 - Consensus Forecasting of Species Distributions -  The Effects of Niche Model Performance and Niche Properties - SDMs - ENSEMBLE - SPATIAL SIMILARITY - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/FHM5G2UG/zhang et al 2015 - Consensus Forecasting of Species Distributions -  The Effects of Niche Model Performance and Niche Properties - SDMs - ENSEMBLE - SPATIAL SIMILARITY - UNCERTAINTY.pdf:application/pdf},
}

@article{cheokSympathyDevilDetailing2016,
	title = {Sympathy for the {Devil}: {Detailing} the {Effects} of {Planning}-{Unit} {Size}, {Thematic} {Resolution} of {Reef} {Classes}, and {Socioeconomic} {Costs} on {Spatial} {Priorities} for {Marine} {Conservation}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Sympathy for the {Devil}},
	url = {https://dx.plos.org/10.1371/journal.pone.0164869},
	doi = {10.1371/journal.pone.0164869},
	abstract = {Spatial data characteristics have the potential to influence various aspects of prioritising biodiversity areas for systematic conservation planning. There has been some exploration of the combined effects of size of planning units and level of classification of physical environments on the pattern and extent of priority areas. However, these data characteristics have yet to be explicitly investigated in terms of their interaction with different socioeconomic cost data during the spatial prioritisation process. We quantify the individual and interacting effects of three factors—planning-unit size, thematic resolution of reef classes, and spatial variability of socioeconomic costs—on spatial priorities for marine conservation, in typical marine planning exercises that use reef classification maps as a proxy for biodiversity. We assess these factors by creating 20 unique prioritisation scenarios involving combinations of different levels of each factor. Because output data from these scenarios are analogous to ecological data, we applied ecological statistics to determine spatial similarities between reserve designs. All three factors influenced prioritisations to different extents, with cost variability having the largest influence, followed by planning-unit size and thematic resolution of reef classes. The effect of thematic resolution on spatial design depended on the variability of cost data used. In terms of incidental representation of conservation objectives derived from finer-resolution data, scenarios prioritised with uniform cost outperformed those prioritised with variable cost. Following our analyses, we make recommendations to help maximise the spatial and cost efficiency and potential effectiveness of future marine conservation plans in similar planning scenarios. We recommend that planners: employ the smallest planning-unit size practical; invest in data at the highest possible resolution; and, when planning across regional extents with the intention of incidentally representing fine-resolution features, prioritise the whole region with uniform costs rather than using coarse-resolution data on variable costs.},
	language = {en},
	number = {11},
	urldate = {2020-01-24},
	journal = {PLOS ONE},
	author = {Cheok, Jessica and Pressey, Robert L. and Weeks, Rebecca and Andréfouët, Serge and Moloney, James},
	editor = {Baldwin, Robert F.},
	month = nov,
	year = {2016},
	keywords = {marxan, reserve selection, bdpg, uncertainty, spatial similarity, scale, cost, planning unit size},
	pages = {e0164869},
	file = {cheok pressey et al 2016 - Sympathy for the Devil - Detailing the Effects of Planning-Unit Size, Thematic Resolution of Reef Classes, and Socioeconomic Costs on Spatial Priorities for Marine Conservation - RESERVE SELECTION - SPATIAL SIMILARITY.PDF:/Users/bill/D/Zotero/storage/ZT3GKMGJ/cheok pressey et al 2016 - Sympathy for the Devil - Detailing the Effects of Planning-Unit Size, Thematic Resolution of Reef Classes, and Socioeconomic Costs on Spatial Priorities for Marine Conservation - RE.PDF:application/pdf},
}

@article{lahtinenPortfolioDecisionAnalysis2017,
	title = {Portfolio decision analysis methods in environmental decision making},
	volume = {94},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136481521730364X},
	doi = {10.1016/j.envsoft.2017.04.001},
	abstract = {Environmental modellers recurrently work with decisions where a portfolio of actions has to be formed to effectively address the overall situation at hand. When creating the portfolio, one needs to consider multiple objectives and constraints, identify promising action candidates and examine interactions among them. The area of portfolio decision analysis deals with such tasks. This paper reviews portfolio modelling approaches and software that are applicable in environmental management. A framework for environmental portfolio decision analysis is provided that consists of steps ranging from problem framing to modelling and optimization, as well as to the analysis of results. The use of this framework is demonstrated with an illustrative case describing planning of urban water services. The problem is analyzed with a recently introduced portfolio decision analysis method called Robust Portfolio Modelling, which enables the use of incomplete preference information and consequence data. This feature can be particularly useful in environmental applications.},
	language = {en},
	urldate = {2020-01-25},
	journal = {Environmental Modelling \& Software},
	author = {Lahtinen, Tuomas J. and Hämäläinen, Raimo P. and Liesiö, Juuso},
	month = aug,
	year = {2017},
	keywords = {bdpg, portfolio analysis},
	pages = {73--86},
	file = {lahtinen et al 2017 - Portfolio decision analysis methods in environmental decision making - BDPG - PORTFOLIO ANALYSIS.pdf:/Users/bill/D/Zotero/storage/XNUAM4MU/lahtinen et al 2017 - Portfolio decision analysis methods in environmental decision making - BDPG - PORTFOLIO ANALYSIS.pdf:application/pdf},
}

@article{whiteMeasuringAccuracySpecies,
	title = {Measuring the accuracy of species distribution models: a review},
	abstract = {Species distribution models (SDMs) are empirical models relating species occurrence to environmental variables based on statistical or other response surfaces. Species distribution modeling can be used as a tool to solve many theoretical and applied ecological and environmental problems, which include testing biogeographical, ecological and evolutionary hypotheses, assessing species invasion and climate change impact, and supporting conservation planning and reserve selection. The utility of SDM in real world applications requires the knowledge of the model’s accuracy. The accuracy of a model includes two aspects: discrimination capacity and reliability. The former is the power of the model to differentiate presences from absences; and the latter refers to the capability of the predicted probabilities to reflect the observed proportion of sites occupied by the subject species.},
	language = {en},
	author = {White, M and Newell, G},
	keywords = {sdm, guppy, EFs, metrics, accuracy, EF EFs, calibration},
	pages = {7},
	file = {liu white newell 2009 - Measuring the accuracy of species distribution models - a review - GUPPY - SDMs - EF EFs.pdf:/Users/bill/D/Zotero/storage/G6JQZM5K/liu white newell 2009 - Measuring the accuracy of species distribution models - a review - GUPPY - SDMs - EF EFs.pdf:application/pdf},
}

@article{liuMeasuringComparingAccuracy2011,
	title = {Measuring and comparing the accuracy of species distribution models with presence-absence data},
	volume = {34},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/j.1600-0587.2010.06354.x},
	doi = {10.1111/j.1600-0587.2010.06354.x},
	language = {en},
	number = {2},
	urldate = {2020-01-27},
	journal = {Ecography},
	author = {Liu, Canran and White, Matt and Newell, Graeme},
	month = apr,
	year = {2011},
	keywords = {sdm, guppy, accuracy, EF EFs, EF},
	pages = {232--243},
	file = {liu white newell 2011 - Measuring and comparing the accuracy of species distribution models with presence absence data - GUPPY SDM EVALUATION.pdf:/Users/bill/D/Zotero/storage/NL2QUW7P/liu white newell 2011 - Measuring and comparing the accuracy of species distribution models with presence absence data - GUPPY SDM EVALUATION.pdf:application/pdf},
}

@article{neelyPerformanceMeasurementSystem,
	title = {Performance measurement system design: {A} literature review and research agenda},
	language = {en},
	author = {Neely, Andy and Gregory, Mike and Platts, Ken},
	keywords = {EFs, EF EFs},
	pages = {36},
	file = {performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf:/Users/bill/D/Zotero/storage/ITDIVIAM/performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf:application/pdf},
}

@article{cookeSocialContextRole2012,
	title = {Social context and the role of collaborative policy making for private land conservation},
	volume = {55},
	issn = {0964-0568, 1360-0559},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09640568.2011.608549},
	doi = {10.1080/09640568.2011.608549},
	language = {en},
	number = {4},
	urldate = {2020-11-03},
	journal = {Journal of Environmental Planning and Management},
	author = {Cooke, Benjamin and Langford, William T. and Gordon, Ascelin and Bekessy, Sarah},
	month = may,
	year = {2012},
	pages = {469--485},
	file = {cooke langford et al 2012 - Social context and the role of collaborative policy making for private land conservation.pdf:/Users/bill/D/Zotero/storage/ZPN836E7/cooke langford et al 2012 - Social context and the role of collaborative policy making for private land conservation.pdf:application/pdf},
}

@article{gordonSimulatingValueCollaboration2013,
	title = {Simulating the value of collaboration in multi-actor conservation planning},
	volume = {249},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380012003341},
	doi = {10.1016/j.ecolmodel.2012.07.009},
	language = {en},
	urldate = {2020-11-03},
	journal = {Ecological Modelling},
	author = {Gordon, Ascelin and Bastin, Lucy and Langford, William T. and Lechner, Alex M. and Bekessy, Sarah A.},
	month = jan,
	year = {2013},
	pages = {19--25},
	file = {gordon bastin langford et al 2013 - simulating the value of collaboration in multi-actor conservation planning.pdf:/Users/bill/D/Zotero/storage/HIPMKV9T/gordon bastin langford et al 2013 - simulating the value of collaboration in multi-actor conservation planning.pdf:application/pdf},
}

@article{gordonAssessingImpactsBiodiversity2011,
	title = {Assessing the impacts of biodiversity offset policies},
	volume = {26},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815211001824},
	doi = {10.1016/j.envsoft.2011.07.021},
	abstract = {In response to the increasing loss of native vegetation and biodiversity, a growing number of countries have adopted “offsetting” policies that seek to balance local habitat destruction by restoring, enhancing and/or protecting similar but separate habitat. Although these policies often have a stated aim of producing a “net gain” or “no net loss” in environmental beneﬁts, it is challenging to determine the potential impacts of a policy and if, or when, it will achieve its objectives. In this paper we address these questions with a general approach that uses predictive modelling under uncertainty to quantify the ecological impacts of different offset policies. This is demonstrated with a case study to the west of Melbourne, Australia where a proposed expansion of Melbourne’s urban growth boundary would result in a loss of endangered native grassland, requiring offsets to be implemented as compensation. Three different offset policies were modelled: i) no restrictions on offset location, ii) offset locations spatially restricted to a strategically deﬁned area and iii) offset locations spatially and temporally restricted, requiring all offsets to be implemented before commencing development. The ecological impact of the policies was determined with a system model that predicts future changes in the extent and condition of native grassland. The case study demonstrates how relative and absolute policy performance can be quantiﬁed in relation to best and worst-case scenarios. The study also shows that the ecological beneﬁts of being temporally and spatially strategic in choosing offsets locations are substantially greater than being spatially strategic alone. We also show that even with considerable uncertainties in the system model predicting future grassland condition, the performance of the three offset policies can still be differentiated. Finally, we show the extent to which a policy achieves a “net gain” is dependent on the baseline against which policy performance is measured. The quantitative framework presented here can also be used to evaluate other offset policies or extended to deal with different types of environmental policies.},
	language = {en},
	number = {12},
	urldate = {2020-11-03},
	journal = {Environmental Modelling \& Software},
	author = {Gordon, Ascelin and Langford, William T. and Todd, James A. and White, Matt D. and Mullerworth, Daniel W. and Bekessy, Sarah A.},
	month = dec,
	year = {2011},
	pages = {1481--1488},
	file = {gordon langford et al 2011 - Assessing the impacts of biodiversity offset policies.pdf:/Users/bill/D/Zotero/storage/FNPZ2IZJ/gordon langford et al 2011 - Assessing the impacts of biodiversity offset policies.pdf:application/pdf},
}

@article{gordonModellingTradeOffs2011,
	title = {Modelling trade offs between public and private conservation policies},
	volume = {144},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320710004544},
	doi = {10.1016/j.biocon.2010.10.011},
	abstract = {To reduce global biodiversity loss, there is an urgent need to determine the most efﬁcient allocation of conservation resources. Recently, there has been a growing trend for many governments to supplement public ownership and management of reserves with incentive programs for conservation on private land. At the same time, policies to promote conservation on private land are rarely evaluated in terms of their ecological consequences. This raises important questions, such as the extent to which private land conservation can improve conservation outcomes, and how it should be mixed with more traditional public land conservation. We address these questions, using a general framework for modelling environmental policies and a case study examining the conservation of endangered native grasslands to the west of Melbourne, Australia. Speciﬁcally, we examine three policies that involve: (i) spending all resources on creating public conservation areas; (ii) spending all resources on an ongoing incentive program where private landholders are paid to manage vegetation on their property with 5-year contracts; and (iii) splitting resources between these two approaches. The performance of each strategy is quantiﬁed with a vegetation condition change model that predicts future changes in grassland quality. Of the policies tested, no one policy was always best and policy performance depended on the objectives of those enacting the policy. This work demonstrates a general method for evaluating environmental policies and highlights the utility of a model which combines ecological and socioeconomic processes.},
	language = {en},
	number = {1},
	urldate = {2020-11-03},
	journal = {Biological Conservation},
	author = {Gordon, Ascelin and Langford, William T. and White, Matt D. and Todd, James A. and Bastin, Lucy},
	month = jan,
	year = {2011},
	pages = {558--566},
	file = {gordon langford et al 2011 - Modelling trade offs between public and private conservation policies.pdf:/Users/bill/D/Zotero/storage/HUQA3BEA/gordon langford et al 2011 - Modelling trade offs between public and private conservation policies.pdf:application/pdf},
}

@article{langfordRaisingBarSystematic2011,
	title = {Raising the bar for systematic conservation planning},
	volume = {26},
	issn = {01695347},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534711002333},
	doi = {10.1016/j.tree.2011.08.001},
	language = {en},
	number = {12},
	urldate = {2020-11-03},
	journal = {Trends in Ecology \& Evolution},
	author = {Langford, William T. and Gordon, Ascelin and Bastin, Lucy and Bekessy, Sarah A. and White, Matt D. and Newell, Graeme},
	month = dec,
	year = {2011},
	pages = {634--640},
	file = {langford et al 2011 - raising the bar for systematic conservation planning - final published copy.pdf:/Users/bill/D/Zotero/storage/FS25UHGK/langford et al 2011 - raising the bar for systematic conservation planning - final published copy.pdf:application/pdf},
}

@article{langfordMapMisclassificationCan2006,
	title = {Map {Misclassification} {Can} {Cause} {Large} {Errors} in {Landscape} {Pattern} {Indices}: {Examples} from {Habitat} {Fragmentation}},
	volume = {9},
	issn = {1432-9840, 1435-0629},
	shorttitle = {Map {Misclassification} {Can} {Cause} {Large} {Errors} in {Landscape} {Pattern} {Indices}},
	url = {http://link.springer.com/10.1007/s10021-005-0119-1},
	doi = {10.1007/s10021-005-0119-1},
	language = {en},
	number = {3},
	urldate = {2020-11-03},
	journal = {Ecosystems},
	author = {Langford, William T. and Gergel, Sarah E. and Dietterich, Thomas G. and Cohen, Warren},
	month = apr,
	year = {2006},
	pages = {474--488},
	file = {langford et al. - 2006 - Map Misclassification Can Cause Large Errors in Landscape Pattern Indices Examples from Habitat Fragmentation - Ecosystems.PDF:/Users/bill/D/Zotero/storage/NX4TEVM3/langford et al. - 2006 - Map Misclassification Can Cause Large Errors in Landscape Pattern Indices Examples from Habitat Fragmentation - Ecosystems.PDF:application/pdf},
}

@article{langfordWhenConservationPlanning2009,
	title = {When do conservation planning methods deliver? {Quantifying} the consequences of uncertainty},
	volume = {4},
	issn = {15749541},
	shorttitle = {When do conservation planning methods deliver?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574954109000223},
	doi = {10.1016/j.ecoinf.2009.04.002},
	abstract = {The rapid global loss of biodiversity has led to a proliferation of systematic conservation planning methods. In spite of their utility and mathematical sophistication, these methods only provide approximate solutions to realworld problems where there is uncertainty and temporal change. The consequences of errors in these solutions are seldom characterized or addressed. We propose a conceptual structure for exploring the consequences of input uncertainty and oversimpliﬁed approximations to real-world processes for any conservation planning tool or strategy. We then present a computational framework based on this structure to quantitatively model species representation and persistence outcomes across a range of uncertainties. These include factors such as land costs, landscape structure, species composition and distribution, and temporal changes in habitat. We demonstrate the utility of the framework using several reserve selection methods including simple rules of thumb and more sophisticated tools such as Marxan and Zonation. We present new results showing how outcomes can be strongly affected by variation in problem characteristics that are seldom compared across multiple studies. These characteristics include number of species prioritized, distribution of species richness and rarity, and uncertainties in the amount and quality of habitat patches. We also demonstrate how the framework allows comparisons between conservation planning strategies and their response to error under a range of conditions. Using the approach presented here will improve conservation outcomes and resource allocation by making it easier to predict and quantify the consequences of many different uncertainties and assumptions simultaneously. Our results show that without more rigorously generalizable results, it is very difﬁcult to predict the amount of error in any conservation plan. These results imply the need for standard practice to include evaluating the effects of multiple real-world complications on the behavior of any conservation planning method.},
	language = {en},
	number = {3},
	urldate = {2020-11-03},
	journal = {Ecological Informatics},
	author = {Langford, William T. and Gordon, Ascelin and Bastin, Lucy},
	month = aug,
	year = {2009},
	pages = {123--135},
	file = {langford, Gordon, Bastin - 2009 - When do conservation planning methods deliver Quantifying the consequences of uncertainty - Ecological Informatics.PDF:/Users/bill/D/Zotero/storage/7MTYHKPF/langford, Gordon, Bastin - 2009 - When do conservation planning methods deliver Quantifying the consequences of uncertainty - Ecological Informatics.PDF:application/pdf},
}

@article{lechnerInvestigatingSpeciesEnvironment2012,
	title = {Investigating species–environment relationships at multiple scales: {Differentiating} between intrinsic scale and the modifiable areal unit problem},
	volume = {11},
	issn = {1476945X},
	shorttitle = {Investigating species–environment relationships at multiple scales},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1476945X12000396},
	doi = {10.1016/j.ecocom.2012.04.002},
	language = {en},
	urldate = {2020-11-03},
	journal = {Ecological Complexity},
	author = {Lechner, Alex M. and Langford, William T. and Jones, Simon D. and Bekessy, Sarah A. and Gordon, Ascelin},
	month = sep,
	year = {2012},
	pages = {91--102},
	file = {lechner langford et al 2012 - Investigating species–environment relationships at multiple scales - Differentiating between intrinsic scale and the modifiable areal unit problem - in press corrected proof.pdf:/Users/bill/D/Zotero/storage/9V4ACC63/lechner langford et al 2012 - Investigating species–environment relationships at multiple scales - Differentiating between intrinsic scale and the modifiable areal unit problem - in press corrected proof.pdf:application/pdf},
}

@article{lechnerAreLandscapeEcologists2012,
	title = {Are landscape ecologists addressing uncertainty in their remote sensing data?},
	volume = {27},
	issn = {0921-2973, 1572-9761},
	url = {http://link.springer.com/10.1007/s10980-012-9791-7},
	doi = {10.1007/s10980-012-9791-7},
	language = {en},
	number = {9},
	urldate = {2020-11-03},
	journal = {Landscape Ecology},
	author = {Lechner, Alex M. and Langford, William T. and Bekessy, Sarah A. and Jones, Simon D.},
	month = nov,
	year = {2012},
	pages = {1249--1261},
	file = {Lechner_2012_Are_landscape_ecologists_addressing_uncertainty_in_their_remote_sensing_data.pdf:/Users/bill/D/Zotero/storage/J289F36S/Lechner_2012_Are_landscape_ecologists_addressing_uncertainty_in_their_remote_sensing_data.pdf:application/pdf},
}

@article{matchettDetectingInfluenceRare2015,
	title = {Detecting the influence of rare stressors on rare species in {Yosemite} {National} {Park} using a novel stratified permutation test},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10702},
	doi = {10.1038/srep10702},
	language = {en},
	number = {1},
	urldate = {2020-11-03},
	journal = {Scientific Reports},
	author = {Matchett, J. R. and Stark, Philip B. and Ostoja, Steven M. and Knapp, Roland A. and McKenny, Heather C. and Brooks, Matthew L. and Langford, William T. and Joppa, Lucas N. and Berlow, Eric L.},
	month = sep,
	year = {2015},
	pages = {10702},
	file = {matchett ... langford ... berlow 2015 - Detecting the influence of rare stressors on rare species in Yosemite National Park using a novel stratified permutation test - proofs.pdf:/Users/bill/D/Zotero/storage/NV2ZDMSY/matchett ... langford ... berlow 2015 - Detecting the influence of rare stressors on rare species in Yosemite National Park using a novel stratified permutation test - proofs.pdf:application/pdf},
}

@article{moretLearningPredictChannel2006,
	title = {Learning to predict channel stability using biogeomorphic features},
	volume = {191},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380005003480},
	doi = {10.1016/j.ecolmodel.2005.08.011},
	abstract = {Current human land use activities are altering many components of the river landscape, resulting in unstable channels. Instability may have serious negative consequences for water quality, aquatic and riparian habitat, and for river-related human infrastructure such as bridges and roads. Resource management agencies have developed rapid bioassessment surveys to help assess stability in a fast and cost-effective way. While this assessment can be done for a single site fairly rapidly, it is still time-consuming to apply over large watersheds and assessment activities must be prioritized. We constructed a system that employs commonly available map data as inputs to cost-sensitive variants of decision tree algorithms to predict the relative channel stability of different sites. In particular, we use bagged lazy option trees (LOTs) and bagged probability estimation trees (PETs) to identify all unstable channels while making the smallest number of errors of classifying stable channels as unstable, thereby minimizing cost and maximizing safety. We measured the performance of the classiﬁers using ROC curves and found that the PETs performed better than the LOTs in situations where the number of instances of the stable and unstable classes were relatively balanced, but the LOTs did better where unstable examples were relatively rare compared to stable, perhaps due to the LOTs’ ability to focus on individual examples.},
	language = {en},
	number = {1},
	urldate = {2020-11-03},
	journal = {Ecological Modelling},
	author = {Moret, Stephanie L. and Langford, William T. and Margineantu, Dragos D.},
	month = jan,
	year = {2006},
	pages = {47--57},
	file = {moret langford margineantu 2006 - Learning to predict channel stability using biogeomorphic features.pdf:/Users/bill/D/Zotero/storage/VVI3CUA4/moret langford margineantu 2006 - Learning to predict channel stability using biogeomorphic features.pdf:application/pdf},
}

@article{reganROBUSTDECISIONMAKINGSEVERE2005,
	title = {{ROBUST} {DECISION}-{MAKING} {UNDER} {SEVERE} {UNCERTAINTY} {FOR} {CONSERVATION} {MANAGEMENT}},
	volume = {15},
	issn = {1051-0761},
	url = {http://doi.wiley.com/10.1890/03-5419},
	doi = {10.1890/03-5419},
	abstract = {In conservation biology it is necessary to make management decisions for endangered and threatened species under severe uncertainty. Failure to acknowledge and treat uncertainty can lead to poor decisions. To illustrate the importance of considering uncertainty, we reanalyze a decision problem for the Sumatran rhino, Dicerorhinus sumatrensis, using information-gap theory to propagate uncertainties and to rank management options. Rather than requiring information about the extent of parameter uncertainty at the outset, information-gap theory addresses the question of how much uncertainty can be tolerated before our decision would change. It assesses the robustness of decisions in the face of severe uncertainty. We show that different management decisions may result when uncertainty in utilities and probabilities are considered in decision-making problems. We highlight the importance of a full assessment of uncertainty in conservation management decisions to avoid, as much as possible, undesirable outcomes.},
	language = {en},
	number = {4},
	urldate = {2020-11-03},
	journal = {Ecological Applications},
	author = {Regan, Helen M. and Ben-Haim, Yakov and Langford, Bill and Wilson, William G. and Lundberg, Per and Andelman, Sandy J. and Burgman, Mark A.},
	month = aug,
	year = {2005},
	pages = {1471--1477},
	file = {regan ben-haim langford et al 2005 - ROBUST DECISION-MAKING UNDER SEVERE UNCERTAINTY FOR CONSERVATION MANAGEMENT.pdf:/Users/bill/D/Zotero/storage/C22Q7IRR/regan ben-haim langford et al 2005 - ROBUST DECISION-MAKING UNDER SEVERE UNCERTAINTY FOR CONSERVATION MANAGEMENT.pdf:application/pdf},
}

@article{wilsonBiodiversitySpeciesInteractions2003,
	title = {Biodiversity and species interactions: extending {Lotka}-{Volterra} community theory},
	volume = {6},
	issn = {1461-023X, 1461-0248},
	shorttitle = {Biodiversity and species interactions},
	url = {http://doi.wiley.com/10.1046/j.1461-0248.2003.00521.x},
	doi = {10.1046/j.1461-0248.2003.00521.x},
	abstract = {A new analysis of the nearly century-old Lotka–Volterra theory allows us to link species interactions to biodiversity patterns, including: species abundance distributions, estimates of total community size, patterns of community invasibility, and predicted responses to disturbance. Based on a few restrictive assumptions about species interactions, our calculations require only that the community is sufﬁciently large to allow a mean-ﬁeld approximation. We develop this analysis to show how an initial assemblage of species with varying interaction strengths is predicted to sort out into the ﬁnal community based on the speciesÕ predicted target densities. The sorting process yields predictions of covarying patterns of species abundance, community size, and species interaction strengths. These predictions can be tested using enrichment experiments, examination of latitudinal and productivity gradients, and features of community assembly.},
	language = {en},
	number = {10},
	urldate = {2020-11-03},
	journal = {Ecology Letters},
	author = {Wilson, W. G. and Lundberg, P. and Vazquez, D. P. and Shurin, J. B. and Smith, M. D. and Langford, W. and Gross, K. L. and Mittelbach, G. G.},
	month = oct,
	year = {2003},
	pages = {944--952},
	file = {wilson et al 2003 - Biodiversity and species interactions - extending Lotka–Volterra community theory.pdf:/Users/bill/D/Zotero/storage/LT354Z4M/wilson et al 2003 - Biodiversity and species interactions - extending Lotka–Volterra community theory.pdf:application/pdf},
}

@inproceedings{goodchildSpatialAccuracy2008,
	address = {Shanghai, P. R. China},
	title = {Spatial {Accuracy} 2.0},
	isbn = {1-84626-170-8},
	abstract = {Research on spatial accuracy assessment occurs within a broader context that provides its motivation. That broader context is dynamic, and has been changing at an accelerating rate. The concept of the Geospatial Web imagines a world of distributed, interoperable, georeferenced information in which it is possible to know where everything of importance is located in real time. It assumes an ability to conflate that is far beyond today’s capabilities. Web 2.0 describes a substantial involvement of the user in creating the content of the Web, and has particular relevance to geospatial information. Metadata 2.0 shifts the onus for metadata production to the user, and addresses some of the growing issues surrounding existing standards. There is a growing need to address the accuracy assessment of the vast quantities of geospatial data being contributed by individual Web users.},
	booktitle = {Proceedings of the 8th {International} {Symposium} on {Spatial} {Accuracy} {Assessment} in {Natural} {Resources} and {Environmental} {Sciences}},
	publisher = {World Academic Union (World Academic Press)},
	author = {Goodchild, Michael F.},
	month = jun,
	year = {2008},
	keywords = {metadata, Geospatial Web, user-generated content, volunteered geographic information, Web 2.0},
	pages = {1--7},
	file = {goodchild 2008 - spatial accuracy 2.0.pdf:/Users/bill/D/Zotero/storage/2XCKXFVY/goodchild 2008 - spatial accuracy 2.0.pdf:application/pdf},
}

@misc{HowReadUnderstand,
	title = {How\_to\_read\_understand\_and\_write\_Discussion\_sect.pdf},
	file = {How_to_read_understand_and_write_Discussion_sect.pdf:/Users/bill/D/Zotero/storage/SG7XCAJ4/How_to_read_understand_and_write_Discussion_sect.pdf:application/pdf},
}

@article{tierneyRealisticGuideMaking2020,
	title = {A {Realistic} {Guide} to {Making} {Data} {Available} {Alongside} {Code} to {Improve} {Reproducibility}},
	url = {http://arxiv.org/abs/2002.11626},
	abstract = {Data makes science possible. Sharing data improves visibility, and makes the research process transparent. This increases trust in the work, and allows for independent reproduction of results. However, a large proportion of data from published research is often only available to the original authors. Despite the obvious beneﬁts of sharing data, and scientists’ advocating for the importance of sharing data, most advice on sharing data discusses its broader beneﬁts, rather than the practical considerations of sharing. This paper provides practical, actionable advice on how to actually share data alongside research. The key message is sharing data falls on a continuum, and entering it should come with minimal barriers.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:2002.11626 [cs]},
	author = {Tierney, Nicholas J. and Ram, Karthik},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.11626},
	keywords = {Computer Science - Digital Libraries},
	annote = {Comment: Both authors contributed equally to the work, 35 pages, 7 figures, 3 tables},
	annote = {Extracted Annotations (11/4/2020, 11:29:07 AM)
"Tooling for producing data documentation information speeds up and simplifies the process of sharing data. Machine-readable metadata that can be indexed by google is created using the dataspice package (Dataspice: Create Lightweight Schema.org Descriptions of Dataset 2018). To help create a data dictionary, we recommend the codebook package in R (Arslan 2019), which also generates machine readable metadata. Data dictionaries are implemented in other software such as STATA, which provides a "codebook" command. Data can be packaged up in a "data package" with DataPackageR (Greg Finak 2019), which provides tools to wrap up data into an R package, while providing helpers with MD5sum checks that allow checksum file comparisons for versioning. Note that is different to Frictionless Data's tabular data package spec (Fowler, Barratt, and Walsh 2017)." (Tierney and Ram 2020:23)
This is just a test note to see if Zotero/Zotfile can find it. (note on p.23)},
	file = {tierney ram 2020 - A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility - REPRODUCIBILITY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/QJC66JG4/tierney ram 2020 - A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility - REPRODUCIBILITY - BDPG - ANNO.pdf:application/pdf},
}

@book{kanevskiMachineLearningSpatial2009,
	address = {Lausanne},
	series = {Environmental sciences, environmental engineering},
	title = {Machine learning for spatial environmental data: theory, applications and software},
	isbn = {978-0-8493-8237-6 978-2-940222-24-7},
	shorttitle = {Machine learning for spatial environmental data},
	language = {eng},
	publisher = {Epfel},
	author = {Kanevski, Mikhail and Pozdnoukhov, Alexei and Timonin, Vadim},
	year = {2009},
	note = {OCLC: 635693164},
	annote = {CD-ROM u.d.T.: "Machine Learning Office : Software for Environmental Spatial Data Analysis Full Research Version" contains: Machine Learning Office, MLO Guide (PDF), Examples of data Literaturverz. S. [347] - 371},
}

@book{zhangSpatialUncertaintyProceedings2008,
	address = {Liverpool},
	series = {Spacial accuracy 2008},
	title = {Spatial uncertainty: proceedings of the 8th {International} symposium on spatial accuracy assessment in natural resources and environmental sciences, {Shanghai}, {China}, {June} 25 - 27, 28},
	isbn = {978-1-84626-170-1},
	shorttitle = {Spatial uncertainty},
	language = {eng},
	number = {the 8th International symposium on spatial accuracy assessment in natural resources and environmental sciences, Shanghai, China, June 25 - 27, 2008 ; Vol. 1},
	publisher = {World Academic Union (World Academic Press)},
	editor = {Zhang, Jingxiong and International Symposium on Spatial Accuracy Assessment in Natural Resources {and} Environmental Sciences},
	year = {2008},
	note = {Meeting Name: International Symposium on Spatial Accuracy Assessment in Natural Resources and Environmental Sciences
OCLC: 552441241},
}

@article{chen,
	title = {Statistical-{Computational} {Tradeoffs} in {Planted} {Problems} and {Submatrix} {Localization} with a {Growing} {Number} of {Clusters} and {Submatrices}},
	abstract = {We consider two closely related problems: planted clustering and submatrix localization. In the planted clustering problem, a random graph is generated based on an underlying cluster structure of the nodes; the task is to recover these clusters given the graph. The submatrix localization problem concerns locating hidden submatrices with elevated means inside a large real-valued random matrix. Of particular interest is the setting where the number of clusters/submatrices is allowed to grow unbounded with the problem size. These formulations cover several classical models such as planted clique, planted densest subgraph, planted partition, planted coloring, and the stochastic block model, which are widely used for studying community detection, graph clustering and bi-clustering.},
	language = {en},
	author = {Chen, Yudong and Xu, Jiaming},
	keywords = {bdpg, planted solution, planted problem},
	pages = {57},
	file = {14-330.pdf:/Users/bill/D/Zotero/storage/E44M4RVF/14-330.pdf:application/pdf},
}

@article{sakataStatisticalMechanicsDictionary2013,
	title = {Statistical mechanics of dictionary learning},
	volume = {103},
	issn = {0295-5075, 1286-4854},
	url = {https://iopscience.iop.org/article/10.1209/0295-5075/103/28008},
	doi = {10.1209/0295-5075/103/28008},
	abstract = {Finding a basis matrix (dictionary) by which objective signals are represented sparsely is of major relevance in various scientiﬁc and technological ﬁelds. We consider a problem to learn a dictionary from a set of training signals. We employ techniques of statistical mechanics of disordered systems to evaluate the size of the training set necessary to typically succeed in the dictionary learning. The results indicate that the necessary size is much smaller than previously estimated, which theoretically supports and/or encourages the use of dictionary learning in practical situations.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {EPL (Europhysics Letters)},
	author = {Sakata, Ayaka and Kabashima, Yoshiyuki},
	month = jul,
	year = {2013},
	pages = {28008},
	file = {0295-5075_103_2_28008.pdf:/Users/bill/D/Zotero/storage/TSLEMIL4/0295-5075_103_2_28008.pdf:application/pdf},
}

@article{ben-shimonMessagePassingColoring2007,
	title = {Message passing for the coloring problem: {Gallager} meets {Alon} and {Kahale}},
	shorttitle = {Message passing for the coloring problem},
	url = {http://arxiv.org/abs/0710.3928},
	abstract = {Message passing algorithms are popular in many combinatorial optimization problems. For example, experimental results show that survey propagation (a certain message passing algorithm) is eﬀective in ﬁnding proper k-colorings of random graphs in the near-threshold regime. In 1962 Gallager introduced the concept of Low Density Parity Check (LDPC) codes, and suggested a simple decoding algorithm based on message passing. In 1994 Alon and Kahale exhibited a coloring algorithm and proved its usefulness for ﬁnding a k-coloring of graphs drawn from a certain planted-solution distribution over kcolorable graphs. In this work we show an interpretation of Alon and Kahale’s coloring algorithm in light of Gallager’s decoding algorithm, thus showing a connection between the two problems - coloring and decoding. This also provides a rigorous evidence for the usefulness of the message passing paradigm for the graph coloring problem. Our techniques can be applied to several other combinatorial optimization problems and networking-related issues.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:0710.3928 [cs, math]},
	author = {Ben-Shimon, Sonny and Vilenchik, Dan},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.3928},
	keywords = {Computer Science - Discrete Mathematics, Mathematics - Combinatorics, Mathematics - Probability},
	annote = {Comment: 11 pages},
	file = {0710.3928.pdf:/Users/bill/D/Zotero/storage/WLXNHC7Q/0710.3928.pdf:application/pdf},
}

@article{reichardtDetectableClusterStructure2008,
	title = {({Un})detectable cluster structure in sparse networks},
	volume = {101},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/0711.1452},
	doi = {10.1103/PhysRevLett.101.078701},
	abstract = {We study the problem of recovering a known cluster structure in a sparse network, also known as the planted partitioning problem, by means of statistical mechanics. We find a sharp transition from un-recoverable to recoverable structure as a function of the separation of the clusters. For multivariate data, such transitions have been observed frequently, but always as a function of the number of data points provided, i.e. given a large enough data set, two point clouds can always be recognized as different clusters, as long as their separation is non-zero. In contrast, for the sparse networks studied here, a cluster structure remains undetectable even in an infinitely large network if a critical separation is not exceeded. We give analytic formulas for this critical separation as a function of the degree distribution of the network and calculate the shape of the recoverability-transition. Our findings have implications for unsupervised learning and data-mining in relational data bases and provide bounds on the achievable performance of graph clustering algorithms.},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {Physical Review Letters},
	author = {Reichardt, Joerg and Leone, Michele},
	month = aug,
	year = {2008},
	note = {arXiv: 0711.1452},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {078701},
	annote = {Comment: 4 Pages, 2 Figures},
	file = {0711.1452.pdf:/Users/bill/D/Zotero/storage/AR9WC8NB/0711.1452.pdf:application/pdf},
}

@article{coja-oghlanSpectralApproachAnalyzing2009,
	title = {A {Spectral} {Approach} to {Analyzing} {Belief} {Propagation} for 3-{Coloring}},
	volume = {18},
	issn = {0963-5483, 1469-2163},
	url = {http://arxiv.org/abs/0712.0171},
	doi = {10.1017/S096354830900981X},
	abstract = {Belief Propagation (BP) is a message-passing algorithm that computes the exact marginal distributions at every vertex of a graphical model without cycles. While BP is designed to work correctly on trees, it is routinely applied to general graphical models that may contain cycles, in which case neither convergence, nor correctness in the case of convergence is guaranteed. Nonetheless, BP gained popularity as it seems to remain effective in many cases of interest, even when the underlying graph is “far” from being a tree. However, the theoretical understanding of BP (and its new relative Survey Propagation) when applied to CSPs is poor.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Combinatorics, Probability and Computing},
	author = {Coja-Oghlan, Amin and Mossel, Elchanan and Vilenchik, Dan},
	month = nov,
	year = {2009},
	note = {arXiv: 0712.0171},
	keywords = {Computer Science - Discrete Mathematics, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity},
	pages = {881--912},
	file = {0712.0171v1.pdf:/Users/bill/D/Zotero/storage/5YEUZMWG/0712.0171v1.pdf:application/pdf},
}

@article{achlioptasAlgorithmicBarriersPhase2008,
	title = {Algorithmic barriers from phase transitions},
	url = {http://arxiv.org/abs/0803.2122},
	doi = {10.1109/FOCS.2008.11},
	abstract = {For many random Constraint Satisfaction Problems, by now, we have asymptotically tight estimates of the largest constraint density for which they have solutions. At the same time, all known polynomial-time algorithms for many of these problems already completely fail to ﬁnd solutions at much smaller densities. For example, it is well-known that it is easy to color a random graph using twice as many colors as its chromatic number. Indeed, some of the simplest possible coloring algorithms already achieve this goal. Given the simplicity of those algorithms, one would expect there is a lot of room for improvement. Yet, to date, no algorithm is known that uses (2 − ǫ)χ colors, in spite of efforts by numerous researchers over the years.},
	language = {en},
	urldate = {2020-11-04},
	journal = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
	author = {Achlioptas, Dimitris and Coja-Oghlan, Amin},
	month = oct,
	year = {2008},
	note = {arXiv: 0803.2122},
	keywords = {Mathematics - Combinatorics, Mathematics - Probability},
	pages = {793--802},
	annote = {Comment: extended abstract},
	file = {0803.2122.pdf:/Users/bill/D/Zotero/storage/4QA68EIL/0803.2122.pdf:application/pdf},
}

@article{liOneSolution3satisfiability2009,
	title = {From one solution of a 3-satisfiability formula to a solution cluster: {Frozen} variables and entropy},
	volume = {79},
	issn = {1539-3755, 1550-2376},
	shorttitle = {From one solution of a 3-satisfiability formula to a solution cluster},
	url = {http://arxiv.org/abs/0809.4332},
	doi = {10.1103/PhysRevE.79.031102},
	abstract = {A solution to a 3-satisfiability (3-SAT) formula can be expanded into a cluster, all other solutions of which are reachable from this one through a sequence of single-spin flips. Some variables in the solution cluster are frozen to the same spin values by one of two different mechanisms: frozen-core formation and long-range frustrations. While frozen cores are identified by a local whitening algorithm, long-range frustrations are very difficult to trace, and they make an entropic belief-propagation (BP) algorithm fail to converge. For BP to reach a fixed point the spin values of a tiny fraction of variables (chosen according to the whitening algorithm) are externally fixed during the iteration. From the calculated entropy values, we infer that, for a large random 3-SAT formula with constraint density close to the satisfiability threshold, the solutions obtained by the survey-propagation or the walksat algorithm belong neither to the most dominating clusters of the formula nor to the most abundant clusters. This work indicates that a single solution cluster of a random 3-SAT formula may have further community structures.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Physical Review E},
	author = {Li, Kang and Ma, Hui and Zhou, Haijun},
	month = mar,
	year = {2009},
	note = {arXiv: 0809.4332},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Computational Complexity},
	pages = {031102},
	annote = {Comment: 13 pages, 6 figures. Final version as published in PRE},
	file = {0809.4332.pdf:/Users/bill/D/Zotero/storage/AH5N4G9K/0809.4332.pdf:application/pdf},
}

@article{youngFirstOrderPhase2010,
	title = {First order phase transition in the {Quantum} {Adiabatic} {Algorithm}},
	volume = {104},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/0910.1378},
	doi = {10.1103/PhysRevLett.104.020502},
	abstract = {We simulate the quantum adiabatic algorithm (QAA) for the exact cover problem for sizes up to N=256 using quantum Monte Carlo simulations incorporating parallel tempering. At large N we find that some instances have a discontinuous (first order) quantum phase transition during the evolution of the QAA. This fraction increases with increasing N and may tend to 1 for N -{\textgreater} infinity.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Physical Review Letters},
	author = {Young, A. P. and Knysh, S. and Smelyanskiy, V. N.},
	month = jan,
	year = {2010},
	note = {arXiv: 0910.1378},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Quantum Physics},
	pages = {020502},
	annote = {Comment: 5 pages, 3 figures. Replaced with published version; two figures slightly changed and some small changes to the text},
	file = {0910.1378.pdf:/Users/bill/D/Zotero/storage/3GHZGWCM/0910.1378.pdf:application/pdf},
}

@article{coja-oghlanWhyAlmostAll,
	title = {Why almost all satisﬁable k-{CNF} formulas are easy},
	abstract = {Finding a satisfying assignment for a k-CNF formula (k ≥ 3), assuming such exists, is a notoriously hard problem. In this work we consider the uniform distribution over satisﬁable k-CNF formulas with a linear number of clauses (clause-variable ratio greater than some constant). We rigorously analyze the structure of the space of satisfying assignments of a random formula in that distribution, showing that basically all satisfying assignments are clustered in one cluster, and agree on all but a small, though linear, number of variables. This observation enables us to describe a polynomial time algorithm that ﬁnds whp a satisfying assignment for such formulas, thus asserting that most satisﬁable k-CNF formulas are easy (whenever the clause-variable ratio is greater than some constant). This should be contrasted with the setting of very sparse k-CNF formulas (which are satisﬁable whp), where experimental results show some regime of clause density to be difﬁcult for many SAT heuristics. One explanation for this phenomena, backed up by partially non-rigorous analytical tools from statistical physics, is the complicated clustering of the solution space at that regime, unlike the more “regular” structure that denser formulas possess. Thus in some sense, our result rigorously supports this explanation.},
	language = {en},
	author = {Coja-Oghlan, Amin and Krivelevich, Michael and Vilenchik, Dan},
	pages = {14},
	file = {911-3036-2-PB.pdf:/Users/bill/D/Zotero/storage/4DP8HIP2/911-3036-2-PB.pdf:application/pdf},
}

@article{feldman2018acm,
	title = {On the {Complexity} of {Random} {Satisfiability} {Problems} with {Planted} {Solutions}},
	url = {http://arxiv.org/abs/1311.4821},
	abstract = {For a planted satisﬁability problem on n variables with k variables per constraint, the planted assignment becomes the unique solution after O(n log n) random clauses. However, the bestknown algorithms need at least nr/2 to eﬃciently identify any assignment weakly correlated with the planted one for clause distributions that are (r − 1)-wise independent (r can be as high as k). Our main result is an unconditional lower bound, tight up to logarithmic factors, of Ω˜ (nr/2) clauses for statistical algorithms, a broad class of algorithms introduced in [50, 34]. We complement this with a nearly matching upper bound using a statistical algorithm. As known approaches for problems over distributions in general, and planted satisﬁability problems in particular, all have statistical analogues (spectral, MCMC, gradient-based, convex optimization etc.), this provides a rigorous explanation of the large gap between the identiﬁability and algorithmic identiﬁability thresholds for random satisﬁability problems with planted solutions. Our results imply that a strong form of Feige’s refutation hypothesis for average-case SAT instances [31] holds for statistical algorithms. We also consider the closely related problem of ﬁnding the planted assignment of a random planted k-CSP which is the basis of Goldreich’s proposed one-way function [41]. Our bounds extend to this problem and give concrete evidence for the security of the one-way function and the associated pseudorandom generator when used with a suﬃciently hard predicate.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1311.4821 [cs, math]},
	author = {Feldman, Vitaly and Perkins, Will and Vempala, Santosh},
	month = mar,
	year = {2018},
	note = {arXiv: 1311.4821},
	keywords = {bdpg, Computer Science - Discrete Mathematics, Mathematics - Combinatorics, Mathematics - Probability, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, planted solution},
	annote = {Comment: Extended abstract appeared in STOC 2015},
	file = {1311.4821v3.pdf:/Users/bill/D/Zotero/storage/SPPXESCP/1311.4821v3.pdf:application/pdf},
}

@article{feldman2015acm,
	title = {Subsampled {Power} {Iteration}: a {Unified} {Algorithm} for {Block} {Models} and {Planted} {CSP}'s},
	shorttitle = {Subsampled {Power} {Iteration}},
	url = {http://arxiv.org/abs/1407.2774},
	abstract = {We present a new algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems, via a common generalization in terms of random bipartite graphs. Our algorithm achieves the best-known bounds for the number of edges needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is signiﬁcantly better than both spectral and SDP-based approaches. The main new features of the algorithm are two-fold: (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) it can be implemented statistically, i.e., with very limited access to the input distribution.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1407.2774 [cs, math]},
	author = {Feldman, Vitaly and Perkins, Will and Vempala, Santosh},
	month = apr,
	year = {2015},
	note = {arXiv: 1407.2774},
	keywords = {bdpg, Mathematics - Combinatorics, Mathematics - Probability, Computer Science - Data Structures and Algorithms, planted solution, CSP},
	file = {1407.2774v1.pdf:/Users/bill/D/Zotero/storage/I65DT4YA/1407.2774v1.pdf:application/pdf},
}

@article{tullerDiscoveringLocalPatterns2010,
	title = {Discovering local patterns of co - evolution: computational aspects and biological examples},
	volume = {11},
	issn = {1471-2105},
	shorttitle = {Discovering local patterns of co - evolution},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-43},
	doi = {10.1186/1471-2105-11-43},
	abstract = {Background: Co-evolution is the process in which two (or more) sets of orthologs exhibit a similar or correlative pattern of evolution. Co-evolution is a powerful way to learn about the functional interdependencies between sets of genes and cellular functions and to predict physical interactions. More generally, it can be used for answering fundamental questions about the evolution of biological systems. Orthologs that exhibit a strong signal of co-evolution in a certain part of the evolutionary tree may show a mild signal of co-evolution in other branches of the tree. The major reasons for this phenomenon are noise in the biological input, genes that gain or lose functions, and the fact that some measures of co-evolution relate to rare events such as positive selection. Previous publications in the field dealt with the problem of finding sets of genes that co-evolved along an entire underlying phylogenetic tree, without considering the fact that often co-evolution is local.
Results: In this work, we describe a new set of biological problems that are related to finding patterns of local coevolution. We discuss their computational complexity and design algorithms for solving them. These algorithms outperform other bi-clustering methods as they are designed specifically for solving the set of problems mentioned above. We use our approach to trace the co-evolution of fungal, eukaryotic, and mammalian genes at high resolution across the different parts of the corresponding phylogenetic trees. Specifically, we discover regions in the fungi tree that are enriched with positive evolution. We show that metabolic genes exhibit a remarkable level of co-evolution and different patterns of co-evolution in various biological datasets. In addition, we find that protein complexes that are related to gene expression exhibit non-homogenous levels of co-evolution across different parts of the fungi evolutionary line. In the case of mammalian evolution, signaling pathways that are related to neurotransmission exhibit a relatively higher level of co-evolution along the primate subtree.
Conclusions: We show that finding local patterns of co-evolution is a computationally challenging task and we offer novel algorithms that allow us to solve this problem, thus opening a new approach for analyzing the evolution of biological systems.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {BMC Bioinformatics},
	author = {Tuller, Tamir and Felder, Yifat and Kupiec, Martin},
	month = dec,
	year = {2010},
	pages = {43},
	file = {1471-2105-11-43.pdf:/Users/bill/D/Zotero/storage/CQEU26YC/1471-2105-11-43.pdf:application/pdf},
}

@article{amesHowFindHidden,
	title = {How to find a hidden clique},
	language = {en},
	author = {Ames, Brendan},
	pages = {35},
	file = {2013-08-16_MOPTA.pdf:/Users/bill/D/Zotero/storage/6XDB2QB9/2013-08-16_MOPTA.pdf:application/pdf},
}

@misc{sutton,
	title = {Towards the {Rigorous} {Analysis} of {Evolutionary} {Algorithms} on {Random} k-{Satisfiability} {Formulas}},
	language = {en},
	author = {Sutton, Andrew M},
	keywords = {slides},
	file = {AMS_220812.pdf:/Users/bill/D/Zotero/storage/EY5RSBT9/AMS_220812.pdf:application/pdf},
}

@article{berthet2015acms,
	title = {Optimal {Testing} for {Planted} {Satisfiability} {Problems}},
	url = {http://arxiv.org/abs/1401.2205},
	abstract = {We study the problem of detecting planted solutions in a random satisﬁability formula. Adopting the formalism of hypothesis testing in statistical analysis, we describe the minimax optimal rates. Our analysis relies on the study of the number of satisfying assignments, for which we prove new results. We also address algorithmic issues, and describe the performance of a new computationally eﬃcient test. This result is compared to a related hypothesis on the hardness of detecting satisﬁability of random formulas.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1401.2205 [cs, math, stat]},
	author = {Berthet, Quentin},
	month = feb,
	year = {2015},
	note = {arXiv: 1401.2205},
	keywords = {bdpg, Mathematics - Probability, Computer Science - Computational Complexity, Mathematics - Statistics Theory, planted solution, satisfiability},
	file = {berthet 2014 - Optimal Testing for Planted Satisfiability Problems.pdf:/Users/bill/D/Zotero/storage/AZCEDL73/berthet 2014 - Optimal Testing for Planted Satisfiability Problems.pdf:application/pdf},
}

@incollection{amiriPushingRandomWalk2007,
	address = {Berlin, Heidelberg},
	title = {Pushing {Random} {Walk} {Beyond} {Golden} {Ratio}},
	volume = {4649},
	isbn = {978-3-540-74509-9 978-3-540-74510-5},
	url = {http://link.springer.com/10.1007/978-3-540-74510-5_8},
	abstract = {We propose a simple modiﬁcation of a well-known Random Walk algorithm for solving the Satisﬁability problem and analyze its performance on random CNFs with a planted solution. We rigorously prove that the new algorithm solves the Full CNF with high probability, and for random CNFs with a planted solution of high density ﬁnds an assignment that diﬀers from the planted in only ε-fraction of variables. In the experiments the algorithm solves random CNFs with a planted solution of any density.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Computer {Science} – {Theory} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Amiri, Ehsan and Skvortsov, Evgeny},
	editor = {Diekert, Volker and Volkov, Mikhail V. and Voronkov, Andrei},
	year = {2007},
	doi = {10.1007/978-3-540-74510-5_8},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {44--55},
	file = {chp%3A10.1007%2F978-3-540-74510-5_8.pdf:/Users/bill/D/Zotero/storage/63XJBV7G/chp%3A10.1007%2F978-3-540-74510-5_8.pdf:application/pdf},
}

@incollection{yamamoto2007aac,
	address = {Berlin, Heidelberg},
	title = {A {Spectral} {Method} for {MAX2SAT} in the {Planted} {Solution} {Model}},
	volume = {4835},
	isbn = {978-3-540-77118-0},
	url = {http://link.springer.com/10.1007/978-3-540-77120-3_12},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Algorithms and {Computation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Yamamoto, Masaki},
	editor = {Tokuyama, Takeshi},
	year = {2007},
	doi = {10.1007/978-3-540-77120-3_12},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {bdpg, planted solution},
	pages = {112--123},
	file = {chp%3A10.1007%2F978-3-540-77120-3_12.pdf:/Users/bill/D/Zotero/storage/RK3KFAXI/chp%3A10.1007%2F978-3-540-77120-3_12.pdf:application/pdf},
}

@article{berkePropagationConnectivityRandom,
	title = {Propagation {Connectivity} of {Random} {Hypergraphs}},
	abstract = {We consider the average-case MLS-3LIN problem, the problem of ﬁnding a most likely solution for a given system of perturbed 3LIN-equations generated with equation probability p and perturbation probability q. Our purpose is to investigate the situation for certain message passing algorithms to work for this problem whp We consider the case q = 0 (no perturbation occurs) and analyze the execution of (a simple version of) our algorithm. For characterizing problem instances for which the execution succeeds, we consider their natural 3-uniform hypergraph representation and introduce the notion of “propagation connectivity”, a generalized connectivity property on 3-uniform hypergraphs. The execution succeeds on a given system of 3LIN-equations if and only if the representing hypergraph is propagation connected. We show upper and lower bounds for equation probability p such that propagation connectivity holds whp on random hypergraphs representing MLS-3LIN instances.},
	language = {en},
	author = {Berke, Robert and Onsjo, Mikael},
	pages = {10},
	file = {chp%3A10.1007%2F978-3-642-04944-6_10.pdf:/Users/bill/D/Zotero/storage/T2UJQYEY/chp%3A10.1007%2F978-3-642-04944-6_10.pdf:application/pdf},
}

@article{watanabeAverageCaseAnalysisMAX2SAT,
	title = {Average-{Case} {Analysis} for the {MAX}-{2SAT} {Problem}},
	abstract = {We propose a “planted solution model” for discussing the average-case complexity of the MAX-2SAT problem. We show that for a large range of parameters, the planted solution (more precisely, one of the planted solution pair) is the optimal solution for the generated instance with high probability. We then give a simple linear time algorithm based on a message passing method, and we prove that it solves the MAX-2SAT problem with high probability under our planted solution model.},
	language = {en},
	author = {Watanabe, Osamu and Yamamoto, Masaki},
	pages = {6},
	file = {chp%3A10.1007%2F11814948_27.pdf:/Users/bill/D/Zotero/storage/VY95BPTW/chp%3A10.1007%2F11814948_27.pdf:application/pdf},
}

@incollection{onsjoSimpleMessagePassing2006,
	address = {Berlin, Heidelberg},
	title = {A {Simple} {Message} {Passing} {Algorithm} for {Graph} {Partitioning} {Problems}},
	volume = {4288},
	isbn = {978-3-540-49694-6 978-3-540-49696-0},
	url = {http://link.springer.com/10.1007/11940128_51},
	abstract = {Motivated by the belief propagation, we propose a simple and deterministic message passing algorithm for the Graph Bisection problem and related problems. The running time of the main algorithm is linear w.r.t. the number of vertices and edges. For evaluating its average-case correctness, planted solution models are used. For the Graph Bisection problem under the standard planted solution model with probability parameters p and r, we prove that our algorithm yields a planted solution with probability {\textgreater} 1 − δ if p − r = Ω(n−1/2 log(n/δ)).},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Algorithms and {Computation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Onsjö, Mikael and Watanabe, Osamu},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Asano, Tetsuo},
	year = {2006},
	doi = {10.1007/11940128_51},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {507--516},
	file = {chp%3A10.1007%2F11940128_51.pdf:/Users/bill/D/Zotero/storage/I44BLF2D/chp%3A10.1007%2F11940128_51.pdf:application/pdf},
}

@book{koolenHedgingStructuredConcepts2010,
	title = {Hedging structured concepts},
	isbn = {978-0-9822529-2-5},
	abstract = {We develop an online algorithm called Component Hedge for learning structured concept classes when the loss of a structured concept sums over its components. Example classes include paths through a graph (composed of edges) and partial permutations (composed of assignments). The algorithm maintains a parameter vector with one non-negative weight per component, which always lies in the convex hull of the structured concept class. The algorithm predicts by decomposing the current parameter vector into a convex combination of concepts and choosing one of those concepts at random. The parameters are updated by first performing a multiplicative update and then projecting back into the convex hull. We show that Component Hedge has optimal regret bounds for a large variety of structured concept classes.},
	language = {en},
	publisher = {Omnipress},
	author = {Koolen, W.M and Warmuth, M.K and Kivinen, J and Kalai, A.T and Mohri, M},
	year = {2010},
	note = {OCLC: 6899968046},
	file = {christiano14.pdf:/Users/bill/D/Zotero/storage/TKY963RU/christiano14.pdf:application/pdf},
}

@article{dimitriou,
	title = {On {SAT} {Distributions} with {Planted} {Assignments} ({Extended} {Abstract})},
	abstract = {While it is known how to generate satisfiable instances by reducing certain computational problems to SAT, it is not known how a similar generator can be developed directly for k-SAT. In this work we almost answer this question affirmatively by improving upon previous results in many ways. First, we give a generator for instances of MAX k-SAT, the version of k-SAT where one wants to maximize the number of satisfied clauses. Second, we provide a useful characterization of the optimal solution. In our model not only we know how the optimal solution looks like but we also prove it is unique. Finally, we show that our generator has certain useful computational properties among which is the ability to control the hardness of the generated instances, the appearance of an easy-hard-easy pattern in the search complexity for good assignments and a new type of phase transition which is related to the uniqueness of the optimal solution.},
	language = {en},
	author = {Dimitriou, Tassos},
	keywords = {bdpg, planted solution},
	pages = {6},
	file = {dimitriou - On SAT Distributions with Planted Assignments - Extended Abstract.pdf:/Users/bill/D/Zotero/storage/ZURL45N2/dimitriou - On SAT Distributions with Planted Assignments - Extended Abstract.pdf:application/pdf},
}

@article{etesamiPseudorandomnessDepth2Circuits,
	title = {Pseudorandomness against {Depth}-2 {Circuits} and {Analysis} of {Goldreich}’s {Candidate} {One}-{Way} {Function}},
	language = {en},
	author = {Etesami, Seyed Omid},
	pages = {84},
	file = {EECS-2010-180.pdf:/Users/bill/D/Zotero/storage/JBJGXTCJ/EECS-2010-180.pdf:application/pdf},
}

@article{berthet2015ejs,
	title = {Optimal testing for planted satisfiability problems},
	volume = {9},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1425398209},
	doi = {10.1214/15-EJS1001},
	abstract = {We study the problem of detecting planted solutions in a random satisﬁability formula. Adopting the formalism of hypothesis testing in statistical analysis, we describe the minimax optimal rates of detection. Our analysis relies on the study of the number of satisfying assignments, for which we prove new results. We also address algorithmic issues, and give a computationally eﬃcient test with optimal statistical performance. This result is compared to an average-case hypothesis on the hardness of refuting satisﬁability of random formulas.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Electronic Journal of Statistics},
	author = {Berthet, Quentin},
	year = {2015},
	keywords = {bdpg, planted solution, satisfiability},
	pages = {298--317},
	file = {Berthet_2015_Optimal testing for planted satisfiability problems.pdf:/Users/bill/D/Zotero/storage/Z6FGBAKF/Berthet_2015_Optimal testing for planted satisfiability problems.pdf:application/pdf},
}

@inproceedings{krivelevichSolvingRandomSatisfiable2006,
	address = {Miami, Florida},
	title = {Solving random satisfiable {3CNF} formulas in expected polynomial time},
	isbn = {978-0-89871-605-4},
	url = {http://portal.acm.org/citation.cfm?doid=1109557.1109608},
	doi = {10.1145/1109557.1109608},
	abstract = {We present an algorithm for solving 3SAT instances. Several algorithms have been proved to work whp (with high probability) for various SAT distributions. However, an algorithm that works whp has a drawback. Indeed for typical instances it works well, however for some rare inputs it does not provide a solution at all. Alternatively, one could require that the algorithm always produce a correct answer but perform well on average. Expected polynomial time formalizes this notion. We prove that for some natural distribution on 3CNF formulas, called planted 3SAT, our algorithm has expected polynomial (in fact, almost linear) running time. The planted 3SAT distribution is the set of satisﬁable 3CNF formulas generated in the following manner. First, a truth assignment is picked uniformly at random. Then, each clause satisﬁed by it is included in the formula with probability p. Extending previous work for the planted 3SAT distribution, we present, for the ﬁrst time for a satisﬁable SAT distribution, an expected polynomial time algorithm. Namely, it solves all 3SAT instances, and over the planted distribution (with p = d/n2, d {\textgreater} 0 a suﬃciently large constant) it runs in expected polynomial time. Our results extend to k-SAT for any constant k.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the seventeenth annual {ACM}-{SIAM} symposium on {Discrete} algorithm  - {SODA} '06},
	publisher = {ACM Press},
	author = {Krivelevich, Michael and Vilenchik, Dan},
	year = {2006},
	pages = {454--463},
	file = {ExpectedPoly3SAT.pdf:/Users/bill/D/Zotero/storage/SFWTMDNT/ExpectedPoly3SAT.pdf:application/pdf},
}

@article{feigeRigorousAnalysisHeuristics,
	title = {Rigorous {Analysis} of {Heuristics} for {NP}-hard {Problems}},
	abstract = {The known NP-hardness results imply that for many combinatorial optimization problems there are no eﬃcient algorithms that ﬁnd an optimal solution, or even a near optimal solution, on every instance. A heuristic for an NP-hard problem is a polynomial time algorithm that produces optimal or near optimal solutions on some input instances, but may fail on others. The study of heuristics involves both an algorithmic issue (the design of the heuristic algorithm) and a conceptual challenge, namely, how does one evaluate the quality of a heuristic. Current methods for evaluating heuristics include experimental evidence, hand waving arguments, and rigorous analysis of the performance of the heuristic on some wide (in a sense that depends on the context) classes of inputs. This talk is concerned with the latter method. On the conceptual side, several frameworks that have been used in order to model the classes of inputs of interest (including random models, semi-random models, smoothed analysis) will be discussed. On the algorithmic side, several algorithmic techniques and principles of analysis that are often useful in these frameworks will be presented.},
	language = {en},
	author = {Feige, Uriel},
	pages = {10},
	file = {japan.pdf:/Users/bill/D/Zotero/storage/UXPJZC9D/japan.pdf:application/pdf},
}

@article{krzakala2009prla,
	title = {Hiding {Quiet} {Solutions} in {Random} {Constraint} {Satisfaction} {Problems}},
	volume = {102},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/0901.2130},
	doi = {10.1103/PhysRevLett.102.238701},
	abstract = {We study constraint satisfaction problems on the so-called 'planted' random ensemble. We show that for a certain class of problems, e.g. graph coloring, many of the properties of the usual random ensemble are quantitatively identical in the planted random ensemble. We study the structural phase transitions, and the easy/hard/easy pattern in the average computational complexity. We also discuss the finite temperature phase diagram, finding a close connection with the liquid/glass/solid phenomenology.},
	language = {en},
	number = {23},
	urldate = {2020-11-04},
	journal = {Physical Review Letters},
	author = {Krzakala, Florent and Zdeborová, Lenka},
	month = jun,
	year = {2009},
	note = {arXiv: 0901.2130},
	keywords = {problem difficulty, bdpg, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, bdpg\_P1, planted solution, planted problem},
	pages = {238701},
	annote = {Comment: 4 pages, 3 figures},
	file = {krzakala et al 2009 - hiding quiet solutions in random constraint satisfaction problems.pdf:/Users/bill/D/Zotero/storage/59H2EIW3/krzakala et al 2009 - hiding quiet solutions in random constraint satisfaction problems.pdf:application/pdf},
}

@article{roughgardenCS369NWorstCaseAnalysis,
	title = {{CS369N}: {Beyond} {Worst}-{Case} {Analysis} {Lecture} \#4: {Probabilistic} and {Semirandom} {Models} for {Clustering} and {Graph} {Partitioning}},
	language = {en},
	author = {Roughgarden, Tim},
	pages = {12},
	file = {l4.pdf:/Users/bill/D/Zotero/storage/5HDX7YT3/l4.pdf:application/pdf},
}

@article{henInformationSciencesInstitute,
	title = {Information {Sciences} {Institute}},
	language = {en},
	author = {Hen, Itay},
	pages = {43},
	file = {lucas2.pdf:/Users/bill/D/Zotero/storage/QGY2VE72/lucas2.pdf:application/pdf},
}

@article{alekhnovichMoreAverageCase2003,
	title = {More on average case vs approximation complexity},
	language = {en},
	journal = {th Annual IEEE Symposium on Foundations of Computer Science},
	author = {Alekhnovich, Michael},
	year = {2003},
	pages = {10},
	file = {misha.pdf:/Users/bill/D/Zotero/storage/4WBJ4A74/misha.pdf:application/pdf},
}

@inproceedings{applebaumPublickeyCryptographyDifferent2010,
	address = {Cambridge, Massachusetts, USA},
	title = {Public-key cryptography from different assumptions},
	isbn = {978-1-4503-0050-6},
	url = {http://dl.acm.org/citation.cfm?doid=1806689.1806715},
	doi = {10.1145/1806689.1806715},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 42nd {ACM} symposium on {Theory} of computing - {STOC} '10},
	publisher = {ACM Press},
	author = {Applebaum, Benny and Barak, Boaz and Wigderson, Avi},
	year = {2010},
	pages = {171},
	file = {ncpkcFull1.pdf:/Users/bill/D/Zotero/storage/WYJWAT67/ncpkcFull1.pdf:application/pdf},
}

@inproceedings{makarychevSortingNoisyData2013,
	address = {Berkeley, California, USA},
	title = {Sorting noisy data with partial information},
	isbn = {978-1-4503-1859-4},
	url = {http://dl.acm.org/citation.cfm?doid=2422436.2422492},
	doi = {10.1145/2422436.2422492},
	abstract = {In this paper, we propose two semi-random models for the Minimum Feedback Arc Set Problem and present approximation algorithms for them. In the ﬁrst model, which we call the Random Edge Flipping model, an instance is generated as follows. We start with an arbitrary acyclic directed graph and then randomly ﬂip its edges (the adversary may later un-ﬂip some of them). In the second model, which we call the Random Backward Edge model, again we start with an arbitrary acyclic graph but now add new random backward edges (the adversary may delete some of them). For the ﬁrst model, we give an approximation algorithm that ﬁnds a solution of cost (1 + δ) opt-cost +n polylog n, where opt-cost is the cost of the optimal solution. For the second model, we give an approximation algorithm that ﬁnds a solution of cost O(planted-cost)+n polylog n, where planted-cost is the cost of the planted solution.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 4th conference on {Innovations} in {Theoretical} {Computer} {Science} - {ITCS} '13},
	publisher = {ACM Press},
	author = {Makarychev, Konstantin and Makarychev, Yury and Vijayaraghavan, Aravindan},
	year = {2013},
	pages = {515},
	file = {noisy_FAS.pdf:/Users/bill/D/Zotero/storage/NVJGYUWM/noisy_FAS.pdf:application/pdf},
}

@article{angeliniStatisticalPhysicsInference,
	title = {Statistical {Physics} of {Inference} problems},
	language = {en},
	author = {Angelini, Maria Chiara and Caltagirone, Francesco and Krzakala, Florent},
	pages = {36},
	file = {notes.pdf:/Users/bill/D/Zotero/storage/HAEZUZ5P/notes.pdf:application/pdf},
}

@article{edmonds,
	title = {Search {Time} when {Solving} {Random} {Jigsaw} {Puzzles} with a {Planted} {Solution}},
	abstract = {John Tsotsos asked his AI students to work on having an AI solve a jigsaw puzzle, though it is NP-complete. This paper considers the search time of the recursive backtracking algorithm for solving it when the puzzle pieces are generated randomly with a planted solution. Feeling that a jigsaw puzzle should have a particular image when completed, we set the probability that two pieces ﬁt together locally to be just small enough that with high probability the planted solution is unique. We were surprised to see that in this case the recursive backtracking algorithm which expands a rectangle from a corner has expected 1 + ǫ branching width. In contrast, this width is exponential if it attempts to put together just the edge pieces into a frame, build a triangle from a corner, or build a block in the middle. If the probability of pieces ﬁtting together is increased just to the point that there is an exponential number of complete solutions, then no matter which order the pieces are put together it takes an exponential amount of time to ﬁnd one of these exponentially many solutions. It is also interesting that for the diﬀerent orders of completing the puzzle, there are very diﬀerent “reasons” for the exponential blowup. We coded the algorithm and it ran just as described.},
	language = {en},
	author = {Edmonds, Jeﬀ and Edmonds, Alex},
	keywords = {planted solution},
	pages = {14},
	file = {puzzle.pdf:/Users/bill/D/Zotero/storage/TZY6XBVV/puzzle.pdf:application/pdf},
}

@article{krivelevichRandomSatisfiableProcess2009,
	title = {On the {Random} {Satisfiable} {Process}},
	volume = {18},
	issn = {0963-5483, 1469-2163},
	url = {https://www.cambridge.org/core/product/identifier/S0963548309990356/type/journal_article},
	doi = {10.1017/S0963548309990356},
	abstract = {In this work we suggest a new model for generating random satisfiable
              k
              -CNF formulas. To generate such formulas. randomly permute all
              
                \$2{\textasciicircum}k{\textbackslash}binom\{n\}\{k\}\$
              
              possible clauses over the variables
              x
              1
              ,. . .,
              
                x
                n
              
              , and starting from the empty formula, go over the clauses one by one, including each new clause as you go along if, after its addition, the formula remains satisfiable. We study the evolution of this process, namely the distribution over formulas obtained after scanning through the first
              m
              clauses (in the random permutation's order).
            
            
              Random processes with conditioning on a certain property being respected are widely studied in the context of graph properties. This study was pioneered by Ruciński and Wormald in 1992 for graphs with a fixed degree sequence, and also by Erdős, Suen and Winkler in 1995 for triangle-free and bipartite graphs. Since then many other graph properties have been studied, such as planarity and
              H
              -freeness. Thus our model is a natural extension of this approach to the satisfiability setting.
            
            
              Our main contribution is as follows. For
              m
              ≥
              cn
              ,
              c
              =
              c
              (
              k
              ) a sufficiently large constant, we are able to characterize the structure of the solution space of a typical formula in this distribution. Specifically, we show that typically all satisfying assignments are essentially clustered in one cluster, and all but
              e
              
                −Ω(
                m
                /
                n
                )
              
              n
              of the variables take the same value in all satisfying assignments. We also describe a polynomial-time algorithm that finds w.h.p. a satisfying assignment for such formulas.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Combinatorics, Probability and Computing},
	author = {Krivelevich, Michael and Sudakov, Benny and Vilenchik, Dan},
	month = sep,
	year = {2009},
	pages = {775--801},
	file = {random-sat-model.pdf:/Users/bill/D/Zotero/storage/6SFSYIR5/random-sat-model.pdf:application/pdf},
}

@inproceedings{dinurComputationalBenefitCorrelated2015,
	address = {Rehovot, Israel},
	title = {The {Computational} {Benefit} of {Correlated} {Instances}},
	isbn = {978-1-4503-3333-7},
	url = {http://dl.acm.org/citation.cfm?doid=2688073.2688082},
	doi = {10.1145/2688073.2688082},
	abstract = {The starting point of this paper is that instances of computational problems often do not exist in isolation. Rather, multiple and correlated instances of the same problem arise naturally in the real world. The challenge is how to gain computationally from instance correlations when they exist. We will be interested in settings where signiﬁcant computational gain can be made in solving a single primary instance by having access to additional auxiliary instances which are correlated to the primary instance via the solution space.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 2015 {Conference} on {Innovations} in {Theoretical} {Computer} {Science} - {ITCS} '15},
	publisher = {ACM Press},
	author = {Dinur, Irit and Goldwasser, Shafi and Lin, Huijia},
	year = {2015},
	pages = {219--228},
	file = {TR14-083.pdf:/Users/bill/D/Zotero/storage/MPPCMSNJ/TR14-083.pdf:application/pdf},
}

@article{kollaPlayingRandomExpanding,
	title = {Playing {Random} and {Expanding} {Unique} {Games}},
	abstract = {In this work, we present a spectral algorithm that ﬁnds good assignments for instances of Unique Games when the underlying graph has some signiﬁcant expansion and the constraints are arbitrary Γ-max-lin.},
	language = {en},
	author = {Kolla, Alexandra and Tulsiani, Madhur},
	pages = {17},
	file = {UGspec.pdf:/Users/bill/D/Zotero/storage/DULAJRFM/UGspec.pdf:application/pdf},
}

@article{zdeborova2011sjdm,
	title = {Quiet {Planting} in the {Locked} {Constraint} {Satisfaction} {Problems}},
	volume = {25},
	issn = {0895-4801, 1095-7146},
	url = {http://arxiv.org/abs/0902.4185},
	doi = {10.1137/090750755},
	abstract = {We study the planted ensemble of locked constraint satisfaction problems. We describe the connection between the random and planted ensembles. The use of the cavity method is combined with arguments from reconstruction on trees and the ﬁrst and the second moment considerations. Our main result is the location of the hard region in the planted ensemble. In a part of that hard region instances have with high probability a single satisfying assignment.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {SIAM Journal on Discrete Mathematics},
	author = {Zdeborová, Lenka and Krzakala, Florent},
	month = jan,
	year = {2011},
	note = {arXiv: 0902.4185},
	keywords = {problem difficulty, bdpg, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Computer Science - Computational Complexity, bdpg\_P1, planted solution, planted problem},
	pages = {750--770},
	annote = {Comment: 21 pages, revised version},
	file = {zdeborova krzakala 2010 - QUIET PLANTING IN THE LOCKED CONSTRAINT SATISFACTION PROBLEMS.pdf:/Users/bill/D/Zotero/storage/9S5M9MFH/zdeborova krzakala 2010 - QUIET PLANTING IN THE LOCKED CONSTRAINT SATISFACTION PROBLEMS.pdf:application/pdf},
}

@techreport{kaminerApplyingGAsSearching,
	title = {Applying {GAs} in {Searching} {Motif} {Patterns} in {Gene} {Expression} {Data}},
	author = {Kaminer, Tal and Laor, Nir},
	file = {MotifDensity.pdf:/Users/bill/D/Zotero/storage/RQ3H4MAC/MotifDensity.pdf:application/pdf},
}

@article{vanhemertEvolvingCombinatorialProblem2006,
	title = {Evolving {Combinatorial} {Problem} {Instances} {That} {Are} {Difficult} to {Solve}},
	volume = {14},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco.2006.14.4.433},
	doi = {10.1162/evco.2006.14.4.433},
	abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difﬁcult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisﬁability, and the travelling salesman problem. Problem instances acquired through this technique are more difﬁcult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difﬁculty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {van Hemert, Jano I.},
	month = dec,
	year = {2006},
	pages = {433--462},
	file = {EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:/Users/bill/D/Zotero/storage/PN7Q6JNG/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:application/pdf},
}

@article{hallGeneratingExperimentalData2001,
	title = {Generating {Experimental} {Data} for {Computational} {Testing} with {Machine} {Scheduling} {Applications}},
	volume = {49},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.49.6.854.10014},
	doi = {10.1287/opre.49.6.854.10014},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	month = dec,
	year = {2001},
	pages = {854--865},
	file = {hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf:/Users/bill/D/Zotero/storage/35ZZ3MGH/hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf:application/pdf},
}

@article{hallPerformancePredictionPreselection2007,
	title = {Performance {Prediction} and {Preselection} for {Optimization} and {Heuristic} {Solution} {Procedures}},
	volume = {55},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0398},
	doi = {10.1287/opre.1070.0398},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	month = aug,
	year = {2007},
	pages = {703--716},
	file = {hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf:/Users/bill/D/Zotero/storage/QRSHQTFI/hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf:application/pdf},
}

@article{hansenNeuralNetworkEnsembles1990,
	title = {Neural network ensembles},
	volume = {12},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/58871/},
	doi = {10.1109/34.58871},
	abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hansen, L.K. and Salamon, P.},
	month = oct,
	year = {1990},
	pages = {993--1001},
	file = {hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:/Users/bill/D/Zotero/storage/KHJZ2P5K/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:application/pdf},
}

@article{langfordCanWeBetter,
	title = {Can {We} do {Better} than {R}-squared? {\textbar} {Tom} {Hopper}},
	language = {en},
	author = {Langford, Bill},
	pages = {8},
	file = {hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf:/Users/bill/D/Zotero/storage/4R5L87S9/hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf:application/pdf},
}

@article{romeijnGeneratingExperimentalData2001,
	title = {Generating {Experimental} {Data} for the {Generalized} {Assignment} {Problem}},
	volume = {49},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.49.6.866.10021},
	doi = {10.1287/opre.49.6.866.10021},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Romeijn, H. Edwin and Romero Morales, Dolores},
	month = dec,
	year = {2001},
	pages = {866--878},
	file = {romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf:/Users/bill/D/Zotero/storage/TVJUSSZJ/romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf:application/pdf},
}

@article{smith-milesMeasuringInstanceDifficulty2012,
	title = {Measuring instance difficulty for combinatorial optimization problems},
	volume = {39},
	issn = {03050548},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054811001997},
	doi = {10.1016/j.cor.2011.07.006},
	abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies – studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances – provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difﬁculty in terms of algorithmic performance, and how such features can be deﬁned and measured for various optimization problems. We provide a comprehensive survey of the research ﬁeld with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems – which are important abstractions of many real-world problems – we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Lopes, Leo},
	month = may,
	year = {2012},
	pages = {875--889},
	file = {smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:/Users/bill/D/Zotero/storage/6FCV7WIL/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:application/pdf},
}

@article{hoggRefiningPhaseTransition1996,
	title = {Refining the phase transition in combinatorial search},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000437029500050X},
	doi = {10.1016/0004-3702(95)00050-X},
	abstract = {Many recent studies have identified phase transitions from under- to overconstrained instances in classes of constraint satisfaction search problems, and their relation to search cost. For example, cases that are difficult to solve with a variety of search methods are concentrated near these transitions. These studies show that a simple parameter describing the problem structure predicts the difficulty of solving the problem, on average. However, this prediction is associated with a large variance and depends on the somewhat arbitrary choice of the problem class. Thus these results are of limited direct use for individual instances. To help address this limitation, additional parameters, describing problem structure as well as heuristic effectiveness, are introduced. These are shown to reduce the variation in some cases. This also provides further insight into the nature of intrinsically hard search problems.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Hogg, Tad},
	month = mar,
	year = {1996},
	pages = {127--154},
	file = {1-s2.0-000437029500050X-main.pdf:/Users/bill/D/Zotero/storage/WG2S8HMK/1-s2.0-000437029500050X-main.pdf:application/pdf},
}

@article{hoggHardestConstraintProblems1994,
	title = {The hardest constraint problems: {A} double phase transition},
	volume = {69},
	issn = {00043702},
	shorttitle = {The hardest constraint problems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370294900884},
	doi = {10.1016/0004-3702(94)90088-4},
	abstract = {The distribution of hard graph coloring problems as a function of graph connectivity is shown to have two distinct transition behaviors. The first, previously recognized, is a peak in the median search cost near the connectivity at which half the graphs have solutions. This region contains a high proportion of relatively hard problem instances. However, the hardest instances are in fact concentrated at a second, lower, transition point. Near this point, most problems are quite easy, but there are also a few very hard cases. This region of exceptionally hard problems corresponds to the transition between polynomial and exponential scaling of the average search cost, whose location we also estimate theoretically. These behaviors also appear to arise in other constraint problems. This work also shows the limitations of simple measures of the cost distribution, such as mean or median, for identifying outlying cases.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Hogg, Tad and Williams, Colin P.},
	month = sep,
	year = {1994},
	pages = {359--377},
	file = {1-s2.0-0004370294900884-main.pdf:/Users/bill/D/Zotero/storage/WRGC3HHR/1-s2.0-0004370294900884-main.pdf:application/pdf},
}

@article{hoggPhaseTransitionsSearch1996,
	title = {Phase transitions and the search problem},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000445},
	doi = {10.1016/0004-3702(95)00044-5},
	abstract = {We describe how techniques that were originally developed in statistical mechanics can be applied to search problems that arise commonly in artificial intelligence. This approach is useful for understanding the typical behavior of classes of problems. In particular, these techniques predict that abrupt changes in computational cost, analogous to physical phase transitions, should occur universally, as heuristic effectiveness or search space topology is varied. We also present a number of open questions raised by these studies.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Hogg, Tad and Huberman, Bernardo A. and Williams, Colin P.},
	month = mar,
	year = {1996},
	keywords = {Hogg},
	pages = {1--15},
	file = {1-s2.0-0004370295000445-main.pdf:/Users/bill/D/Zotero/storage/4J2YTHBW/1-s2.0-0004370295000445-main.pdf:application/pdf},
}

@article{leyton-brownIncentiveMechanismsSmoothing2003,
	title = {Incentive mechanisms for smoothing out a focused demand for network resources},
	volume = {26},
	issn = {01403664},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140366402001391},
	doi = {10.1016/S0140-3664(02)00139-1},
	abstract = {We explore the problem of sharing network resources when users’ preferences lead to temporally concentrated loads, resulting in an inefﬁcient use of the network. In such cases external incentives can be supplied to smooth out demand, obviating the need for expensive technological mechanisms. Taking a game-theoretic approach, we consider a setting in which bandwidth or access to service is available during different time slots at a ﬁxed cost, but all agents have a natural preference for choosing the same time slot. We present four mechanisms that motivate users to distribute the load by probabilistically waiving the cost for each time slot, and analyze the equilibria that arise under these mechanisms.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Computer Communications},
	author = {Leyton-Brown, Kevin and Porter, Ryan and Prabhakar, Balaji and Shoham, Yoav and Venkataraman, Shobha},
	month = feb,
	year = {2003},
	keywords = {Leyton-Brown},
	pages = {237--250},
	file = {03incentivemechanisms.pdf:/Users/bill/D/Zotero/storage/FIDGI83P/03incentivemechanisms.pdf:application/pdf},
}

@incollection{hutterPerformancePredictionAutomated2006,
	address = {Berlin, Heidelberg},
	title = {Performance {Prediction} and {Automated} {Tuning} of {Randomized} and {Parametric} {Algorithms}},
	volume = {4204},
	isbn = {978-3-540-46267-5 978-3-540-46268-2},
	url = {http://link.springer.com/10.1007/11889205_17},
	abstract = {Machine learning can be utilized to build models that predict the runtime of search algorithms for hard combinatorial problems. Such empirical hardness models have previously been studied for complete, deterministic search algorithms. In this work, we demonstrate that such models can also make surprisingly accurate predictions of the run-time distributions of incomplete and randomized search methods, such as stochastic local search algorithms. We also show for the ﬁrst time how information about an algorithm’s parameter settings can be incorporated into a model, and how such models can be used to automatically adjust the algorithm’s parameters on a per-instance basis in order to optimize its performance. Empirical results for Novelty+ and SAPS on structured and unstructured SAT instances show very good predictive performance and signiﬁcant speedups of our automatically determined parameter settings when compared to the default and best ﬁxed distribution-speciﬁc parameter settings.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} - {CP} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hamadi, Youssef and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Benhamou, Frédéric},
	year = {2006},
	doi = {10.1007/11889205_17},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Leyton-Brown},
	pages = {213--228},
	file = {0912f508ebd0e0b6a0000000.pdf:/Users/bill/D/Zotero/storage/NR22CAAD/0912f508ebd0e0b6a0000000.pdf:application/pdf},
}

@article{bhatComputingNashEquilibria2004,
	title = {Computing {Nash} {Equilibria} of {Action}-{Graph} {Games}},
	abstract = {Action-graph games (AGGs) are a fully expressive game representation which can compactly express both strict and context-speciﬁc independence between players’ utility functions. Actions are represented as nodes in a graph G, and the payoff to an agent who chose the action s depends only on the numbers of other agents who chose actions connected to s. We present algorithms for computing both symmetric and arbitrary equilibria of AGGs using a continuation method. We analyze the worst-case cost of computing the Jacobian of the payoff function, the exponential-time bottleneck step, and in all cases achieve exponential speedup. When the indegree of G is bounded by a constant and the game is symmetric, the Jacobian can be computed in polynomial time.},
	language = {en},
	author = {Bhat, Navin A R and Leyton-Brown, Kevin},
	year = {2004},
	keywords = {Leyton-Brown},
	pages = {8},
	file = {1207.4128.pdf:/Users/bill/D/Zotero/storage/9ZWM29GN/1207.4128.pdf:application/pdf},
}

@article{thorntonAutoWEKACombinedSelection2013,
	title = {Auto-{WEKA}: {Combined} {Selection} and {Hyperparameter} {Optimization} of {Classification} {Algorithms}},
	shorttitle = {Auto-{WEKA}},
	url = {http://arxiv.org/abs/1208.3719},
	abstract = {Many diﬀerent machine learning algorithms exist; taking into account each algorithm’s hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Speciﬁcally, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classiﬁcation approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classiﬁers, and hyperparameter settings for each classiﬁer. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classiﬁcation performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more eﬀectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1208.3719 [cs]},
	author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	month = mar,
	year = {2013},
	note = {arXiv: 1208.3719},
	keywords = {Computer Science - Machine Learning, Leyton-Brown},
	annote = {Comment: 9 pages, 3 figures},
	file = {1208.3719.pdf:/Users/bill/D/Zotero/storage/NJUKHPT9/1208.3719.pdf:application/pdf},
}

@article{hutterAlgorithmRuntimePrediction2013,
	title = {Algorithm {Runtime} {Prediction}: {Methods} \& {Evaluation}},
	shorttitle = {Algorithm {Runtime} {Prediction}},
	url = {http://arxiv.org/abs/1211.0906},
	abstract = {Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm’s runtime as a function of problem-speciﬁc instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic conﬁguration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and—perhaps most importantly—a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisﬁability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1211.0906 [cs, stat]},
	author = {Hutter, Frank and Xu, Lin and Hoos, Holger H. and Leyton-Brown, Kevin},
	month = oct,
	year = {2013},
	note = {arXiv: 1211.0906},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning, Leyton-Brown},
	annote = {Comment: 51 pages, 13 figures, 8 tables. Added references, feature cost, and experiments with subsets of features; reworded Sections 1\&2},
	file = {1211.0906.pdf:/Users/bill/D/Zotero/storage/AN5KSPP2/1211.0906.pdf:application/pdf},
}

@article{hutterBayesianOptimizationCensored2013,
	title = {Bayesian {Optimization} {With} {Censored} {Response} {Data}},
	url = {http://arxiv.org/abs/1310.1947},
	abstract = {Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm conﬁguration problem: ﬁnding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting rightcensored observations can substantially improve the state of the art in model-based algorithm conﬁguration.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1310.1947 [cs, stat]},
	author = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.1947},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Leyton-Brown},
	annote = {Comment: Extended version of NIPS 2011 workshop paper},
	file = {1310.1947.pdf:/Users/bill/D/Zotero/storage/WK7AEIDL/1310.1947.pdf:application/pdf},
}

@article{wrightEquilibriumPredictingHuman,
	title = {Beyond {Equilibrium}: {Predicting} {Human} {Behavior} in {Normal}-{Form} {Games}},
	abstract = {It is standard in multiagent settings to assume that agents will adopt Nash equilibrium strategies. However, studies in experimental economics demonstrate that Nash equilibrium is a poor description of human players’ initial behavior in normal-form games. In this paper, we consider a wide range of widely-studied models from behavioral game theory. For what we believe is the ﬁrst time, we evaluate each of these models in a meta-analysis, taking as our data set large-scale and publicly-available experimental data from the literature. We then propose modiﬁcations to the best-performing model that we believe make it more suitable for practical prediction of initial play by humans in normal-form games.},
	language = {en},
	author = {Wright, James R and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {7},
	file = {1946-8218-1-PB.pdf:/Users/bill/D/Zotero/storage/27TQHIBN/1946-8218-1-PB.pdf:application/pdf},
}

@article{xuPredictingSatisfiabilityPhase,
	title = {Predicting {Satisfiability} at the {Phase} {Transition}},
	abstract = {Uniform random 3-SAT at the solubility phase transition is one of the most widely studied and empirically hardest distributions of SAT instances. For 20 years, this distribution has been used extensively for evaluating and comparing algorithms. In this work, we demonstrate that simple rules can predict the solubility of these instances with surprisingly high accuracy. Speciﬁcally, we show how classiﬁcation accuracies of about 70\% can be obtained based on cheaply (polynomial-time) computable features on a wide range of instance sizes. We argue in two ways that classiﬁcation accuracy does not decrease with instance size: ﬁrst, we show that our models’ predictive accuracy remains roughly constant across a wide range of problem sizes; second, we show that a classiﬁer trained on small instances is sufﬁcient to achieve very accurate predictions across the entire range of instance sizes currently solvable by complete methods. Finally, we demonstrate that a simple decision tree based on only two features, and again trained only on the smallest instances, achieves predictive accuracies close to those of our most complex model. We conjecture that this twofeature model outperforms random guessing asymptotically; due to the model’s extreme simplicity, we believe that this conjecture is a worthwhile direction for future theoretical work.},
	language = {en},
	author = {Xu, Lin and Hoos, Holger H and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {7},
	file = {2012-PredictingSatisfiability.pdf:/Users/bill/D/Zotero/storage/JZHIPAJC/2012-PredictingSatisfiability.pdf:application/pdf},
}

@article{leyton-brownUnderstandingEmpiricalHardness2014,
	title = {Understanding the empirical hardness of \textit{{NP}} -complete problems},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2594413.2594424},
	doi = {10.1145/2594413.2594424},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Communications of the ACM},
	author = {Leyton-Brown, Kevin and Hoos, Holger H. and Hutter, Frank and Xu, Lin},
	month = may,
	year = {2014},
	keywords = {Leyton-Brown},
	pages = {98--107},
	file = {2014-CACM-EHMs.pdf:/Users/bill/D/Zotero/storage/YPXIA7QG/2014-CACM-EHMs.pdf:application/pdf},
}

@article{xuPredictingSatisfiabilityPhasea,
	title = {Predicting {Satisfiability} at the {Phase} {Transition}},
	abstract = {Uniform random 3-SAT at the solubility phase transition is one of the most widely studied and empirically hardest distributions of SAT instances. For 20 years, this distribution has been used extensively for evaluating and comparing algorithms. In this work, we demonstrate that simple rules can predict the solubility of these instances with surprisingly high accuracy. Speciﬁcally, we show how classiﬁcation accuracies of about 70\% can be obtained based on cheaply (polynomial-time) computable features on a wide range of instance sizes. We argue in two ways that classiﬁcation accuracy does not decrease with instance size: ﬁrst, we show that our models’ predictive accuracy remains roughly constant across a wide range of problem sizes; second, we show that a classiﬁer trained on small instances is sufﬁcient to achieve very accurate predictions across the entire range of instance sizes currently solvable by complete methods. Finally, we demonstrate that a simple decision tree based on only two features, and again trained only on the smallest instances, achieves predictive accuracies close to those of our most complex model. We conjecture that this twofeature model outperforms random guessing asymptotically; due to the model’s extreme simplicity, we believe that this conjecture is a worthwhile direction for future theoretical work.},
	language = {en},
	author = {Xu, Lin and Hoos, Holger H and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {7},
	file = {5098-22365-1-PB.pdf:/Users/bill/D/Zotero/storage/4F9UGZD6/5098-22365-1-PB.pdf:application/pdf},
}

@article{leyton-brownEmpiricalHardnessModels2009,
	title = {Empirical hardness models: {Methodology} and a case study on combinatorial auctions},
	volume = {56},
	issn = {0004-5411, 1557-735X},
	shorttitle = {Empirical hardness models},
	url = {https://dl.acm.org/doi/10.1145/1538902.1538906},
	doi = {10.1145/1538902.1538906},
	abstract = {Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Journal of the ACM},
	author = {Leyton-Brown, Kevin and Nudelman, Eugene and Shoham, Yoav},
	month = jun,
	year = {2009},
	keywords = {Leyton-Brown},
	pages = {1--52},
	file = {a22-leyton-brown.pdf:/Users/bill/D/Zotero/storage/C7SU63L6/a22-leyton-brown.pdf:application/pdf},
}

@article{thompsonValuationUncertaintyImperfect,
	title = {Valuation {Uncertainty} and {Imperfect} {Introspection} in {Second}-{Price} {Auctions}},
	abstract = {In auction theory, agents are typically presumed to have perfect knowledge of their valuations. In practice, though, they may face barriers to this knowledge due to transaction costs or bounded rationality. Modeling and analyzing such settings has been the focus of much recent work, though a canonical model of such domains has not yet emerged. We begin by proposing a taxonomy of auction models with valuation uncertainty and showing how it categorizes previous work. We then restrict ourselves to single-good sealed-bid auctions, in which agents have (uncertain) independent private values and can introspect about their own (but not others’) valuations through possibly costly and imperfect queries. We investigate second-price auctions, performing equilibrium analysis for cases with both discrete and continuous valuation distributions. We identify cases where every equilibrium involves either randomized or asymmetric introspection. We contrast the revenue properties of different equilibria, discuss steps the seller can take to improve revenue, and identify a form of revenue equivalence across mechanisms.},
	language = {en},
	author = {Thompson, David R M},
	pages = {6},
	file = {AAAI07-022.pdf:/Users/bill/D/Zotero/storage/ZI9FZGUC/AAAI07-022.pdf:application/pdf},
}

@article{jiangActionGraphGames2011,
	title = {Action-{Graph} {Games}},
	volume = {71},
	issn = {08998256},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0899825610001752},
	doi = {10.1016/j.geb.2010.10.012},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Games and Economic Behavior},
	author = {Jiang, Albert Xin and Leyton-Brown, Kevin and Bhat, Navin A.R.},
	month = jan,
	year = {2011},
	keywords = {Leyton-Brown},
	pages = {141--173},
	file = {AGG.pdf:/Users/bill/D/Zotero/storage/T5HKD2D3/AGG.pdf:application/pdf},
}

@article{nudelmanUnderstandingGameTheoreticAlgorithms,
	title = {Understanding {Game}-{Theoretic} {Algorithms}:},
	language = {en},
	author = {Nudelman, Eugene and Wortman, Jennifer and Shoham, Yoav and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {28},
	file = {GAMUT (GAMES 2004).pdf:/Users/bill/D/Zotero/storage/73Z6DVXK/GAMUT (GAMES 2004).pdf:application/pdf},
}

@article{ryanTutorialRationalGenerating,
	title = {A {Tutorial} on {Rational} {Generating} {Functions}},
	language = {en},
	author = {Ryan, Christopher Thomas and Jiang, Albert Xin and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {6},
	file = {gf_tutorial.pdf:/Users/bill/D/Zotero/storage/V8238PZS/gf_tutorial.pdf:application/pdf},
}

@article{hutterTradeoffsEmpiricalEvaluation2010,
	title = {Tradeoffs in the empirical evaluation of competing algorithm designs},
	volume = {60},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-010-9191-0},
	doi = {10.1007/s10472-010-9191-0},
	abstract = {We propose an empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs. Our approach can provide insight into performance variation both across candidate algorithms and across instances. It can also identify the best tradeoff between evaluating a larger number of candidate algorithm designs, performing these evaluations on a larger number of problem instances, and allocating more time to each algorithm run. We applied our approach to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming, considering six different distributions of problem instances. We demonstrate that the resulting algorithm design scenarios differ in many ways, with important consequences for both automatic and manual algorithm design. We expect that both our methods and our findings will lead to tangible improvements in algorithm design methods.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	month = oct,
	year = {2010},
	keywords = {Leyton-Brown},
	pages = {65--89},
	file = {hutter hoos leyton-brown 2010 - Tradeoffs in the empirical evaluation of competing algorithm designs.pdf:/Users/bill/D/Zotero/storage/W2G8MM3K/hutter hoos leyton-brown 2010 - Tradeoffs in the empirical evaluation of competing algorithm designs.pdf:application/pdf},
}

@article{hutterEfficientApproachAssessing,
	title = {An {Efficient} {Approach} for {Assessing} {Hyperparameter} {Importance}},
	abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efﬁcient methods that can be used to gain such insight, leveraging random forest models ﬁt on the data already gathered by Bayesian optimization. We ﬁrst introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very highdimensional cases—most performance variation is attributable to just a few hyperparameters.},
	language = {en},
	author = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {9},
	file = {hutter14.pdf:/Users/bill/D/Zotero/storage/JLBQSAS3/hutter14.pdf:application/pdf},
}

@article{hutterParamILSAutomaticAlgorithm,
	title = {{ParamILS}: {An} {Automatic} {Algorithm} {Conﬁguration} {Framework}},
	abstract = {The identiﬁcation of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm conﬁguration problem. More formally, we provide methods for optimizing a target algorithm’s performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm conﬁguration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual conﬁgurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the conﬁguration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the ﬁrst published work on automatically conﬁguring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identiﬁed with considerable effort. Nevertheless, using our automated algorithm conﬁguration procedures, we achieved substantial and consistent performance improvements.},
	language = {en},
	author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {40},
	file = {JAIR-3606.pdf:/Users/bill/D/Zotero/storage/SMU7EHDP/JAIR-3606.pdf:application/pdf},
}

@article{leyton-brownMechanismDesignAuctions,
	title = {Mechanism {Design} and {Auctions}},
	language = {en},
	author = {Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {176},
	file = {Lecture-2_KLeyton-Brown.pdf:/Users/bill/D/Zotero/storage/X5CB8R5C/Lecture-2_KLeyton-Brown.pdf:application/pdf},
}

@incollection{leyton-brownLearningEmpiricalHardness2002,
	address = {Berlin, Heidelberg},
	title = {Learning the {Empirical} {Hardness} of {Optimization} {Problems}: {The} {Case} of {Combinatorial} {Auctions}},
	volume = {2470},
	isbn = {978-3-540-44120-5 978-3-540-46135-7},
	shorttitle = {Learning the {Empirical} {Hardness} of {Optimization} {Problems}},
	url = {http://link.springer.com/10.1007/3-540-46135-3_37},
	abstract = {We propose a new approach for understanding the algorithm-speciﬁc empirical hardness of N P-Hard problems. In this work we focus on the empirical hardness of the winner determination problem—an optimization problem arising in combinatorial auctions—when solved by ILOG’s CPLEX software. We consider nine widely-used problem distributions and sample randomly from a continuum of parameter settings for each distribution. We identify a large number of distribution-nonspeciﬁc features of data instances and use statistical regression techniques to learn, evaluate and interpret a function from these features to the predicted hardness of an instance.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} - {CP} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Leyton-Brown, Kevin and Nudelman, Eugene and Shoham, Yoav},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Van Hentenryck, Pascal},
	year = {2002},
	doi = {10.1007/3-540-46135-3_37},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Leyton-Brown},
	pages = {556--572},
	file = {leyton-brown et al 2002 - Learning the Empirical Hardness of Optimization Problems - The case of combinatorial auctions.pdf:/Users/bill/D/Zotero/storage/AP37K6R7/leyton-brown et al 2002 - Learning the Empirical Hardness of Optimization Problems - The case of combinatorial auctions.pdf:application/pdf},
}

@article{lipsonEmpiricallyEvaluatingMultiagent,
	title = {Empirically {Evaluating} {Multiagent} {Reinforcement} {Learning} {Algorithms}},
	abstract = {This article makes two contributions. First, we present a platform for running and analyzing multiagent reinforcement learning experiments. Second, to demonstrate this platform we undertook and evaluated an empirical test of multiagent reinforcement learning algorithms from the literature, which to our knowledge is the largest such test ever conducted. We summarize some conclusions from our experiments, comparing algorithms on a variety of metrics including reward, regret, convergence to a Nash equilibrium and behavior in self play1.},
	language = {en},
	author = {Lipson, Asher and Leyton-Brown, Kevin},
	keywords = {Leyton-Brown},
	pages = {40},
	file = {malt.pdf:/Users/bill/D/Zotero/storage/MC2H3Q9Z/malt.pdf:application/pdf},
}

@incollection{nudelmanUnderstandingRandomSAT2004a,
	address = {Berlin, Heidelberg},
	title = {Understanding {Random} {SAT}: {Beyond} the {Clauses}-to-{Variables} {Ratio}},
	volume = {3258},
	isbn = {978-3-540-23241-4 978-3-540-30201-8},
	shorttitle = {Understanding {Random} {SAT}},
	url = {http://link.springer.com/10.1007/978-3-540-30201-8_33},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} – {CP} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Nudelman, Eugene and Leyton-Brown, Kevin and Hoos, Holger H. and Devkar, Alex and Shoham, Yoav},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Wallace, Mark},
	year = {2004},
	doi = {10.1007/978-3-540-30201-8_33},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Leyton-Brown},
	pages = {438--452},
	file = {nudelman ... leyton-brown - understanding SAT - beyond the clauses-to-variables ratio.pdf:/Users/bill/D/Zotero/storage/TM3KPB4P/nudelman ... leyton-brown - understanding SAT - beyond the clauses-to-variables ratio.pdf:application/pdf},
}

@inproceedings{leyton-brownUniversalTestSuite2000,
	address = {Minneapolis, Minnesota, United States},
	title = {Towards a universal test suite for combinatorial auction algorithms},
	isbn = {978-1-58113-272-4},
	url = {http://portal.acm.org/citation.cfm?doid=352871.352879},
	doi = {10.1145/352871.352879},
	abstract = {General combinatorial auctions—auctions in which bidders place unrestricted bids for bundles of goods—are the subject of increasing study. Much of this work has focused on algorithms for ﬁnding an optimal or approximately optimal set of winning bids. Comparatively little attention has been paid to methodical evaluation and comparison of these algorithms. In particular, there has not been a systematic discussion of appropriate data sets that can serve as universally accepted and well motivated benchmarks. In this paper we present a suite of distribution families for generating realistic, economically motivated combinatorial bids in ﬁve broad real-world domains. We hope that this work will yield many comments, criticisms and extensions, bringing the community closer to a universal combinatorial auction test suite.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 2nd {ACM} conference on {Electronic} commerce  - {EC} '00},
	publisher = {ACM Press},
	author = {Leyton-Brown, Kevin and Pearson, Mark and Shoham, Yoav},
	year = {2000},
	keywords = {Leyton-Brown},
	pages = {66--76},
	file = {p66-leyton-brown.pdf:/Users/bill/D/Zotero/storage/MQNKTZVI/p66-leyton-brown.pdf:application/pdf},
}

@article{thompsonEmpiricalAnalysisPlurality,
	title = {Empirical {Analysis} of {Plurality} {Election} {Equilibria}},
	abstract = {Voting is widely used to aggregate the diﬀerent preferences of agents, even though these agents are often able to manipulate the outcome through strategic voting. Most research on manipulation of voting methods studies (1) limited solution concepts, (2) limited preferences, or (3) scenarios with a few manipulators that have a common goal. In contrast, we study voting in plurality elections through the lens of Nash equilibrium, which allows for the possibility that any number of agents, with arbitrary diﬀerent goals, could all be manipulators. This is possible thanks to recent advances in (Bayes-)Nash equilibrium computation for large games. Although plurality has numerous pure-strategy Nash equilibria, we demonstrate how a simple equilibrium reﬁnement—assuming that agents only deviate from truthfulness when it will change the outcome—dramatically reduces this set. We also use symmetric Bayes-Nash equilibria to investigate the case where voters are uncertain of each others’ preferences. This reﬁnement does not completely eliminate the problem of multiple equilibria. However, it does show that even when agents manipulate, plurality still tends to lead to good outcomes (e.g., Condorcet winners, candidates that would win if voters were truthful, outcomes with high social welfare).},
	language = {en},
	author = {Thompson, David R M and Lev, Omer},
	pages = {8},
	file = {p391.pdf:/Users/bill/D/Zotero/storage/P8AKHDVQ/p391.pdf:application/pdf},
}

@incollection{xuHierarchicalHardnessModels2007,
	address = {Berlin, Heidelberg},
	title = {Hierarchical {Hardness} {Models} for {SAT}},
	volume = {4741},
	isbn = {978-3-540-74969-1},
	url = {http://link.springer.com/10.1007/978-3-540-74970-7_49},
	abstract = {Empirical hardness models predict a solver’s runtime for a given instance of an N P-hard problem based on eﬃciently computable features. Previous research in the SAT domain has shown that better prediction accuracy and simpler models can be obtained when models are trained separately on satisﬁable and unsatisﬁable instances. We extend this work by training separate hardness models for each class, predicting the probability that a novel instance belongs to each class, and using these predictions to build a hierarchical hardness model using a mixture-of-experts approach. We describe and analyze classiﬁers and hardness models for four well-known distributions of SAT instances and nine high-performance solvers. We show that surprisingly accurate classiﬁcations can be achieved very eﬃciently. Our experiments show that hierarchical hardness models achieve higher prediction accuracy than the previous state of the art. Furthermore, the classiﬁer’s conﬁdence correlates strongly with prediction error, giving a useful per-instance estimate of prediction error.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} – {CP} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Xu, Lin and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Bessière, Christian},
	year = {2007},
	doi = {10.1007/978-3-540-74970-7_49},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Leyton-Brown},
	pages = {696--711},
	file = {XuEtAl07.pdf:/Users/bill/D/Zotero/storage/VVHKUC3Q/XuEtAl07.pdf:application/pdf},
}

@book{balintProceedingsSATCHALLENGE2012,
	title = {Proceedings of {SAT} {CHALLENGE} 2012 {Solver} and {Benchmark} {Descriptions}},
	isbn = {978-952-10-8106-4},
	shorttitle = {{SAT} {CHALLENGE} 2012},
	author = {Balint, Adrian},
	year = {2012},
	file = {sc2012_proceedings.pdf:/Users/bill/D/Zotero/storage/TBLWSH3I/sc2012_proceedings.pdf:application/pdf},
}

@inproceedings{leyton-brownLocalEffectGames2005,
	title = {Local-{Effect} {Games}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2005/219},
	abstract = {We present a new class of games, local-effect games (LEGs), which exploit structure in a different way from other compact game representations studied in AI. We show both theoretically and empirically that these games often (but not always) have pure-strategy Nash equilibria. Finding a potential function is a good technique for finding such equilibria. We give a complete characterization of which LEGs have potential functions and provide the functions in each case; we also show a general case where pure-strategy equilibria exist in the absence of potential functions. In experiments, we show that myopic best-response dynamics converge quickly to pure strategy equilibria in games not covered by our positive theoretical results.},
	author = {Leyton-Brown, Kevin and Tennenholtz, Moshe},
	year = {2005},
	file = {05011.LeytonBrownKevin.Paper.219.pdf:/Users/bill/D/Zotero/storage/7AHLDTED/05011.LeytonBrownKevin.Paper.219.pdf:application/pdf},
}

@inproceedings{lehmannComputingMarkets2005,
	title = {Computing and {Markets}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2005/224},
	abstract = {The seminar Computing and Markets facilitated a very fruit- ful interaction between economists and computer scientists, which intensified the understanding of the other disciplines’ tool sets. The seminar helped to pave the way to a unified theory of markets that takes into account both the economic and the computational issues—and their deep interaction.},
	booktitle = {Dagstuhl {Seminar} {Proceedings} 05011 {Computing} and {Markets}},
	author = {Lehmann, Daniel and Muller, Rudolf and Sandholm, Tuomas},
	year = {2005},
	file = {05011.Summary.proceedings_intro.224.pdf:/Users/bill/D/Zotero/storage/QQSIZAVJ/05011.Summary.proceedings_intro.224.pdf:application/pdf},
}

@inproceedings{xuFeaturesSAT,
	title = {Features for {SAT}},
	author = {Xu, Lin and Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
	file = {xu hoos leyton-brown - features for SAT.pdf:/Users/bill/D/Zotero/storage/AY3N77CU/xu hoos leyton-brown - features for SAT.pdf:application/pdf},
}

@article{vanhemertEvolvingCombinatorialProblem2006a,
	title = {Evolving {Combinatorial} {Problem} {Instances} {That} {Are} {Difficult} to {Solve}},
	volume = {14},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco.2006.14.4.433},
	doi = {10.1162/evco.2006.14.4.433},
	abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difﬁcult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisﬁability, and the travelling salesman problem. Problem instances acquired through this technique are more difﬁcult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difﬁculty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {van Hemert, Jano I.},
	month = dec,
	year = {2006},
	pages = {433--462},
	file = {EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:/Users/bill/D/Zotero/storage/58JHUFMR/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:application/pdf},
}

@article{hallGeneratingExperimentalData2001a,
	title = {Generating {Experimental} {Data} for {Computational} {Testing} with {Machine} {Scheduling} {Applications}},
	volume = {49},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.49.6.854.10014},
	doi = {10.1287/opre.49.6.854.10014},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	month = dec,
	year = {2001},
	pages = {854--865},
	file = {hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf:/Users/bill/D/Zotero/storage/G9HNQX3R/hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf:application/pdf},
}

@article{hallPerformancePredictionPreselection2007a,
	title = {Performance {Prediction} and {Preselection} for {Optimization} and {Heuristic} {Solution} {Procedures}},
	volume = {55},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0398},
	doi = {10.1287/opre.1070.0398},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	month = aug,
	year = {2007},
	pages = {703--716},
	file = {hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf:/Users/bill/D/Zotero/storage/BT6J66G7/hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf:application/pdf},
}

@article{hansenNeuralNetworkEnsembles1990a,
	title = {Neural network ensembles},
	volume = {12},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/58871/},
	doi = {10.1109/34.58871},
	abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hansen, L.K. and Salamon, P.},
	month = oct,
	year = {1990},
	pages = {993--1001},
	file = {hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:/Users/bill/D/Zotero/storage/NGLLYADA/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:application/pdf},
}

@misc{hopperCanWeBetter2014,
	title = {Can {We} do {Better} than {R}-squared?},
	language = {en},
	author = {Hopper, Tom},
	year = {2014},
	file = {hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf:/Users/bill/D/Zotero/storage/7J77USEG/hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf:application/pdf},
}

@article{romeijnGeneratingExperimentalData2001a,
	title = {Generating {Experimental} {Data} for the {Generalized} {Assignment} {Problem}},
	volume = {49},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.49.6.866.10021},
	doi = {10.1287/opre.49.6.866.10021},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Operations Research},
	author = {Romeijn, H. Edwin and Romero Morales, Dolores},
	month = dec,
	year = {2001},
	pages = {866--878},
	file = {romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf:/Users/bill/D/Zotero/storage/XTTVXTVB/romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf:application/pdf},
}

@article{smith-milesMeasuringInstanceDifficulty2012a,
	title = {Measuring instance difficulty for combinatorial optimization problems},
	volume = {39},
	issn = {03050548},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054811001997},
	doi = {10.1016/j.cor.2011.07.006},
	abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies – studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances – provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difﬁculty in terms of algorithmic performance, and how such features can be deﬁned and measured for various optimization problems. We provide a comprehensive survey of the research ﬁeld with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems – which are important abstractions of many real-world problems – we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Lopes, Leo},
	month = may,
	year = {2012},
	pages = {875--889},
	file = {smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:/Users/bill/D/Zotero/storage/7Z9NLMMM/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:application/pdf},
}

@article{beyer2016em,
	title = {Solving conservation planning problems with integer linear programming},
	volume = {328},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380016300217},
	doi = {10.1016/j.ecolmodel.2016.02.005},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Beyer, Hawthorne L. and Dujardin, Yann and Watts, Matthew E. and Possingham, Hugh P.},
	month = may,
	year = {2016},
	pages = {14--22},
	file = {beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/SD4BI2GB/beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - BDPG - ANNO.pdf:application/pdf;beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - supplementary materials - mmc1.pdf:/Users/bill/D/Zotero/storage/K4KDTT7L/beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - supplementary materials - mmc1.pdf:application/pdf},
}

@article{runtingReducingRiskReserve2018b,
	title = {Reducing risk in reserve selection using {Modern} {Portfolio} {Theory}: {Coastal} planning under sea-level rise},
	volume = {55},
	issn = {00218901},
	shorttitle = {Reducing risk in reserve selection using {Modern} {Portfolio} {Theory}},
	url = {http://doi.wiley.com/10.1111/1365-2664.13190},
	doi = {10.1111/1365-2664.13190},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Journal of Applied Ecology},
	author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
	editor = {Magrach, Ainhoa},
	month = sep,
	year = {2018},
	pages = {2193--2203},
	file = {runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise - RESERVE SELECTION - PORTFOLIO - ANNO.pdf:/Users/bill/D/Zotero/storage/BF4IE58B/runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise - RESERVE SELECTION - PORTFOLIO - ANNO.pdf:application/pdf},
}

@article{eatonSpatialConservationPlanning2019a,
	title = {Spatial conservation planning under uncertainty: adapting to climate change risks using modern portfolio theory},
	volume = {29},
	issn = {1051-0761, 1939-5582},
	shorttitle = {Spatial conservation planning under uncertainty},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/eap.1962},
	doi = {10.1002/eap.1962},
	abstract = {Climate change and urban growth impact habitats, species, and ecosystem services. To buffer against global change, an established adaptation strategy is designing protected areas to increase representation and complementarity of biodiversity features. Uncertainty regarding the scale and magnitude of landscape change complicates reserve planning and exposes decision makers to the risk of failing to meet conservation goals. Conservation planning tends to treat risk as an absolute measure, ignoring the context of the management problem and risk preferences of stakeholders. Application of risk management theory to conservation emphasizes the diversification of a portfolio of assets, with the goal of reducing the impact of system volatility on investment return. We use principles of Modern Portfolio Theory (MPT), which quantifies risk as the variance and correlation among assets, to formalize diversification as an explicit strategy for managing risk in climate-driven reserve design. We extend MPT to specify a framework that evaluates multiple conservation objectives, allows decision makers to balance management benefits and risk when preferences are contested or unknown, and includes additional decision options such as parcel divestment when evaluating candidate reserve designs. We apply an efficient search algorithm that optimizes portfolio design for large conservation problems and a game theoretic approach to evaluate portfolio trade-offs that satisfy decision makers with divergent benefit and risk tolerances, or when a single decision maker cannot resolve their own preferences. Evaluating several risk profiles for a case study in South Carolina, our results suggest that a reserve design may be somewhat robust to differences in risk attitude but that budgets will likely be important determinants of conservation planning strategies, particularly when divestment is considered a viable alternative. We identify a possible fiscal threshold where adequate resources allow protecting a sufficiently diverse portfolio of habitats such that the risk of failing to achieve conservation objectives is considerably lower. For a range of sea-level rise projections, conversion of habitat to open water (14–180\%) and wetland loss (1–7\%) are unable to be compensated under the current protected network. In contrast, optimal reserve design outcomes are predicted to ameliorate expected losses relative to current and future habitat protected under the existing conservation estate.},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {Ecological Applications},
	author = {Eaton, Mitchell J. and Yurek, Simeon and Haider, Zulqarnain and Martin, Julien and Johnson, Fred A. and Udell, Bradley J. and Charkhgard, Hadi and Kwon, Changhyun},
	month = oct,
	year = {2019},
	file = {eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/HIHG8RUH/eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG - ANNO.pdf:application/pdf},
}

@article{wolpertWhatNoFree,
	title = {What the no free lunch theorems really mean; how to improve search algorithms},
	language = {en},
	author = {Wolpert, David H},
	keywords = {NFL theorems},
	pages = {14},
	file = {12-10-017.pdf:/Users/bill/D/Zotero/storage/BIP8DMII/12-10-017.pdf:application/pdf},
}

@misc{wolpertWHATDOESDINNER,
	title = {{WHAT} {DOES} {DINNER} {COST}?},
	language = {en},
	author = {Wolpert, David H},
	keywords = {NFL theorems},
	file = {coev.pdf:/Users/bill/D/Zotero/storage/XPLRC7IK/coev.pdf:application/pdf},
}

@article{langfordNoFreeLunch,
	title = {No {Free} {Lunch} {Theorems}},
	language = {en},
	author = {Langford, Bill},
	keywords = {NFL theorems},
	pages = {7},
	file = {No Free Lunch Theorems - good NFL web page - www.no-free-lunch.org.pdf:/Users/bill/D/Zotero/storage/CFP5RPW3/No Free Lunch Theorems - good NFL web page - www.no-free-lunch.org.pdf:application/pdf},
}

@inproceedings{hoSimpleExplanationNo2001,
	address = {Orlando, Florida, USA},
	title = {Simple {Explanation} of the {No} {Free} {Lunch} {Theorem} of {Opimization}},
	isbn = {0-7803-7061-9},
	abstract = {The No Free Lunch Theorem of Optimziation (NFLT) is an impossibility theorem telling us that a general-purpose universal optimization strategy is impossible, and the only way one strategy can outperform another is if it is specialized to the structure of the specific problem under consideration.  Since virtually all decision and control problems can be cast as optimziation problems, an appreciation of the NFLT and its conseqeunces is essential for controls engineers.  In this paper we present a framework for conceptualizing optimization problems that leads useful insights and a simple explanation of the NFLT.},
	booktitle = {Proceedings of the 40th {IEEE} {Conference} on {Decision} and {Control}},
	author = {Ho, Yu-Chi and Pepyne, David L.},
	month = dec,
	year = {2001},
	pages = {4409--4414},
	file = {nfl-optimization-explanation.pdf:/Users/bill/D/Zotero/storage/RS8VS9HB/nfl-optimization-explanation.pdf:application/pdf},
}

@article{franklinEffectSpeciesRarity2009,
	title = {Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern {California}},
	volume = {15},
	issn = {13669516, 14724642},
	url = {http://doi.wiley.com/10.1111/j.1472-4642.2008.00536.x},
	doi = {10.1111/j.1472-4642.2008.00536.x},
	abstract = {Aim Several studies have found that more accurate predictive models of species’ occurrences can be developed for rarer species; however, one recent study found the relationship between range size and model performance to be an artefact of sample prevalence, that is, the proportion of presence versus absence observations in the data used to train the model. We examined the effect of model type, species rarity class, species’ survey frequency, detectability and manipulated sample prevalence on the accuracy of distribution models developed for 30 reptile and amphibian species.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Diversity and Distributions},
	author = {Franklin, Janet and Wejnert, Katherine E. and Hathaway, Stacie A. and Rochester, Carlton J. and Fisher, Robert N.},
	month = jan,
	year = {2009},
	keywords = {problem attributes},
	pages = {167--177},
	file = {franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf:/Users/bill/D/Zotero/storage/DKQ4IVI7/franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf:application/pdf},
}

@article{naimiStellaRSoftwareTranslate2012,
	title = {{StellaR}: {A} software to translate {Stella} models into {R} open-source environment},
	volume = {38},
	issn = {13648152},
	shorttitle = {{StellaR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815212001673},
	doi = {10.1016/j.envsoft.2012.05.012},
	abstract = {Stella is a popular system dynamics modeling tool, which helps to put together conceptual diagrams and converts them into numeric computer models. Although it can be very useful, especially in participatory modeling, it lacks the power and ﬂexibility of a programming language. This paper presents the StellaR software which translates a Stella model into a model in R, an open source high level programming language. This allows using conceptual modeling tools provided in Stella, together with computational functionality and programming ﬂexibility provided in R. It also opens access to powerful software libraries available in R, which is especially useful for spatial modeling.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Environmental Modelling \& Software},
	author = {Naimi, Babak and Voinov, Alexey},
	month = dec,
	year = {2012},
	keywords = {problem attributes},
	pages = {117--118},
	file = {naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf:/Users/bill/D/Zotero/storage/IXKAE245/naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf:application/pdf},
}

@incollection{leyton-brownBoostingMetaphorAlgorithm2003,
	address = {Berlin, Heidelberg},
	title = {Boosting as a {Metaphor} for {Algorithm} {Design}},
	volume = {2833},
	isbn = {978-3-540-20202-8 978-3-540-45193-8},
	url = {http://link.springer.com/10.1007/978-3-540-45193-8_75},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} – {CP} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Leyton-Brown, Kevin and Nudelman, Eugene and Andrew, Galen and McFadden, Jim and Shoham, Yoav},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Rossi, Francesca},
	year = {2003},
	doi = {10.1007/978-3-540-45193-8_75},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {problem difficulty, guppy, problem difficulty guppy},
	pages = {899--903},
	file = {leyton-brown et al 2003 - Boosting as a Metaphor for Algorithm Design.pdf:/Users/bill/D/Zotero/storage/PM4SY2V2/leyton-brown et al 2003 - Boosting as a Metaphor for Algorithm Design.pdf:application/pdf},
}

@article{herbrichAlgorithmicLuckiness,
	title = {Algorithmic {Luckiness}},
	abstract = {Classical statistical learning theory studies the generalisation performance of machine learning algorithms rather indirectly. One of the main detours is that algorithms are studied in terms of the hypothesis class that they draw their hypotheses from. In this paper, motivated by the luckiness framework of Shawe-Taylor et al. (1998), we study learning algorithms more directly and in a way that allows us to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits the margin, the sparsity of the resultant weight vector, and the degree of clustering of the training data in feature space.},
	author = {Herbrich, Ralf and Williamson, Robert C.},
	keywords = {problem difficulty, guppy, problem difficulty guppy},
	file = {herbrich williamson 2002 - Algorithmic Luckiness.pdf:/Users/bill/D/Zotero/storage/H8KRBDMU/herbrich williamson 2002 - Algorithmic Luckiness.pdf:application/pdf;herbrich williamson 2002 - Errata for Algorithmic Luckiness.pdf:/Users/bill/D/Zotero/storage/4SBWR2DA/herbrich williamson 2002 - Errata for Algorithmic Luckiness.pdf:application/pdf},
}

@article{hooker1995jh,
	title = {Testing heuristics: {We} have it all wrong},
	volume = {1},
	abstract = {The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. It is hard to make fair comparisons between algorithms and to assemble realistic test problems. Competitive testing tells us which algorithm is faster but not why. Because it requires polished code, it consumes time and energy that could be better spent doing more experiments. This article argues that a more scientific approach of controlled experimentation, similar to that used in other empirical sciences, avoids or alleviates these problems. We have confused research and development; competitive testing is suited only for the latter.},
	number = {1},
	journal = {Journal of heuristics},
	author = {Hooker, John N.},
	month = sep,
	year = {1995},
	pages = {33--42},
	file = {hooker 1995 - testing heuristics - we have it all wrong - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/B7XPF86W/hooker 1995 - testing heuristics - we have it all wrong - BDPG - ANNO.pdf:application/pdf},
}

@article{oliveiraThesisMappingEffectiveness,
	title = {Thesis: {Mapping} the {Eﬀectiveness} of {Automated} {Test} {Suite} {Generation} {Techniques}},
	language = {en},
	author = {Oliveira, Carlos},
	pages = {167},
	file = {OLIVEIRA-Carlos Thesis.pdf:/Users/bill/D/Zotero/storage/DMJC4JD2/OLIVEIRA-Carlos Thesis.pdf:application/pdf},
}

@article{malanAlgorithmUsingSupport,
	title = {An algorithm using support vector classiﬁcation based constraint approximations},
	language = {en},
	author = {Malan, Maria Magdalena},
	pages = {101},
	file = {malan_particle_2019.pdf:/Users/bill/D/Zotero/storage/8IGCSS93/malan_particle_2019.pdf:application/pdf},
}

@article{khalilDissertationPresentedAcademic,
	title = {A {Dissertation} {Presented} to {The} {Academic} {Faculty}},
	language = {en},
	journal = {DISCRETE OPTIMIZATION},
	author = {Khalil, Elias B},
	pages = {172},
	file = {KHALIL-DISSERTATION-2019.pdf:/Users/bill/D/Zotero/storage/BLGBFHNJ/KHALIL-DISSERTATION-2019.pdf:application/pdf},
}

@article{lonsingAnecdoteEvaluatingQBF,
	title = {An {Anecdote} on {Evaluating} {QBF} {Solvers} and {Quantiﬁer} {Alternations}},
	abstract = {On the occasion of the 25th anniversary of the International Conference on Principles and Practice of Constraint Programming (CP), we are glad to present the history of our paper entitled Evaluating QBF Solvers: Quantiﬁer Alternations Matter that was presented at CP 2018. Our paper was ﬁnally accepted at CP 2018 after an 18-month odyssey, where it was rejected three times (in diﬀerent versions) from other top-tier conferences.},
	language = {en},
	author = {Lonsing, Florian and Egly, Uwe},
	pages = {5},
	file = {Lonsing-Egly-25CP-anniversary-commentary-2019.pdf:/Users/bill/D/Zotero/storage/ZYEKRI85/Lonsing-Egly-25CP-anniversary-commentary-2019.pdf:application/pdf},
}

@article{leymanHowCanWe,
	title = {How can we do worthwhile metaheuristic research?},
	abstract = {We present a metaheuristic development framework, to deal with problems typical of the design of new metaheuristic implementations. We discuss the framework with a component-based view in mind, meaning that we believe added value can be created by doing research on operators instead of some “novel” metaheuristic framework. With our framework, we hope to make it easier for researchers to determine where the added value of a speci c metaheuristic implementation comes from.},
	language = {en},
	author = {Leyman, Pieter and Causmaecker, Patrick De},
	pages = {2},
	file = {Abstract_ORBEL33_PieterLeyman.pdf:/Users/bill/D/Zotero/storage/SIS8E77X/Abstract_ORBEL33_PieterLeyman.pdf:application/pdf},
}

@incollection{louEvolvingBenchmarkFunctions2019,
	address = {Boca Raton, Florida : CRC Press, [2019] {\textbar} Produced in celebration of the 25th anniversary of the International Journal of Parallel, Emergent, and Distributed Systems.},
	edition = {1},
	title = {Evolving {Benchmark} {Functions} for {Optimization} {Algorithms}},
	isbn = {978-1-315-16708-4},
	url = {https://www.taylorfrancis.com/books/9781351681926/chapters/10.1201/9781315167084-11},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {From {Parallel} to {Emergent} {Computing}},
	publisher = {CRC Press},
	author = {Lou, Yang and Yuen, Shiu Yin and Chen, Guanrong},
	editor = {Adamatzky, Andrew and Akl, Selim and Sirakoulis, Georgios Ch.},
	collaborator = {Adamatzky, Andrew and Akl, Selim G. and Sirakoulis, Georgios Ch.},
	month = mar,
	year = {2019},
	doi = {10.1201/9781315167084-11},
	pages = {239--260},
	file = {chp11.pdf:/Users/bill/D/Zotero/storage/IVPH4A5J/chp11.pdf:application/pdf},
}

@article{corstjensCombinedApproachAnalysing2019,
	title = {A combined approach for analysing heuristic algorithms},
	volume = {25},
	issn = {1381-1231, 1572-9397},
	url = {http://link.springer.com/10.1007/s10732-018-9388-7},
	doi = {10.1007/s10732-018-9388-7},
	abstract = {When developing optimisation algorithms, the focus often lies on obtaining an algorithm that is able to outperform other existing algorithms for some performance measure. It is not common practice to question the reasons for possible performance diﬀerences observed. These types of questions relate to evaluating the impact of the various heuristic parameters and often remain unanswered. In this paper, the focus is on gaining insight in the behaviour of a heuristic algorithm by investigating how the various elements operating within the algorithm correlate with performance, obtaining indications of which combinations work well and which do not, and how all these eﬀects are inﬂuenced by the speciﬁc problem instance the algorithm is solving. We consider two approaches for analysing algorithm parameters and components — functional analysis of variance and multilevel regression analysis — and study the beneﬁts of using both approaches jointly. We present the results of a combined methodology that is able to provide more insights than when the two approaches are used separately. The illustrative case studies in this paper analyse a Large Neighbourhood Search algorithm applied to the Vehicle Routing Problem with Time Windows and an Iterated Local Search algorithm for the Unrelated Parallel Machine Scheduling Problem with Sequence-dependent Setup Times.},
	language = {en},
	number = {4-5},
	urldate = {2020-11-04},
	journal = {Journal of Heuristics},
	author = {Corstjens, Jeroen and Dang, Nguyen and Depaire, Benoît and Caris, An and De Causmaecker, Patrick},
	month = oct,
	year = {2019},
	pages = {591--628},
	file = {A_combined_approach_for_analysing_heuristic_algorithms.pdf:/Users/bill/D/Zotero/storage/QUA4I3CN/A_combined_approach_for_analysing_heuristic_algorithms.pdf:application/pdf},
}

@article{campeloSampleSizeEstimation2019,
	title = {Sample size estimation for power and accuracy in the experimental comparison of algorithms},
	volume = {25},
	issn = {1381-1231, 1572-9397},
	url = {http://arxiv.org/abs/1808.02997},
	doi = {10.1007/s10732-018-9396-7},
	abstract = {Experimental comparisons of performance represent an important aspect of research on optimization algorithms. In this work we present a methodology for deﬁning the required sample sizes for designing experiments with desired statistical properties for the comparison of two methods on a given problem class. The proposed approach allows the experimenter to deﬁne desired levels of accuracy for estimates of mean performance diﬀerences on individual problem instances, as well as the desired statistical power for comparing mean performances over a problem class of interest. The method calculates the required number of problem instances, and runs the algorithms on each test instance so that the accuracy of the estimated diﬀerences in performance is controlled at the predeﬁned level. Two examples illustrate the application of the proposed method, and its ability to achieve the desired statistical properties with a methodologically sound deﬁnition of the relevant sample sizes.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Journal of Heuristics},
	author = {Campelo, Felipe and Takahashi, Fernanda},
	month = apr,
	year = {2019},
	note = {arXiv: 1808.02997},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	pages = {305--338},
	annote = {Comment: Main text: 31 pages, 5 figures; Supplemental materials: 20 pages, 3 figures; Submitted to the Journal of Heuristics on October 2017},
	file = {1808.02997.pdf:/Users/bill/D/Zotero/storage/5F3NTZBA/1808.02997.pdf:application/pdf},
}

@article{louConstructingAlternativeBenchmark2019,
	title = {On constructing alternative benchmark suite for evolutionary algorithms},
	volume = {44},
	issn = {22106502},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650217305151},
	doi = {10.1016/j.swevo.2018.04.005},
	abstract = {Benchmark testing offers performance measurement for an evolutionary algorithm before it is put into applications. In this paper, a systematic method to construct a benchmark test suite is proposed. A set of established algorithms are employed. For each algorithm, a uniquely easy problem instance is generated by evolution. The resulting instances consist of a novel benchmark test suite. Each problem instance is favorable (uniquely easy) to one algorithm only. A hierarchical fitness assignment method, which is based on statistical test results, is designed to generate uniquely easy (or hard) problem instances for an algorithm. Experimental results show that each algorithm performs the best robustly on its uniquely favorable problem. The testing results are repeatable. The distribution of algorithm performance in the suite is unbiased (or uniform), which mimics any subset of real-world problems that is uniformly distributed. The resulting suite offers 1) an alternative benchmark suite to evolutionary algorithms; 2) a novel method of accessing novel algorithms; and 3) meaningful training and testing problems for evolutionary algorithm selectors and portfolios.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Swarm and Evolutionary Computation},
	author = {Lou, Yang and Yuen, Shiu Yin},
	month = feb,
	year = {2019},
	pages = {287--292},
	file = {sec_2018.pdf:/Users/bill/D/Zotero/storage/DXM3S642/sec_2018.pdf:application/pdf},
}

@article{jordanEvaluatingPerformanceReinforcement2020,
	title = {Evaluating the {Performance} of {Reinforcement} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/2006.16958},
	abstract = {Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difﬁcult to replicate. In this work, we argue that the inconsistency of performance stems from the use of ﬂawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:2006.16958 [cs, stat]},
	author = {Jordan, Scott M. and Chandak, Yash and Cohen, Daniel and Zhang, Mengxue and Thomas, Philip S.},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16958},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 30 pages, 9 figures, Thirty-seventh International Conference on Machine Learning (ICML 2020)},
	file = {2006.16958.pdf:/Users/bill/D/Zotero/storage/2VX76Z3D/2006.16958.pdf:application/pdf},
}

@inproceedings{shandEvolvingControllablyDifficult2019,
	address = {Prague Czech Republic},
	title = {Evolving controllably difficult datasets for clustering},
	isbn = {978-1-4503-6111-8},
	url = {https://dl.acm.org/doi/10.1145/3321707.3321761},
	doi = {10.1145/3321707.3321761},
	abstract = {Synthetic datasets play an important role in evaluating clustering algorithms, as they can help shed light on consistent biases, strengths, and weaknesses of particular techniques, thereby supporting sound conclusions. Despite this, there is a surprisingly small set of established clustering benchmark data, and many of these are currently handcrafted. Even then, their difficulty is typically not quantified or considered, limiting the ability to interpret algorithmic performance on these datasets. Here, we introduce HAWKS, a new data generator that uses an evolutionary algorithm to evolve cluster structure of a synthetic data set. We demonstrate how such an approach can be used to produce datasets of a pre-specified difficulty, to trade off different aspects of problem difficulty, and how these interventions directly translate into changes in the clustering performance of established algorithms.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {ACM},
	author = {Shand, Cameron and Allmendinger, Richard and Handl, Julia and Webb, Andrew and Keane, John},
	month = jul,
	year = {2019},
	pages = {463--471},
	file = {ClusterGen_GECCO2019_Deposit_nonacm.pdf:/Users/bill/D/Zotero/storage/P4JRUZI3/ClusterGen_GECCO2019_Deposit_nonacm.pdf:application/pdf},
}

@article{eggenspergerPitfallsBestPractices2019,
	title = {Pitfalls and {Best} {Practices} in {Algorithm} {Configuration}},
	volume = {64},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/11420},
	doi = {10.1613/jair.1.11420},
	abstract = {Good parameter settings are crucial to achieve high performance in many areas of artiﬁcial intelligence (AI), such as propositional satisﬁability solving, AI planning, scheduling, and machine learning (in particular deep learning). Automated algorithm conﬁguration methods have recently received much attention in the AI community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. However, practical applications of algorithm conﬁguration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineﬀective. We identify several common issues and propose best practices for avoiding them. As one possibility for automatically handling as many of these as possible, we also propose a tool called GenericWrapper4AC.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Journal of Artificial Intelligence Research},
	author = {Eggensperger, Katharina and Lindauer, Marius and Hutter, Frank},
	month = apr,
	year = {2019},
	pages = {861--893},
	file = {11420-Article (PDF)-21322-1-10-20190416.pdf:/Users/bill/D/Zotero/storage/9N2IH6KL/11420-Article (PDF)-21322-1-10-20190416.pdf:application/pdf},
}

@article{bartz-beielstein2020acms,
	title = {Benchmarking in {Optimization}: {Best} {Practice} and {Open} {Issues}},
	shorttitle = {Benchmarking in {Optimization}},
	url = {http://arxiv.org/abs/2007.03488},
	abstract = {This survey compiles ideas and recommendations from more than a dozen researchers with diﬀerent backgrounds and from diﬀerent institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, wellspeciﬁed problems, suitable algorithms, adequate performance measures, thoughtful analysis, eﬀective and eﬃcient designs, comprehensible presentations, and guaranteed reproducibility. The ﬁnal goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving ﬁeld of research this manuscript is meant to co-evolve over time by means of periodic updates.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:2007.03488 [cs, math, stat]},
	author = {Bartz-Beielstein, Thomas and Doerr, Carola and Bossek, Jakob and Chandrasekaran, Sowmya and Eftimov, Tome and Fischbach, Andreas and Kerschke, Pascal and Lopez-Ibanez, Manuel and Malan, Katherine M. and Moore, Jason H. and Naujoks, Boris and Orzechowski, Patryk and Volz, Vanessa and Wagner, Markus and Weise, Thomas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03488},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Applications, Computer Science - Performance, benchmarking, Mathematics - Optimization and Control},
	file = {2007.03488.pdf:/Users/bill/D/Zotero/storage/WTD7J625/2007.03488.pdf:application/pdf},
}

@article{colletteMultiobjectiveOptimization,
	title = {Multiobjective {Optimization}},
	language = {en},
	author = {Collette, Yann and Siarry, Patrick},
	pages = {7},
	file = {Multiobjective_Optimization_Principles_and_Case_St.pdf:/Users/bill/D/Zotero/storage/UMNN4648/Multiobjective_Optimization_Principles_and_Case_St.pdf:application/pdf},
}

@article{woinarskiApplicationTaxonPriority1996,
	title = {Application of a taxon priority system for conservation planning by selecting areas which are most distinct from environments already reserved},
	volume = {76},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006320795001069},
	doi = {10.1016/0006-3207(95)00106-9},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Woinarski, J.C.Z. and Price, O. and Faith, D.P.},
	year = {1996},
	pages = {147--159},
	file = {1-s2.0-0006320795001069-main.pdf:/Users/bill/D/Zotero/storage/A3DYTNHD/1-s2.0-0006320795001069-main.pdf:application/pdf},
}

@article{sarkarBiodiversityConservationPlanning2006,
	title = {Biodiversity {Conservation} {Planning} {Tools}: {Present} {Status} and {Challenges} for the {Future}},
	volume = {31},
	issn = {1543-5938, 1545-2050},
	shorttitle = {Biodiversity {Conservation} {Planning} {Tools}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.energy.31.042606.085844},
	doi = {10.1146/annurev.energy.31.042606.085844},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Annual Review of Environment and Resources},
	author = {Sarkar, Sahotra and Pressey, Robert L. and Faith, Daniel P. and Margules, Christopher R. and Fuller, Trevon and Stoms, David M. and Moffett, Alexander and Wilson, Kerrie A. and Williams, Kristen J. and Williams, Paul H. and Andelman, Sandy},
	month = nov,
	year = {2006},
	pages = {123--159},
	file = {2006_Sarkar_etal_BiodiversityConservationPlanning.pdf:/Users/bill/D/Zotero/storage/Z4K5QMWR/2006_Sarkar_etal_BiodiversityConservationPlanning.pdf:application/pdf},
}

@article{bordewichNatureReserveSelection2008,
	title = {Nature {Reserve} {Selection} {Problem}: {A} {Tight} {Approximation} {Algorithm}},
	volume = {5},
	issn = {1545-5963},
	shorttitle = {Nature {Reserve} {Selection} {Problem}},
	url = {http://ieeexplore.ieee.org/document/4359901/},
	doi = {10.1109/TCBB.2007.70252},
	abstract = {The Nature Reserve Selection Problem is a problem that arises in the context of studying biodiversity conservation. Subject to budgetary constraints, the problem is to select a set of regions to be conserved so that the phylogenetic diversity of the set of species contained within those regions is maximized. Recently, it has been shown in a paper by Moulton et al. that this problem is NP-hard. In this paper, we establish a tight polynomial-time approximation algorithm for the Nature Reserve Section Problem. Furthermore, we resolve a question on the computational complexity of a related problem left open by Moulton et al.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Bordewich, M. and Semple, C.},
	month = apr,
	year = {2008},
	pages = {275--280},
	file = {6307.pdf:/Users/bill/D/Zotero/storage/XGYZ33EG/6307.pdf:application/pdf},
}

@article{sempleNatureReserveSelection1992,
	title = {Nature {Reserve} {Selection} and {Other} {Problems} in {Phylogenetic} {Diversity}},
	language = {en},
	journal = {Conservation Biology},
	author = {Semple, Charles},
	year = {1992},
	pages = {42},
	file = {2007090611301.pdf:/Users/bill/D/Zotero/storage/ZGE3RZI6/2007090611301.pdf:application/pdf},
}

@article{bordewichNatureReserveSelection2008a,
	title = {Nature {Reserve} {Selection} {Problem}: {A} {Tight} {Approximation} {Algorithm}},
	volume = {5},
	issn = {1545-5963},
	shorttitle = {Nature {Reserve} {Selection} {Problem}},
	url = {http://ieeexplore.ieee.org/document/4359901/},
	doi = {10.1109/TCBB.2007.70252},
	abstract = {The Nature Reserve Selection Problem is a problem that arises in the context of studying biodiversity conservation. Subject to budgetary constraints, the problem is to select a set of regions to conserve so that the phylogenetic diversity of the set of species contained within those regions is maximized. Recently, it was shown in a paper by Moulton et al. that this problem is NP-hard. In this paper, we establish a tight polynomial-time approximation algorithm for the Nature Reserve Section Problem. Furthermore, we resolve a question on the computational complexity of a related problem left open in Moulton et al.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Bordewich, M. and Semple, C.},
	month = apr,
	year = {2008},
	pages = {275--280},
	file = {BS07.pdf:/Users/bill/D/Zotero/storage/HAVRPXG2/BS07.pdf:application/pdf},
}

@article{bordewichBudgetedNatureReserve2012,
	title = {Budgeted {Nature} {Reserve} {Selection} with diversity feature loss and arbitrary split systems},
	volume = {64},
	issn = {0303-6812, 1432-1416},
	url = {http://link.springer.com/10.1007/s00285-011-0405-9},
	doi = {10.1007/s00285-011-0405-9},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Journal of Mathematical Biology},
	author = {Bordewich, Magnus and Semple, Charles},
	month = jan,
	year = {2012},
	pages = {69--85},
	file = {BS11.pdf:/Users/bill/D/Zotero/storage/XBTLKHZ7/BS11.pdf:application/pdf},
}

@article{presseyEffectivenessAlternativeHeuristic1997,
	title = {Effectiveness of alternative heuristic algorithms for identifying indicative minimum requirements for conservation reserves},
	volume = {80},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320796000456},
	doi = {10.1016/S0006-3207(96)00045-6},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Pressey, R.L. and Possingham, H.P. and Day, J.R.},
	month = may,
	year = {1997},
	pages = {207--219},
	file = {Effectiveness_of.pdf:/Users/bill/D/Zotero/storage/26TMVFPE/Effectiveness_of.pdf:application/pdf},
}

@article{presseyOptimalityReserveSelection1996,
	title = {Optimality in reserve selection algorithms: {When} does it matter and how much?},
	volume = {76},
	issn = {00063207},
	shorttitle = {Optimality in reserve selection algorithms},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006320795001204},
	doi = {10.1016/0006-3207(95)00120-4},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Pressey, R.L. and Possingham, H.P. and Margules, C.R.},
	year = {1996},
	pages = {259--267},
	file = {hp_bc_76_96.pdf:/Users/bill/D/Zotero/storage/MDLUTMMH/hp_bc_76_96.pdf:application/pdf},
}

@article{lawlerRareSpeciesUse2003,
	title = {Rare {Species} and the {Use} of {Indicator} {Groups} for {Conservation} {Planning}},
	volume = {17},
	issn = {0888-8892, 1523-1739},
	url = {http://doi.wiley.com/10.1046/j.1523-1739.2003.01638.x},
	doi = {10.1046/j.1523-1739.2003.01638.x},
	abstract = {Indicators of biodiversity have been proposed as a potential tool for selecting areas for conservation when information about species distributions is scarce. Although tests of the concept have produced varied results, sites selected to address indicator groups can include a high proportion of other species. We tested the hypothesis that species at risk of extinction are not likely to be included in sites selected to protect indicator groups. Using a reserve-selection approach, we compared the ability of seven indicator groups—freshwater fish, birds, mammals, freshwater mussels, reptiles, amphibians, and at-risk species of those six taxa—to provide protection for other species in general and at-risk species in particular in the Middle Atlantic region of the United States. Although sites selected with single taxonomic indicator groups provided protection for between 61\% and 82\% of all other species, no taxonomic group provided protection for more than 58\% of all other at-risk species. The failure to cover at-risk species is likely linked to their rarity. By examining the relationship between a species’ probability of coverage by each indicator group and the extent of its geographic range within the study area, we found that species with more restricted ranges were less likely to be protected than more widespread species. Furthermore, we found that although sites selected with indicator groups composed primarily of terrestrial species (birds and mammals) included relatively high percentages of those species (82–85\%) they included smaller percentages of strictly aquatic species (27–55\%). Finally, of both importance and possible utility, we found that at-risk species themselves performed well as an indicator group, covering an average of 84\% of all other species.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Lawler, Joshua J. and White, Denis and Sifneos, Jean C. and Master, Lawrence L.},
	month = jun,
	year = {2003},
	pages = {875--882},
	file = {Indicators.pdf:/Users/bill/D/Zotero/storage/H55DIP76/Indicators.pdf:application/pdf},
}

@article{gerrardSelectingConservationReserves1997,
	title = {Selecting conservation reserves using species-covering models: {Adapting} the {ARC}/{INFO} {GIS}},
	volume = {2},
	issn = {1361-1682, 1467-9671},
	shorttitle = {Selecting conservation reserves using species-covering models},
	url = {http://doi.wiley.com/10.1111/j.1467-9671.1997.tb00004.x},
	doi = {10.1111/j.1467-9671.1997.tb00004.x},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Transactions in GIS},
	author = {Gerrard, Ross A and Church, Richard L and Stoms, David M and Davis, Frank W},
	month = oct,
	year = {1997},
	pages = {45--60},
	file = {j.1467-9671.1997.tb00004.x.pdf:/Users/bill/D/Zotero/storage/F3FBLEC4/j.1467-9671.1997.tb00004.x.pdf:application/pdf},
}

@article{mcdonnellMathematicalMethodsSpatially,
	title = {Mathematical methods for spatially cohesive reserve design},
	abstract = {The problem of designing spatially cohesive nature reserve systems that meet biodiversity objectives is formulated as a nonlinear integer programming problem. The multiobjective function minimises a combination of boundary length, area and failed representation of the biological attributes we are trying to conserve. The task is to reserve a subset of sites that best meet this objective. We use data on the distribution of habitats in the Northern Territory, Australia, to show how simulated annealing and a greedy heuristic algorithm can be used to generate good solutions to such large reserve design problems, and to compare the effectiveness of these methods.},
	language = {en},
	author = {McDonnell, Mark D and Possingham, Hugh P and Ball, Ian R and Cousins, Elizabeth A},
	pages = {8},
	file = {McDonnell_JEMA02.pdf:/Users/bill/D/Zotero/storage/PX64DFYZ/McDonnell_JEMA02.pdf:application/pdf},
}

@article{megiddoMaximumCoverageLocation1983,
	title = {The {Maximum} {Coverage} {Location} {Problem}},
	volume = {4},
	issn = {0196-5212, 2168-345X},
	url = {https://epubs.siam.org/doi/10.1137/0604028},
	doi = {10.1137/0604028},
	abstract = {In this paper we define and discuss the following problem which we call the maximum coverage location problem. A transportation network is given together with the locations of customers and facilities. Thus, for each customer i, a radius ri is known such that customer i can currently be served by a facility which is located within a distance of r, from the location of customer i. We consider the problem from the point of view of a new company which is interested in establishing new facilities on the network so as to maximize the company's "share of the market." Specifically, assume that the company gains an amount of wi in case customer i decides to switch over to one of the new facilities. Moreover, we assume that the decision to switch over is based on proximity only, i.e., customer i switches over to a new facility only if the latter is located at a distance less than ri from i. The problem is to locate p new facilities so as to maximize the total gain.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {SIAM Journal on Algebraic Discrete Methods},
	author = {Megiddo, Nimrod and Zemel, Eitan and Hakimi, S. Louis},
	month = jun,
	year = {1983},
	pages = {253--261},
	file = {megiddo-zemel-hakimi.pdf:/Users/bill/D/Zotero/storage/ECKVN7TW/megiddo-zemel-hakimi.pdf:application/pdf},
}

@article{xueSchedulingConservationDesigns,
	title = {Scheduling {Conservation} {Designs} via {Network} {Cascade} {Optimization}},
	abstract = {We introduce the problem of scheduling land purchases to conserve an endangered species in a way that achieves maximum population spread but delays purchases as long as possible, so that conservation planners retain maximum ﬂexibility and use available budgets in the most efﬁcient way. We develop the problem formally as a stochastic optimization problem over a network cascade model describing the population spread, and present a solution approach that reduces the stochastic problem to a novel variant of a Steiner tree problem. We give a primal-dual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution. Our experiments, using actual conservation data and a standard diffusion model, show that the approach produces near optimal results and is much more scalable than more generic off-the-shelf optimizers.},
	language = {en},
	author = {Xue, Shan and Fern, Alan and Sheldon, Daniel},
	pages = {7},
	file = {xue fern sheldon 2012 - Scheduling Conservation Designs via Network Cascade Optimization - aaai2012 paper by OSU CS authors - GUPPY.pdf:/Users/bill/D/Zotero/storage/2MW79YX4/xue fern sheldon 2012 - Scheduling Conservation Designs via Network Cascade Optimization - aaai2012 paper by OSU CS authors - GUPPY.pdf:application/pdf},
}

@article{dasRecentAdvancesDifferential2016,
	title = {Recent advances in differential evolution – {An} updated survey},
	volume = {27},
	issn = {22106502},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650216000146},
	doi = {10.1016/j.swevo.2016.01.004},
	abstract = {Differential Evolution (DE) is arguably one of the most powerful and versatile evolutionary optimizers for the continuous parameter spaces in recent times. Almost 5 years have passed since the ﬁrst comprehensive survey article was published on DE by Das and Suganthan in 2011. Several developments have been reported on various aspects of the algorithm in these 5 years and the research on and with DE have now reached an impressive state. Considering the huge progress of research with DE and its applications in diverse domains of science and technology, we ﬁnd that it is a high time to provide a critical review of the latest literatures published and also to point out some important future avenues of research. The purpose of this paper is to summarize and organize the information on these current developments on DE. Beginning with a comprehensive foundation of the basic DE family of algorithms, we proceed through the recent proposals on parameter adaptation of DE, DE-based single-objective global optimizers, DE adopted for various optimization scenarios including constrained, large-scale, multi-objective, multi-modal and dynamic optimization, hybridization of DE with other optimizers, and also the multifaceted literature on applications of DE. The paper also presents a dozen of interesting open problems and future research issues on DE.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Swarm and Evolutionary Computation},
	author = {Das, Swagatam and Mullick, Sankha Subhra and Suganthan, P.N.},
	month = apr,
	year = {2016},
	pages = {1--30},
	file = {1-s2.0-S2210650216000146-main.pdf:/Users/bill/D/Zotero/storage/8Z65ZJWK/1-s2.0-S2210650216000146-main.pdf:application/pdf},
}

@incollection{mccreeshUnderstandingEmpiricalHardness2019,
	address = {Cham},
	title = {Understanding the {Empirical} {Hardness} of {Random} {Optimisation} {Problems}},
	volume = {11802},
	isbn = {978-3-030-30047-0 978-3-030-30048-7},
	url = {http://link.springer.com/10.1007/978-3-030-30048-7_20},
	abstract = {We look at the empirical complexity of the maximum clique problem, the graph colouring problem, and the maximum satisﬁability problem, in randomly generated instances. Although each is NP-hard, we encounter exponential behaviour only with certain choices of instance generation parameters. To explain this, we link the diﬃculty of optimisation to the diﬃculty of a small number of decision problems, which are already better-understood through phenomena like phase transitions with associated complexity peaks. However, our results show that individual decision problems can interact in very diﬀerent ways, leading to diﬀerent behaviour for each optimisation problem. Finally, we uncover a conﬂict between anytime and overall behaviour in algorithm design, and discuss the implications for the design of experiments and of search strategies such as variable- and value-ordering heuristics.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {McCreesh, Ciaran and Pettersson, William and Prosser, Patrick},
	editor = {Schiex, Thomas and de Givry, Simon},
	year = {2019},
	doi = {10.1007/978-3-030-30048-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {333--349},
	file = {190408.pdf:/Users/bill/D/Zotero/storage/NK99QMI3/190408.pdf:application/pdf},
}

@incollection{abadiGeneratingSolvedInstances1990,
	address = {New York, NY},
	title = {On {Generating} {Solved} {Instances} of {Computational} {Problems}},
	volume = {403},
	isbn = {978-0-387-97196-4 978-0-387-34799-8},
	url = {http://link.springer.com/10.1007/0-387-34799-2_23},
	abstract = {We consider the efficient generation of solved instances of computational problems. In particular, we consider ;nvulnerable generators. Let S be a subset of ( 0 , l ) and be a Turing Machine that accepts S; a n accepting computation w of M on input x is called a “witness” that x E S. Informally, a program is an winvulnerable generator if, on b p u t I”,it produces instance-witness pairs (2,w),with 1x1 = n,according to a distribution under which any polynomial-time adversary who is given I fails to 6nd a witness that x E S , with probability at least a,for infinitely many lengths n.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Advances in {Cryptology} — {CRYPTO}’ 88},
	publisher = {Springer New York},
	author = {Abadi, Martín and Allendert, Eric and Broder, Andrei and Feigenbaum, Joan and Hemachandra, Lane A.},
	editor = {Goldwasser, Shafi},
	year = {1990},
	doi = {10.1007/0-387-34799-2_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {297--310},
	file = {AABFH-Crypto88.pdf:/Users/bill/D/Zotero/storage/8VDFQYIW/AABFH-Crypto88.pdf:application/pdf},
}

@incollection{abadiGeneratingSolvedInstances1990a,
	address = {New York, NY},
	title = {On {Generating} {Solved} {Instances} of {Computational} {Problems}},
	volume = {403},
	isbn = {978-0-387-97196-4 978-0-387-34799-8},
	url = {http://link.springer.com/10.1007/0-387-34799-2_23},
	abstract = {We consider the efficient generation of solved instances of computational problems. In particular, we consider ;nvulnerable generators. Let S be a subset of ( 0 , l ) and be a Turing Machine that accepts S; a n accepting computation w of M on input x is called a “witness” that x E S. Informally, a program is an winvulnerable generator if, on b p u t I”,it produces instance-witness pairs (2,w),with 1x1 = n,according to a distribution under which any polynomial-time adversary who is given I fails to 6nd a witness that x E S , with probability at least a,for infinitely many lengths n.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Advances in {Cryptology} — {CRYPTO}’ 88},
	publisher = {Springer New York},
	author = {Abadi, Martín and Allendert, Eric and Broder, Andrei and Feigenbaum, Joan and Hemachandra, Lane A.},
	editor = {Goldwasser, Shafi},
	year = {1990},
	doi = {10.1007/0-387-34799-2_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {297--310},
	file = {Abadi1990_Chapter_OnGeneratingSolvedInstancesOfC.pdf:/Users/bill/D/Zotero/storage/R7UDBTAM/Abadi1990_Chapter_OnGeneratingSolvedInstancesOfC.pdf:application/pdf},
}

@article{brockington1994,
	title = {Camouflaging {Independent} {Sets} in {Quasi}-{Random} {Graphs}},
	abstract = {In this paper, we look at the problem of how one might try to hide a large independent set in a graph in which all other independent sets are signi cantly smaller.},
	language = {en},
	author = {Brockington, Mark and Culberson, Joseph C},
	year = {1994},
	keywords = {bdpg, test generation, solution hiding},
	pages = {15},
	file = {brockington culberson 1994 - camouflaging independent sets in squasi-random graphs - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/CVM5RYFN/brockington culberson 1994 - camouflaging independent sets in squasi-random graphs - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{fischerEvaluationMethodologyVirtual,
	title = {An {Evaluation} {Methodology} for {Virtual} {Network} {Embedding}},
	language = {en},
	author = {Fischer, Andreas and Passau, Universität},
	pages = {199},
	file = {Fischer_Andreas-An_Evaluation_Methodology_for_Virtual_Network_Embedding.pdf:/Users/bill/D/Zotero/storage/FVC26LTP/Fischer_Andreas-An_Evaluation_Methodology_for_Virtual_Network_Embedding.pdf:application/pdf},
}

@article{palubeckisGeneratingHardTest,
	title = {Generating {Hard} {Test} {Instances} with {Known} {Optimal} {Solution} for the {Rectilinear} {Quadratic} {Assignment} {Problem}},
	abstract = {In this paper we consider the rectilinear version of the quadratic assignment problem (QAP). We deﬁne a class of edge-weighted graphs with nonnegatively valued bisections. For one important type of such graphs we provide a characterization of point sets on the plane for which the optimal value of the related QAP is zero. These graphs are used in the algorithms for generating rectilinear QAP instances with known provably optimal solutions. The basic algorithm of such type uses only triangles. Making a reduction from 3-dimensional matching, it is shown that the set of instances which can be generated by this algorithm is hard. The basic algorithm is extended to process graphs larger than triangles. We give implementation details of this extension and of four other variations of the basic algorithm. We compare these ﬁve and also two existing generators experimentally employing multi-start descent heuristic for the QAP as an examiner. The graphs with nonnegatively valued bisections can also be used in the construction of lower bounds on the optimal value for the rectilinear QAP.},
	language = {en},
	author = {Palubeckis, G},
	pages = {30},
	file = {fulltext.pdf:/Users/bill/D/Zotero/storage/UTKLJL8P/fulltext.pdf:application/pdf},
}

@article{rardinExperimentalEvaluationHeuristic,
	title = {Experimental {Evaluation} of {Heuristic} {Optimization} {Algorithms}: {A} {Tutorial}},
	abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically—by applying procedures to a collection of speciﬁc instances and comparing the observed solution quality and computational burden.},
	language = {en},
	author = {Rardin, Ronald L and Uzsoy, Reha},
	pages = {44},
	file = {fulltext(8).pdf:/Users/bill/D/Zotero/storage/2ATTQTGK/fulltext(8).pdf:application/pdf},
}

@article{grundelPROBABILISTICANALYSISRESULTS,
	title = {{PROBABILISTIC} {ANALYSIS} {AND} {RESULTS} {OF} {COMBINATORIAL} {PROBLEMS} {WITH} {MILITARY} {APPLICATIONS}},
	language = {en},
	author = {Grundel, Don A},
	pages = {135},
	file = {grundel 2004 - Probabilistic analysis and results of combinatorial problems with military applications - THESIS.pdf:/Users/bill/D/Zotero/storage/SHRSZXF6/grundel 2004 - Probabilistic analysis and results of combinatorial problems with military applications - THESIS.pdf:application/pdf},
}

@article{hasselbergTestCaseGenerators1993,
	title = {Test case generators and computational results for the maximum clique problem},
	volume = {3},
	issn = {0925-5001, 1573-2916},
	url = {http://link.springer.com/10.1007/BF01096415},
	doi = {10.1007/BF01096415},
	abstract = {In the last years many algorithms have been proposed for solving the maximum clique problem. Most of these algorithms have been tested on randomly generated graphs. In this paper we present different test problem generators that arise from a variety of practical applications, as well as graphs with known maximum cliques. In addition, we provide computational experience with two exact algorithms using the generated test problems.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Journal of Global Optimization},
	author = {Hasselberg, Jonas and Pardalos, Panos M. and Vairaktarakis, George},
	year = {1993},
	pages = {463--482},
	file = {Hasselberg et al 1993 - Test Case Generators And Computational results for the maximum clique problem - BDPG - SANCHIS VERTEX COVER.pdf:/Users/bill/D/Zotero/storage/URFH7XQJ/Hasselberg et al 1993 - Test Case Generators And Computational results for the maximum clique problem - BDPG - SANCHIS VERTEX COVER.pdf:application/pdf},
}

@article{mccreeshSOLVINGHARDSUBGRAPH,
	title = {{SOLVING} {HARD} {SUBGRAPH} {PROBLEMS} {IN} {PARALLEL}},
	language = {en},
	author = {Mccreesh, Ciaran},
	pages = {264},
	file = {mccreesh 2017 - solving hard subgraph problems in paralle - THESIS.pdf:/Users/bill/D/Zotero/storage/XKFDHNFL/mccreesh 2017 - solving hard subgraph problems in paralle - THESIS.pdf:application/pdf},
}

@book{goldwasserDataStructuresNeighbor2002,
	address = {Providence, Rhode                     Island},
	series = {{DIMACS} {Series} in {Discrete} {Mathematics} and                         {Theoretical} {Computer} {Science}},
	title = {Data {Structures}, {Near} {Neighbor} {Searches}, and {Methodology}: {Fifth} and {Sixth} {DIMACS} {Implementation} {Challenges}},
	volume = {59},
	isbn = {978-0-8218-2892-2 978-1-4704-4017-6},
	shorttitle = {Data {Structures}, {Near} {Neighbor} {Searches}, and {Methodology}},
	url = {http://www.ams.org/dimacs/059},
	language = {en},
	urldate = {2020-11-04},
	publisher = {American Mathematical                     Society},
	editor = {Goldwasser, Michael and Johnson, David and McGeoch, Catherine},
	month = dec,
	year = {2002},
	doi = {10.1090/dimacs/059},
	file = {mcgeoch 1999 - A Bibliography of Algorithm Experimentation.pdf:/Users/bill/D/Zotero/storage/KF8T22NW/mcgeoch 1999 - A Bibliography of Algorithm Experimentation.pdf:application/pdf},
}

@article{rardinExperimentalEvaluationHeuristica,
	title = {Experimental {Evaluation} of {Heuristic} {Optimization} {Algorithms}: {A} {Tutorial}},
	abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically—by applying procedures to a collection of speciﬁc instances and comparing the observed solution quality and computational burden.},
	language = {en},
	author = {Rardin, Ronald L and Uzsoy, Reha},
	pages = {44},
	file = {Rardin Uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/LXXVE6YL/Rardin Uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{rardinExperimentalEvaluationHeuristicb,
	title = {Experimental {Evaluation} of {Heuristic} {Optimization} {Algorithms}: {A} {Tutorial}},
	abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically—by applying procedures to a collection of speciﬁc instances and comparing the observed solution quality and computational burden.},
	language = {en},
	author = {Rardin, Ronald L and Uzsoy, Reha},
	pages = {44},
	file = {rardin uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial.pdf:/Users/bill/D/Zotero/storage/FM7J3R4T/rardin uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial.pdf:application/pdf},
}

@techreport{sanchis1988,
	address = {Rochester, NY},
	type = {{TR}},
	title = {Test {Instance} {Construction} for {NP}-hard {Problems}},
	abstract = {The performance of heuristic approximation algorithms for NP-hard problems can often only be determined by experimentation. This paper explores some of the issues involved in the efficient generation of useful test sets for such problems, i.e. test sets consisting of instances of the problem for which the answer is known and having other properties useful for testing the performance of approximation algorithms. Some theoretical results are given concerning what kinds of test sets can and cannot be generated; these are derived by examining the complexity of length-restricted instance generation for languages in NP and co-NP. Also examples of test set generation procedures are presented.},
	language = {en},
	number = {206},
	institution = {Computer Science Dept, University of Rochester},
	author = {Sanchis, Laura A},
	year = {1988},
	keywords = {bdpg, test generation, NP-hard problems},
	pages = {22},
	file = {sanchis 1988 - test instance construction for NP-hard problems - technical report - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/D27MKTC2/sanchis 1988 - test instance construction for NP-hard problems - technical report - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{sanchisComplexityTestCase1990,
	title = {On the complexity of test case generation for {NP}-hard problems},
	volume = {36},
	issn = {00200190},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0020019090900829},
	doi = {10.1016/0020-0190(90)90082-9},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Information Processing Letters},
	author = {Sanchis, Laura A.},
	month = nov,
	year = {1990},
	pages = {135--140},
	file = {sanchis 1990 - on the complexity of test case generation for NP-hard problems - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/G6BCT84F/sanchis 1990 - on the complexity of test case generation for NP-hard problems - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{sanchisGeneratingHardDiverse1995a,
	title = {Generating hard and diverse test sets for {NP}-hard graph problems},
	volume = {58},
	issn = {0166218X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0166218X93E0140T},
	doi = {10.1016/0166-218X(93)E0140-T},
	abstract = {In evaluating the performance of approximation algorithms for NP-hard problems, it is often necessary to resort to empirical testing. In order to do such testing it is useful to have test instances of the problem for which the correct answer is known. We present algorithms for efficiently generating test instances for some NP-hard graph problems in such a way that the sets of instances generated can be shown to be both diverse and computationally hard. The techniques used involve combining extremal graph theory results with NP-hardness reductions.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Discrete Applied Mathematics},
	author = {Sanchis, Laura A.},
	month = mar,
	year = {1995},
	pages = {35--66},
	file = {sanchis 1995 - generating hard and diverse test sets for NP-hard graph problems - BDPG - PROBLEM DIFFICULTY - VERTEX COVER.pdf:/Users/bill/D/Zotero/storage/WYG5FVD9/sanchis 1995 - generating hard and diverse test sets for NP-hard graph problems - BDPG - PROBLEM DIFFICULTY - VERTEX COVER.pdf:application/pdf},
}

@article{sanchisExperimentalAnalysisHeuristic2002,
	title = {Experimental {Analysis} of {Heuristic} {Algorithms} for the {Dominating} {Set} {Problem}},
	volume = {33},
	issn = {0178-4617, 1432-0541},
	url = {http://link.springer.com/10.1007/s00453-001-0101-z},
	doi = {10.1007/s00453-001-0101-z},
	abstract = {We say a vertex v in a graph G covers a vertex w if v = w or if v and w are adjacent. A subset of vertices of G is a dominating set if it collectively covers all vertices in the graph. The dominating set problem, which is NP-hard, consists of ﬁnding a smallest possible dominating set for a graph. The straightforward greedy strategy for ﬁnding a small dominating set in a graph consists of successively choosing vertices which cover the largest possible number of previously uncovered vertices. Several variations on this greedy heuristic are described and the results of extensive testing of these variations is presented. A more sophisticated procedure for choosing vertices, which takes into account the number of ways in which an uncovered vertex may be covered, appears to be the most successful of the algorithms which are analyzed. For our experimental testing, we used both random graphs and graphs constructed by test case generators which produce graphs with a given density and a speciﬁed size for the smallest dominating set. We found that these generators were able to produce challenging graphs for the algorithms, thus helping to discriminate among them, and allowing a greater variety of graphs to be used in the experiments.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Algorithmica},
	author = {Sanchis, L. A.},
	month = may,
	year = {2002},
	pages = {3--18},
	file = {Sanchis 2002 - Experimental Analysis Of Heuristic algorithms for the dominating set problem.pdf:/Users/bill/D/Zotero/storage/YT539468/Sanchis 2002 - Experimental Analysis Of Heuristic algorithms for the dominating set problem.pdf:application/pdf},
}

@article{sanchisRelatingSizeConnected2004,
	title = {Relating the size of a connected graph to its total and restricted domination numbers},
	volume = {283},
	issn = {0012365X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0012365X04000263},
	doi = {10.1016/j.disc.2003.11.011},
	abstract = {A dominating set for a graph G = (V; E) is a subset of vertices D ⊆ V such that for all v ∈ V − D there exists some u ∈ D adjacent to v. The domination number of G is the size of its smallest dominating set. A dominating set D is a total dominating set if every vertex in D has a neighbor in D. We give a tight upper bound on the number of edges that a connected graph with a given total domination number can have, and characterize the extremal graphs attaining the bound. We do the same for the k-restricted domination number, which is the smallest number d, such that for any subset U ⊆ V where {\textbar}U {\textbar} = k there exists a dominating set for G of size at most d, and containing all vertices in U .},
	language = {en},
	number = {1-3},
	urldate = {2020-11-04},
	journal = {Discrete Mathematics},
	author = {Sanchis, Laura A.},
	month = jun,
	year = {2004},
	pages = {205--216},
	file = {sanchis 2004 - relating the size of a connected graph to its total and restricted domination numbers.pdf:/Users/bill/D/Zotero/storage/UJZBTUEM/sanchis 2004 - relating the size of a connected graph to its total and restricted domination numbers.pdf:application/pdf},
}

@article{sanchisExperimentalTheoreticalResults1996a,
	title = {Some {Experimental} and {Theoretical} {Results} on {Test} {Case} {Generators} for the {Maximum} {Clique} {Problem}},
	volume = {8},
	issn = {1091-9856, 1526-5528},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/ijoc.8.2.87},
	doi = {10.1287/ijoc.8.2.87},
	abstract = {We describe and analyze test case generators for the maximum clique problem or equivalently for the maximum independent set or vertex cover problems. The generators produce graphs with speci ed number of vertices and edges, and known maximum clique size. The experimental hardness of the test cases is evaluated in relation to several heuristics for the maximum clique problem, based on neural networks, and derived from the work of A. Jagota.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {INFORMS Journal on Computing},
	author = {Sanchis, Laura A. and Jagota, Arun},
	month = may,
	year = {1996},
	pages = {87--102},
	file = {sanchis jagota 1995 - Some Experimental and Theoretical Results on Test Case Generators for the Maximum Clique Problem - BDPG.pdf:/Users/bill/D/Zotero/storage/RMMG3UIE/sanchis jagota 1995 - Some Experimental and Theoretical Results on Test Case Generators for the Maximum Clique Problem - BDPG.pdf:application/pdf},
}

@article{papathanasiouEquilibriumPointBased,
	title = {An {Equilibrium} {Point} {Based} {Humanoids} {Control} {Model}},
	language = {en},
	author = {Papathanasiou, Anthanasios},
	pages = {16},
	file = {sanchis obituary - university of rochester newsletter-2006 - woman who generated NP-complet problems with known solutions - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/28MKX2LU/sanchis obituary - university of rochester newsletter-2006 - woman who generated NP-complet problems with known solutions - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{aliMetalearningApproachAutomatic2006,
	title = {A meta-learning approach to automatic kernel selection for support vector machines},
	volume = {70},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231206001056},
	doi = {10.1016/j.neucom.2006.03.004},
	abstract = {Appropriate choice of a kernel is the most important ingredient of the kernel-based learning methods such as support vector machine (SVM). Automatic kernel selection is a key issue given the number of kernels available, and the current trial-and-error nature of selecting the best kernel for a given problem. This paper introduces a new method for automatic kernel selection, with empirical results based on classiﬁcation. The empirical study has been conducted among ﬁve kernels with 112 different classiﬁcation problems, using the popular kernel based statistical learning algorithm SVM. We evaluate the kernels’ performance in terms of accuracy measures. We then focus on answering the question: which kernel is best suited to which type of classiﬁcation problem? Our meta-learning methodology involves measuring the problem characteristics using classical, distance and distribution-based statistical information. We then combine these measures with the empirical results to present a rule-based method to select the most appropriate kernel for a classiﬁcation problem. The rules are generated by the decision tree algorithm C5.0 and are evaluated with 10 fold cross validation. All generated rules offer high accuracy ratings.},
	language = {en},
	number = {1-3},
	urldate = {2020-11-04},
	journal = {Neurocomputing},
	author = {Ali, Shawkat and Smith-Miles, Kate A.},
	month = dec,
	year = {2006},
	pages = {173--186},
	file = {ali smith-miles 2006 - a meta-learning approach to automatic kernel selection for support vector machines.pdf:/Users/bill/D/Zotero/storage/2VNFKXS7/ali smith-miles 2006 - a meta-learning approach to automatic kernel selection for support vector machines.pdf:application/pdf},
}

@incollection{lopesPitfallsInstanceGeneration2010,
	address = {Berlin, Heidelberg},
	title = {Pitfalls in {Instance} {Generation} for {Udine} {Timetabling}},
	volume = {6073},
	isbn = {978-3-642-13799-0 978-3-642-13800-3},
	url = {http://link.springer.com/10.1007/978-3-642-13800-3_31},
	abstract = {In many randomly generated instances for Udine timetabling very diﬀerent solvers achieved the same objective value (with diﬀerent solutions). This paper summarises observations concerning the structure of the instances and their consequences for eﬀective test instance generation and reporting of computational results in Udine Timetabling problems.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lopes, Leo and Smith-Miles, Kate},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Blum, Christian and Battiti, Roberto},
	year = {2010},
	doi = {10.1007/978-3-642-13800-3_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {299--302},
	file = {lopes smith-miles 2010 - pitfalls in instance generation for udine timetabling.pdf:/Users/bill/D/Zotero/storage/FSK93F3C/lopes smith-miles 2010 - pitfalls in instance generation for udine timetabling.pdf:application/pdf},
}

@article{HonoursProjects,
	title = {Honours {Projects}},
	language = {en},
	pages = {68},
	file = {monash honours projects 2013.pdf:/Users/bill/D/Zotero/storage/6ESVRPHD/monash honours projects 2013.pdf:application/pdf},
}

@article{smith-milesIntelligentOptimization,
	title = {= {Intelligent} {Optimization}},
	language = {en},
	author = {Smith-Miles, Professor Kate},
	pages = {51},
	file = {smith-miles - how can data mining help to understand what makes an optimization problem hard, which algorithm will perform best, and why - talk.PDF:/Users/bill/D/Zotero/storage/BDDFQ835/smith-miles - how can data mining help to understand what makes an optimization problem hard, which algorithm will perform best, and why - talk.PDF:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = jan,
	year = {2009},
	pages = {1--25},
	file = {smith-miles 2008 - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:/Users/bill/D/Zotero/storage/N228KTGV/smith-miles 2008 - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:application/pdf},
}

@article{smith-milesUnderstandingRelationshipScheduling,
	title = {Understanding the {Relationship} between {Scheduling} {Problem} {Structure} and {Heuristic} {Performance} using {Knowledge} {Discovery}},
	abstract = {Using a knowledge discovery approach, we seek insights into the relationships between problem structure and the effectiveness of scheduling heuristics. A large collection of 75,000 instances of the single machine early/tardy scheduling problem is generated, characterized by six features, and used to explore the performance of two common scheduling heuristics. The best heuristic is selected using rules from a decision tree with accuracy exceeding 97\%. A self-organizing map is used to visualize the feature space and generate insights into heuristic performance. This paper argues for such a knowledge discovery approach to be applied to other optimization problems, to contribute to automation of algorithm selection as well as insightful algorithm design.},
	language = {en},
	author = {Smith-Miles, Kate A and James, Ross J W and Giffin, John W and Tu, Yiqing},
	pages = {15},
	file = {smith-miles et al - understanding the relationship between scheduling problem structure and heuristic performance using knowledge discovery.pdf:/Users/bill/D/Zotero/storage/A7D2PKJC/smith-miles et al - understanding the relationship between scheduling problem structure and heuristic performance using knowledge discovery.pdf:application/pdf},
}

@incollection{smith-milesKnowledgeDiscoveryApproach2009,
	address = {Berlin, Heidelberg},
	title = {A {Knowledge} {Discovery} {Approach} to {Understanding} {Relationships} between {Scheduling} {Problem} {Structure} and {Heuristic} {Performance}},
	volume = {5851},
	isbn = {978-3-642-11168-6 978-3-642-11169-3},
	url = {http://link.springer.com/10.1007/978-3-642-11169-3_7},
	abstract = {Using a knowledge discovery approach, we seek insights into the relationships between problem structure and the effectiveness of scheduling heuristics. A large collection of 75,000 instances of the single machine early/tardy scheduling problem is generated, characterized by six features, and used to explore the performance of two common scheduling heuristics. The best heuristic is selected using rules from a decision tree with accuracy exceeding 97\%. A self-organizing map is used to visualize the feature space and generate insights into heuristic performance. This paper argues for such a knowledge discovery approach to be applied to other optimization problems, to contribute to automation of algorithm selection as well as insightful algorithm design.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smith-Miles, Kate A. and James, Ross J. W. and Giffin, John W. and Tu, Yiqing},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Stützle, Thomas},
	year = {2009},
	doi = {10.1007/978-3-642-11169-3_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {89--103},
	file = {smith-miles et al 2009 - a knowledge discovery approach to understanding relationships between scheduling problem structure and heuristic performance.pdf:/Users/bill/D/Zotero/storage/7JWRVFSN/smith-miles et al 2009 - a knowledge discovery approach to understanding relationships between scheduling problem structure and heuristic performance.pdf:application/pdf},
}

@inproceedings{smith-milesMetalearningDataSummarization2010,
	address = {Barcelona, Spain},
	title = {Meta-learning for data summarization based on instance selection method},
	isbn = {978-1-4244-6909-3},
	url = {http://ieeexplore.ieee.org/document/5585986/},
	doi = {10.1109/CEC.2010.5585986},
	abstract = {The purpose of instance selection is to identify which instances (examples, patterns) in a large dataset should be selected as representatives of the entire dataset, without significant loss of information. When a machine learning method is applied to the reduced dataset, the accuracy of the model should not be significantly worse than if the same method were applied to the entire dataset. The reducibility of any dataset, and hence the success of instance selection methods, surely depends on the characteristics of the dataset, as well as the machine learning method. This paper adopts a meta-learning approach, via an empirical study of 112 classification datasets from the UCI Repository [1], to explore the relationship between data characteristics, machine learning methods, and the success of instance selection method.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {{IEEE} {Congress} on {Evolutionary} {Computation}},
	publisher = {IEEE},
	author = {Smith-Miles, Kate and Islam, Rafiqul},
	month = jul,
	year = {2010},
	pages = {1--8},
	file = {smith-miles islam 2010 - meta-learning for data summarization based on instance selection method.pdf:/Users/bill/D/Zotero/storage/VRJ4VPQW/smith-miles islam 2010 - meta-learning for data summarization based on instance selection method.pdf:application/pdf},
}

@incollection{smith-milesMetaLearningInstanceSelection2011,
	address = {Berlin, Heidelberg},
	title = {Meta-{Learning} of {Instance} {Selection} for {Data} {Summarization}},
	volume = {358},
	isbn = {978-3-642-20979-6 978-3-642-20980-2},
	url = {http://link.springer.com/10.1007/978-3-642-20980-2_2},
	abstract = {The goal of instance selection is to identify which instances (examples, patterns) in a large dataset should be selected as representatives of the entire dataset, without significant loss of information. When a machine learning method is applied to the reduced dataset, the accuracy of the model should not be significantly worse than if the same method were applied to the entire dataset. The reducibility of any dataset, and hence the success of instance selection methods, surely depends on the characteristics of the dataset. However the relationship between data characteristics and the reducibility achieved by instance selection methods has not been extensively tested. This chapter adopts a meta-learning approach, via an empirical study of 112 classification datasets, to explore the relationship between data characteristics and the success of a naïve instance selection method. The approach can be readily extended to explore how the data characteristics influence the performance of many more sophisticated instance selection methods.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Meta-{Learning} in {Computational} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smith-Miles, Kate A. and Islam, Rafiqul M. D.},
	editor = {Kacprzyk, Janusz and Jankowski, Norbert and Duch, Włodzisław and Gra̧bczewski, Krzysztof},
	year = {2011},
	doi = {10.1007/978-3-642-20980-2_2},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {77--95},
	file = {smith-miles islam 2011 - meta-learning of instance selection for data summarization.pdf:/Users/bill/D/Zotero/storage/AM83HNHR/smith-miles islam 2011 - meta-learning of instance selection for data summarization.pdf:application/pdf},
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011a,
	address = {Berlin, Heidelberg},
	title = {Generalising {Algorithm} {Performance} in {Instance} {Space}: {A} {Timetabling} {Case} {Study}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	shorttitle = {Generalising {Algorithm} {Performance} in {Instance} {Space}},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_41},
	abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a “footprint” in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the diﬀerences between the instance generation methods, and the suitability of diﬀerent algorithms.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smith-Miles, Kate and Lopes, Leo},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_41},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {524--538},
	file = {smith-miles lopes 2011 - generalising algorithm performance in instance space - a timetabling case study.pdf:/Users/bill/D/Zotero/storage/RHL3FW4V/smith-miles lopes 2011 - generalising algorithm performance in instance space - a timetabling case study.pdf:application/pdf},
}

@inproceedings{smith-milesMeasuringAlgorithmFootprints2012,
	address = {Brisbane, Australia},
	title = {Measuring algorithm footprints in instance space},
	isbn = {978-1-4673-1509-8 978-1-4673-1510-4 978-1-4673-1508-1},
	url = {http://ieeexplore.ieee.org/document/6252992/},
	doi = {10.1109/CEC.2012.6252992},
	abstract = {This paper proposes a new methodology to determine the relative performance of optimization algorithms across various classes of instances. Rather than reporting performance based on a chosen test set of benchmark instances, we aim to develop metrics for an algorithm’s performance generalized across a diverse set of instances. Instances are summarized by a set of features that correlate with difficulty, and we propose methods for visualizing instances and algorithm performance in this high-dimensional feature space. The footprint of an algorithm is where good performance can be expected, and we propose new metrics to measure the relative size of an algorithm’s footprint in instance space. The methodology is demonstrated using the Traveling Salesman Problem as a case study.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2012 {IEEE} {Congress} on {Evolutionary} {Computation}},
	publisher = {IEEE},
	author = {Smith-Miles, Kate and Tan, Thomas T.},
	month = jun,
	year = {2012},
	pages = {1--8},
	file = {smith-miles tan 2012 - measuring algorithm footprints in instance space.pdf:/Users/bill/D/Zotero/storage/AIRWLG2N/smith-miles tan 2012 - measuring algorithm footprints in instance space.pdf:application/pdf},
}

@article{smith-milesDiscoveringSuitabilityOptimisation2011,
	title = {Discovering the suitability of optimisation algorithms by learning from evolved instances},
	volume = {61},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-011-9230-5},
	doi = {10.1007/s10472-011-9230-5},
	abstract = {The suitability of an optimisation algorithm selected from within an algorithm portfolio depends upon the features of the particular instance to be solved. Understanding the relative strengths and weaknesses of different algorithms in the portfolio is crucial for effective performance prediction, automated algorithm selection, and to generate knowledge about the ideal conditions for each algorithm to influence better algorithm design. Relying on well-studied benchmark instances, or randomly generated instances, limits our ability to truly challenge each of the algorithms in a portfolio and determine these ideal conditions. Instead we use an evolutionary algorithm to evolve instances that are uniquely easy or hard for each algorithm, thus providing a more direct method for studying the relative strengths and weaknesses of each algorithm. The proposed methodology ensures that the meta-data is sufficient to be able to learn the features of the instances that uniquely characterise the ideal conditions for each algorithm. A case study is presented based on a comprehensive study of the performance of two heuristics on the Travelling Salesman Problem. The results show that prediction of search effort as well as the best performing algorithm for a given instance can be achieved with high accuracy.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Smith-Miles, Kate and van Hemert, Jano},
	month = feb,
	year = {2011},
	pages = {87--104},
	file = {smith-miles van hemert 2011 - discovering the suitability of optimisation algorithms by learning from evolved instances.pdf:/Users/bill/D/Zotero/storage/RBX5XNJP/smith-miles van hemert 2011 - discovering the suitability of optimisation algorithms by learning from evolved instances.pdf:application/pdf},
}

@article{kwokOptimizationIntermittencySelfOrganizing2005,
	title = {Optimization via {Intermittency} with a {Self}-{Organizing} {Neural} {Network}},
	volume = {17},
	issn = {0899-7667, 1530-888X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/0899766054796860},
	doi = {10.1162/0899766054796860},
	abstract = {One of the major obstacles in using neural networks to solve combinatorial optimization problems is the convergence toward one of the many local minima instead of the global minima. In this letter, we propose a technique that enables a self-organizing neural network to escape from local minima by virtue of the intermittency phenomenon. It gives rise to novel search dynamics that allow the system to visit multiple global minima as meta-stable states. Numerical experiments performed suggest that the phenomenon is a combined effect of Kohonen-type competitive learning and the iterated softmax function operating near bifurcation. The resultant intermittent search exhibits fractal characteristics when the optimization performance is at its peak in the form of 1/f signals in the time evolution of the cost, as well as power law distributions in the meta-stable solution states. The N-Queens problem is used as an example to illustrate the meta-stable convergence process that sequentially generates, in a single run, 92 solutions to the 8-Queens problem and 4024 solutions to the 17-Queens problem.},
	language = {en},
	number = {11},
	urldate = {2020-11-04},
	journal = {Neural Computation},
	author = {Kwok, Terence and Smith, Kate A.},
	month = nov,
	year = {2005},
	pages = {2454--2481},
	file = {0899766054796860.pdf:/Users/bill/D/Zotero/storage/ZIIL4U5Y/0899766054796860.pdf:application/pdf},
}

@article{daolioProblemFeaturesAlgorithm2017,
	title = {Problem {Features} versus {Algorithm} {Performance} on {Rugged} {Multiobjective} {Combinatorial} {Fitness} {Landscapes}},
	volume = {25},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco_a_00193},
	doi = {10.1162/evco_a_00193},
	abstract = {In this article, we attempt to understand and to contrast the impact of problem features on the performance of randomized search heuristics for black-box multiobjective combinatorial optimization problems. At ﬁrst, we measure the performance of two conventional dominance-based approaches with unbounded archive on a benchmark of enumerable binary optimization problems with tunable ruggedness, objective space dimension, and objective correlation (ρMNK-landscapes). Precisely, we investigate the expected runtime required by a global evolutionary optimization algorithm with an ergodic variation operator (GSEMO) and by a neighborhood-based local search heuristic (PLS), to identify a (1 + ε)−approximation of the Pareto set. Then, we deﬁne a number of problem features characterizing the ﬁtness landscape, and we study their intercorrelation and their association with algorithm runtime on the benchmark instances. At last, with a mixed-effects multilinear regression we assess the individual and joint effect of problem features on the performance of both algorithms, within and across the instance classes deﬁned by benchmark parameters. Our analysis reveals further insights into the importance of ruggedness and multimodality to characterize instance hardness for this family of multiobjective optimization problems and algorithms.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {Daolio, Fabio and Liefooghe, Arnaud and Verel, Sébastien and Aguirre, Hernán and Tanaka, Kiyoshi},
	month = dec,
	year = {2017},
	pages = {555--585},
	file = {daolio et al 2017 - Problem Features versus Algorithm Performance on Rugged Multiobjective Combinatorial Fitness Landscapes.pdf:/Users/bill/D/Zotero/storage/EBXA6UVM/daolio et al 2017 - Problem Features versus Algorithm Performance on Rugged Multiobjective Combinatorial Fitness Landscapes.pdf:application/pdf},
}

@article{franklinEffectSpeciesRarity2009a,
	title = {Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern {California}},
	volume = {15},
	issn = {13669516, 14724642},
	url = {http://doi.wiley.com/10.1111/j.1472-4642.2008.00536.x},
	doi = {10.1111/j.1472-4642.2008.00536.x},
	abstract = {Aim Several studies have found that more accurate predictive models of species’ occurrences can be developed for rarer species; however, one recent study found the relationship between range size and model performance to be an artefact of sample prevalence, that is, the proportion of presence versus absence observations in the data used to train the model. We examined the effect of model type, species rarity class, species’ survey frequency, detectability and manipulated sample prevalence on the accuracy of distribution models developed for 30 reptile and amphibian species.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Diversity and Distributions},
	author = {Franklin, Janet and Wejnert, Katherine E. and Hathaway, Stacie A. and Rochester, Carlton J. and Fisher, Robert N.},
	month = jan,
	year = {2009},
	pages = {167--177},
	file = {franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf:/Users/bill/D/Zotero/storage/U8B8GE5G/franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf:application/pdf},
}

@article{naimiStellaRSoftwareTranslate2012a,
	title = {{StellaR}: {A} software to translate {Stella} models into {R} open-source environment},
	volume = {38},
	issn = {13648152},
	shorttitle = {{StellaR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815212001673},
	doi = {10.1016/j.envsoft.2012.05.012},
	abstract = {Stella is a popular system dynamics modeling tool, which helps to put together conceptual diagrams and converts them into numeric computer models. Although it can be very useful, especially in participatory modeling, it lacks the power and ﬂexibility of a programming language. This paper presents the StellaR software which translates a Stella model into a model in R, an open source high level programming language. This allows using conceptual modeling tools provided in Stella, together with computational functionality and programming ﬂexibility provided in R. It also opens access to powerful software libraries available in R, which is especially useful for spatial modeling.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Environmental Modelling \& Software},
	author = {Naimi, Babak and Voinov, Alexey},
	month = dec,
	year = {2012},
	pages = {117--118},
	file = {naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf:/Users/bill/D/Zotero/storage/M6BMIF94/naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf:application/pdf},
}

@article{dvovrakMaximizingSubmodularFunction,
	title = {Maximizing a {Submodular} {Function} with {Viability} {Constraints}},
	language = {en},
	author = {Dvoˇrak, Wolfgang and Henzinger, Monika and Williamson, David P},
	pages = {15},
	file = {dvorak et al - maximizing a submodular function with viability constraints.pdf:/Users/bill/D/Zotero/storage/83DBBMKI/dvorak et al - maximizing a submodular function with viability constraints.pdf:application/pdf},
}

@article{dvovrakMaximizingSubmodularFunctiona,
	title = {Maximizing a {Submodular} {Function} with {Viability} {Constraints}},
	language = {en},
	author = {Dvoˇrak, Wolfgang and Henzinger, Monika and Williamson, David P},
	pages = {15},
	file = {dvorak et al - maximizing a submodular function with viability constraints.pdf:/Users/bill/D/Zotero/storage/HQATPSXW/dvorak et al - maximizing a submodular function with viability constraints.pdf:application/pdf},
}

@article{smithLocatingPhaseTransition1996,
	title = {Locating the phase transition in binary constraint satisfaction problems},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000526},
	doi = {10.1016/0004-3702(95)00052-6},
	abstract = {The phase transition in binary constraint satisfaction problems, i.e. the transition from a region in which almost all problems have many solutions to a region in which almost all problems have no solutions, as the constraints become tighter, is investigated by examining the behaviour of samples of randomly-generated problems. In contrast to theoretical work, which is concerned with the asymptotic behaviour of problems as the number of variables becomes larger, this paper is concerned with the location of the phase transition in finite problems. The accuracy of a prediction based on the expected number of solutions is discussed; it is shown that the variance of the number of solutions can be used to set bounds on the phase transition and to indicate the accuracy of the prediction. A class of sparse problems, for which the prediction is known to be inaccurate, is considered in detail; it is shown that, for these problems, the phase transition depends on the topology of the constraint graph as well as on the tightness of the constraints.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Smith, Barbara M. and Dyer, Martin E.},
	month = mar,
	year = {1996},
	pages = {155--181},
	file = {smith dyer 1996 - locating the phase transition in binary constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/K4L3DEWL/smith dyer 1996 - locating the phase transition in binary constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:application/pdf},
}

@inproceedings{xu2005ipnijcai,
	address = {Edinburgh, Scotland, UK},
	title = {A {Simple} {Model} to {Generate} {Hard} {Satisﬁable} {Instances}},
	url = {http://www.ijcai.org/Proceedings/05/Papers/0989.pdf},
	abstract = {In this paper, we try to further demonstrate that the models of random CSP instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisﬁable instances whose hardness is similar to unforced satisﬁable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.},
	language = {en},
	booktitle = {{IJCAI}-05, {Proceedings} of the {Nineteenth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Xu, Ke and Boussemart, Frederic and Hemery, Fred and Lecoutre, Christophe},
	year = {2005},
	keywords = {problem difficulty, bdpg, bdpg\_P1},
	pages = {337--342},
	file = {xu et al 2005 - A Simple Model to Generate Hard Satisfiable Instances - ijcai05 - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/Z3NISNDA/xu et al 2005 - A Simple Model to Generate Hard Satisfiable Instances - ijcai05 - BDPG - ANNO.pdf:application/pdf},
}

@article{xu2007ai,
	title = {Random constraint satisfaction: {Easy} generation of hard (satisfiable) instances},
	volume = {171},
	issn = {00043702},
	shorttitle = {Random constraint satisfaction},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370207000653},
	doi = {10.1016/j.artint.2007.04.001},
	abstract = {In this paper, we show that the models of random CSP instances proposed by Xu and Li [K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artiﬁcial Intelligence Research 12 (2000) 93–103; K. Xu, W. Li, Many hard examples in exact phase transitions with application to generating hard satisﬁable instances, Technical report, CoRR Report cs.CC/0302001, Revised version in Theoretical Computer Science 355 (2006) 291–302] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisﬁable instances whose hardness is similar to unforced satisﬁable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.},
	language = {en},
	number = {8-9},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Xu, Ke and Boussemart, Frédéric and Hemery, Fred and Lecoutre, Christophe},
	month = jun,
	year = {2007},
	keywords = {problem difficulty, bdpg, bdpg\_P1},
	pages = {514--534},
	file = {xu et al 2007 - random constraint satisfaction - easy generation of hard satisfiable problems - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/BVDE8EK9/xu et al 2007 - random constraint satisfaction - easy generation of hard satisfiable problems - BDPG - ANNO.pdf:application/pdf},
}

@article{xu,
	title = {Many {Hard} {Examples} in {Exact} {Phase} {Transitions} with {Application} to {Generating} {Hard} {Satisﬁable} {Instances}},
	abstract = {This paper ﬁrst analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisﬁable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisﬁable and forced satisﬁable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three diﬀerent kinds of phase transition behavior in NP-complete problems.},
	language = {en},
	author = {Xu, Ke and Li, Wei},
	keywords = {problem difficulty, bdpg, bdpg\_P1},
	pages = {19},
	file = {xu li - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/3MQZ2AWX/xu li - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - BDPG - ANNO.pdf:application/pdf},
}

@article{xu2006tcs,
	title = {Many hard examples in exact phase transitions},
	volume = {355},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397506000181},
	doi = {10.1016/j.tcs.2006.01.001},
	abstract = {This paper analyzes the resolution complexity of two random constraint satisfaction problem (CSP) models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CSPs and CNF formulas hard to solve, which can be useful in the experimental evaluation of CSP and SAT algorithms, but also propose models with both many hard instances and exact phase transitions. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Theoretical Computer Science},
	author = {Xu, Ke and Li, Wei},
	month = apr,
	year = {2006},
	keywords = {problem difficulty, bdpg, bdpg\_P1},
	pages = {291--302},
	file = {xu li 2006 - many hard examples in exact phase transitions - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/BSRFL4WP/xu li 2006 - many hard examples in exact phase transitions - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:application/pdf},
}

@article{langfordBenchmarksHiddenOptimum,
	title = {Benchmarks with {Hidden} {Optimum} {Solutions} for {Set} {Covering}, {Set} {Packing} and {Winner} {Determination}},
	language = {en},
	author = {Langford, Bill},
	pages = {3},
	file = {xu web page good - Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination- httpCOLON__www.nlsde.buaa.edu.cn_TILDAkexu_benchmarks_set-benchmarks.htm.pdf:/Users/bill/D/Zotero/storage/TB9KFQWS/xu web page good - Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination- httpCOLON__www.nlsde.buaa.edu.cn_TILDAkexu_benchmarks_set-benchmarks.htm.pdf:application/pdf},
}

@article{xuManyHardExamplesb,
	title = {Many {Hard} {Examples} in {Exact} {Phase} {Transitions} with {Application} to {Generating} {Hard} {Satisfiable} {Instances}},
	abstract = {This paper first analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisfiable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.},
	language = {en},
	author = {Xu, K},
	pages = {2},
	file = {xu web page - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - Resolution Complexity of Random CSP.pdf:/Users/bill/D/Zotero/storage/NWZADC6W/xu web page - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - Resolution Complexity of Random CSP.pdf:application/pdf},
}

@article{BenchmarksHiddenOptimum,
	title = {Benchmarks with {Hidden} {Optimum} {Solutions} for {Set} {Covering}, {Set} {Packing} and {Winner} {Determination}},
	language = {en},
	pages = {3},
	file = {xu web page- Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination.pdf:/Users/bill/D/Zotero/storage/S6BPQ692/xu web page- Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination.pdf:application/pdf},
}

@article{kanIntroductionAnalysisApproximation1986,
	title = {An introduction to the analysis of approximation algorithms},
	volume = {14},
	issn = {0166218X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0166218X86900594},
	doi = {10.1016/0166-218X(86)90059-4},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Discrete Applied Mathematics},
	author = {Kan, A.H.G.Rinnooy},
	month = jun,
	year = {1986},
	pages = {171--185},
	file = {1-s2.0-0166218X86900594-main.pdf:/Users/bill/D/Zotero/storage/985GJ8FD/1-s2.0-0166218X86900594-main.pdf:application/pdf},
}

@article{prosserEmpiricalStudyPhase1996,
	title = {An empirical study of phase transitions in binary constraint satisfaction problems},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000488},
	doi = {10.1016/0004-3702(95)00048-8},
	abstract = {An empirical study of randomly generated binary constraint satisfaction problems reveals that for problems with a given number of variables, domain size, and connectivity there is a critical level of constraint tightness at which a phase transition occurs. At the phase transition, problems change from being soluble to insoluble, and the difficulty of problems increases dramatically. A theory developed by Williams and Hogg [44], and independently developed by Smith [37], predicts where the hardest problems should occur. It is shown that the theory is in close agreement with the empirical results, except when constraint graphs are sparse.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Prosser, Patrick},
	month = mar,
	year = {1996},
	pages = {81--109},
	file = {1-s2.0-0004370295000488-main.pdf:/Users/bill/D/Zotero/storage/B2VCWB7W/1-s2.0-0004370295000488-main.pdf:application/pdf},
}

@article{pembertonEpsilontransformationExploitingPhase1996,
	title = {Epsilon-transformation: exploiting phase transitions to solve combinatorial optimization problems},
	volume = {81},
	issn = {00043702},
	shorttitle = {Epsilon-transformation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000577},
	doi = {10.1016/0004-3702(95)00057-7},
	abstract = {It has been shown that there exists a transition in the average-case complexity of tree search problems, from exponential to polynomial in the search depth. We develop a new method, called E-transformation, which makes use of this complexity transition, to find a suboptimal solution. With a random tree model, we show that, as the tree depth approaches infinity, the expected number of nodes expanded by branch-and-bound (BnB) using stransformation is at most cubic in the search depth, and that the relative error of the solution cost found with respect to the optimal solution cost is bounded above by a small constant. We also present an iterative version of c-transformation that can be used to find both suboptimal and optimal goal nodes. Depth-first BnB (DFBnB) using iterative Hransformation significantly improves upon DPBnB on random trees with large branching factors and deep goal nodes, finding a better solution sooner. We then present experimental results for ctransformation and iterative e-transformation on the asymmetric traveling salesman problem ( ATSP) and tbe maximum boolean satisfiability problem, and identify the conditions under which these two methods are effective. On the ATSP, DFBnB using Etransformation outperforms a well-known local search method, and DFBnB using iterative Etransformation improves upon DFBnB.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Pemberton, Joseph C. and Zhang, Weixiong},
	month = mar,
	year = {1996},
	pages = {297--325},
	file = {1-s2.0-0004370295000577-main.pdf:/Users/bill/D/Zotero/storage/V97NIGIE/1-s2.0-0004370295000577-main.pdf:application/pdf},
}

@article{holteMaximizingMultiplePattern2006,
	title = {Maximizing over multiple pattern databases speeds up heuristic search},
	volume = {170},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370206000804},
	doi = {10.1016/j.artint.2006.09.002},
	abstract = {A pattern database (PDB) is a heuristic function stored as a lookup table. This paper considers how best to use a ﬁxed amount (m units) of memory for storing pattern databases. In particular, we examine whether using n pattern databases of size m/n instead of one pattern database of size m improves search performance. In all the state spaces considered, the use of multiple smaller pattern databases reduces the number of nodes generated by IDA*. The paper provides an explanation for this phenomenon based on the distribution of heuristic values that occur during search.},
	language = {en},
	number = {16-17},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Holte, Robert C. and Felner, Ariel and Newton, Jack and Meshulam, Ram and Furcy, David},
	month = nov,
	year = {2006},
	pages = {1123--1136},
	file = {1-s2.0-S0004370206000804-main.pdf:/Users/bill/D/Zotero/storage/Z6CSSD2S/1-s2.0-S0004370206000804-main.pdf:application/pdf},
}

@article{dicksonSystematicIdentificationPotential2014,
	title = {Systematic identification of potential conservation priority areas on roadless {Bureau} of {Land} {Management} lands in the western {United} {States}},
	volume = {178},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320714002882},
	doi = {10.1016/j.biocon.2014.08.001},
	abstract = {With ongoing global change, there is an urgent need to expand existing networks of important conservation areas around the world. In the western United States, vast areas of public land, including those administered by the Bureau of Land Management (BLM), present substantial conservation opportunities. For 11 contiguous western states, we used a novel multiple-criteria analysis to model and map contiguous areas of roadless BLM land that possessed important ecological indicators of high biodiversity, resilience to climate change, and landscape connectivity. Speciﬁcally, we leveraged available spatial datasets to implement a systematic and statistically robust analysis of seven key indicators at three different spatial scales, and to identify the locations of potential conservation priority areas (CPAs) across 294,274 km2 of roadless BLM land. Within this extent, and based on conservative thresholds in our results, we identiﬁed 43,417 km2 of land with relatively high conservation value and 117 unique CPAs totaling 6291 km2. Most CPA lands were located in Utah, Colorado, Arizona, Oregon, and Nevada. Overall, CPAs had higher species richness, vegetation community diversity, topographic complexity, and surface water availability than existing BLM protected areas. CPAs often corresponded with locations known to have important wilderness characteristics or were adjacent to established areas of ecological, social, or cultural importance. These CPAs represent a diverse set of places that can be used by multiple stakeholders in ongoing or future landscape conservation and special designation efforts in BLM and adjacent ownerships. Our methodological framework and novel weighting approach can accommodate a wide range of input variables and is readily applicable to other jurisdictions and regions within the U.S. and beyond. Ó 2014 Elsevier Ltd. All rights reserved.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Dickson, Brett G. and Zachmann, Luke J. and Albano, Christine M.},
	month = oct,
	year = {2014},
	pages = {117--127},
	file = {1-s2.0-S0006320714002882-main.pdf:/Users/bill/D/Zotero/storage/RS2XXYXJ/1-s2.0-S0006320714002882-main.pdf:application/pdf},
}

@article{shyloRestartStrategiesOptimization2011,
	title = {Restart strategies in optimization: parallel and serial cases},
	volume = {37},
	issn = {01678191},
	shorttitle = {Restart strategies in optimization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819110001110},
	doi = {10.1016/j.parco.2010.08.004},
	abstract = {This paper addresses the problem of minimizing the average running time of the Las Vegas type algorithm, both in serial and parallel setups. The necessary conditions for the existence of an effective restart strategy are presented. We clarify the counter-intuitive empirical observations of super linear speedup and relate parallel speedup with the restart properties of serial algorithms. The general property of restart distributions is derived. The computational experiments involving the state-of-the-art optimization algorithm are provided.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Parallel Computing},
	author = {Shylo, Oleg V. and Middelkoop, Timothy and Pardalos, Panos M.},
	month = jan,
	year = {2011},
	pages = {60--68},
	file = {1-s2.0-S0167819110001110-main.pdf:/Users/bill/D/Zotero/storage/GP9RGEGE/1-s2.0-S0167819110001110-main.pdf:application/pdf},
}

@article{tanResearchAdvanceSwarm2013,
	title = {Research {Advance} in {Swarm} {Robotics}},
	volume = {9},
	issn = {22149147},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S221491471300024X},
	doi = {10.1016/j.dt.2013.03.001},
	abstract = {The research progress of swarm robotics is reviewed in details. The swarm robotics inspired from nature is a combination of swarm intelligence and robotics, which shows a great potential in several aspects. First of all, the cooperation of nature swarm and swarm intelligence are brieﬂy introduced, and the special features of the swarm robotics are summarized compared to a single robot and other multi-individual systems. Then the modeling methods for swarm robotics are described, followed by a list of several widely used swarm robotics entity projects and simulation platforms. Finally, as a main part of this paper, the current research on the swarm robotic algorithms are presented in detail, including cooperative control mechanisms in swarm robotics for ﬂocking, navigating and searching applications.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Defence Technology},
	author = {Tan, Ying and Zheng, Zhong-yang},
	month = mar,
	year = {2013},
	pages = {18--39},
	file = {1-s2.0-S221491471300024X-main.pdf:/Users/bill/D/Zotero/storage/LJQI5MII/1-s2.0-S221491471300024X-main.pdf:application/pdf},
}

@article{tulowieckiUsingVegetationData2014,
	title = {Using vegetation data within presettlement land survey records for species distribution modeling: {A} tale of two datasets},
	volume = {291},
	issn = {03043800},
	shorttitle = {Using vegetation data within presettlement land survey records for species distribution modeling},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014003627},
	doi = {10.1016/j.ecolmodel.2014.07.025},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Tulowiecki, Stephen J.},
	month = nov,
	year = {2014},
	pages = {109--120},
	file = {1-s2.0-S0304380014003627-main.pdf:/Users/bill/D/Zotero/storage/NVJJ4QZ8/1-s2.0-S0304380014003627-main.pdf:application/pdf},
}

@article{meiyappanSpatialModelingAgricultural2014,
	title = {Spatial modeling of agricultural land use change at global scale},
	volume = {291},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014003640},
	doi = {10.1016/j.ecolmodel.2014.07.027},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Meiyappan, Prasanth and Dalton, Michael and O’Neill, Brian C. and Jain, Atul K.},
	month = nov,
	year = {2014},
	pages = {152--174},
	file = {1-s2.0-S0304380014003640-main.pdf:/Users/bill/D/Zotero/storage/GU52A42Z/1-s2.0-S0304380014003640-main.pdf:application/pdf},
}

@article{antonarakisUncertaintyInitialForest2014,
	title = {Uncertainty in initial forest structure and composition when predicting carbon dynamics in a temperate forest},
	volume = {291},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014003780},
	doi = {10.1016/j.ecolmodel.2014.07.030},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Antonarakis, A.S.},
	month = nov,
	year = {2014},
	pages = {134--141},
	file = {1-s2.0-S0304380014003780-main.pdf:/Users/bill/D/Zotero/storage/FFUKDVHF/1-s2.0-S0304380014003780-main.pdf:application/pdf},
}

@article{larocqueIntegratedModellingSoftware2015,
	title = {Integrated modelling software platform development for effective use of ecosystem models},
	volume = {306},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014003809},
	doi = {10.1016/j.ecolmodel.2014.08.003},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Larocque, Guy R. and Bhatti, Jagtar and Arsenault, André},
	month = jun,
	year = {2015},
	pages = {318--325},
	file = {1-s2.0-S0304380014003809-main.pdf:/Users/bill/D/Zotero/storage/TWRJ3IRW/1-s2.0-S0304380014003809-main.pdf:application/pdf},
}

@article{maPlantFunctionalDiversity2014,
	title = {Plant functional diversity in agricultural margins and fallow fields varies with landscape complexity level: {Conservation} implications},
	volume = {22},
	issn = {16171381},
	shorttitle = {Plant functional diversity in agricultural margins and fallow fields varies with landscape complexity level},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1617138114000909},
	doi = {10.1016/j.jnc.2014.08.006},
	abstract = {A consensus has been established that functional traits rather than taxonomic diversity play a fundamental role in linking biodiversity with ecosystem processes and associated services. This study from Finland addressed an issue of relative values of fallow and field margin biotopes in conservation of plant functional diversity (based on six functional traits of relevance to ecosystem services, and diversity of multiple traits) in agricultural landscapes differing in their structural complexity. Relative covers of plant species were surveyed in sampling plots located in perennial fallow fields and three types of perennial margins (margins between crop fields, along forest edges and by river) in three types of landscape context (simple, intermediate and complex). Fallow fields significantly contributed to the total functional diversity only in simple landscapes. The river margins provided the greatest functional diversity, especially in reproduction and regeneration traits while crop margins were consistently characterised by the lowest functional diversity. Substantial functional diversity of fallow patches in simple landscapes was due to high abundance of functional species, while that of river margins stemmed from presence of unique species. The plant functional diversity progressively declined with agricultural landscapes becoming simplified. The study indicates non-cropped biotopes having complementary roles in ensuring multifunctionality of agro-landscapes and confirms importance of biotope mosaic for functional diversity.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Journal for Nature Conservation},
	author = {Ma, Maohua and Herzon, Irina},
	month = dec,
	year = {2014},
	pages = {525--531},
	file = {1-s2.0-S1617138114000909-main.pdf:/Users/bill/D/Zotero/storage/5FN8KJUT/1-s2.0-S1617138114000909-main.pdf:application/pdf},
}

@article{har-peledWEIGHTEDGEOMETRICSET2012,
	title = {{WEIGHTED} {GEOMETRIC} {SET} {COVER} {PROBLEMS} {REVISITED}},
	abstract = {We study several set cover problems in low dimensional geometric settings. Speciﬁcally, we describe a PTAS for the problem of computing a minimum cover of given points by a set of weighted fat objects. Here, we allow the objects to expand by some prespeciﬁed δ-fraction of their diameter.},
	language = {en},
	author = {Har-Peled, Sariel and Lee, Mira},
	year = {2012},
	pages = {21},
	file = {77-357-1-PB.pdf:/Users/bill/D/Zotero/storage/DTY6J2QH/77-357-1-PB.pdf:application/pdf},
}

@article{monassonDeterminingComputationalComplexity1999,
	title = {Determining computational complexity from characteristic ‘phase transitions’},
	volume = {400},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/22055},
	doi = {10.1038/22055},
	language = {en},
	number = {6740},
	urldate = {2020-11-04},
	journal = {Nature},
	author = {Monasson, Rémi and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
	month = jul,
	year = {1999},
	pages = {133--137},
	file = {400133a0.pdf:/Users/bill/D/Zotero/storage/CPWF7QBG/400133a0.pdf:application/pdf},
}

@inproceedings{minhSDASimpleUnifying2010,
	address = {Hanoi, Vietnam},
	title = {{SDA}*: {A} {Simple} and {Unifying} {Solution} to {Recent} {Bioinformatic} {Challenges} for {Conservation} {Genetics}},
	isbn = {978-1-4244-8334-1},
	shorttitle = {{SDA}*},
	url = {http://ieeexplore.ieee.org/document/5632157/},
	doi = {10.1109/KSE.2010.24},
	abstract = {Recently, several algorithms have been proposed to tackle different conservation questions under phylogenetic diversity. Such questions are variants of the more general problem of budgeted reserve selection under split diversity, an NP-hard problem. Here, we present a novel framework, Split Diversity Algorithm* (SDA*), to unify all these attempts. More speciﬁcally, SDA* transforms the budgeted reserve selection problem into a binary linear programming (BLP), that can be solved by available linear optimization techniques. SDA* guarantees to ﬁnd optimal solutions in reasonable time.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2010 {Second} {International} {Conference} on {Knowledge} and {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Minh, Bui Quang and Klaere, Steffen and von Haeseler, Arndt},
	month = oct,
	year = {2010},
	pages = {33--37},
	file = {05632157.pdf:/Users/bill/D/Zotero/storage/FJEPJPQB/05632157.pdf:application/pdf},
}

@article{alonOnlineSetCover,
	title = {The {Online} {Set} {Cover} {Problem}},
	abstract = {Let X = \{1, 2, . . . , n\} be a ground set of n elements, and let S be a family of subsets of X, {\textbar}S{\textbar} = m, with a positive cost cS associated with each S ∈ S.},
	language = {en},
	author = {Alon, Noga and Awerbuch, Baruch and Azar, Yossi and Buchbinder, Niv},
	pages = {6},
	file = {aaabnproc2.pdf:/Users/bill/D/Zotero/storage/ZN3TUJTY/aaabnproc2.pdf:application/pdf},
}

@inproceedings{achlioptas2000psncaia,
	address = {Austin, TX},
	title = {Generating {Satisfiable} {Problem} {Instances}},
	abstract = {A major difﬁculty in evaluating incomplete local search style algorithms for constraint satisfaction problems is the need for a source of hard problem instances that are guaranteed to be satisﬁable. A standard approach to evaluate incomplete search methods has been to use a general problem generator and a complete search method to ﬁlter out the unsatisﬁable instances. Unfortunately, this approach cannot be used to create problem instances that are beyond the reach of complete search methods. So far, it has proven to be surprisingly difﬁcult to develop a direct generator for satisﬁable instances only. In this paper, we propose a generator that only outputs satisﬁable problem instances. We also show how one can ﬁnely control the hardness of the satisﬁable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. Finally, we use our problem distribution to show the easy-hard-easy pattern in search complexity for local search procedures, analogous to the previously reported pattern for complete search methods.},
	language = {en},
	booktitle = {Proceedings of the {Seventeenth} {National} {Conference} on {Artificial} {Intelligence} ({AAAI}-00),},
	author = {Achlioptas, Dimitris and Gomes, Carla and Kautz, Henry and Selman, Bart},
	year = {2000},
	pages = {256--261},
	file = {achlioptas et al 2000 - Generating Satisfiable Problem Instances.pdf:/Users/bill/D/Zotero/storage/BKKMWQ6U/achlioptas et al 2000 - Generating Satisfiable Problem Instances.pdf:application/pdf},
}

@article{achlioptasRigorousLocationPhase2005,
	title = {Rigorous location of phase transitions in hard optimization problems},
	volume = {435},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature03602},
	doi = {10.1038/nature03602},
	language = {en},
	number = {7043},
	urldate = {2020-11-04},
	journal = {Nature},
	author = {Achlioptas, Dimitris and Naor, Assaf and Peres, Yuval},
	month = jun,
	year = {2005},
	pages = {759--764},
	file = {achlioptas naor peres 2005 - Rigorous location of phase transitions in hard optimization problems.pdf:/Users/bill/D/Zotero/storage/47VM9J3Q/achlioptas naor peres 2005 - Rigorous location of phase transitions in hard optimization problems.pdf:application/pdf},
}

@article{biroliPhaseTransitionsComplexity2002,
	title = {Phase transitions and complexity in computer science: an overview of the statistical physics approach to the random satisÿability problem},
	abstract = {Phase transitions, ubiquitous in condensed matter physics, are encountered in computer science too. The existence of critical phenomena has deep consequences on computational complexity, that is the resolution times of various optimization or decision problems. Concepts and methods borrowed from the statistical physics of disordered and out-of-equilibrium systems shed new light on the dynamical operation of solving algorithms. c 2002 Published by Elsevier Science B.V.},
	language = {en},
	journal = {Physica A},
	author = {Biroli, Giulio and Cocco, Simona and Monasson, ReÃmi},
	year = {2002},
	pages = {14},
	file = {biroli et al 2002 - Phase transitions and complexity in computer science - an overview of the statistical physics approach to the random satisfiability problem.pdf:/Users/bill/D/Zotero/storage/I5UAUIZG/biroli et al 2002 - Phase transitions and complexity in computer science - an overview of the statistical physics approach to the random satisfiability problem.pdf:application/pdf},
}

@article{huntNCApproximationSchemesNP1998,
	title = {{NC}-{Approximation} {Schemes} for {NP}- and {PSPACE}-{Hard} {Problems} for {Geometric} {Graphs}},
	volume = {26},
	issn = {01966774},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0196677497909032},
	doi = {10.1006/jagm.1997.0903},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Journal of Algorithms},
	author = {Hunt, Harry B and Marathe, Madhav V and Radhakrishnan, Venkatesh and Ravi, S.S and Rosenkrantz, Daniel J and Stearns, Richard E},
	month = feb,
	year = {1998},
	pages = {238--274},
	file = {bunt-1998.pdf:/Users/bill/D/Zotero/storage/LYNF78LB/bunt-1998.pdf:application/pdf},
}

@incollection{gomesHeavytailedDistributionsCombinatorial1997,
	address = {Berlin, Heidelberg},
	title = {Heavy-tailed distributions in combinatorial search},
	volume = {1330},
	isbn = {978-3-540-63753-0 978-3-540-69642-1},
	url = {http://link.springer.com/10.1007/BFb0017434},
	abstract = {Combinatorial search methods often exhibit a large variability in performance. We study the cost profiles of combinatorial search procedures. Our study reveals some intriguing properties of such cost profiles. The distributions are often characterized by very long tails or "heavy tails". We will show that these distributions are best characterized by a general class of distributions that have no moments (i.e., an infinite mean, variance, etc.). Such non-standard distributions have recently been observed in areas as diverse as economics, statistical physics, and geophysics. They are closely related to fractal phenomena, whose study was introduced by Mandelbrot. We believe this is the first finding of these distributions in a purely computational setting. We also show how random restarts can effectively eliminate heavy-tailed behavior, thereby dramatically improving the overall performance of a search procedure.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}-{CP97}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gomes, Carla P. and Selman, Bart and Crato, Nuno},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Smolka, Gert},
	year = {1997},
	doi = {10.1007/BFb0017434},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {121--135},
	file = {chp%3A10.1007%2FBFb0017434.pdf:/Users/bill/D/Zotero/storage/MTBWHEJL/chp%3A10.1007%2FBFb0017434.pdf:application/pdf},
}

@article{davidsonPaper782Identifying,
	title = {Paper \#782:{Identifying} and {Generating} {Easy} {Sets} of {Constraints} {For} {Clustering}},
	abstract = {Clustering under constraints is a recent innovation in the artiﬁcial intelligence community that has yielded signiﬁcant practical beneﬁt. However, recent work has shown that for some negative forms of constraints the associated subproblem of just ﬁnding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufﬁcient conditions from graph theory to identify apriori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.},
	language = {en},
	author = {Davidson, Ian and Ravi, S S},
	pages = {7},
	file = {davidson ravi 2006 - Identifying and Generating Easy Sets of Constraints For Clustering - aaai06.pdf:/Users/bill/D/Zotero/storage/KIAZYPSZ/davidson ravi 2006 - Identifying and Generating Easy Sets of Constraints For Clustering - aaai06.pdf:application/pdf},
}

@article{dimitriouSATDistributionsPhase2003,
	title = {{SAT} {Distributions} with {Phase} {Transitions} between {Decision} and {Optimization} {Problems}},
	volume = {16},
	issn = {15710653},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1571065304004597},
	doi = {10.1016/S1571-0653(04)00459-7},
	abstract = {We present a generator for weighted instances of MAX k-SAT in which every clause has a weight associated with it and the goal is to maximize the total weight of satis¯ed clauses. Our generator produces formulas whose hardness can be ¯nely tuned by two parameters p and ± that control the weights of the clauses. Under the right choice of these parameters an easy-hard-easy pattern in the search complexity emerges which is similar to the patterns observed for traditional SAT distributions. What is remarkable, however, is that the generated distributions seem to lie in the middle ground between decision and optimization problems. Increasing the value of p from 0 to 1 has the e®ect of changing the shape of the computational cost from an easy-hard-easy pattern which is typical of decision problems to an easy-hard pattern which is typical of optimization problems. Thus our distributions seem to bridge the gap between decision and optimization versions of SAT.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Electronic Notes in Discrete Mathematics},
	author = {Dimitriou, Tassos},
	month = oct,
	year = {2003},
	pages = {1--14},
	file = {dimitriou 2003 - SAT Distributions with Phase Transitions between Decision and Optimization Problems.pdf:/Users/bill/D/Zotero/storage/IBD3VTU7/dimitriou 2003 - SAT Distributions with Phase Transitions between Decision and Optimization Problems.pdf:application/pdf},
}

@article{vanhemertEvolvingCombinatorialProblem2006b,
	title = {Evolving {Combinatorial} {Problem} {Instances} {That} {Are} {Difficult} to {Solve}},
	volume = {14},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco.2006.14.4.433},
	doi = {10.1162/evco.2006.14.4.433},
	abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difﬁcult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisﬁability, and the travelling salesman problem. Problem instances acquired through this technique are more difﬁcult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difﬁculty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {van Hemert, Jano I.},
	month = dec,
	year = {2006},
	pages = {433--462},
	file = {EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:/Users/bill/D/Zotero/storage/SS246XGB/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:application/pdf},
}

@article{guerriLearningTechniquesAutomatic,
	title = {Learning techniques for {Automatic} {Algorithm} {Portfolio} {Selection}},
	abstract = {The purpose of this paper is to show that a well known machine learning technique based on Decision Trees can be eﬀectively used to select the best approach (in terms of eﬃciency) in an algorithm portfolio for a particular case study: the Bid Evaluation Problem (BEP) in Combinatorial Auctions. In particular, we are interested in deciding when to use a Constraint Programming (CP) approach and when an Integer Programming (IP) approach, on the basis of the structure of the instance considered. Diﬀerent instances of the same problem present a diﬀerent structure, and one aspect (e.g. feasibility or optimality) can prevail on the other. We have extracted from a set of BEP instances, a number of parameters representing the instance structure. Some of them (few indeed) precisely identify the best strategy and its corresponding tuning to be used to face that instance. We will show that this approach is very promising, since it identiﬁes the most eﬃcient algorithm in the 90\% of the cases.},
	language = {en},
	author = {Guerri, Alessio and Milano, Michela},
	pages = {5},
	file = {ECAI2004.pdf:/Users/bill/D/Zotero/storage/9XV3P3RY/ECAI2004.pdf:application/pdf},
}

@article{eugster2014cs&da,
	title = {({Psycho}-)analysis of benchmark experiments: {A} formal framework for investigating the relationship between data sets and learning algorithms},
	volume = {71},
	issn = {01679473},
	shorttitle = {({Psycho}-)analysis of benchmark experiments},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947313002946},
	doi = {10.1016/j.csda.2013.08.007},
	abstract = {It is common knowledge that the performance of different learning algorithms depends on certain characteristics of the data—such as dimensionality, linear separability or sample size. However, formally investigating this relationship in an objective and reproducible way is not trivial. A new formal framework for describing the relationship between data set characteristics and the performance of different learning algorithms is proposed. The framework combines the advantages of benchmark experiments with the formal description of data set characteristics by means of statistical and information-theoretic measures and with the recursive partitioning of Bradley–Terry models for comparing the algorithms’ performances. The formal aspects of each component are introduced and illustrated by means of an artificial example. Its real-world usage is demonstrated with an application example consisting of thirteen widely-used data sets and six common learning algorithms. The Appendix provides information on the implementation and the usage of the framework within the R language.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Computational Statistics \& Data Analysis},
	author = {Eugster, Manuel J.A. and Leisch, Friedrich and Strobl, Carolin},
	month = mar,
	year = {2014},
	keywords = {bdpg, benchmarking},
	pages = {986--1000},
	file = {eugster leisch strobl 2014 - (Psycho-)analysis of benchmark experiments - A formal framework for investigating the relationship between data sets and learning algorithms - GUPPY PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/WRJ75F24/eugster leisch strobl 2014 - (Psycho-)analysis of benchmark experiments - A formal framework for investigating the relationship between data sets and learning algorithms - GUPPY PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{fujitoApproximationAlgorithmsSubmodular2000,
	title = {Approximation {Algorithms} for {Submodular} {Set} {Cover} with {Applications}},
	abstract = {The main problem considered is submodular set cover, the problem of minimizing a linear function under a nondecreasing submodular constraint, which generalizes both wellknown set cover and minimum matroid base problems. The problem is NP-hard, and two natural greedy heuristics are introduced along with analysis of their performance. As applications of these heuristics we consider various special cases of submodular set cover, including partial cover variants of set cover and vertex cover, and node-deletion problems for hereditary and matroidal properties. An approximation bound derived for each of them is either matching or generalizing the best existing bounds.},
	language = {en},
	author = {Fujito, Toshihiro},
	year = {2000},
	pages = {8},
	file = {fujito.pdf:/Users/bill/D/Zotero/storage/A6VRFEIG/fujito.pdf:application/pdf},
}

@article{grandoniSetCoveringOur,
	title = {Set {Covering} with {Our} {Eyes} {Closed}},
	language = {en},
	author = {Grandoni, Fabrizio and Gupta, Anupam and Sankowski, Piotr and Leonardi, Stefano and Singh, Mohit and Miettinen, Pauli},
	pages = {10},
	file = {GGLMSS08focs.pdf:/Users/bill/D/Zotero/storage/5RHV9ST2/GGLMSS08focs.pdf:application/pdf},
}

@article{grandoniSETCOVERINGOUR,
	title = {{SET} {COVERING} {WITH} {OUR} {EYES} {CLOSED}},
	language = {en},
	author = {Grandoni, Fabrizio and Gupta, Anupam and Leonardi, Stefano and Sankowski, Piotr and Singh, Mohit},
	pages = {23},
	file = {grandoni et al 2013 - SET COVERING WITH OUR EYES CLOSED.pdf:/Users/bill/D/Zotero/storage/8CK8GTZA/grandoni et al 2013 - SET COVERING WITH OUR EYES CLOSED.pdf:application/pdf},
}

@article{guoUncertaintyEnsembleModelling2015,
	title = {Uncertainty in ensemble modelling of large-scale species distribution: {Effects} from species characteristics and model techniques},
	volume = {306},
	issn = {03043800},
	shorttitle = {Uncertainty in ensemble modelling of large-scale species distribution},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014003792},
	doi = {10.1016/j.ecolmodel.2014.08.002},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Guo, Chuanbo and Lek, Sovan and Ye, Shaowen and Li, Wei and Liu, Jiashou and Li, Zhongjie},
	month = jun,
	year = {2015},
	pages = {67--75},
	file = {guo et al 2014 - Uncertainty in ensemble modelling of large-scale species distribution - Effects from species characteristics and model techniques.pdf:/Users/bill/D/Zotero/storage/HVPD6VUM/guo et al 2014 - Uncertainty in ensemble modelling of large-scale species distribution - Effects from species characteristics and model techniques.pdf:application/pdf},
}

@article{hansenNeuralNetworkEnsembles1990b,
	title = {Neural network ensembles},
	volume = {12},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/58871/},
	doi = {10.1109/34.58871},
	abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hansen, L.K. and Salamon, P.},
	month = oct,
	year = {1990},
	pages = {993--1001},
	file = {hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:/Users/bill/D/Zotero/storage/NB7FAGFV/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf:application/pdf},
}

@article{tinkamho2002itpami,
	title = {Complexity measures of supervised classification problems},
	volume = {24},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/990132/},
	doi = {10.1109/34.990132},
	abstract = {We studied a number of measures that characterize the difficulty of a classification problem, focusing on the geometrical complexity of the class boundary. We compared a set of real-world problems to random labelings of points and found that real problems contain structures in this measurement space that are significantly different from the random sets. Distributions of problems in this space show that there exist at least two independent factors affecting a problem's difficulty. We suggest using this space to describe a classifier's domain of competence. This can guide static and dynamic selection of classifiers for specific problems as well as subproblems formed by confinement, projection, and transformations of the feature vectors.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Tin Kam Ho} and Basu, M.},
	month = mar,
	year = {2002},
	pages = {289--300},
	file = {ho basu 2002 - complexity measures of supervised classification problems - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/VP3BMADZ/ho basu 2002 - complexity measures of supervised classification problems - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{odonnellLectureDefinitionsGreedy,
	title = {Lecture 1: {Deﬁnitions}; greedy algorithm for {Set}-{Cover} \& {Max}-{Coverage}},
	language = {en},
	author = {O’Donnell, Ryan and Shahaf, Dafna},
	pages = {5},
	file = {lecture1.pdf:/Users/bill/D/Zotero/storage/VZJ3NEN6/lecture1.pdf:application/pdf},
}

@article{leyton-brownPortfolioApproachAlgorithm,
	title = {A {Portfolio} {Approach} to {Algorithm} {Selection}},
	language = {en},
	author = {Leyton-Brown, Kevin and Nudelman, Eugene and Andrew, Galen and McFadden, Jim and Shoham, Yoav},
	pages = {2},
	file = {leyton-brown et al 2003 - A Portfolio Approach to Algorithm Selection - IJCAI03 - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/D8F8ZRHN/leyton-brown et al 2003 - A Portfolio Approach to Algorithm Selection - IJCAI03 - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{liLearningClusterbasedStructure2010,
	title = {Learning cluster-based structure to solve constraint satisfaction problems},
	volume = {60},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-010-9212-z},
	doi = {10.1007/s10472-010-9212-z},
	abstract = {The hybrid search algorithm for constraint satisfaction problems described here first uses local search to detect crucial substructures and then applies that knowledge to solve the problem. This paper shows the difficulties encountered by traditional and state-of-the-art learning heuristics when these substructures are overlooked. It introduces a new algorithm, Foretell, to detect dense and tight substructures called clusters with local search. It also develops two ways to use clusters during global search: one supports variable-ordering heuristics and the other makes inferences adapted to them. Together they improve performance on both benchmark and real-world problems.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Li, Xingjian and Epstein, Susan L.},
	month = oct,
	year = {2010},
	pages = {91--117},
	file = {li epstein 2010 - learning cluster-based structure to solve constraint satisfaction problems.pdf:/Users/bill/D/Zotero/storage/QFHH6SBU/li epstein 2010 - learning cluster-based structure to solve constraint satisfaction problems.pdf:application/pdf},
}

@article{likhachevAnytimeSearchDynamic2008,
	title = {Anytime search in dynamic graphs},
	volume = {172},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437020800060X},
	doi = {10.1016/j.artint.2007.11.009},
	abstract = {Agents operating in the real world often have limited time available for planning their next actions. Producing optimal plans is infeasible in these scenarios. Instead, agents must be satisﬁed with the best plans they can generate within the time available. One class of planners well-suited to this task are anytime planners, which quickly ﬁnd an initial, highly suboptimal plan, and then improve this plan until time runs out.},
	language = {en},
	number = {14},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Likhachev, Maxim and Ferguson, Dave and Gordon, Geoff and Stentz, Anthony and Thrun, Sebastian},
	month = sep,
	year = {2008},
	pages = {1613--1643},
	file = {likhachev ... thrun 2008 - anytime search in dynamic graphs.pdf:/Users/bill/D/Zotero/storage/V8TWP7KF/likhachev ... thrun 2008 - anytime search in dynamic graphs.pdf:application/pdf},
}

@article{mezardAnalyticAlgorithmicSolution2002,
	title = {Analytic and {Algorithmic} {Solution} of {Random} {Satisfiability} {Problems}},
	volume = {297},
	issn = {00368075, 10959203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1073287},
	doi = {10.1126/science.1073287},
	language = {en},
	number = {5582},
	urldate = {2020-11-04},
	journal = {Science},
	author = {Mezard, M.},
	month = aug,
	year = {2002},
	pages = {812--815},
	file = {Mezard.Science.297_812.pdf:/Users/bill/D/Zotero/storage/AUVWLJPQ/Mezard.Science.297_812.pdf:application/pdf},
}

@article{moorePhaseTransitionsNPcomplete,
	title = {Phase transitions in {NP}-complete problems: a challenge for probability, combinatorics, and computer science},
	language = {en},
	author = {Moore, Cristopher},
	pages = {36},
	file = {moore 2010 - Phase transitions in NP-complete problems - a challenge for probability, combinatorics, and computer science.pdf:/Users/bill/D/Zotero/storage/3KGC8M7J/moore 2010 - Phase transitions in NP-complete problems - a challenge for probability, combinatorics, and computer science.pdf:application/pdf},
}

@article{naudtsComparisonPredictiveMeasures2000,
	title = {A comparison of predictive measures of problem difficulty in evolutionary algorithms},
	volume = {4},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/843491/},
	doi = {10.1109/4235.843491},
	abstract = {This paper studies a number of predictive measures of problem difficulty, among which epistasis variance and fitness distance correlation are the most widely known. Our approach is based on comparing the reference class of a measure to a number of known easy function classes. First, we generalize the reference classes of fitness distance correlation and epistasis variance, and construct a new predictive measure that is insensitive to nonlinear fitness scaling. We then investigate the relations between the reference classes of the measures and a number of intuitively easy classes such as the steepest ascent optimizable functions. Within the latter class, functions that fool the predictive quality of all of the measures are easily found. This points out the need to further identify which functions are easy for a given class of evolutionary algorithms in order to design more efficient hardness indicators for them. We finally restrict attention to the genetic algorithm (GA), and consider both GA-easy and GA-hard fitness functions, and give experimental evidence that the values of the measures, based on random samples, can be completely unreliable and entirely uncorrelated to the convergence quality and convergence speed of GA instances using either proportional or ranking selection.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Naudts, B. and Kallel, L.},
	month = apr,
	year = {2000},
	pages = {1--15},
	file = {naudts and kallel 2000 - a comparison of predictive measures of problem difficulty in evolutionary algorithms - ieeeevocomp.pdf:/Users/bill/D/Zotero/storage/Q6I3KVA7/naudts and kallel 2000 - a comparison of predictive measures of problem difficulty in evolutionary algorithms - ieeeevocomp.pdf:application/pdf},
}

@article{fortnowStatusNPProblem2009,
	title = {The status of the {P} versus {NP} problem},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1562164.1562186},
	doi = {10.1145/1562164.1562186},
	language = {en},
	number = {9},
	urldate = {2020-11-04},
	journal = {Communications of the ACM},
	author = {Fortnow, Lance},
	month = sep,
	year = {2009},
	pages = {78--86},
	file = {p78-fortnow.pdf:/Users/bill/D/Zotero/storage/G5I9RPDU/p78-fortnow.pdf:application/pdf},
}

@article{schusterRevivalLandscapeParadigm2012,
	title = {A revival of the landscape paradigm: {Large} scale data harvesting provides access to fitness landscapes},
	volume = {17},
	issn = {10762787},
	shorttitle = {A revival of the landscape paradigm},
	url = {http://doi.wiley.com/10.1002/cplx.21401},
	doi = {10.1002/cplx.21401},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Complexity},
	author = {Schuster, Peter},
	month = may,
	year = {2012},
	pages = {6--10},
	file = {schuster 2012 - Solving problems involving the distribution of a species of unknown distribution via ecological niche modeling - GUPPY - EFs - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/496HFVLZ/schuster 2012 - Solving problems involving the distribution of a species of unknown distribution via ecological niche modeling - GUPPY - EFs - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{selmanGeneratingHardSatisfiability1996,
	title = {Generating hard satisfiability problems},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000453},
	doi = {10.1016/0004-3702(95)00045-3},
	abstract = {We report results from large-scale experiments in satisfiabilitytesting. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability testing procedures.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Selman, Bart and Mitchell, David G. and Levesque, Hector J.},
	month = mar,
	year = {1996},
	pages = {17--29},
	file = {selman mitchell levesque 1996 - generating hard satisfiability problems.pdf:/Users/bill/D/Zotero/storage/P6VKHJHV/selman mitchell levesque 1996 - generating hard satisfiability problems.pdf:application/pdf},
}

@article{sternWhatSetCover,
	title = {What is the set cover problem},
	language = {en},
	author = {Stern, Tamara I},
	pages = {5},
	file = {setcover-tamara.pdf:/Users/bill/D/Zotero/storage/HZX2NB9W/setcover-tamara.pdf:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009a,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = jan,
	year = {2009},
	pages = {1--25},
	file = {smith-miles - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:/Users/bill/D/Zotero/storage/R4VRBDRW/smith-miles - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:application/pdf},
}

@article{smith-milesMeasuringInstanceDifficulty2012b,
	title = {Measuring instance difficulty for combinatorial optimization problems},
	volume = {39},
	issn = {03050548},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054811001997},
	doi = {10.1016/j.cor.2011.07.006},
	abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies – studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances – provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difﬁculty in terms of algorithmic performance, and how such features can be deﬁned and measured for various optimization problems. We provide a comprehensive survey of the research ﬁeld with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems – which are important abstractions of many real-world problems – we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Lopes, Leo},
	month = may,
	year = {2012},
	pages = {875--889},
	file = {smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:/Users/bill/D/Zotero/storage/CRBN3D2H/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf:application/pdf},
}

@article{vanhemertEvolvingCombinatorialProblem2006c,
	title = {Evolving {Combinatorial} {Problem} {Instances} {That} {Are} {Difficult} to {Solve}},
	volume = {14},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco.2006.14.4.433},
	doi = {10.1162/evco.2006.14.4.433},
	abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difﬁcult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisﬁability, and the travelling salesman problem. Problem instances acquired through this technique are more difﬁcult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difﬁculty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {van Hemert, Jano I.},
	month = dec,
	year = {2006},
	pages = {433--462},
	file = {van hemert - Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:/Users/bill/D/Zotero/storage/V4RLYNBL/van hemert - Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf:application/pdf},
}

@incollection{xuSATzilla07DesignAnalysis2007,
	address = {Berlin, Heidelberg},
	title = {{SATzilla}-07: {The} {Design} and {Analysis} of an {Algorithm} {Portfolio} for {SAT}},
	volume = {4741},
	isbn = {978-3-540-74969-1},
	shorttitle = {{SATzilla}-07},
	url = {http://link.springer.com/10.1007/978-3-540-74970-7_50},
	abstract = {It has been widely observed that there is no “dominant” SAT solver; instead, diﬀerent solvers perform best on diﬀerent instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe a per-instance solver portfolio for SAT, SATzilla-07, which uses socalled empirical hardness models to choose among its constituent solvers. We leverage new model-building techniques such as censored sampling and hierarchical hardness models, and demonstrate the eﬀectiveness of our techniques by building a portfolio of state-of-the-art SAT solvers and evaluating it on several widely-studied SAT data sets. Overall, we show that our portfolio signiﬁcantly outperforms its constituent algorithms on every data set. Our approach has also proven itself to be eﬀective in practice: in the 2007 SAT competition, SATzilla-07 won three gold medals, one silver, and one bronze; it is available online at http://www.cs.ubc.ca/labs/beta/Projects/SATzilla .},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} – {CP} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Xu, Lin and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Bessière, Christian},
	year = {2007},
	doi = {10.1007/978-3-540-74970-7_50},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {712--727},
	file = {XuEtAl07b.pdf:/Users/bill/D/Zotero/storage/BE2ZI637/XuEtAl07b.pdf:application/pdf},
}

@article{zhangAlgorithmsConnectedSet2009,
	title = {Algorithms for connected set cover problem and fault-tolerant connected set cover problem},
	volume = {410},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397508008554},
	doi = {10.1016/j.tcs.2008.11.005},
	abstract = {Given a set V of elements, S a family of subsets of V , and G a connected graph on vertex set S,a connected set cover (CSC) is a subfamily R of S such that every element in V is covered by at least one set of R, and the subgraph G[R] of G induced by R is connected. If furthermore G[R] is k-connected and every element in V is covered by at least m sets in R, then R is a (k, m)-CSC. In this paper, we present two approximation algorithms for the minimum CSC problem, and one approximation algorithm for the minimum (2, m)-CSC problem. Performance ratios are analyzed. These are the first approximation algorithms for CSC problems in general graphs with guaranteed performance ratios.},
	language = {en},
	number = {8-10},
	urldate = {2020-11-04},
	journal = {Theoretical Computer Science},
	author = {Zhang, Zhao and Gao, Xiaofeng and Wu, Weili},
	month = mar,
	year = {2009},
	pages = {812--817},
	file = {zhang et al 2009 - Algorithms for connected set cover problem and fault-tolerant connected set cover problem.pdf:/Users/bill/D/Zotero/storage/6ELAH9UR/zhang et al 2009 - Algorithms for connected set cover problem and fault-tolerant connected set cover problem.pdf:application/pdf},
}

@inproceedings{gunnersenSpecVCMVImprovingCluster2011a,
	address = {Melbourne, Vic, Australia},
	title = {{SpecVCMV}: {Improving} cluster visualisation},
	isbn = {978-1-61284-972-0 978-1-61284-969-0 978-1-61284-971-3},
	shorttitle = {{SpecVCMV}},
	url = {http://ieeexplore.ieee.org/document/6119660/},
	doi = {10.1109/IECON.2011.6119660},
	abstract = {This paper proposes a new approach to validating and visualising cluster structure by combining fuzzy membership functions and spectral clustering. By modifying the Visual Cluster Validity algorithm (VCV) to use an external fuzzy membership function as the distance measure and using sum of cluster membership as the sorting function, computational experiments on both the Zelnik-Manor synthetic and UCI real datasets show the proposed method, SpecVCMV, more clearly identifies the underlying cluster structure in the data.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {{IECON} 2011 - 37th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Gunnersen, Sverre and Smith-Miles, Kate and Lee, Vincent},
	month = nov,
	year = {2011},
	pages = {2255--2260},
	file = {06119660.pdf:/Users/bill/D/Zotero/storage/6XCC3923/06119660.pdf:application/pdf},
}

@inproceedings{wuApproximateBayesianComputation2013a,
	address = {Shanghai, China},
	title = {Approximate {Bayesian} computation for estimating rate constants in biochemical reaction systems},
	isbn = {978-1-4799-1309-1},
	url = {http://ieeexplore.ieee.org/document/6732528/},
	doi = {10.1109/BIBM.2013.6732528},
	abstract = {To study the dynamic properties of complex biological systems, mathematical modeling has been used widely in systems biology. Apart from the well-established knowledge for modeling techniques, there are still some difﬁculties while understanding the dynamics in system biology. One of the major challenges is how to infer unknown parameters in mathematical models based on the experimentally observed data sets. This is extremely difﬁcult when the experimental data are sparse and the biological systems are stochastic. To tackle this problem, in this work we revised one computation method for inference called approximate Bayesian computation (ABC) and conducted extensive computing tests to examine the inﬂuence of a number of factors on the performance of ABC. Based on simulation results, we found that the number of stochastic simulations and step size of the observation data have substantial inﬂuence on the estimation accuracy. We applied the ABC method to two stochastic systems to test the efﬁciency and effectiveness of the ABC and obtained promising approximation for the unknown parameters in the systems. This work raised a number of important issues for designing effective inference methods for estimating rate constants in biochemical reaction systems.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2013 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine}},
	publisher = {IEEE},
	author = {Wu, Qianqian and Smith-Miles, Kate and Tian, Tianhai},
	month = dec,
	year = {2013},
	pages = {416--421},
	file = {06732528.pdf:/Users/bill/D/Zotero/storage/V4QB5E3N/06732528.pdf:application/pdf},
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009b,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/1456650.1456656},
	doi = {10.1145/1456650.1456656},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = jan,
	year = {2009},
	pages = {1--25},
	file = {a6-smith-miles.pdf:/Users/bill/D/Zotero/storage/K7RQZAFU/a6-smith-miles.pdf:application/pdf},
}

@article{bowlyGenerationTechniquesLinear2020,
	title = {Generation techniques for linear programming instances with controllable properties},
	volume = {12},
	issn = {1867-2949, 1867-2957},
	url = {http://link.springer.com/10.1007/s12532-019-00170-6},
	doi = {10.1007/s12532-019-00170-6},
	abstract = {This paper addresses the problem of generating synthetic test cases for experimentation in linear programming. We propose a method which maps instance generation and instance space search to an alternative encoded space. This allows us to develop a generator for feasible bounded linear programming instances with controllable properties. We show that this method is capable of generating any feasible bounded linear program, and that parameterised generators and search algorithms using this approach generate only feasible bounded instances. Our results demonstrate that controlled generation and instance space search using this method achieves feature diversity more eﬀectively than using a direct representation.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Mathematical Programming Computation},
	author = {Bowly, Simon and Smith-Miles, Kate and Baatar, Davaatseren and Mittelmann, Hans},
	month = sep,
	year = {2020},
	pages = {389--415},
	file = {bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:/Users/bill/D/Zotero/storage/HSWS7UXS/bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf:application/pdf},
}

@incollection{corneOptimisationGeneralisationFootprints2010b,
	address = {Berlin, Heidelberg},
	title = {Optimisation and {Generalisation}: {Footprints} in {Instance} {Space}},
	isbn = {978-3-642-15843-8 978-3-642-15844-5},
	shorttitle = {Optimisation and {Generalisation}},
	url = {http://link.springer.com/10.1007/978-3-642-15844-5_3},
	abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for speciﬁc problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its ‘seen’ instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm ‘footprints’, indicating how performance generalises in instance space. We ﬁnd much evidence that typical ways of choosing the ‘best’ algorithm, via tests over a distribution of instances, are seriously ﬂawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Parallel {Problem} {Solving} from {Nature}, {PPSN} {XI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Corne, David W. and Reynolds, Alan P.},
	editor = {Schaefer, Robert and Cotta, Carlos and Kołodziej, Joanna and Rudolph, Günter},
	year = {2010},
	doi = {10.1007/978-3-642-15844-5_3},
	pages = {22--31},
	file = {corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/ISCVIZUK/corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{munozPerformanceAnalysisContinuous2017a,
	title = {Performance {Analysis} of {Continuous} {Black}-{Box} {Optimization} {Algorithms} via {Footprints} in {Instance} {Space}},
	volume = {25},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco_a_00194},
	doi = {10.1162/evco_a_00194},
	abstract = {This article presents a method for the objective assessment of an algorithm’s strengths and weaknesses. Instead of examining the performance of only one or more algorithms on a benchmark set, or generating custom problems that maximize the performance difference between two algorithms, our method quantiﬁes both the nature of the test instances and the algorithm performance. Our aim is to gather information about possible phase transitions in performance, that is, the points in which a small change in problem structure produces algorithm failure. The method is based on the accurate estimation and characterization of the algorithm footprints, that is, the regions of instance space in which good or exceptional performance is expected from an algorithm. A footprint can be estimated for each algorithm and for the overall portfolio. Therefore, we select a set of features to generate a common instance space, which we validate by constructing a sufﬁciently accurate prediction model. We characterize the footprints by their area and density. Our method identiﬁes complementary performance between algorithms, quantiﬁes the common features of hard problems, and locates regions where a phase transition may lie.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {Muñoz, Mario A. and Smith-Miles, Kate A.},
	month = dec,
	year = {2017},
	pages = {529--554},
	file = {evco_a_00194.pdf:/Users/bill/D/Zotero/storage/FVW6VZKH/evco_a_00194.pdf:application/pdf},
}

@article{munozGeneratingNewSpaceFilling2020,
	title = {Generating {New} {Space}-{Filling} {Test} {Instances} for {Continuous} {Black}-{Box} {Optimization}},
	volume = {28},
	issn = {1063-6560, 1530-9304},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/evco_a_00262},
	doi = {10.1162/evco_a_00262},
	abstract = {This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a twodimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = sep,
	year = {2020},
	pages = {379--404},
	file = {evco_a_00262.pdf:/Users/bill/D/Zotero/storage/P3XI3U4V/evco_a_00262.pdf:application/pdf},
}

@article{xingengFaceImageModeling2011a,
	title = {Face {Image} {Modeling} by {Multilinear} {Subspace} {Analysis} {With} {Missing} {Values}},
	volume = {41},
	issn = {1083-4419, 1941-0492},
	url = {http://ieeexplore.ieee.org/document/5678656/},
	doi = {10.1109/TSMCB.2010.2097588},
	abstract = {Multilinear subspace analysis (MSA) is a promising methodology for pattern-recognition problems due to its ability in decomposing the data formed from the interaction of multiple factors. The MSA requires a large training set, which is well organized in a single tensor, which consists of data samples with all possible combinations of the contributory factors. However, such a “complete” training set is difﬁcult (or impossible) to obtain in many real applications. The missing-value problem is therefore crucial to the practicality of the MSA but has been hardly investigated up to present. To solve the problem, this paper proposes an algorithm named M2SA, which is advantageous in real applications due to the following: 1) it inherits the ability of the MSA to decompose the interlaced semantic factors; 2) it does not depend on any assumptions on the data distribution; and 3) it can deal with a high percentage of missing values. M2SA is evaluated by face image modeling on two typical multifactorial applications, i.e., face recognition and facial age estimation. Experimental results show the effectiveness of M2SA even when the majority of the values in the training tensor are missing.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	author = {{Xin Geng} and Smith-Miles, K and {Zhi-Hua Zhou} and {Liang Wang}},
	month = jun,
	year = {2011},
	pages = {881--892},
	file = {geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf:/Users/bill/D/Zotero/storage/YD7JGAFQ/geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf:application/pdf},
}

@inproceedings{insaniSelectingSuitableSolution2013a,
	address = {Yogyakarta, Indonesia},
	title = {Selecting suitable solution strategies for {Classes} of graph coloring instances using data mining},
	isbn = {978-1-4799-0425-9 978-1-4799-0423-5 978-1-4799-0424-2},
	url = {http://ieeexplore.ieee.org/document/6676240/},
	doi = {10.1109/ICITEED.2013.6676240},
	abstract = {The Maximal Independent Set (MIS) formulation tackles the graph coloring problem (GCP) as the partitioning of vertices of a graph into a minimum number of maximal independent sets as each MIS can be assigned a unique color. Mehrotra and Trick [5] solved the MIS formulation with an exact IP approach, but they were restricted to solving smaller or easier instances. For harder instances, it might be impossible to get the optimal solution within a reasonable computation time. We develop a heuristic algorithm, hoping that we can solve these problems in more reasonable time. However, though heuristics can find a near-optimal solution extremely fast compared to the exact approaches, there is still significant variations in performance that can only be explained by the fact that certain structures or properties in graphs may be better suited to some heuristics more than others. Selecting the best algorithm on average across all instances does not help us pick the best one for a particular instance. The need to understand how the best heuristic for a particular class of instance depends on these graph properties is an important issue. In this research, we use data mining to select the best solution strategies for classes of graph coloring instances.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2013 {International} {Conference} on {Information} {Technology} and {Electrical} {Engineering} ({ICITEE})},
	publisher = {IEEE},
	author = {Insani, Nur and Smith-Miles, Kate and Baatar, Davaatseren},
	month = oct,
	year = {2013},
	pages = {208--215},
	file = {insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/4YSW5P4B/insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{munozReliabilityExploratoryLandscapea,
	title = {Reliability of {Exploratory} {Landscape} {Analysis}},
	abstract = {The inherent difﬁculty of solving a black-box optimization problem depends on the characteristics of the problem’s ﬁtness landscape and the algorithm being used. Exploratory Landscape Analysis (ELA) methods can be used to describe the complexities of the problem using numerical features generated via a sampling process of the search space. Despite their success in a number of applications, ELA methods have signiﬁcant limitations typically related with the computational costs associated with generating accurate features. Consequently, only approximate features are available in practice which may be unreliable, leading to systemic errors. The overarching aim of this paper, is to evaluate the reliability of landscape features generated by well-known ELA methods. We describe a comprehensive evaluative framework combining exploratory and statistical validation stages. The results show that particular landscape features are highly volatile. In addition, instances of the same function can have feature values that are signiﬁcantly different. This implies that the ﬁtness landscapes may be statistically anisotropic. Finally, the results show evidence of a curse of the modality, meaning that the sample size should increase with the number of local optima.},
	language = {en},
	author = {Munoz, Mario A and Kirley, Michael and Smith-Miles, Kate},
	pages = {21},
	file = {J_RobustnessAnalysisELA_R5_preprint.pdf:/Users/bill/D/Zotero/storage/XNZH85SL/J_RobustnessAnalysisELA_R5_preprint.pdf:application/pdf},
}

@inproceedings{kangHowExtractMeaningful2013,
	address = {Singapore, Singapore},
	title = {How to extract meaningful shapes from noisy time-series subsequences?},
	isbn = {978-1-4673-5895-8},
	url = {http://ieeexplore.ieee.org/document/6597219/},
	doi = {10.1109/CIDM.2013.6597219},
	abstract = {A method for extracting and classifying shapes from noisy time series is proposed. The method consists of two steps. The ﬁrst step is to perform a noise test on each subsequence extracted from the series using a sliding window. All the subsequences recognised as noise are removed from further analysis, and the shapes are extracted from the remaining nonnoise subsequences. The second step is to cluster these extracted shapes. Although extracted from subsequences, these shapes form a non-overlapping set of time series subsequences and are hence amenable to meaningful clustering. The method is primarily designed for extracting and classifying shapes from very noisy real-world time series. Tests using artiﬁcial data with different levels of white noise and the red noise, and the real-world atmospheric turbulence data naturally characterised by strong red noise show that the method is able to correctly extract and cluster shapes from artiﬁcial data and that it has great potential for locating shapes in very noisy real-world time series.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2013 {IEEE} {Symposium} on {Computational} {Intelligence} and {Data} {Mining} ({CIDM})},
	publisher = {IEEE},
	author = {Kang, Yanfei and Smith-Miles, Kate and Belusic, Danijel},
	month = apr,
	year = {2013},
	pages = {65--72},
	file = {kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf:/Users/bill/D/Zotero/storage/8TW79ZQ7/kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf:application/pdf},
}

@article{munozInstanceSpacesMachine2018,
	title = {Instance spaces for machine learning classification},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-017-5629-5},
	doi = {10.1007/s10994-017-5629-5},
	abstract = {This paper tackles the issue of objective performance evaluation of machine learning classiﬁers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset aﬀect the diﬃculty of an instance for particular classiﬁcation algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classiﬁcation dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classiﬁers to be identiﬁed. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be aﬀorded by the current UCI repository.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Machine Learning},
	author = {Muñoz, Mario A. and Villanova, Laura and Baatar, Davaatseren and Smith-Miles, Kate},
	month = jan,
	year = {2018},
	pages = {109--147},
	file = {munoz acosta ... smith-miles 2017 - Instance Spaces for Machine Learning Classification - ML - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/B5TCX2BR/munoz acosta ... smith-miles 2017 - Instance Spaces for Machine Learning Classification - ML - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{munozAlgorithmSelectionBlackbox2015,
	title = {Algorithm selection for black-box continuous optimization problems: {A} survey on methods and challenges},
	volume = {317},
	issn = {00200255},
	shorttitle = {Algorithm selection for black-box continuous optimization problems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025515003680},
	doi = {10.1016/j.ins.2015.05.010},
	abstract = {Selecting the most appropriate algorithm to use when attempting to solve a black-box continuous optimization problem is a challenging task. Such problems typically lack algebraic expressions. It is not possible to calculate derivative information, and the problem may exhibit uncertainty or noise. In many cases, the input and output variables are analyzed without considering the internal details of the problem. Algorithm selection requires expert knowledge of search algorithm eﬃcacy and skills in algorithm engineering and statistics. Even with the necessary knowledge and skills, success is not guaranteed.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Information Sciences},
	author = {Muñoz, Mario A. and Sun, Yuan and Kirley, Michael and Halgamuge, Saman K.},
	month = oct,
	year = {2015},
	pages = {224--245},
	file = {munoz acosta et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/G5CJHNQN/munoz acosta et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@incollection{munozMetalearningPredictionModel2012,
	address = {Berlin, Heidelberg},
	title = {A {Meta}-learning {Prediction} {Model} of {Algorithm} {Performance} for {Continuous} {Optimization} {Problems}},
	volume = {7491},
	isbn = {978-3-642-32936-4 978-3-642-32937-1},
	url = {http://link.springer.com/10.1007/978-3-642-32937-1_23},
	abstract = {Algorithm selection and conﬁguration is a challenging problem in the continuous optimization domain. An approach to tackle this problem is to develop a model that links landscape analysis measures and algorithm parameters to performance. This model can be then used to predict algorithm performance when a new optimization problem is presented. In this paper, we investigate the use of a machine learning framework to build such a model. We demonstrate the effectiveness of our technique using CMA-ES as a representative algorithm and a feed-forward backpropagation neural network as the learning strategy. Our experimental results show that we can build sufﬁciently accurate predictions of an algorithm’s expected performance. This information is used to rank the algorithm parameter settings based on the current problem instance, hence increasing the probability of selecting the best conﬁguration for a new problem.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Parallel {Problem} {Solving} from {Nature} - {PPSN} {XII}},
	publisher = {Springer Berlin Heidelberg},
	author = {Muñoz, Mario A. and Kirley, Michael and Halgamuge, Saman K.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Coello, Carlos A. Coello and Cutello, Vincenzo and Deb, Kalyanmoy and Forrest, Stephanie and Nicosia, Giuseppe and Pavone, Mario},
	year = {2012},
	doi = {10.1007/978-3-642-32937-1_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {226--235},
	file = {munoz acosta et al 20012 - A Meta-Learning Prediction Model of Algorithm Performance for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/S2384QC8/munoz acosta et al 20012 - A Meta-Learning Prediction Model of Algorithm Performance for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@incollection{munozMetalearningPredictionModel2012a,
	address = {Berlin, Heidelberg},
	title = {A {Meta}-learning {Prediction} {Model} of {Algorithm} {Performance} for {Continuous} {Optimization} {Problems}},
	volume = {7491},
	isbn = {978-3-642-32936-4 978-3-642-32937-1},
	url = {http://link.springer.com/10.1007/978-3-642-32937-1_23},
	abstract = {Algorithm selection and conﬁguration is a challenging problem in the continuous optimization domain. An approach to tackle this problem is to develop a model that links landscape analysis measures and algorithm parameters to performance. This model can be then used to predict algorithm performance when a new optimization problem is presented. In this paper, we investigate the use of a machine learning framework to build such a model. We demonstrate the effectiveness of our technique using CMA-ES as a representative algorithm and a feed-forward backpropagation neural network as the learning strategy. Our experimental results show that we can build sufﬁciently accurate predictions of an algorithm’s expected performance. This information is used to rank the algorithm parameter settings based on the current problem instance, hence increasing the probability of selecting the best conﬁguration for a new problem.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Parallel {Problem} {Solving} from {Nature} - {PPSN} {XII}},
	publisher = {Springer Berlin Heidelberg},
	author = {Muñoz, Mario A. and Kirley, Michael and Halgamuge, Saman K.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Coello, Carlos A. Coello and Cutello, Vincenzo and Deb, Kalyanmoy and Forrest, Stephanie and Nicosia, Giuseppe and Pavone, Mario},
	year = {2012},
	doi = {10.1007/978-3-642-32937-1_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {226--235},
	file = {munoz et al 2012 - A Meta-Learning Prediction Model of Algorithm Performance  or Continuous Optimization Problems - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/6ISQHAPF/munoz et al 2012 - A Meta-Learning Prediction Model of Algorithm Performance  or Continuous Optimization Problems - BDPG - GUPPY.pdf:application/pdf},
}

@inproceedings{munozICARUSIdentificationComplementary2016,
	address = {Vancouver, BC, Canada},
	title = {{ICARUS}: {Identification} of complementary algorithms by uncovered sets},
	isbn = {978-1-5090-0623-6},
	shorttitle = {{ICARUS}},
	url = {http://ieeexplore.ieee.org/document/7744089/},
	doi = {10.1109/CEC.2016.7744089},
	abstract = {Since there is no single best performing algorithm for all problems, an algorithm portfolio would leverage the strengths of complementary algorithms to achieve the best performance. In this paper, we present and evaluate a new technique for designing algorithm portfolios for continuous black-box optimization problems, based on social choice and voting theory concepts. Our technique, which we call ICARUS, models the portfolio design task as an election, in which each problem ‘votes’ for a subset of preferred algorithms guided by a performance metric such as the number of ﬁtness evaluations. The resulting ‘uncovered set’ of algorithms forms the portfolio. We demonstrate the efﬁcacy of ICARUS using a suite of stateof-the-art evolutionary algorithms and benchmark continuous optimization problems. Our analysis conﬁrms that ICARUS creates an algorithm portfolio where the expected performance is superior to a manually constructed portfolio.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2016 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Munoz, Mario A. and Kirley, Michael},
	month = jul,
	year = {2016},
	pages = {2427--2432},
	file = {munoz kirley 2016 - ICARUS - Identification of Complementary algoRithms by Uncovered Sets - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/HIKEDIM3/munoz kirley 2016 - ICARUS - Identification of Complementary algoRithms by Uncovered Sets - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:application/pdf},
}

@inproceedings{munozEffectsFunctionTranslation2015,
	address = {Sendai, Japan},
	title = {Effects of function translation and dimensionality reduction on landscape analysis},
	isbn = {978-1-4799-7492-4},
	url = {http://ieeexplore.ieee.org/document/7257043/},
	doi = {10.1109/CEC.2015.7257043},
	abstract = {Exploratory Landscape Analysis (ELA) measures have been shown to predict algorithm performance; hence, they are being applied on critical tasks such as automatic algorithm selection and problem generation. This paper provides a cautionary examination on their use in black-box continuous optimization. We explore the effect that translations have on the measures, when the cost function is deﬁned within a boundconstrained region. Furthermore, we examine the robustness of the neighborhood structure after dimensionality reduction. The results demonstrate that a measure may transition abruptly due a translation. Therefore, we should not generalize the measures of an instance nor report average values of a measure as belonging to the generating function. Moreover, dimensionality reduction could alter the neighborhood structure, such that the regions corresponding to signiﬁcantly different functions overlap.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2015 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Munoz, Mario A. and Smith-Miles, Kate},
	month = may,
	year = {2015},
	pages = {1336--1342},
	file = {munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis copy.pdf:/Users/bill/D/Zotero/storage/GLKRUSTH/munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis copy.pdf:application/pdf},
}

@inproceedings{munozEffectsFunctionTranslation2015a,
	address = {Sendai, Japan},
	title = {Effects of function translation and dimensionality reduction on landscape analysis},
	isbn = {978-1-4799-7492-4},
	url = {http://ieeexplore.ieee.org/document/7257043/},
	doi = {10.1109/CEC.2015.7257043},
	abstract = {Exploratory Landscape Analysis (ELA) measures have been shown to predict algorithm performance; hence, they are being applied on critical tasks such as automatic algorithm selection and problem generation. This paper provides a cautionary examination on their use in black-box continuous optimization. We explore the effect that translations have on the measures, when the cost function is deﬁned within a boundconstrained region. Furthermore, we examine the robustness of the neighborhood structure after dimensionality reduction. The results demonstrate that a measure may transition abruptly due a translation. Therefore, we should not generalize the measures of an instance nor report average values of a measure as belonging to the generating function. Moreover, dimensionality reduction could alter the neighborhood structure, such that the regions corresponding to signiﬁcantly different functions overlap.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2015 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Munoz, Mario A. and Smith-Miles, Kate},
	month = may,
	year = {2015},
	pages = {1336--1342},
	file = {munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis.pdf:/Users/bill/D/Zotero/storage/Q6VR4WYL/munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis.pdf:application/pdf},
}

@article{munozInstanceSpacesMachine2018a,
	title = {Instance spaces for machine learning classification},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-017-5629-5},
	doi = {10.1007/s10994-017-5629-5},
	abstract = {This paper tackles the issue of objective performance evaluation of machine learning classiﬁers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difﬁculty of an instance for particular classiﬁcation algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classiﬁcation dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classiﬁers to be identiﬁed. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Machine Learning},
	author = {Muñoz, Mario A. and Villanova, Laura and Baatar, Davaatseren and Smith-Miles, Kate},
	month = jan,
	year = {2018},
	pages = {109--147},
	file = {munoz villanova baatar miles-smith 2018 - instance spaces for machine learning classification - ML - PROBLEM DIFFICULTY - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/3FK4GIVA/munoz villanova baatar miles-smith 2018 - instance spaces for machine learning classification - ML - PROBLEM DIFFICULTY - BDPG - GUPPY.pdf:application/pdf},
}

@inproceedings{phuaAdaptiveCommunalDetection2007a,
	address = {San Jose, California},
	title = {Adaptive communal detection in search of adversarial identity crime},
	isbn = {978-1-59593-846-6},
	url = {http://portal.acm.org/citation.cfm?doid=1288552.1288553},
	doi = {10.1145/1288552.1288553},
	abstract = {This paper is on adaptive real-time searching of credit application data streams for identity crime with many search parameters. Specifically, we concentrated on handling our domain-specific adversarial activity problem with the adaptive Communal Analysis Suspicion Scoring (CASS) algorithm. CASS’s main novel theoretical contribution is in the formulation of State-ofAlert (SoA) which sets the condition of reduced, same, or heightened watchfulness; and Parameter-of-Change (PoC) which improves detection ability with pre-defined parameter values for each SoA. With pre-configured SoA policy and PoC strategy, CASS determines when, what, and how much to adapt its search parameters to ongoing adversarial activity. The above approach is validated with three sets of experiments, where each experiment is conducted on several million real credit applications and measured with three appropriate performance metrics. Significant improvements are achieved over previous work, with the discovery of some practical insights of adaptivity into our domain.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 2007 international workshop on {Domain} driven data mining  - {DDDM} '07},
	publisher = {ACM Press},
	author = {Phua, Clifton and Lee, Vincent and Smith-Miles, Kate and Gayler, Ross},
	year = {2007},
	pages = {1--10},
	file = {p1-phua.pdf:/Users/bill/D/Zotero/storage/RB8ZSDSA/p1-phua.pdf:application/pdf},
}

@inproceedings{munozNonparametricModelSpace2017a,
	address = {Berlin Germany},
	title = {Non-parametric model of the space of continuous black-box optimization problems},
	isbn = {978-1-4503-4939-0},
	url = {https://dl.acm.org/doi/10.1145/3067695.3075971},
	doi = {10.1145/3067695.3075971},
	abstract = {Exploratory Landscape Analysis are data driven methods used for automated algorithm selection in continuous black-box optimization. Most of these methods follow strong assumptions that limit their characterization power, or loose information by compressing the data into a few scalar features. A more exible approach is to avoid explicit measuring and comparing of speci c structures. In this paper we present a proof-of-concept for a more general method, which produces non-parametric models of the space of problems. Using non-metric multidimensional scaling, we generate synthetic features for each problem, which could replace or complement the existing ones. We demonstrate approaches to produce algorithm recommendations and visual representations of the space. To validate the model, we compare our results with those obtained through existing methods, which show that our models have competitive performance.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = jul,
	year = {2017},
	pages = {175--176},
	file = {p175-munoz.pdf:/Users/bill/D/Zotero/storage/CUUZAJE7/p175-munoz.pdf:application/pdf},
}

@article{gengFaceImageModelinga,
	title = {Face image modeling by multilinear subspace analysis with missing values},
	abstract = {The main difﬁculty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-n product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difﬁcult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M2SA, which can work on the training tensor with massive missing values. Thus M2SA can be used to model face images even when there are only a small number of face images with limited variations (which will cause missing values in the training tensor). Experiments on face recognition show that M2SA can work reasonably well with up to 70\% missing values in the training tensor.},
	language = {en},
	author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua and Wang, Liang},
	pages = {4},
	file = {p629-geng.pdf:/Users/bill/D/Zotero/storage/IR275TVY/p629-geng.pdf:application/pdf},
}

@inproceedings{gengFacialAgeEstimation2008a,
	address = {Vancouver, British Columbia, Canada},
	title = {Facial age estimation by nonlinear aging pattern subspace},
	isbn = {978-1-60558-303-7},
	url = {http://portal.acm.org/citation.cfm?doid=1459359.1459469},
	doi = {10.1145/1459359.1459469},
	abstract = {Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are deﬁned as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identiﬁed.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceeding of the 16th {ACM} international conference on {Multimedia} - {MM} '08},
	publisher = {ACM Press},
	author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua},
	year = {2008},
	pages = {721},
	file = {p721-geng.pdf:/Users/bill/D/Zotero/storage/SSWLC9PI/p721-geng.pdf:application/pdf},
}

@inproceedings{chanEvolvingStellarModels2019a,
	address = {Prague Czech Republic},
	title = {Evolving stellar models to find the origins of our galaxy},
	isbn = {978-1-4503-6111-8},
	url = {https://dl.acm.org/doi/10.1145/3321707.3321714},
	doi = {10.1145/3321707.3321714},
	abstract = {A er the Big Bang, it took about 200 million years before the very rst stars would form – now more than 13 billion years ago. Unfortunately, we will not be able to observe these stars directly. Instead, we can observe the ’fossil’ records that these stars have le behind, preserved in the oldest stars of our own galaxy. When the rst stars exploded as supernovae, their ashes were dispersed and the next generation of stars formed, incorporating some of the debris. We can now measure the chemical abundances in those old stars, which is similar to a genetic ngerprint that allows us to identify the parents.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {ACM},
	author = {Chan, Conrad and Aleti, Aldeida and Heger, Alexander and Smith-Miles, Kate},
	month = jul,
	year = {2019},
	pages = {1129--1137},
	file = {p1129-chan.pdf:/Users/bill/D/Zotero/storage/KQNDXL2P/p1129-chan.pdf:application/pdf},
}

@inproceedings{munozGeneratingCustomClassification2017b,
	address = {Berlin Germany},
	title = {Generating custom classification datasets by targeting the instance space},
	isbn = {978-1-4503-4939-0},
	url = {https://dl.acm.org/doi/10.1145/3067695.3082532},
	doi = {10.1145/3067695.3082532},
	abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = jul,
	year = {2017},
	pages = {1582--1588},
	file = {p1582-munoz.pdf:/Users/bill/D/Zotero/storage/EEL27K8Z/p1582-munoz.pdf:application/pdf},
}

@article{smith-milesDiscoveringSuitabilityOptimisation2011a,
	title = {Discovering the suitability of optimisation algorithms by learning from evolved instances},
	volume = {61},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-011-9230-5},
	doi = {10.1007/s10472-011-9230-5},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Smith-Miles, Kate and van Hemert, Jano},
	month = feb,
	year = {2011},
	pages = {87--104},
	file = {smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/VGFRKLI7/smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011b,
	address = {Berlin, Heidelberg},
	title = {Generalising {Algorithm} {Performance} in {Instance} {Space}: {A} {Timetabling} {Case} {Study}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	shorttitle = {Generalising {Algorithm} {Performance} in {Instance} {Space}},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_41},
	abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a “footprint” in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the diﬀerences between the instance generation methods, and the suitability of diﬀerent algorithms.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smith-Miles, Kate and Lopes, Leo},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_41},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {524--538},
	file = {smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/X49UWU9A/smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@inproceedings{munozGeneratingCustomClassification2017c,
	address = {Berlin Germany},
	title = {Generating custom classification datasets by targeting the instance space},
	isbn = {978-1-4503-4939-0},
	url = {https://dl.acm.org/doi/10.1145/3067695.3082532},
	doi = {10.1145/3067695.3082532},
	abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = jul,
	year = {2017},
	pages = {1582--1588},
	file = {W_ClassProblemGenerator.pdf:/Users/bill/D/Zotero/storage/M4K459LS/W_ClassProblemGenerator.pdf:application/pdf},
}

@article{cohenHowEvaluationGuides,
	title = {How {Evaluation} {Guides} {AI} {Research}: {The} {Message} {Still} {Counts} {More} than the {Medium}},
	language = {en},
	author = {Cohen, Paul and Howe, Adele},
	pages = {10},
	file = {cohen howe 1988 - How Evaluation Guides AI Research - The Message Still Counts More than the Medium.pdf:/Users/bill/D/Zotero/storage/BT9CTGS8/cohen howe 1988 - How Evaluation Guides AI Research - The Message Still Counts More than the Medium.pdf:application/pdf},
}

@article{deliaSurrogatebasedEnsembleGrouping2017,
	title = {Surrogate-based {Ensemble} {Grouping} {Strategies} for {Embedded} {Sampling}-based {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/1705.02003},
	abstract = {The embedded ensemble propagation approach introduced in [49] has been demonstrated to be a powerful means of reducing the computational cost of sampling-based uncertainty quantiﬁcation methods, particularly on emerging computational architectures. A substantial challenge with this method however is ensemble-divergence, whereby diﬀerent samples within an ensemble choose diﬀerent code paths. This can reduce the eﬀectiveness of the method and increase computational cost. Therefore grouping samples together to minimize this divergence is paramount in making the method eﬀective for challenging computational simulations. In this work, a new grouping approach based on a surrogate for computational cost built up during the uncertainty propagation is developed and applied to model diﬀusion problems where computational cost is driven by the number of (preconditioned) linear solver iterations. The approach is developed within the context of locally adaptive stochastic collocation methods, where a surrogate for the number of linear solver iterations, generated from previous levels of the adaptive grid generation, is used to predict iterations for subsequent samples, and group them based on similar numbers of iterations. The eﬀectiveness of the method is demonstrated by applying it to highly anisotropic diﬀusion problems with a wide variation in solver iterations from sample to sample. It extends the parameter-based grouping approach developed in [17] to more general problems without requiring detailed knowledge of how the uncertain parameters aﬀect the simulation’s cost, and is also less intrusive to the simulation code.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1705.02003 [stat]},
	author = {D'Elia, Marta and Phipps, Eric and Rushdi, Ahmad and Ebeida, Mohamed},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02003},
	keywords = {Statistics - Computation},
	annote = {Comment: 24 pages, 4 figures},
	file = {d elia et al 2017 - Surrogate-based Ensemble Grouping Strategies for Embedded Sampling-based Uncertainty Quantification - PROBLEM DIFFICULTY - UNCERTAINTY -BDPG.pdf:/Users/bill/D/Zotero/storage/B7AMVXJJ/d elia et al 2017 - Surrogate-based Ensemble Grouping Strategies for Embedded Sampling-based Uncertainty Quantification - PROBLEM DIFFICULTY - UNCERTAINTY -BDPG.pdf:application/pdf},
}

@techreport{swilerEfficientAlgorithmsMixed2009,
	title = {Efficient algorithms for mixed aleatory-epistemic uncertainty quantification with application to radiation-hardened electronics. {Part} {I}, algorithms and benchmark results.},
	url = {http://www.osti.gov/servlets/purl/972887-jMlSv7/},
	abstract = {This report documents the results of an FY09 ASC V\&V Methods level 2 milestone demonstrating new algorithmic capabilities for mixed aleatory-epistemic uncertainty quantiﬁcation. Through the combination of stochastic expansions for computing aleatory statistics and interval optimization for computing epistemic bounds, mixed uncertainty analysis studies are shown to be more accurate and eﬃcient than previously achievable. Part I of the report describes the algorithms and presents benchmark performance results. Part II applies these new algorithms to UQ analysis of radiation eﬀects in electronic devices and circuits for the QASPR program.},
	language = {en},
	number = {SAND2009-5805, 972887},
	urldate = {2020-11-04},
	author = {Swiler, Laura Painton and Eldred, Michael Scott},
	month = sep,
	year = {2009},
	doi = {10.2172/972887},
	pages = {SAND2009--5805, 972887},
	file = {eldred swiler 2009 - Efficient algorithms for mixed aleatory-epistemic uncertainty quantification with application to radiation-hardened electronics - Part I - algorithms and benchmark results - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/37KAPNZX/eldred swiler 2009 - Efficient algorithms for mixed aleatory-epistemic uncertainty quantification with application to radiation-hardened electronics - Part I - algorithms and benchmark results - PROBLEM DIFFI.pdf:application/pdf},
}

@article{goodfellowExplainingHarnessingAdversarial2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {goodfellow et al 2015 - Explaining and Harnessing Adversarial Examples - ML BOOK - BDPG - GUPPY - UNCERTAINTY ENSEMBLE.pdf:/Users/bill/D/Zotero/storage/HWCUQN3L/goodfellow et al 2015 - Explaining and Harnessing Adversarial Examples - ML BOOK - BDPG - GUPPY - UNCERTAINTY ENSEMBLE.pdf:application/pdf},
}

@article{universitatpolitecnicadevalenciaUniversitatPolitecnicaValencia2014,
	title = {Universitat {Politècnica} de {València}},
	volume = {18},
	issn = {1886-4996, 1134-2196},
	url = {http://polipapers.upv.es/index.php/IA/article/view/3293},
	doi = {10.4995/ia.2014.3293},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Ingeniería del agua},
	author = {Universitat Politècnica de València, Editorial},
	month = sep,
	year = {2014},
	pages = {ix},
	file = {How to measure the difficulty of a Mixed-Linear Integer....pdf:/Users/bill/D/Zotero/storage/BLQVXXEI/How to measure the difficulty of a Mixed-Linear Integer....pdf:application/pdf},
}

@article{howe2002j,
	title = {A {Critical} {Assessment} of {Benchmark} {Comparison} in {Planning}},
	volume = {17},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10305},
	doi = {10.1613/jair.935},
	abstract = {Recent trends in planning research have led to empirical comparison becoming commonplace. The eld has started to settle into a methodology for such comparisons, which for obvious practical reasons requires running a subset of planners on a subset of problems. In this paper, we characterize the methodology and examine eight implicit assumptions about the problems, planners and metrics used in many of these comparisons. The problem assumptions are: PR1) the performance of a general purpose planner should not be penalized/biased if executed on a sampling of problems and domains, PR2) minor syntactic di erences in representation do not a ect performance, and PR3) problems should be solvable by STRIPS capable planners unless they require ADL. The planner assumptions are: PL1) the latest version of a planner is the best one to use, PL2) default parameter settings approximate good performance, and PL3) time cut-o s do not unduly bias outcome. The metrics assumptions are: M1) performance degrades similarly for each planner when run on degraded runtime environments (e.g., machine platform) and M2) the number of plan steps distinguishes performance. We nd that most of these assumptions are not supported empirically; in particular, that planners are a ected di erently by these assumptions. We conclude with a call to the community to devote research resources to improving the state of the practice and especially to enhancing the available benchmark problems.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Journal of Artificial Intelligence Research},
	author = {Howe, A. E. and Dahlman, E.},
	month = jul,
	year = {2002},
	pages = {1--33},
	file = {howe dahlman 1993 - a critical assessment of benchmark comparison in planning.pdf:/Users/bill/D/Zotero/storage/X9GJR757/howe dahlman 1993 - a critical assessment of benchmark comparison in planning.pdf:application/pdf},
}

@incollection{howeExploitingCompetitivePlanner2000,
	address = {Berlin, Heidelberg},
	title = {Exploiting {Competitive} {Planner} {Performance}},
	volume = {1809},
	isbn = {978-3-540-67866-3 978-3-540-44657-6},
	url = {http://link.springer.com/10.1007/10720246_5},
	abstract = {To date, no one planner has demonstrated clearly superior performance. Although researchers have hypothesized that this should be the case, no one has performed a large study to test its limits. In this research, we tested performance of a set of planners to determine which is best on what types of problems. The study included six planners and over 200 problems. We found that performance, as measured by number of problems solved and computation time, varied with no one planner solving all the problems or being consistently fastest. Analysis of the data also showed that most planners either fail or succeed quickly and that performance depends at least in part on some easily observable problem/domain features. Based on these results, we implemented a meta-planner that interleaves execution of six planners on a problem until one of them solves it. The control strategy for ordering the planners and allocating time is derived from the performance study data. We found that our meta-planner is able to solve more problems than any single planner, but at the expense of computation time.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Recent {Advances} in {AI} {Planning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Howe, Adele E. and Dahlman, Eric and Hansen, Christopher and Scheetz, Michael and von Mayrhauser, Anneliese},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Biundo, Susanne and Fox, Maria},
	year = {2000},
	doi = {10.1007/10720246_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {62--72},
	file = {howe et al 1999 - Exploiting Competitive Planner Performance.pdf:/Users/bill/D/Zotero/storage/RRF7FEY7/howe et al 1999 - Exploiting Competitive Planner Performance.pdf:application/pdf},
}

@article{ayadIndependentSetVertex,
	title = {Independent {Set} and {Vertex} {Cover}},
	language = {en},
	author = {Ayad, Hanan},
	pages = {2},
	file = {IndependentSet - independent set and vertex cover.pdf:/Users/bill/D/Zotero/storage/SG9EWH52/IndependentSet - independent set and vertex cover.pdf:application/pdf},
}

@article{ApproximationAlgorithmsWeighted,
	title = {Approximation {Algorithms} for {Weighted} {Vertex} {Cover}},
	language = {en},
	pages = {16},
	file = {iowa state 2010 - approximation algorithms for weighted vertex cover - SLIDES.pdf:/Users/bill/D/Zotero/storage/IAIBQN6Q/iowa state 2010 - approximation algorithms for weighted vertex cover - SLIDES.pdf:application/pdf},
}

@article{maciaUCIMindfulRepository2014,
	title = {Towards {UCI}+: {A} mindful repository design},
	volume = {261},
	issn = {00200255},
	shorttitle = {Towards {UCI}+},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
	doi = {10.1016/j.ins.2013.08.059},
	abstract = {Public repositories have contributed to the maturation of experimental methodology in machine learning. Publicly available data sets have allowed researchers to empirically assess their learners and, jointly with open source machine learning software, they have favoured the emergence of comparative analyses of learners’ performance over a common framework. These studies have brought standard procedures to evaluate machine learning techniques. However, current claims—such as the superiority of enhanced algorithms—are biased by unsustained assumptions made throughout some praxes.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Information Sciences},
	author = {Macià, Núria and Bernadó-Mansilla, Ester},
	month = mar,
	year = {2014},
	pages = {237--262},
	file = {macia bernado-mansilla 2014 - towards uci plus - a mindful repository design - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/YP5B3EWN/macia bernado-mansilla 2014 - towards uci plus - a mindful repository design - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{munozAlgorithmSelectionBlackbox2015a,
	title = {Algorithm selection for black-box continuous optimization problems: {A} survey on methods and challenges},
	volume = {317},
	issn = {00200255},
	shorttitle = {Algorithm selection for black-box continuous optimization problems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025515003680},
	doi = {10.1016/j.ins.2015.05.010},
	abstract = {Selecting the most appropriate algorithm to use when attempting to solve a black-box continuous optimization problem is a challenging task. Such problems typically lack algebraic expressions. It is not possible to calculate derivative information, and the problem may exhibit uncertainty or noise. In many cases, the input and output variables are analyzed without considering the internal details of the problem. Algorithm selection requires expert knowledge of search algorithm eﬃcacy and skills in algorithm engineering and statistics. Even with the necessary knowledge and skills, success is not guaranteed.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Information Sciences},
	author = {Muñoz, Mario A. and Sun, Yuan and Kirley, Michael and Halgamuge, Saman K.},
	month = oct,
	year = {2015},
	pages = {224--245},
	file = {munoz et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/QQHX84U8/munoz et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@inproceedings{munozGeneratingCustomClassification2017d,
	address = {Berlin Germany},
	title = {Generating custom classification datasets by targeting the instance space},
	isbn = {978-1-4503-4939-0},
	url = {https://dl.acm.org/doi/10.1145/3067695.3082532},
	doi = {10.1145/3067695.3082532},
	abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Muñoz, Mario A. and Smith-Miles, Kate},
	month = jul,
	year = {2017},
	pages = {1582--1588},
	file = {munoz smith-miles 2017 - Generating custom classification datasets by targeting the instance space - PROBLEM DIFFICULTY - BDPG - ANNP.pdf:/Users/bill/D/Zotero/storage/HC8DY2NP/munoz smith-miles 2017 - Generating custom classification datasets by targeting the instance space - PROBLEM DIFFICULTY - BDPG - ANNP.pdf:application/pdf},
}

@article{kingsfordCMSC451Reductions,
	title = {{CMSC} 451: {Reductions} \& {NP}-completeness},
	language = {en},
	author = {Kingsford, Carl},
	pages = {22},
	file = {npcomplete.pdf:/Users/bill/D/Zotero/storage/R6CYMCVR/npcomplete.pdf:application/pdf},
}

@article{olson2017bm,
	title = {{PMLB}: a large benchmark suite for machine learning evaluation and comparison},
	volume = {10},
	issn = {1756-0381},
	shorttitle = {{PMLB}},
	url = {https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4},
	doi = {10.1186/s13040-017-0154-4},
	abstract = {The selection, development, or comparison of machine learning methods in data mining can be a diﬃcult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from diﬀerent sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating speciﬁc benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identiﬁcation of the strengths and weaknesses of diﬀerent machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important ﬁrst step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and eﬃcient standards in the future.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {BioData Mining},
	author = {Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
	month = dec,
	year = {2017},
	keywords = {bdpg, benchmarking, machine learning},
	pages = {36},
	file = {olson et al 2017 - PMLB - A Large Benchmark Suite for Machine Learning Evaluation and Comparison - EVALUATION - ANNO.pdf:/Users/bill/D/Zotero/storage/GD37UKDE/olson et al 2017 - PMLB - A Large Benchmark Suite for Machine Learning Evaluation and Comparison - EVALUATION - ANNO.pdf:application/pdf},
}

@article{phippsImprovingSamplingbasedUncertainty,
	title = {Improving {Sampling}-based {Uncertainty} {Quantiﬁcation} {Performance} {Through} {Embedded} {Ensemble} {Propagation}},
	language = {en},
	author = {Phipps, E and D’Elia, M and Ebeida, M and Rushdi, A},
	pages = {1},
	file = {phipps d elia et al 2017 - Improving Sampling-based Uncertainty Quantification Performance Through Embedded Ensemble Propagation - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/95PMKJGV/phipps d elia et al 2017 - Improving Sampling-based Uncertainty Quantification Performance Through Embedded Ensemble Propagation - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{rushdiVPSVORONOIPIECEWISE2017,
	title = {{VPS}: {VORONOI} {PIECEWISE} {SURROGATE} {MODELS} {FOR} {HIGH}-{DIMENSIONAL} {DATA} {FITTING}},
	volume = {7},
	issn = {2152-5080},
	shorttitle = {{VPS}},
	url = {http://www.dl.begellhouse.com/journals/52034eb04b657aea,7bd16ae14fe9cbcf,1c04fb773044c729.html},
	doi = {10.1615/Int.J.UncertaintyQuantification.2016018697},
	abstract = {Global surrogate models (metamodels) are indispensable for numerical simulations over high-dimensional spaces. They typically use well-selected samples of the expensive code runs to produce a cheap-to-evaluate model. We introduce a new method to construct credible global surrogates with local accuracy without dictating where to sample: Voronoi Piecewise Surrogate (VPS) models. The key component in our method is to decompose the high-dimensional parameter space using an implicit Voronoi tessellation around the sample points as seeds. VPS assigns samples to cells using a simple nearest seed search, and ﬁnds cell neighbors via local hyperplane sampling, without constructing an explicit mesh. To avoid the intractable complexity of high-dimensional Voronoi cells, we construct an approximate dual Delaunay graph to establish a neighborhood network between cells. Each cell then uses information at its neighbors to build its own local piece of the global surrogate. This approach breaks down the high-order approximation problem into a set of piecewise low-order problems in the neighborhood of each function evaluation. The one-to-one mapping between the number of function evaluations and the number of Voronoi cells, regardless of the number of dimensions, eliminates the curse of dimensionality associated with standard domain decompositions. Furthermore, the Voronoi tessellation is naturally updated with the addition of new function evaluations. Due to its piecewise nature, VPS accurately handles smooth functions as well as functions with discontinuities, and can adopt a parallel implementation.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {International Journal for Uncertainty Quantification},
	author = {Rushdi, Ahmad A. and Swiler, Laura P. and Phipps, Eric T. and D'Elia, Marta and Ebeida, Mohamed S.},
	year = {2017},
	pages = {1--21},
	file = {rushdi swiler phipps d elia ebida 2016 - vps - voronoi piecewise surrogate models for high-dimensional data fitting - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/3MBSY338/rushdi swiler phipps d elia ebida 2016 - vps - voronoi piecewise surrogate models for high-dimensional data fitting - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@inproceedings{sunSelectionFitnessLandscape2014,
	address = {Colombo, Sri Lanka},
	title = {On the selection of fitness landscape analysis metrics for continuous optimization problems},
	isbn = {978-1-4799-4598-6},
	url = {https://ieeexplore.ieee.org/document/7069635},
	doi = {10.1109/ICIAFS.2014.7069635},
	abstract = {Selecting the best algorithm for a given optimization problem is non-trivial due to large number of existing algorithms and high complexity of problems. A possible way to tackle this challenge is to attempt to understand the problem complexity. Fitness Landscape Analysis (FLA) metrics are widely used techniques to extract characteristics from problems. Based on the extracted characteristics, machine learning methods are employed to select the optimal algorithm for a given problem. Therefore, the accuracy of the algorithm selection framework heavily relies on the choice of FLA metrics. Although researchers have paid great attention to designing FLA metrics to quantify the problem characteristics, there is still no agreement on which combination of FLA metrics should be employed. In this paper, we present some well-performed FLA metrics, discuss their contributions and limitations in detail, and map each FLA metric to the captured problem characteristics. Moreover, computational complexity of each FLA metric is carefully analysed. We propose two criteria to follow when selecting FLA metrics. We hope our work can help researchers identify the best combination of FLA metrics.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {7th {International} {Conference} on {Information} and {Automation} for {Sustainability}},
	publisher = {IEEE},
	author = {Sun, Yuan and Halgamuge, Saman K. and Kirley, Michael and Munoz, Mario A.},
	month = dec,
	year = {2014},
	pages = {1--6},
	file = {sun ... kirley munoz 2014 - On the Selection of Fitness Landscape Analysis Metrics for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/UB2DP49J/sun ... kirley munoz 2014 - On the Selection of Fitness Landscape Analysis Metrics for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@inproceedings{suttonApproximatingDistributionFitness2011,
	address = {Schwarzenberg, Austria},
	title = {Approximating the distribution of fitness over hamming regions},
	isbn = {978-1-4503-0633-1},
	url = {http://portal.acm.org/citation.cfm?doid=1967654.1967663},
	doi = {10.1145/1967654.1967663},
	abstract = {The distribution of ﬁtness values across a set of states sharply inﬂuences the dynamics of evolutionary processes and heuristic search in combinatorial optimization. In this paper we present a method for approximating the distribution of ﬁtness values over Hamming regions by solving a linear programming problem that incorporates low order moments of the target function. These moments can be retrieved in polynomial time for select problems such as MAX-k-SAT using Walsh analysis. The method is applicable to any real function on binary strings that is epistatically bounded and discrete with asymptotic bounds on the cardinality of its codomain.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 11th workshop proceedings on {Foundations} of genetic algorithms - {FOGA} '11},
	publisher = {ACM Press},
	author = {Sutton, Andrew M. and Whitley, Darrell and Howe, Adele E.},
	year = {2011},
	pages = {93},
	file = {sutton whitley howe 2011 - approximating the distribution of fitness over hamming regions.pdf:/Users/bill/D/Zotero/storage/FASPKNTK/sutton whitley howe 2011 - approximating the distribution of fitness over hamming regions.pdf:application/pdf},
}

@article{submissionMachineLearningThat,
	title = {Machine {Learning} that {Matters}},
	abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the ﬁeld’s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
	language = {en},
	author = {Submission, Anonymous},
	pages = {6},
	file = {wagstaff 2012 - Machine Learning that Matters.pdf:/Users/bill/D/Zotero/storage/QAXUBL9F/wagstaff 2012 - Machine Learning that Matters.pdf:application/pdf},
}

@article{whitleyLauraBarbulescuCarnegie,
	title = {Laura {Barbulescu} {Carnegie} {Mellon} {University}},
	abstract = {Test suites for many domains often fail to model features present in real-world problems. For the permutation ow-shop sequencing problem (PFSP), the most popular test suite consists of problems whose features are generated from a single uniform random distribution. Synthetic generation of problems with characteristics present in real-world problems is a viable alternative. We compare the performance of several competitive algorithms on problems produced with such a generator. We nd that, as more realistic characteristics are introduced, the performance of a stateof-the-art algorithm degrades rapidly: faster and less complex stochastic algorithms provide superior performance. Our empirical results show that small changes in problem structure or problem size can in uence algorithm performance. We hypothesize that these performance di erences may be partially due to di erences in search space topologies; we show that structured problems produce topologies with performance plateaus. Algorithm sensitivity to problem characteristics suggests the need to construct test suites more representative of real-world applications.},
	language = {en},
	author = {Whitley, Darrell and Howe, Adele},
	pages = {9},
	file = {watson barbulescu howe whitley 1999 - algorithm performance and problem structure for flow-shop scheduling.pdf:/Users/bill/D/Zotero/storage/8M43X6PP/watson barbulescu howe whitley 1999 - algorithm performance and problem structure for flow-shop scheduling.pdf:application/pdf},
}

@article{watsonFocusingIndividualWhy,
	title = {Focusing on the {Individual}: {Why} {We} {Need} {New} {Empirical} {Methods} for {Characterizing} {Problem} {Difﬁculty} - {Position} {Paper}},
	abstract = {A large number of empirical methods for characterizing problem difﬁculty have appeared in the last 15 or so years, including the work on phase transitions, ﬁtness landscape correlation length, analysis of optima distributions, and algorithm run-time distributions. These methods have been successful either in predicting the difﬁculty of an ensemble of problem instances or providing descriptive characterizations of algorithm performance. However, they are of limited use in explaining and predicting the performance of algorithms on individual problem instances. We argue that the development of empirical methods for characterizing problem difﬁculty at the instance level is necessary for an advanced understanding of algorithm behavior. Further, the practical beneﬁt is tremendous, enabling 1) the development of more comprehensive benchmarks, 2) problem-sensitive algorithm selection, and 3) intelligent tuning of problem-sensitive algorithm parameters.},
	language = {en},
	author = {Watson, Jean-Paul and Howe, Adele E and Collins, Fort},
	pages = {7},
	file = {watson howe 2001 - focusing on the individual - why we need new empirical methods for characterizing problem difficulty - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/NIF2KCTK/watson howe 2001 - focusing on the individual - why we need new empirical methods for characterizing problem difficulty - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{albuquerqueRarityWeightedRichnessSimple2015,
	title = {Rarity-{Weighted} {Richness}: {A} {Simple} and {Reliable} {Alternative} to {Integer} {Programming} and {Heuristic} {Algorithms} for {Minimum} {Set} and {Maximum} {Coverage} {Problems} in {Conservation} {Planning}},
	volume = {10},
	issn = {1932-6203},
	shorttitle = {Rarity-{Weighted} {Richness}},
	url = {https://dx.plos.org/10.1371/journal.pone.0119905},
	doi = {10.1371/journal.pone.0119905},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {PLOS ONE},
	author = {Albuquerque, Fabio and Beier, Paul},
	editor = {Sueur, Cédric},
	month = mar,
	year = {2015},
	pages = {e0119905},
	file = {albuquerque beier 2015 - rarity-weighted richness - a simple and reliable alternative to integer programming and heuristic algorithms for minimum set and maximum coverage problems in consrvation planning - BDPG - HEURISTIC RESERVE SELECTION - ANNO.PDF:/Users/bill/D/Zotero/storage/TRMZ6RQN/albuquerque beier 2015 - rarity-weighted richness - a simple and reliable alternative to integer programming and heuristic algorithms for minimum set and maximum coverage problems in consrvation planning - BD.PDF:application/pdf},
}

@article{andoLessonsFinanceNew2011,
	title = {Lessons from {Finance} for {New} {Land}-{Conservation} {Strategies} {Given} {Climate}-{Change} {Uncertainty}: {Lessons} for {Conservation} from {Finance}},
	volume = {25},
	issn = {08888892},
	shorttitle = {Lessons from {Finance} for {New} {Land}-{Conservation} {Strategies} {Given} {Climate}-{Change} {Uncertainty}},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2011.01648.x},
	doi = {10.1111/j.1523-1739.2011.01648.x},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Ando, Amy W. and Hannah, Lee},
	month = apr,
	year = {2011},
	pages = {412--414},
	file = {ando hannah 2011 - Lessons from Finance for New Land-Conservation Strategies Given Climate-Change Uncertainty - BDPG - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/IVN498JA/ando hannah 2011 - Lessons from Finance for New Land-Conservation Strategies Given Climate-Change Uncertainty - BDPG - UNCERTAINTY.pdf:application/pdf},
}

@article{andoOptimalPortfolioDesign2012,
	title = {Optimal portfolio design to reduce climate-related conservation uncertainty in the {Prairie} {Pothole} {Region}},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1114653109},
	doi = {10.1073/pnas.1114653109},
	language = {en},
	number = {17},
	urldate = {2020-11-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ando, A. W. and Mallory, M. L.},
	month = apr,
	year = {2012},
	pages = {6484--6489},
	file = {ando mallory 2012 - Optimal portfolio design to reduce climate-related conservation uncertainty in the Prairie Pothole Region - COST - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/WSZ24MJ6/ando mallory 2012 - Optimal portfolio design to reduce climate-related conservation uncertainty in the Prairie Pothole Region - COST - BDPG - ANNO.pdf:application/pdf},
}

@article{bealeIncorporatingUncertaintyPredictive2012,
	title = {Incorporating uncertainty in predictive species distribution modelling},
	volume = {367},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2011.0178},
	doi = {10.1098/rstb.2011.0178},
	abstract = {Motivated by the need to solve ecological problems (climate change, habitat fragmentation and biological invasions), there has been increasing interest in species distribution models (SDMs). Predictions from these models inform conservation policy, invasive species management and disease-control measures. However, predictions are subject to uncertainty, the degree and source of which is often unrecognized. Here, we review the SDM literature in the context of uncertainty, focusing on three main classes of SDM: niche-based models, demographic models and process-based models. We identify sources of uncertainty for each class and discuss how uncertainty can be minimized or included in the modelling process to give realistic measures of confidence around predictions. Because this has typically not been performed, we conclude that uncertainty in SDMs has often been underestimated and a false precision assigned to predictions of geographical distribution. We identify areas where development of new statistical tools will improve predictions from distribution models, notably the development of hierarchical models that link different types of distribution model and their attendant uncertainties across spatial scales. Finally, we discuss the need to develop more defensible methods for assessing predictive performance, quantifying model goodness-of-fit and for assessing the significance of model covariates.},
	language = {en},
	number = {1586},
	urldate = {2020-11-04},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Beale, Colin M. and Lennon, Jack J.},
	month = jan,
	year = {2012},
	pages = {247--258},
	file = {beale lennon 2012 - incorporating uncertainty in predictive species distribution modelling - BDPG - GUPPY - SDM.pdf:/Users/bill/D/Zotero/storage/SW9YQV2G/beale lennon 2012 - incorporating uncertainty in predictive species distribution modelling - BDPG - GUPPY - SDM.pdf:application/pdf},
}

@article{beechStochasticApproachMarine2008,
	title = {A stochastic approach to marine reserve design: {Incorporating} data uncertainty},
	volume = {3},
	issn = {15749541},
	shorttitle = {A stochastic approach to marine reserve design},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574954108000514},
	doi = {10.1016/j.ecoinf.2008.09.001},
	abstract = {Marine reserves, or protected areas, are used to meet an array of biodiversity and conservation objectives. The design of regional networks of marine reserves is concerned with the problem of where to place the marine protected areas and how to spatially configure them. Quantitative methods for doing this provide important decision support tools for marine managers. The central problem is to balance the costs and benefits of the reserve network, whilst satisfying conservation objectives (hence solving a constrained optimization problem). Current optimization algorithms for reserve design are widely used, but none allow for the systematic incorporation of data uncertainty and its effect on the reserve design solutions. The central purpose of this study is to provide a framework for incorporating uncertain ecological input data into algorithms for designing networks of marine reserves.},
	language = {en},
	number = {4-5},
	urldate = {2020-11-04},
	journal = {Ecological Informatics},
	author = {Beech, Talia and Dowd, Michael and Field, Chris and Hatcher, Bruce and Andréfouët, Serge},
	month = oct,
	year = {2008},
	pages = {321--333},
	file = {beech et al 2008 - a stochastic approach to marine reserve design - incorporating data uncertainty - ecoinf.pdf:/Users/bill/D/Zotero/storage/FNBLVRIM/beech et al 2008 - a stochastic approach to marine reserve design - incorporating data uncertainty - ecoinf.pdf:application/pdf},
}

@article{billionnetSolvingProbabilisticReserve2011,
	title = {Solving the probabilistic reserve selection problem},
	volume = {222},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380010005508},
	doi = {10.1016/j.ecolmodel.2010.10.009},
	abstract = {The Reserve Selection Problem consists in selecting certain sites among a set of potential sites for biodiversity protection. In many models of the literature, the species present and able to survive in each site are supposed to be known. Here, for every potential site and for every species considered, only the probability that the species survives in the site is supposed to be known. The problem to select, under a budgetary constraint, a set of sites which maximizes the expected number of species is known in the literature under the name of probabilistic reserve selection problem. In this article, this problem is studied with species weighting to deal differently with common species and rare species. A spatial constraint is also considered preventing to obtain too fragmented reserve networks. As in Polasky et al. (2000), the problem is formulated by a nonlinear mathematical program in Boolean variables. Camm et al. (2002) developed a mixed-integer linear programming approximation that may be solved with standard integer programming software. The method gives tight approximate solutions but does not allow to tell how far these solutions are from the optimum. In this paper, a slightly different approach is proposed to approximate the problem. The interesting aspect of the approach, which also uses only standard mixed-integer programming software, is that it leads, not only to an approximate solution, but also to an upper limit on the true optimal value. In other words, the method gives an approximate solution with a guarantee on its accuracy. The linear reformulation is based on an upper approximation of the logarithmic function by a piecewise-linear function. The approach is very effective on artiﬁcial instances that include up to 400 sites and 300 species. Within an average CPU time of about 12 min, near-optimal solutions are obtained with an average relative error, in comparison to the optimum, of less than 0.2\%.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Billionnet, Alain},
	month = feb,
	year = {2011},
	pages = {546--554},
	file = {Billionnet 2010 - Solving the probabilistic reserve selection problem - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/MK3PIBVN/Billionnet 2010 - Solving the probabilistic reserve selection problem - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf:application/pdf},
}

@article{brotonsSpeciesDistributionModels2014,
	title = {Species {Distribution} {Models} and {Impact} {Factor} {Growth} in {Environmental} {Journals}: {Methodological} {Fashion} or the {Attraction} of {Global} {Change} {Science}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Species {Distribution} {Models} and {Impact} {Factor} {Growth} in {Environmental} {Journals}},
	url = {https://dx.plos.org/10.1371/journal.pone.0111996},
	doi = {10.1371/journal.pone.0111996},
	abstract = {In this work, I evaluate the impact of species distribution models (SDMs) on the current status of environmental and ecological journals by asking the question to which degree development of SDMs in the literature is related to recent changes in the impact factors of ecological journals. The hypothesis evaluated states that research fronts are likely to attract research attention and potentially drive citation patterns, with journals concentrating papers related to the research front receiving more attention and benefiting from faster increases in their impact on the ecological literature. My results indicate a positive relationship between the number of SDM related articles published in a journal and its impact factor (IF) growth during the period 2000–09. However, the percentage of SDM related papers in a journal was strongly and positively associated with the percentage of papers on climate change and statistical issues. The results support the hypothesis that global change science has been critical in the development of SDMs and that interest in climate change research in particular, rather than the usage of SDM per se, appears as an important factor behind journal IF increases in ecology and environmental sciences. Finally, our results on SDM application in global change science support the view that scientific interest rather than methodological fashion appears to be the major driver of research attraction in the scientific literature.},
	language = {en},
	number = {11},
	urldate = {2020-11-04},
	journal = {PLoS ONE},
	author = {Brotons, Lluís},
	editor = {Fontaneto, Diego},
	month = nov,
	year = {2014},
	pages = {e111996},
	file = {brotons 2014 - Species Distribution Models and Impact Factor Growth in Environmental Journals - Methodological Fashion or the Attraction of Global Change Science - BDPG - GUPPY - SDM.pdf:/Users/bill/D/Zotero/storage/TJGX9QML/brotons 2014 - Species Distribution Models and Impact Factor Growth in Environmental Journals - Methodological Fashion or the Attraction of Global Change Science - BDPG - GUPPY - SDM.pdf:application/pdf},
}

@article{postvanderburgRoleBudgetSufficiency2014a,
	title = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management: {Robust} {Species} {Management}},
	volume = {78},
	issn = {0022541X},
	shorttitle = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management},
	url = {http://doi.wiley.com/10.1002/jwmg.638},
	doi = {10.1002/jwmg.638},
	abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used ﬁeld data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efﬁcient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufﬁcient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufﬁcient budget. Below this budget, the costefﬁcient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufﬁcient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one’s budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. Ó 2013 The Wildlife Society.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {The Journal of Wildlife Management},
	author = {Post van der Burg, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
	month = jan,
	year = {2014},
	pages = {153--163},
	file = {burg et al 2014 - On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management - BDPG.pdf:/Users/bill/D/Zotero/storage/RJA5UU5D/burg et al 2014 - On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management - BDPG.pdf:application/pdf},
}

@article{carvalhoConservationPlanningClimate2011,
	title = {Conservation planning under climate change: {Toward} accounting for uncertainty in predicted species distributions to increase confidence in conservation investments in space and time},
	volume = {144},
	issn = {00063207},
	shorttitle = {Conservation planning under climate change},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320711001649},
	doi = {10.1016/j.biocon.2011.04.024},
	abstract = {Climate warming challenges our approach to building systems of protected areas because it is likely to drive accelerating shifts in species distributions, and the projections of those future species distributions are uncertain. There are several important sources of uncertainty intrinsic to using species occurrence projections for reserve system design including uncertainty in the number of occurrences captured by any reserve selection solution, and uncertainty arising from the different approaches used to ﬁt predictive models. Here we used the present and future predicted distributions of Iberian herptiles to analyze how dynamics and uncertainty in species distributions may affect decisions about resource allocation for conservation in space and time. We identiﬁed priority areas maximizing coverage of current and future (2020 and 2080) predicted distributions of 65 species, under ‘‘Mild’’ and ‘‘Severe’’ uncertainty. Next, we applied a return-on-investment analysis to quantify and make explicit trade-offs between investing in areas selected when optimizing for different times and with different uncertainty levels. Areas identiﬁed as important for conservation in every time frame and uncertainty level were the ones considered to be robust climate adaptation investments, and included chieﬂy already protected areas. Areas identiﬁed only under ‘‘Mild’’ uncertainty were considered good candidates for investment if extra resources are available and were mainly located in northern Iberia. However, areas selected only in the ‘‘Severe’’ uncertainty case should not be completely disregarded as they may become climatic refugia for some species. Our study provides an objective methodology to deliver ‘‘no regrets’’ conservation investments.},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Carvalho, Sílvia B. and Brito, José C. and Crespo, Eduardo G. and Watts, Matthew E. and Possingham, Hugh P.},
	month = jul,
	year = {2011},
	pages = {2020--2030},
	file = {carvalho et al 2011 - Conservation planning under climate change -  Toward accounting for uncertainty in predicted species distributions ... - SDM - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/SA46L8ET/carvalho et al 2011 - Conservation planning under climate change -  Toward accounting for uncertainty in predicted species distributions ... - SDM - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf:application/pdf},
}

@article{carwardineConservationPlanningIrreplaceability2007,
	title = {Conservation planning with irreplaceability: does the method matter?},
	volume = {16},
	issn = {0960-3115, 1572-9710},
	shorttitle = {Conservation planning with irreplaceability},
	url = {http://link.springer.com/10.1007/s10531-006-9055-4},
	doi = {10.1007/s10531-006-9055-4},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Biodiversity and Conservation},
	author = {Carwardine, J. and Rochester, W. A. and Richardson, K. S. and Williams, K. J. and Pressey, R. L. and Possingham, H. P.},
	month = jan,
	year = {2007},
	pages = {245--258},
	file = {carwardine et al 2007 - conservation planning with irreplaceability - does the method matter - BDPG - RESERVE SELECTION - ANNO.pdf:/Users/bill/D/Zotero/storage/2SMX5UAW/carwardine et al 2007 - conservation planning with irreplaceability - does the method matter - BDPG - RESERVE SELECTION - ANNO.pdf:application/pdf},
}

@article{carwardineConservationPlanningWhen2010,
	title = {Conservation {Planning} when {Costs} {Are} {Uncertain}: {Conservation} {Planning}},
	volume = {24},
	issn = {08888892},
	shorttitle = {Conservation {Planning} when {Costs} {Are} {Uncertain}},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2010.01535.x},
	doi = {10.1111/j.1523-1739.2010.01535.x},
	abstract = {Spatially explicit information on the financial costs of conservation actions can improve the ability of conservation planning to achieve ecological and economic objectives, but the magnitude of this improvement may depend on the accuracy of the cost estimates. Data on costs of conservation actions are inherently uncertain. For example, the cost of purchasing a property for addition to a protected-area network depends on the individual landholder’s preferences, values, and aspirations, all of which vary in space and time, and the effect of this uncertainty on the conservation priority of a site is relatively untested. We investigated the sensitivity of the conservation priority of sites to uncertainty in cost estimates. We explored scenarios for expanding (four-fold) the protected-area network in Queensland, Australia to represent a range of vegetation types, species, and abiotic environments, while minimizing the cost of purchasing new properties. We estimated property costs for 17, 790 10 × 10 km sites with data on unimproved land values. We systematically changed property costs and noted how these changes affected conservation priority of a site. The sensitivity of the priority of a site to changes in cost data was largely dependent on a site’s importance for meeting conservation targets. Sites that were essential or unimportant for meeting targets maintained high or low priorities, respectively, regardless of cost estimates. Sites of intermediate conservation priority were sensitive to property costs and represented the best option for efficiency gains, especially if they could be purchased at a lower price than anticipated. Thus, uncertainty in cost estimates did not impede the use of cost data in conservation planning, and information on the sensitivity of the conservation priority of a site to estimates of the price of land can be used to inform strategic conservation planning before the actual price of the land is known.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Carwardine, Josie and Wilson, Kerrie A. and Hajkowicz, Stefan A. and Smith, Robert J. and Klein, Carissa J. and Watts, Matt and Possingham, Hugh P.},
	month = dec,
	year = {2010},
	pages = {1529--1537},
	file = {carwardine et al. - 2010 - Conservation Planning when Costs Are Uncertain - BDPG - COST UNCERTAINTY - RESERVE SELECTION - ANNO.pdf:/Users/bill/D/Zotero/storage/GYSNYMC5/carwardine et al. - 2010 - Conservation Planning when Costs Are Uncertain - BDPG - COST UNCERTAINTY - RESERVE SELECTION - ANNO.pdf:application/pdf},
}

@article{costaManagersModelersMeasuring2018,
	title = {Managers, modelers, and measuring the impact of species distribution model uncertainty on marine zoning decisions},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0204569},
	doi = {10.1371/journal.pone.0204569},
	abstract = {Marine managers routinely use spatial data to make decisions about their marine environment. Uncertainty associated with this spatial data can have profound impacts on these management decisions and their projected outcomes. Recent advances in modeling techniques, including species distribution models (SDMs), make it easier to generate continuous maps showing the uncertainty associated with spatial predictions and maps. However, SDM predictions and maps can be complex and nuanced. This complexity makes their use challenging for non-technical managers, preventing them from having the best available information to make decisions. To help bridge these communication and information gaps, we developed maps to illustrate how SDMs and associated uncertainty can be translated into readily usable products for managers. We also explicitly described the potential impacts of uncertainty on marine zoning decisions. This approach was applied to a case study in Saipan Lagoon, Commonwealth of the Northern Mariana Islands (CNMI). Managers in Saipan are interested in minimizing the potential impacts of personal watercraft (e.g., jet skis) on staghorn Acropora (i.e., Acropora aspera, A. formosa, and A. pulchra), which is an important coral assemblage in the lagoon. We used a recently completed SDM for staghorn Acropora to develop maps showing the sensitivity of zoning options to three different prediction and three different uncertainty thresholds (nine combinations total). Our analysis showed that the amount of area and geographic location of predicted staghorn Acropora presence changed based on these nine combinations. These dramatically different spatial patterns would have significant zoning implications when considering where to exclude and/or allow jet skis operations inside the lagoon. They also show that different uncertainty thresholds may lead managers to markedly different conclusions and courses of action. Defining acceptable levels of uncertainty upfront is critical for ensuring that managers can make more informed decisions, meet their marine resource goals and generate favorable outcomes for their stakeholders.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {PLOS ONE},
	author = {Costa, Bryan and Kendall, Matthew and McKagan, Steven},
	editor = {Januchowski-Hartley, Fraser Andrew},
	month = oct,
	year = {2018},
	pages = {e0204569},
	file = {costa et al 2018 - Managers modelers and measuring the impact of species distribution model uncertainty on marine zoning decisions - RESERVE SELECTION - SDM - SCP - BDPG - GUPPY - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/8GXHRMV7/costa et al 2018 - Managers modelers and measuring the impact of species distribution model uncertainty on marine zoning decisions - RESERVE SELECTION - SDM - SCP - BDPG - GUPPY - UNCERTAINTY.pdf:application/pdf},
}

@article{dujardinSolvingMultiobjectiveOptimization2018,
	title = {Solving multi-objective optimization problems in conservation with the reference point method},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0190748},
	doi = {10.1371/journal.pone.0190748},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {PLOS ONE},
	author = {Dujardin, Yann and Chadès, Iadine},
	editor = {Soleimani-damaneh, Majid},
	month = jan,
	year = {2018},
	pages = {e0190748},
	file = {dujardin chades 2018 - Solving multi-objective optimization problems in conservation with the reference point method - MULTI-OBJECTIVE.pdf:/Users/bill/D/Zotero/storage/K6BYAED3/dujardin chades 2018 - Solving multi-objective optimization problems in conservation with the reference point method - MULTI-OBJECTIVE.pdf:application/pdf},
}

@article{fischerPerfectlyNestedSignificantly2005,
	title = {Á {Perfectly} nested or signiﬁcantly nested / an important difference for conservation management},
	language = {en},
	author = {Fischer, Joern and Lindenmayer, David B},
	year = {2005},
	pages = {10},
	file = {fischer and lindenmayer 2005 - perfectly nested or significantly nested - an important difference for conservation management - oikos.pdf:/Users/bill/D/Zotero/storage/NFJSKSNX/fischer and lindenmayer 2005 - perfectly nested or significantly nested - an important difference for conservation management - oikos.pdf:application/pdf},
}

@article{fischerClusteringCompactnessReserve,
	title = {Clustering and {Compactness} in {Reserve} {Site} {Selection}: {An} {Extension} of the {Biodiversity} {Management} {Area} {Selection} {Model}},
	abstract = {Over the last 15 yr, a number of formal mathematical models and heuristics have been developed for the purpose of selecting sites for biodiversity conservation. One of these models, the Biodiversity Management Area Selection (BMAS) model (Church et al. 1996a), places a major emphasis on protecting at least a certain area for each biodiversity element. Viewed spatially, solutions from this model tend to be a combination of isolated planning units and, sometimes, small clusters. One method to identify solutions with potentially less fragmentation is to add an objective to minimize the outside perimeter of selected areas. Outside perimeter only counts those edges of a planning unit that are not shared in common with another selected planning unit in a cluster, and, therefore, compact clustering is encouraged. This article presents a new math programming model that incorporates this perimeter objective into the BMAS model. We present an application using data from the USDA Forest Service-funded Sierra Nevada Ecosystem Project (Davis et al. 1996) and show that the model can be solved optimally by off-the-shelf software. Our tests indicate that the model can produce dramatic reductions in perimeter of the reserve system (increasing clustering and compactness) at the expense of relatively small decreases in performance against area and suitability measures. FOR. SCI. 49(4): 555–565.},
	language = {en},
	author = {Fischer, Douglas T and Church, Richard L},
	pages = {12},
	file = {fischer church 2003 - Clustering and Compactness in ReserveSite Selection - An Extension of theBiodiversity Management AreaSelection Model - BDPG - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/ZIMGPBIG/fischer church 2003 - Clustering and Compactness in ReserveSite Selection - An Extension of theBiodiversity Management AreaSelection Model - BDPG - RESERVE SELECTION.pdf:application/pdf},
}

@article{fourcadePaintingsPredictDistribution2018a,
	title = {Paintings predict the distribution of species, or the challenge of selecting environmental predictors and evaluation statistics},
	volume = {27},
	issn = {1466822X},
	url = {http://doi.wiley.com/10.1111/geb.12684},
	doi = {10.1111/geb.12684},
	abstract = {Aim: Species distribution modelling, a family of statistical methods that predicts species distributions from a set of occurrences and environmental predictors, is now routinely applied in many macroecological studies. However, the reliability of evaluation metrics usually employed to validate these models remains questioned. Moreover, the emergence of online databases of environmental variables with global coverage, especially climatic, has favoured the use of the same set of standard predictors. Unfortunately, the selection of variables is too rarely based on a careful examination of the species’ ecology. In this context, our aim was to highlight the importance of selecting ad hoc variables in species distribution models, and to assess the ability of classical evaluation statistics to identify models with no biological realism.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Global Ecology and Biogeography},
	author = {Fourcade, Yoan and Besnard, Aurélien G. and Secondi, Jean},
	month = feb,
	year = {2018},
	pages = {245--256},
	file = {fourcade et al 2017 - Paintings predict the distribution of species, or the challenge ofselecting environmental predictors and evaluation statistics - SDM - GUPPY - BDPG - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/GITCCB25/fourcade et al 2017 - Paintings predict the distribution of species, or the challenge ofselecting environmental predictors and evaluation statistics - SDM - GUPPY - BDPG - UNCERTAINTY.pdf:application/pdf},
}

@article{gamePLANNINGPERSISTENCEMARINE2008,
	title = {{PLANNING} {FOR} {PERSISTENCE} {IN} {MARINE} {RESERVES}: {A} {QUESTION} {OF} {CATASTROPHIC} {IMPORTANCE}},
	volume = {18},
	issn = {1051-0761},
	shorttitle = {{PLANNING} {FOR} {PERSISTENCE} {IN} {MARINE} {RESERVES}},
	url = {http://doi.wiley.com/10.1890/07-1027.1},
	doi = {10.1890/07-1027.1},
	abstract = {Large-scale catastrophic events, although rare, lie generally beyond the control of local management and can prevent marine reserves from achieving biodiversity outcomes. We formulate a new conservation planning problem that aims to minimize the probability of missing conservation targets as a result of catastrophic events. To illustrate this approach we formulate and solve the problem of minimizing the impact of large-scale coral bleaching events on a reserve system for the Great Barrier Reef, Australia. We show that by considering the threat of catastrophic events as part of the reserve design problem it is possible to substantially improve the likely persistence of conservation features within reserve networks for a negligible increase in cost. In the case of the Great Barrier Reef, a 2\% increase in overall reserve cost was enough to improve the long-run performance of our reserve network by .60\%. Our results also demonstrate that simply aiming to protect the reefs at lowest risk of catastrophic bleaching does not necessarily lead to the best conservation outcomes, and enormous gains in overall persistence can be made by removing the requirement to represent all bioregions in the reserve network. We provide an explicit and well-deﬁned method that allows the probability of catastrophic disturbances to be included in the site selection problem without creating additional conservation targets or imposing arbitrary presence/absence thresholds on existing data. This research has implications for reserve design in a changing climate.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Ecological Applications},
	author = {Game, Edward T. and Watts, Matthew E. and Wooldridge, Scott and Possingham, Hugh P.},
	month = apr,
	year = {2008},
	pages = {670--680},
	file = {game et al 2008 - planning for persistence in marine reserves - a question of catastrophic importance - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/599JNVJ8/game et al 2008 - planning for persistence in marine reserves - a question of catastrophic importance - RESERVE SELECTION.pdf:application/pdf},
}

@article{gameSixCommonMistakes2013,
	title = {Six {Common} {Mistakes} in {Conservation} {Priority} {Setting}},
	volume = {27},
	issn = {0888-8892, 1523-1739},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cobi.12051},
	doi = {10.1111/cobi.12051},
	abstract = {A vast number of prioritization schemes have been developed to help conservation navigate tough decisions about the allocation of finite resources. However, the application of quantitative approaches to setting priorities in conservation frequently includes mistakes that can undermine their authors’ intention to be more rigorous and scientific in the way priorities are established and resources allocated. Drawing on well-established principles of decision science, we highlight 6 mistakes commonly associated with setting priorities for conservation: not acknowledging conservation plans are prioritizations; trying to solve an illdefined problem; not prioritizing actions; arbitrariness; hidden value judgments; and not acknowledging risk of failure. We explain these mistakes and offer a path to help conservation planners avoid making the same mistakes in future prioritizations.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Game, Edward T. and Kareiva, Peter and Possingham, Hugh P.},
	month = jun,
	year = {2013},
	pages = {480--485},
	file = {game kareiva possingham 2013 - Six Common Mistakes in Conservation Priority Setting.pdf:/Users/bill/D/Zotero/storage/5Z55E27B/game kareiva possingham 2013 - Six Common Mistakes in Conservation Priority Setting.pdf:application/pdf},
}

@article{grandBiasedDataReduce2007,
	title = {Biased data reduce efficiency and effectiveness of conservation reserve networks},
	volume = {10},
	issn = {1461-023X, 1461-0248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2007.01025.x},
	doi = {10.1111/j.1461-0248.2007.01025.x},
	abstract = {Complementarity-based reserve selection algorithms efﬁciently prioritize sites for biodiversity conservation, but they are data-intensive and most regions lack accurate distribution maps for the majority of species. We explored implications of basing conservation planning decisions on incomplete and biased data using occurrence records of the plant family Proteaceae in South Africa. Treating this high-quality database as ÔcompleteÕ, we introduced three realistic sampling biases characteristic of biodiversity databases: a detectability sampling bias and two forms of roads sampling bias. We then compared reserve networks constructed using complete, biased, and randomly sampled data. All forms of biased sampling performed worse than both the complete data set and equal-effort random sampling. Biased sampling failed to detect a median of 1–5\% of species, and resulted in reserve networks that were 9–17\% larger than those designed with complete data. Spatial congruence and the correlation of irreplaceability scores between reserve networks selected with biased and complete data were low. Thus, reserve networks based on biased data require more area to protect fewer species and identify different locations than those selected with randomly sampled or complete data.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Ecology Letters},
	author = {Grand, Joanna and Cummings, Michael P. and Rebelo, Tony G. and Ricketts, Taylor H. and Neel, Maile C.},
	month = may,
	year = {2007},
	pages = {364--374},
	file = {grand et al 2007 - Biased data reduce efficiency and effectiveness of conservation reserve networks - BDPG - RESERVE SELECTION - UNCERTAINTY - SAMPLING ERROR MODEL - RARITY MEASURES - ANNO.pdf:/Users/bill/D/Zotero/storage/QGG36X88/grand et al 2007 - Biased data reduce efficiency and effectiveness of conservation reserve networks - BDPG - RESERVE SELECTION - UNCERTAINTY - SAMPLING ERROR MODEL - RARITY MEASURES - ANNO.pdf:application/pdf},
}

@article{granthamDiminishingReturnInvestment2008,
	title = {Diminishing return on investment for biodiversity data in conservation planning: {Conservation} value of data},
	volume = {1},
	issn = {1755263X},
	shorttitle = {Diminishing return on investment for biodiversity data in conservation planning},
	url = {http://doi.wiley.com/10.1111/j.1755-263X.2008.00029.x},
	doi = {10.1111/j.1755-263X.2008.00029.x},
	abstract = {It is generally assumed that gathering more data is a good investment for conservation planning. However, the beneﬁts of additional data have seldom been evaluated by analyzing the return on investment. If there are diminishing returns in terms of improved planning, then resources might be better directed toward other actions, depending on their relative costs and beneﬁts. Our aim was to determine the return on investment from spending different amounts on survey data before undertaking a program of implementing new protected areas. We estimated how much protea data is obtained as a function of dollars invested in surveying. We then simulated incremental protection and loss of habitat to determine the beneﬁt of investment in that data on the protection of proteas. We found that, after an investment of only US\$100,000 (∼780,000 South Africa Rand [ZAR]), there was little increase in the effectiveness of conservation prioritizations, despite the full data set costing at least 25 times that amount.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Conservation Letters},
	author = {Grantham, Hedley S. and Moilanen, Atte and Wilson, Kerrie A. and Pressey, Robert L. and Rebelo, Tony G. and Possingham, Hugh P.},
	month = sep,
	year = {2008},
	pages = {190--198},
	file = {grantham et al 2008 - Diminishing return on investment for biodiversity data in conservation planning - RESERVE SELECTION - BDPG - COST - ANNO.pdf:/Users/bill/D/Zotero/storage/FN9GUMJU/grantham et al 2008 - Diminishing return on investment for biodiversity data in conservation planning - RESERVE SELECTION - BDPG - COST - ANNO.pdf:application/pdf},
}

@article{moilanenPlanningRobustReserve2006a,
	title = {Planning for robust reserve networks using uncertainty analysis},
	volume = {199},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380006003127},
	doi = {10.1016/j.ecolmodel.2006.07.004},
	abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence–absence in sites, or on species-speciﬁc distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data—erroneous species presence–absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modiﬁcations to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and Ben-Haim, Yakov},
	month = nov,
	year = {2006},
	pages = {115--124},
	file = {moilanen et al 2006 - Planning for robust reserve networks usinguncertainty analysis - BDPG - UNCERTAINTY - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/LAFZDKZ5/moilanen et al 2006 - Planning for robust reserve networks usinguncertainty analysis - BDPG - UNCERTAINTY - RESERVE SELECTION.pdf:application/pdf},
}

@article{norbergComprehensiveEvaluationPredictive2019,
	title = {A comprehensive evaluation of predictive performance of 33 species distribution models at species and community levels},
	volume = {89},
	issn = {0012-9615, 1557-7015},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1370},
	doi = {10.1002/ecm.1370},
	abstract = {A large array of species distribution model (SDM) approaches has been developed for explaining and predicting the occurrences of individual species or species assemblages. Given the wealth of existing models, it is unclear which models perform best for interpolation or extrapolation of existing data sets, particularly when one is concerned with species assemblages. We compared the predictive performance of 33 variants of 15 widely applied and recently emerged SDMs in the context of multispecies data, including both joint SDMs that model multiple species together, and stacked SDMs that model each species individually combining the predictions afterward. We offer a comprehensive evaluation of these SDM approaches by examining their performance in predicting withheld empirical validation data of different sizes representing five different taxonomic groups, and for prediction tasks related to both interpolation and extrapolation. We measure predictive performance by 12 measures of accuracy, discrimination power, calibration, and precision of predictions, for the biological levels of species occurrence, species richness, and community composition. Our results show large variation among the models in their predictive performance, especially for communities comprising many species that are rare. The results do not reveal any major trade-offs among measures of model performance; the same models performed generally well in terms of accuracy, discrimination, and calibration, and for the biological levels of individual species, species richness, and community composition. In contrast, the models that gave the most precise predictions were not well calibrated, suggesting that poorly performing models can make overconfident predictions. However, none of the models performed well for all prediction tasks. As a general strategy, we therefore propose that researchers fit a small set of models showing complementary performance, and then apply a cross-validation procedure involving separate data to establish which of these models performs best for the goal of the study.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Ecological Monographs},
	author = {Norberg, Anna and Abrego, Nerea and Blanchet, F. Guillaume and Adler, Frederick R. and Anderson, Barbara J. and Anttila, Jani and Araújo, Miguel B. and Dallas, Tad and Dunson, David and Elith, Jane and Foster, Scott D. and Fox, Richard and Franklin, Janet and Godsoe, William and Guisan, Antoine and O'Hara, Bob and Hill, Nicole A. and Holt, Robert D. and Hui, Francis K. C. and Husby, Magne and Kålås, John Atle and Lehikoinen, Aleksi and Luoto, Miska and Mod, Heidi K. and Newell, Graeme and Renner, Ian and Roslin, Tomas and Soininen, Janne and Thuiller, Wilfried and Vanhatalo, Jarno and Warton, David and White, Matt and Zimmermann, Niklaus E. and Gravel, Dominique and Ovaskainen, Otso},
	month = aug,
	year = {2019},
	file = {norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY - ANNO.pdf:/Users/bill/D/Zotero/storage/7RCAGYZ3/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY - ANNO.pdf:application/pdf;norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S1.pdf:/Users/bill/D/Zotero/storage/KIZLVNRK/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S1.pdf:application/pdf;norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S2.pdf:/Users/bill/D/Zotero/storage/FVPNNYJE/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S2.pdf:application/pdf;norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S3.pdf:/Users/bill/D/Zotero/storage/B3925UT9/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S3.pdf:application/pdf},
}

@article{pearsonModelbasedUncertaintySpecies2006,
	title = {Model-based uncertainty in species range prediction},
	volume = {33},
	issn = {0305-0270, 1365-2699},
	url = {http://doi.wiley.com/10.1111/j.1365-2699.2006.01460.x},
	doi = {10.1111/j.1365-2699.2006.01460.x},
	abstract = {Aim Many attempts to predict the potential range of species rely on environmental niche (or ‘bioclimate envelope’) modelling, yet the effects of using different niche-based methodologies require further investigation. Here we investigate the impact that the choice of model can have on predictions, identify key reasons why model output may differ and discuss the implications that model uncertainty has for policy-guiding applications.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {Journal of Biogeography},
	author = {Pearson, Richard G. and Thuiller, Wilfried and Araújo, Miguel B. and Martinez-Meyer, Enrique and Brotons, Lluís and McClean, Colin and Miles, Lera and Segurado, Pedro and Dawson, Terence P. and Lees, David C.},
	month = oct,
	year = {2006},
	pages = {1704--1711},
	file = {pearson et al 2006 - model-based uncertainty in species range prediction - BDPG - SDM - GUPPY.pdf:/Users/bill/D/Zotero/storage/RXLKPVRZ/pearson et al 2006 - model-based uncertainty in species range prediction - BDPG - SDM - GUPPY.pdf:application/pdf},
}

@article{presseyEffectsDataCharacteristics1999,
	title = {Effects of data characteristics on the results of reserve selection algorithms},
	volume = {26},
	issn = {0305-0270, 1365-2699},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2699.1999.00258.x},
	doi = {10.1046/j.1365-2699.1999.00258.x},
	abstract = {We tested the effects of four data characteristics on the results of reserve selection algorithms. The data characteristics were nestedness of features (land types in this case), rarity of features, size variation of sites (potential reserves) and size of data sets (numbers of sites and features). We manipulated data sets to produce three levels, with replication, of each of these data characteristics while holding the other three characteristics constant. We then used an optimizing algorithm and three heuristic algorithms to select sites to solve several reservation problems. We measured efﬁciency as the number or total area of selected sites, indicating the relative cost of a reserve system. Higher nestedness increased the efﬁciency of all algorithms (reduced the total cost of new reserves). Higher rarity reduced the efﬁciency of all algorithms (increased the total cost of new reserves). More variation in site size increased the efﬁciency of all algorithms expressed in terms of total area of selected sites. We measured the suboptimality of heuristic algorithms as the percentage increase of their results over optimal (minimum possible) results. Suboptimality is a measure of the reliability of heuristics as indicative costing analyses. Higher rarity reduced the suboptimality of heuristics (increased their reliability) and there is some evidence that more size variation did the same for the total area of selected sites. We discuss the implications of these results for the use of reserve selection algorithms as indicative and real-world planning tools.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Journal of Biogeography},
	author = {Pressey, R. L. and Possingham, H. P. and Logan, V. S. and Day, J. R. and Williams, P. H.},
	month = jan,
	year = {1999},
	pages = {179--191},
	file = {Pressey et al. - 1999 - Effects of data characteristics on the results of reserve selection algorithms - BDPG -  PROBLEM DIFFICULTY - ANNO.pdf:/Users/bill/D/Zotero/storage/JAXE6PB3/Pressey et al. - 1999 - Effects of data characteristics on the results of reserve selection algorithms - BDPG -  PROBLEM DIFFICULTY - ANNO.pdf:application/pdf},
}

@article{ribeiroWhyShouldTrust2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv: 1602.04938},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {ribeiro singh guestrin 2016 - why should i trust you - explaining the predictions of any classifier - ML - SUBMODULAR - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/QRCQ9SPS/ribeiro singh guestrin 2016 - why should i trust you - explaining the predictions of any classifier - ML - SUBMODULAR - BDPG - GUPPY.pdf:application/pdf},
}

@article{robillardAssessingShelfLife2017,
	title = {Assessing the shelf life of cost-efficient conservation plans for species at risk across gradients of agricultural land use: {Cost}-{Efficient} {Conservation}},
	volume = {31},
	issn = {08888892},
	shorttitle = {Assessing the shelf life of cost-efficient conservation plans for species at risk across gradients of agricultural land use},
	url = {http://doi.wiley.com/10.1111/cobi.12886},
	doi = {10.1111/cobi.12886},
	abstract = {High costs of land in agricultural regions warrant spatial prioritization approaches to conservation that explicitly consider land prices to produce protected-area networks that accomplish targets efficiently. However, land-use changes in such regions and delays between plan design and implementation may render optimized plans obsolete before implementation occurs. To measure the shelf life of cost-efficient conservation plans, we simulated a land-acquisition and restoration initiative aimed at conserving species at risk in Canada’s farmlands. We accounted for observed changes in land-acquisition costs and in agricultural intensity based on censuses of agriculture taken from 1986 to 2011. For each year of data, we mapped costs and areas of conservation priority designated using Marxan. We compared plans to test for changes through time in the arrangement of high-priority sites and in the total cost of each plan. For acquisition costs, we measured the savings from accounting for prices during site selection. Land-acquisition costs and land-use intensity generally rose over time independent of inflation (24–78\%), although rates of change were heterogeneous through space and decreased in some areas. Accounting for spatial variation in land price lowered the cost of conservation plans by 1.73–13.9\%, decreased the range of costs by 19–82\%, and created unique solutions from which to choose. Despite the rise in plan costs over time, the high conservation priority of particular areas remained consistent. Delaying conservation in these critical areas may compromise what optimized conservation plans can achieve. In the case of Canadian farmland, rapid conservation action is cost-effective, even with moderate levels of uncertainty in how to implement restoration goals.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Robillard, Cassandra M. and Kerr, Jeremy T.},
	month = aug,
	year = {2017},
	pages = {837--847},
	file = {robillard kerr 2017 - Assessing the shelf life of cost-efﬁcient conservation plans for species at risk across gradients of agricultural land use - BDPG - RESERVE SELECTION - COST.pdf:/Users/bill/D/Zotero/storage/ZAVSPH98/robillard kerr 2017 - Assessing the shelf life of cost-efﬁcient conservation plans for species at risk across gradients of agricultural land use - BDPG - RESERVE SELECTION - COST.pdf:application/pdf},
}

@article{rodewaldTradeoffsValueBiodiversity2019,
	title = {Tradeoffs in the value of biodiversity feature and cost data in conservation prioritization},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-52241-2},
	doi = {10.1038/s41598-019-52241-2},
	abstract = {Abstract
            
              Decision-support tools are commonly used to maximize return on investments (ROI) in conservation. We evaluated how the relative value of information on biodiversity features and land cost varied with data structure and variability, attributes of focal species and conservation targets, and habitat suitability thresholds for contrasting bird communities in the Pacific Northwest of North America. Specifically, we used spatial distribution maps for 20 bird species, land values, and an integer linear programming model to prioritize land units (1 km
              2
              ) that met conservation targets at the lowest estimated cost (hereafter ‘efficiency’). Across scenarios, the relative value of biodiversity data increased with conservation targets, as higher thresholds for suitable habitat were applied, and when focal species occurred disproportionately on land of high assessed value. Incorporating land cost generally improved planning efficiency, but at diminishing rates as spatial variance in biodiversity features relative to land cost increased. Our results offer a precise, empirical demonstration of how spatially-optimized planning solutions are influenced by spatial variation in underlying feature layers. We also provide guidance to planners seeking to maximize efficiency in data acquisition and resolve potential trade-offs when setting targets and thresholds in financially-constrained, spatial planning efforts aimed at maximizing ROI in biodiversity conservation.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Scientific Reports},
	author = {Rodewald, Amanda D. and Strimas-Mackey, Matt and Schuster, Richard and Arcese, Peter},
	month = dec,
	year = {2019},
	pages = {15921},
	file = {rodewald strimas-mackey et al 2019 - Tradeoffs in the value of biodiversity feature and cost data in conservation prioritization - BDPG - RESERVE SELECTION - COST ERROR - VOI - ANNO.pdf:/Users/bill/D/Zotero/storage/WX5K62BN/rodewald strimas-mackey et al 2019 - Tradeoffs in the value of biodiversity feature and cost data in conservation prioritization - BDPG - RESERVE SELECTION - COST ERROR - VOI - ANNO.pdf:application/pdf},
}

@article{rodriguesFlexibilityEfficiencyAccountability2000,
	title = {Flexibility, efﬁciency, and accountability: adapting reserve selection algorithms to more complex conservation problems},
	language = {en},
	author = {Rodrigues, Ana S and Cerdeira, J Orestes and Gaston, Kevin J},
	year = {2000},
	pages = {10},
	file = {rodrigues et al 2000 - flexibility, efficiency, and accountability - adapting reserve selection algorithms to more complex conservation problems - RESERVE SELECTION - ANNO.pdf:/Users/bill/D/Zotero/storage/8R2L6NVX/rodrigues et al 2000 - flexibility, efficiency, and accountability - adapting reserve selection algorithms to more complex conservation problems - RESERVE SELECTION - ANNO.pdf:application/pdf},
}

@article{rodriguesOptimisationReserveSelection2002,
	title = {Optimisation in reserve selection procedures—why not?},
	volume = {107},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320702000423},
	doi = {10.1016/S0006-3207(02)00042-3},
	abstract = {Linear programming techniques provide an appropriate tool for solving reserve selection problems. Although this has long been known, most published analyses persist in the use of intuitive heuristics, which cannot guarantee the optimality of the solutions found. Here, we dispute two of the most common justiﬁcations for the use of intuitive heuristics, namely that optimisation techniques are too slow and cannot solve the most realistic selection problems. By presenting an overview of processing times obtained when solving a diversity of reserve selection problems, we demonstrate that most of those published could almost certainly be solved very quickly by standard optimisation software using current widely available computing technology. Even for those problems that take longer to solve, solutions with low levels of sub-optimality can be obtained quite quickly, presenting a better alternative to intuitive heuristics. \# 2002 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Rodrigues, Ana S.L. and Gaston, Kevin J.},
	month = sep,
	year = {2002},
	pages = {123--129},
	file = {rodrigues gaston 2002 - optimisation in reserve selection procedures - why not - BDPG - RESERVE SELECTION - ANNO.pdf:/Users/bill/D/Zotero/storage/DIXKU65K/rodrigues gaston 2002 - optimisation in reserve selection procedures - why not - BDPG - RESERVE SELECTION - ANNO.pdf:application/pdf},
}

@article{rondininiTradeoffsDifferentTypes2006,
	title = {Tradeoffs of different types of species occurrence data for use in systematic conservation planning: {Species} data for conservation planning},
	volume = {9},
	issn = {1461023X},
	shorttitle = {Tradeoffs of different types of species occurrence data for use in systematic conservation planning},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2006.00970.x},
	doi = {10.1111/j.1461-0248.2006.00970.x},
	abstract = {Data on the occurrence of species are widely used to inform the design of reserve networks. These data contain commission errors (when a species is mistakenly thought to be present) and omission errors (when a species is mistakenly thought to be absent), and the rates of the two types of error are inversely related. Point locality data can minimize commission errors, but those obtained from museum collections are generally sparse, suffer from substantial spatial bias and contain large omission errors. Geographic ranges generate large commission errors because they assume homogenous species distributions. Predicted distribution data make explicit inferences on species occurrence and their commission and omission errors depend on model structure, on the omission of variables that determine species distribution and on data resolution. Omission errors lead to identifying networks of areas for conservation action that are smaller than required and centred on known species occurrences, thus affecting the comprehensiveness, representativeness and efﬁciency of selected areas. Commission errors lead to selecting areas not relevant to conservation, thus affecting the representativeness and adequacy of reserve networks. Conservation plans should include an estimation of commission and omission errors in underlying species data and explicitly use this information to inﬂuence conservation planning outcomes.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {Ecology Letters},
	author = {Rondinini, Carlo and Wilson, Kerrie A. and Boitani, Luigi and Grantham, Hedley and Possingham, Hugh P.},
	month = oct,
	year = {2006},
	pages = {1136--1145},
	file = {Rondinini et al. - 2006 - Tradeoffs of different types of species occurrence data for use in systematic conservation planning. - Ecology letters.pdf:/Users/bill/D/Zotero/storage/PW3EMZHJ/Rondinini et al. - 2006 - Tradeoffs of different types of species occurrence data for use in systematic conservation planning. - Ecology letters.pdf:application/pdf},
}

@article{rothFunctionalEcologyImperfect2018,
	title = {Functional ecology and imperfect detection of species},
	volume = {9},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12950},
	doi = {10.1111/2041-210X.12950},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Roth, Tobias and Allan, Eric and Pearman, Peter B. and Amrhein, Valentin},
	editor = {Carvalheiro, Luísa},
	month = apr,
	year = {2018},
	pages = {917--928},
	file = {roth et al 2017 - Functional ecology and imperfect detection of species - DETECTABILITY - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/EZSVA4DK/roth et al 2017 - Functional ecology and imperfect detection of species - DETECTABILITY - BDPG - GUPPY.pdf:application/pdf},
}

@article{sanchezAnalysisHowTraining2007a,
	title = {An analysis of how training data complexity affects the nearest neighbor classifiers},
	volume = {10},
	issn = {1433-7541, 1433-755X},
	url = {http://link.springer.com/10.1007/s10044-007-0061-2},
	doi = {10.1007/s10044-007-0061-2},
	abstract = {The k-nearest neighbors (k-NN) classiﬁer is one of the most popular supervised classiﬁcation methods. It is very simple, intuitive and accurate in a great variety of real-world domains. Nonetheless, despite its simplicity and effectiveness, practical use of this rule has been historically limited due to its high storage requirements and the computational costs involved. On the other hand, the performance of this classiﬁer appears strongly sensitive to training data complexity. In this context, by means of several problem difﬁculty measures, we try to characterize the behavior of the k-NN rule when working under certain situations. More speciﬁcally, the present analysis focuses on the use of some data complexity measures to describe class overlapping, feature space dimensionality and class density, and discover their relation with the practical accuracy of this classiﬁer.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Pattern Analysis and Applications},
	author = {Sánchez, J. S. and Mollineda, R. A. and Sotoca, J. M.},
	month = jul,
	year = {2007},
	pages = {189--201},
	file = {sanchez et al 2007 - an analysis of how training data complexity affects the nearest neighbor classifiers - KNN - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/Y8TNM63P/sanchez et al 2007 - an analysis of how training data complexity affects the nearest neighbor classifiers - KNN - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{sarkarPlacePrioritizationBiodiversity2004,
	title = {Place prioritization for biodiversity conservation using probabilistic surrogate distribution data: {Prioritization} using distribution data},
	volume = {10},
	issn = {13669516, 14724642},
	shorttitle = {Place prioritization for biodiversity conservation using probabilistic surrogate distribution data},
	url = {http://doi.wiley.com/10.1111/j.1366-9516.2004.00060.x},
	doi = {10.1111/j.1366-9516.2004.00060.x},
	abstract = {We analyse optimal and heuristic place prioritization algorithms for biodiversity conservation area network design which can use probabilistic data on the distribution of surrogates for biodiversity. We show how an Expected Surrogate Set Covering Problem (ESSCP) and a Maximal Expected Surrogate Covering Problem (MESCP) can be linearized for computationally efﬁcient solution. For the ESSCP, we study the performance of two optimization software packages (XPRESS and CPLEX) and ﬁve heuristic algorithms based on traditional measures of complementarity and rarity as well as the Shannon and Simpson indices of α-diversity which are being used in this context for the ﬁrst time. On small artiﬁcial data sets the optimal place prioritization algorithms often produced more economical solutions than the heuristic algorithms, though not always ones guaranteed to be optimal. However, with large data sets, the optimal algorithms often required long computation times and produced no better results than heuristic ones. Thus there is generally little reason to prefer optimal to heuristic algorithms with probabilistic data sets.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Diversity and Distributions},
	author = {Sarkar, Sahotra and Pappas, Christopher and Garson, Justin and Aggarwal, Anshu and Cameron, Susan},
	month = feb,
	year = {2004},
	pages = {125--133},
	file = {sarkar et al 2004 - Place prioritization for biodiversity conservation using probabilistic surrogate distribution data - - ANNO.pdf:/Users/bill/D/Zotero/storage/M6Y4JJH7/sarkar et al 2004 - Place prioritization for biodiversity conservation using probabilistic surrogate distribution data - - ANNO.pdf:application/pdf},
}

@article{sofaerMisleadingPrioritizationsModelling2018,
	title = {Misleading prioritizations from modelling range shifts under climate change},
	volume = {27},
	issn = {1466822X},
	url = {http://doi.wiley.com/10.1111/geb.12726},
	doi = {10.1111/geb.12726},
	abstract = {Aim: Conservation planning requires the prioritization of a subset of taxa and geographical locations to focus monitoring and management efforts. Integration of the threats and opportunities posed by climate change often relies on predictions from species distribution models, particularly for assessments of vulnerability or invasion risk for multiple taxa. We evaluated whether species distribution models could reliably rank changes in species range size under climate and land use change.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Global Ecology and Biogeography},
	author = {Sofaer, Helen R. and Jarnevich, Catherine S. and Flather, Curtis H.},
	month = jun,
	year = {2018},
	pages = {658--666},
	file = {sofaer et al 2018 - Misleading prioritizations from modelling range shifts underclimate change - BDPG - GUPPY - SDM - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/7IYPF63P/sofaer et al 2018 - Misleading prioritizations from modelling range shifts underclimate change - BDPG - GUPPY - SDM - UNCERTAINTY.pdf:application/pdf},
}

@article{svancaraPolicydrivenEvidencebasedConservation2005,
	title = {Policy-driven versus {Evidence}-based {Conservation}: {A} {Review} of {Political} {Targets} and {Biological} {Needs}},
	volume = {55},
	issn = {0006-3568},
	shorttitle = {Policy-driven versus {Evidence}-based {Conservation}},
	url = {https://academic.oup.com/bioscience/article/55/11/989-995/220923},
	doi = {10.1641/0006-3568(2005)055[0989:PVECAR]2.0.CO;2},
	language = {en},
	number = {11},
	urldate = {2020-11-04},
	journal = {BioScience},
	author = {Svancara, Leona K. and Brannon J., Ree and Scott, Michael and Groves, Craig R. and Noss, Reed F. and Pressey, Robert L.},
	year = {2005},
	pages = {989},
	file = {svancara et al 2005 - policy-driven versus evidence-based conservation - a review of political targets and biological needs - GUPPY - TARGETS - MARXAN - BDPG.pdf:/Users/bill/D/Zotero/storage/W544VFSS/svancara et al 2005 - policy-driven versus evidence-based conservation - a review of political targets and biological needs - GUPPY - TARGETS - MARXAN - BDPG.pdf:application/pdf},
}

@article{tullochIncorporatingUncertaintyAssociated2013,
	title = {Incorporating uncertainty associated with habitat data in marine reserve design},
	volume = {162},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320713000700},
	doi = {10.1016/j.biocon.2013.03.003},
	abstract = {One of the most pervasive forms of uncertainty in data used to make conservation decisions is error associated with mapping of conservation features. Whilst conservation planners should consider uncertainty associated with ecological data to make informed decisions, mapping error is rarely, if ever, accommodated in the planning process. Here, we develop a spatial conservation prioritization approach that accounts for the uncertainty inherent in coral reef habitat maps and apply it in the Kubulau District ﬁsheries management area, Fiji. We use accuracy information describing the probability of occurrence of each habitat type, derived from remote sensing data validated by ﬁeld surveys, to design a marine reserve network that has a high probability of protecting a ﬁxed percentage (10–90\%) of every habitat type. We compare the outcomes of our approach to those of standard reserve design approaches, where habitatmapping errors are not known or ignored. We show that the locations of priority areas change between the standard and probabilistic approaches, with errors of omission and commission likely to occur if reserve design does not accommodate mapping accuracy. Although consideration of habitat mapping accuracy leads to bigger reserve networks, they are unlikely to miss habitat conservation targets. We explore the trade-off between conservation feature representation and reserve network area, with smaller reserve networks possible if we give up on trying to meet targets for habitats mapped with a low accuracy. The approach can be used with any habitat type at any scale to inform more robust and defensible conservation decisions in marine or terrestrial environments.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Tulloch, Vivitskaia J. and Possingham, Hugh P. and Jupiter, Stacy D. and Roelfsema, Chris and Tulloch, Ayesha I.T. and Klein, Carissa J.},
	month = jun,
	year = {2013},
	pages = {41--51},
	file = {tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - ANNO.pdf:/Users/bill/D/Zotero/storage/35K7BKXA/tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - ANNO.pdf:application/pdf},
}

@article{tullochIncorporatingUncertaintyAssociated2013a,
	title = {Incorporating uncertainty associated with habitat data in marine reserve design},
	volume = {162},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320713000700},
	doi = {10.1016/j.biocon.2013.03.003},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Tulloch, Vivitskaia J. and Possingham, Hugh P. and Jupiter, Stacy D. and Roelfsema, Chris and Tulloch, Ayesha I.T. and Klein, Carissa J.},
	month = jun,
	year = {2013},
	pages = {41--51},
	file = {tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - APPENDIX.pdf:/Users/bill/D/Zotero/storage/BP4VWNU3/tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - APPENDIX.pdf:application/pdf},
}

@article{tullochEffectRiskAversion2015,
	title = {Effect of risk aversion on prioritizing conservation projects: {Risk}-{Averse} {Species} {Prioritization}},
	volume = {29},
	issn = {08888892},
	shorttitle = {Effect of risk aversion on prioritizing conservation projects},
	url = {http://doi.wiley.com/10.1111/cobi.12386},
	doi = {10.1111/cobi.12386},
	abstract = {Conservation outcomes are uncertain. Agencies making decisions about what threat mitigation actions to take to save which species frequently face the dilemma of whether to invest in actions with high probability of success and guaranteed benefits or to choose projects with a greater risk of failure that might provide higher benefits if they succeed. The answer to this dilemma lies in the decision maker’s aversion to risk—their unwillingness to accept uncertain outcomes. Little guidance exists on how risk preferences affect conservation investment priorities. Using a prioritization approach based on cost effectiveness, we compared 2 approaches: a conservative probability threshold approach that excludes investment in projects with a risk of management failure greater than a fixed level, and a variance-discounting heuristic used in economics that explicitly accounts for risk tolerance and the probabilities of management success and failure. We applied both approaches to prioritizing projects for 700 of New Zealand’s threatened species across 8303 management actions. Both decision makers’ risk tolerance and our choice of approach to dealing with risk preferences drove the prioritization solution (i.e., the species selected for management). Use of a probability threshold minimized uncertainty, but more expensive projects were selected than with variance discounting, which maximized expected benefits by selecting the management of species with higher extinction risk and higher conservation value. Explicitly incorporating risk preferences within the decision making process reduced the number of species expected to be safe from extinction because lower risk tolerance resulted in more species being excluded from management, but the approach allowed decision makers to choose a level of acceptable risk that fit with their ability to accommodate failure. We argue for transparency in risk tolerance and recommend that decision makers accept risk in an adaptive management framework to maximize benefits and avoid potential extinctions due to inefficient allocation of limited resources.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Tulloch, Ayesha I.T. and Maloney, Richard F. and Joseph, Liana N. and Bennett, Joseph R. and Di Fonzo, Martina M.I. and Probert, William J.M. and O'Connor, Shaun M. and Densem, Jodie P. and Possingham, Hugh P.},
	month = apr,
	year = {2015},
	pages = {513--524},
	file = {tulloch et al 2014 - Effect of risk aversion on prioritizing conservation projects - BDPG - RESERVE SELECTION - UNCERTAINTY - RISK.pdf:/Users/bill/D/Zotero/storage/8RA6JRSL/tulloch et al 2014 - Effect of risk aversion on prioritizing conservation projects - BDPG - RESERVE SELECTION - UNCERTAINTY - RISK.pdf:application/pdf},
}

@article{tullochConservationPlannersTend2016,
	title = {Conservation planners tend to ignore improved accuracy of modelled species distributions to focus on multiple threats and ecological processes},
	volume = {199},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320716301604},
	doi = {10.1016/j.biocon.2016.04.023},
	abstract = {Limited conservation resources mean that management decisions are often made on the basis of scarce biological information. Species distribution models (SDMs) are increasingly proposed as a way to improve the representation of biodiversity features in conservation planning, but the extent to which SDMs are used in conservation planning is unclear. We reviewed the peer-reviewed and grey conservation planning literature to explore if and how SDMs are used in conservation prioritisations. We use text mining to analyse 641 peer-reviewed conservation prioritisation articles published between 2006 and 2012 and ﬁnd that only 10\% of articles speciﬁcally mention SDMs in the abstract, title, and/or keywords. We use topic modelling of all peer-reviewed articles plus a detailed review of a random sample of 40 peer-reviewed and grey literature plans to evaluate factors that might inﬂuence whether decision-makers use SDMs to inform prioritisations. Our results reveal that habitat maps, expert-elicited species distributions, or metrics representing landscape processes (e.g. connectivity surfaces) are used more often than SDMs as biodiversity surrogates in prioritisations. We ﬁnd four main reasons for using such alternatives in place of SDMs: (i) insufﬁcient species occurrence data (particularly for threatened species); (ii) lack of biologically-meaningful predictor data relevant to the spatial scale of planning; (iii) low concern about uncertainty in biodiversity data; and (iv) a focus on accounting for ecological, evolutionary, and cumulative threatening processes that requires alternative data to be collected. Our results suggest that SDMs are perceived as best-suited to dealing with traditional reserve selection objectives and accounting for uncertainties such as future climate change or mapping accuracy. The majority of planners in both the grey and peer-reviewed literature appear to trade off the beneﬁts of using SDMs for the beneﬁts of including information on multiple threats and processes. We suggest that increasing the complexity of species distribution modelling methods might have little impact on their use in conservation planning without a corresponding increase in research aiming at better incorporation of a range of ecological, evolutionary, and threatening processes.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Tulloch, Ayesha I.T. and Sutcliffe, Patricia and Naujokaitis-Lewis, Ilona and Tingley, Reid and Brotons, Lluis and Ferraz, Katia Maria P.M.B. and Possingham, Hugh and Guisan, Antoine and Rhodes, Jonathan R.},
	month = jul,
	year = {2016},
	pages = {157--171},
	file = {tulloch et al 2016 - Conservation planners tend to ignore improved accuracy of modelled species distributions to focus on multiple threats and ecological processes - GUPPY - BDPG - RESERVE SELECTION - TOPIC MODELLING - TEXT ANALYSIS - ANNO.pdf:/Users/bill/D/Zotero/storage/V2VKXFJN/tulloch et al 2016 - Conservation planners tend to ignore improved accuracy of modelled species distributions to focus on multiple threats and ecological processes - GUPPY - BDPG - RESERVE SELECTION - TOPIC M.pdf:application/pdf},
}

@article{vanderkamHeuristicAlgorithmsVs2007,
	title = {Heuristic algorithms vs. linear programs for designing efficient conservation reserve networks: {Evaluation} of solution optimality and processing time},
	volume = {137},
	issn = {00063207},
	shorttitle = {Heuristic algorithms vs. linear programs for designing efficient conservation reserve networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320707000869},
	doi = {10.1016/j.biocon.2007.02.018},
	abstract = {Systematic approaches to efﬁcient reserve network design often make use of one of two types of site selection algorithm; linear programs or heuristic algorithms. Unlike with linear programs, heuristic algorithms have been demonstrated to yield suboptimal networks in that more sites are selected in order to meet conservation goals than may be necessary or fewer features are captured than is possible. Although the degree of suboptimality is not known when using heuristics, some researchers have suggested that it is not signiﬁcant in most cases and that heuristics are preferred since they are more ﬂexible and can yield a solution more quickly. Using eight binary datasets, we demonstrate that suboptimality of numbers of sites selected and biodiversity features protected can occur to various degrees depending on the dataset, the model design, and the type of heuristic applied, and that processing time is not dramatically different between optimal and heuristic algorithms. In choosing an algorithm, the degree of suboptimality may not always be as important to planners as the perception that optimal solvers have feasibility issues, and therefore heuristic algorithms might continue to be a popular tool for conservation planning. We conclude that for many datasets, feasibility of optimal algorithms should not be a concern and that the value of heuristic results can be greatly improved by using optimal algorithms to determine the degree of suboptimality of the results.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Vanderkam, Robert P.D. and Wiersma, Yolanda F. and King, Douglas J.},
	month = jul,
	year = {2007},
	pages = {349--358},
	file = {vanderkam et al 2007 - heuristic algorithms vs linear programs for designing efficient conservation reserve networks - evaluation of solution optimality and processing time.pdf:/Users/bill/D/Zotero/storage/3IHKQX4Z/vanderkam et al 2007 - heuristic algorithms vs linear programs for designing efficient conservation reserve networks - evaluation of solution optimality and processing time.pdf:application/pdf},
}

@article{viscontiBuildingRobustConservation2015,
	title = {Building robust conservation plans: {Flexible} and {Efficient} {Conservation} {Planning}},
	volume = {29},
	issn = {08888892},
	shorttitle = {Building robust conservation plans},
	url = {http://doi.wiley.com/10.1111/cobi.12416},
	doi = {10.1111/cobi.12416},
	abstract = {Systematic conservation planning optimizes trade-offs between biodiversity conservation and human activities by accounting for socioeconomic costs while aiming to achieve prescribed conservation objectives. However, the most cost-efficient conservation plan can be very dissimilar to any other plan achieving the set of conservation objectives. This is problematic under conditions of implementation uncertainty (e.g., if all or part of the plan becomes unattainable). We determined through simulations of parallel implementation of conservation plans and habitat loss the conditions under which optimal plans have limited chances of implementation and where implementation attempts would fail to meet objectives. We then devised a new, flexible method for identifying conservation priorities and scheduling conservation actions. This method entails generating a number of alternative plans, calculating the similarity in site composition among all plans, and selecting the plan with the highest density of neighboring plans in similarity space. We compared our method with the classic method that maximizes cost efficiency with synthetic and real data sets. When implementation was uncertain—a common reality—our method provided higher likelihood of achieving conservation targets. We found that χ , a measure of the shortfall in objectives achieved by a conservation plan if the plan could not be implemented entirely, was the main factor determining the relative performance of a flexibility enhanced approach to conservation prioritization. Our findings should help planning authorities prioritize conservation efforts in the face of uncertainty about future condition and availability of sites.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Visconti, Piero and Joppa, Lucas},
	month = apr,
	year = {2015},
	pages = {503--512},
	file = {visconti joppa 2014 - Building robust conservation plans - BDPG - RESERVE SELECTION - METHOD - ERROR MODELS - ANNO.pdf:/Users/bill/D/Zotero/storage/WA3UQFHH/visconti joppa 2014 - Building robust conservation plans - BDPG - RESERVE SELECTION - METHOD - ERROR MODELS - ANNO.pdf:application/pdf},
}

@article{wangHowLargeSpatiallyexplicit2018,
	title = {How large spatially-explicit optimal reserve design models can we solve now? {An} exploration of current models’ computational efficiency},
	volume = {27},
	issn = {1314-3301, 1314-6947},
	shorttitle = {How large spatially-explicit optimal reserve design models can we solve now?},
	url = {https://natureconservation.pensoft.net/articles.php?id=21642},
	doi = {10.3897/natureconservation.27.21642},
	abstract = {Spatially-explicit optimal reserve design models select best sites from a set of candidate sites to assemble nature reserves to protect species (or habitats) and these reserves display certain spatial attributes which are desirable for species. These models are formulated with linear 0–1 programming and solved using standard optimisation software, but they were run on different platforms, resulting in discrepant or even conflicting messages with regard to their computational efficiency. A fair and accurate comparison of the convenience of these models would be important for conservation planners who use these models. In this article, we considered eight models presented in literature and tested their computational efficiency using randomly generated data sets containing up to 2000 sites. We focused on reserve contiguity and compactness which are considered crucial to species persistence. Our results showed that two of these models, namely Williams (2002) and Önal et al. (2016), stand out as the most efficient models. We also found that the relative efficiency of these models depends on the scope of analysis. Specifically, the Williams (2002) model solves more of the test problems when contiguity is the only spatial attribute and a large subset of the candidate sites needs to be selected. When compactness is considered also, the Önal et al. (2016) model generally performs better. Large scale models are found to be difficult to solve in a reasonable period of time. We discussed factors that may affect those models’ computational efficiency, including model size, share of selected sites, model structure and input data. These results provide useful insight and guidance to conservation practitioners and researchers who focus on spatial aspects and work with large-scale data sets.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Nature Conservation},
	author = {Wang, Yicheng and Önal, Hayri and Fang, Qiaoling},
	month = jun,
	year = {2018},
	pages = {17--34},
	file = {wang onal fang 2018 - how large spatially-explicit optimal reserve design models can we solve now - an exploration of current models computational efficiency - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/2GC9PS2Q/wang onal fang 2018 - how large spatially-explicit optimal reserve design models can we solve now - an exploration of current models computational efficiency - BDPG - ANNO.pdf:application/pdf},
}

@article{warmanSensitivitySystematicReserve2004,
	title = {Sensitivity of {Systematic} {Reserve} {Selection} to {Decisions} about {Scale}, {Biological} {Data}, and {Targets}: {Case} {Study} from {Southern} {British} {Columbia}},
	volume = {18},
	issn = {0888-8892, 1523-1739},
	shorttitle = {Sensitivity of {Systematic} {Reserve} {Selection} to {Decisions} about {Scale}, {Biological} {Data}, and {Targets}},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2004.00538.x},
	doi = {10.1111/j.1523-1739.2004.00538.x},
	abstract = {The identification of conservation areas based on systematic reserve-selection algorithms requires decisions related to both spatial and ecological scale. These decisions may affect the distribution and number of sites considered priorities for conservation within a region. We explored the sensitivity of systematic reserve selection by altering values of three essential variables. We used a 1:20,000–scale terrestrial ecosystem map and habitat suitability data for 29 threatened vertebrate species in the Okanagan region of British Columbia, Canada. To these data we applied a reserve-selection algorithm to select conservation sites while altering selection unit size and shape, features of biodiversity (i.e., vertebrate species), and area conservation targets for each biodiversity feature. The spatial similarity, or percentage overlap, of selected sets of conservation sites identified (1) with different selection units was ≤40\%, (2) with different biodiversity features was 59\%, and (3) with different conservation targets was ≥94\%. Because any selected set of sites is only one of many possible sets, we also compared the conservation value (irreplaceability) of all sites in the region for each variation of the data. The correlations of irreplaceability were weak for different selection units (0.23 ≤ r ≤ 0.67), strong for different biodiversity features (r = 0.84), and mixed for different conservation targets (r = 0.16; 0.16; 1.00). Because of the low congruence of selected sites and weak correlations of irreplaceability for different selection units, recommendations from studies that have been applied at only one spatial scale must be considered cautiously.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Warman, Leanna D. and Sinclair, A. R. E. and Scudder, G. G. E. and Klinkenberg, Brian and Pressey, Robert L.},
	month = jun,
	year = {2004},
	pages = {655--666},
	file = {warman ... pressey 2004 - Sensitivity of Systematic Reserve Selection to Decisions about Scale, Biological Data, and Targets - Case Study from Southern British Columbia - BDPG - RESERVE SELECTION - UNCERTAINTY - SPECIES NOT INCLUDED - ANNO copy 2.pdf:/Users/bill/D/Zotero/storage/GCGKCNTP/warman ... pressey 2004 - Sensitivity of Systematic Reserve Selection to Decisions about Scale, Biological Data, and Targets - Case Study from Southern British Columbia - BDPG - RESERVE SELECTION - UNCERTAINT.pdf:application/pdf},
}

@article{warrenEvaluatingPresenceOnly2020,
	title = {Evaluating presence‐only species distribution models with discrimination accuracy is uninformative for many applications},
	volume = {47},
	issn = {0305-0270, 1365-2699},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jbi.13705},
	doi = {10.1111/jbi.13705},
	abstract = {Aim: Species distribution models are used across evolution, ecology, conservation and epidemiology to make critical decisions and study biological phenomena, often in cases where experimental approaches are intractable. Choices regarding optimal models, methods and data are typically made based on discrimination accuracy: a model's ability to predict subsets of species occurrence data that were withheld during model construction. However, empirical applications of these models often involve making biological inferences based on continuous estimates of relative habitat suitability as a function of environmental predictor variables. We term the reliability of these biological inferences ‘functional accuracy.’ We explore the link between discrimination accuracy and functional accuracy.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Journal of Biogeography},
	author = {Warren, Dan L. and Matzke, Nicholas J. and Iglesias, Teresa L.},
	month = jan,
	year = {2020},
	pages = {167--180},
	file = {warren et al 2019 - Evaluating presence‐only species distribution models with discrimination accuracy is uninformative for many applications - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/G5ZNEMBP/warren et al 2019 - Evaluating presence‐only species distribution models with discrimination accuracy is uninformative for many applications - BDPG - GUPPY.pdf:application/pdf},
}

@article{watlingPerformanceMetricsVariance2015,
	title = {Performance metrics and variance partitioning reveal sources of uncertainty in species distribution models},
	volume = {309-310},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380015001283},
	doi = {10.1016/j.ecolmodel.2015.03.017},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Watling, James I. and Brandt, Laura A. and Bucklin, David N. and Fujisaki, Ikuko and Mazzotti, Frank J. and Romañach, Stephanie S. and Speroterra, Carolina},
	month = aug,
	year = {2015},
	pages = {48--59},
	file = {watling et al 2015 - Performance metrics and variance partitioning reveal sources of uncertainty in species distribution models - SDM - GUPPY - BDPG - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/3CA7RST8/watling et al 2015 - Performance metrics and variance partitioning reveal sources of uncertainty in species distribution models - SDM - GUPPY - BDPG - UNCERTAINTY.pdf:application/pdf},
}

@article{weiIntegratedApproachAddressing2012,
	title = {An integrated approach for addressing geographic uncertainty in spatial optimization},
	volume = {26},
	issn = {1365-8816, 1362-3087},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13658816.2011.633918},
	doi = {10.1080/13658816.2011.633918},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {International Journal of Geographical Information Science},
	author = {Wei, Ran and Murray, Alan T.},
	month = jul,
	year = {2012},
	pages = {1231--1249},
	file = {wei murray 2012 - An integrated approach for addressing geographic uncertainty in spatial optimization - BDPG - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/IJY6RK2A/wei murray 2012 - An integrated approach for addressing geographic uncertainty in spatial optimization - BDPG - UNCERTAINTY.pdf:application/pdf},
}

@article{collenConservationPrioritizationContext2015a,
	title = {Conservation prioritization in the context of uncertainty: {Conservation} prioritization in the context of uncertainty},
	volume = {18},
	issn = {13679430},
	shorttitle = {Conservation prioritization in the context of uncertainty},
	url = {http://doi.wiley.com/10.1111/acv.12222},
	doi = {10.1111/acv.12222},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Animal Conservation},
	author = {Collen, B.},
	month = aug,
	year = {2015},
	pages = {315--317},
	file = {collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf:/Users/bill/D/Zotero/storage/SEHG6CKQ/collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf:application/pdf},
}

@article{poisotStructureProbabilisticNetworks2016,
	title = {The structure of probabilistic networks},
	volume = {7},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12468},
	doi = {10.1111/2041-210X.12468},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Poisot, Timothée and Cirtwill, Alyssa R. and Cazelles, Kévin and Gravel, Dominique and Fortin, Marie‐Josée and Stouffer, Daniel B.},
	editor = {Vamosi, Jana},
	month = mar,
	year = {2016},
	pages = {303--312},
	file = {2041-210X.12468.pdf:/Users/bill/D/Zotero/storage/EH3RFBDG/2041-210X.12468.pdf:application/pdf},
}

@techreport{moraPymfinderToolMotif2018,
	type = {preprint},
	title = {pymfinder: a tool for the motif analysis of binary and quantitative complex networks},
	shorttitle = {pymfinder},
	url = {http://biorxiv.org/lookup/doi/10.1101/364703},
	abstract = {Abstract
          
            We developed
            pymfinder
            , a new software to analyze multiple aspects of the so-called network motifs—distinct
            n
            -node patterns of interaction—for any directed, undirected, unipartite or bipartite network. Unlike existing software for the study of network motifs,
            pymfinder
            allows the computation of node- and link-specific motif profiles as well as the analysis of weighted motifs. Beyond the overall characterization of networks, the tools presented in this work therefore allow for the comparison of the “roles” of either nodes or links of a network. Examples include the study of the roles of different species and/or their trophic/mutualistic interactions in ecological networks or the roles of specific proteins and/or their activation/inhibition relationships in protein-protein interaction networks. Here, we show how to apply the main tools from
            pymfinder
            using a predator-prey interaction network from a marine food web.
            pymfinder
            is open source software that can be freely and anonymously downloaded from
            https://github.com/stoufferlab/pymfinder
            , distributed under the MIT License (2018).},
	language = {en},
	urldate = {2020-11-04},
	institution = {Ecology},
	author = {Mora, Bernat Bramon and Cirtwill, Alyssa R. and Stouffer, Daniel B.},
	month = jul,
	year = {2018},
	doi = {10.1101/364703},
	file = {364703.full.pdf:/Users/bill/D/Zotero/storage/WHMWKXYY/364703.full.pdf:application/pdf},
}

@article{poisotSyntheticDatasetsCommunity2016,
	title = {Synthetic datasets and community tools for the rapid testing of ecological hypotheses},
	volume = {39},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/ecog.01941},
	doi = {10.1111/ecog.01941},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Ecography},
	author = {Poisot, Timothée and Gravel, Dominique and Leroux, Shawn and Wood, Spencer A. and Fortin, Marie-Josée and Baiser, Benjamin and Cirtwill, Alyssa R. and Araújo, Miguel B. and Stouffer, Daniel B.},
	month = apr,
	year = {2016},
	pages = {402--408},
	file = {ecog.01941.pdf:/Users/bill/D/Zotero/storage/DT855GPJ/ecog.01941.pdf:application/pdf},
}

@article{simmonsMotifsBipartiteEcological2019,
	title = {Motifs in bipartite ecological networks: uncovering indirect interactions},
	volume = {128},
	issn = {00301299},
	shorttitle = {Motifs in bipartite ecological networks},
	url = {http://doi.wiley.com/10.1111/oik.05670},
	doi = {10.1111/oik.05670},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Oikos},
	author = {Simmons, Benno I. and Cirtwill, Alyssa R. and Baker, Nick J. and Wauchope, Hannah S. and Dicks, Lynn V. and Stouffer, Daniel B. and Sutherland, William J.},
	month = jan,
	year = {2019},
	pages = {154--170},
	file = {oik.05670.pdf:/Users/bill/D/Zotero/storage/8KKSW3ED/oik.05670.pdf:application/pdf},
}

@article{simmons2019mee,
	title = {bmotif: {A} package for motif analyses of bipartite networks},
	volume = {10},
	issn = {2041-210X, 2041-210X},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13149},
	doi = {10.1111/2041-210X.13149},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Simmons, Benno I. and Sweering, Michelle J. M. and Schillinger, Maybritt and Dicks, Lynn V. and Sutherland, William J. and Di Clemente, Riccardo},
	editor = {Matthiopoulos, Jason},
	month = may,
	year = {2019},
	pages = {695--701},
	file = {simmons et al 2019 - bmotif - a package for motif analyses of bipartite networks - BDPG - BIPARTITE NETWORKS.pdf:/Users/bill/D/Zotero/storage/XGYWS24H/simmons et al 2019 - bmotif - a package for motif analyses of bipartite networks - BDPG - BIPARTITE NETWORKS.pdf:application/pdf},
}

@article{schapaughBayesianNetworksQuest2012a,
	title = {Bayesian networks and the quest for reserve adequacy},
	volume = {152},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320712001577},
	doi = {10.1016/j.biocon.2012.03.014},
	abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. We describe a method that integrates correlates of persistence for multiple species into a single currency – site quality. Site quality is, in turn, an explicit measure of performance used in optimization. We develop a Bayesian network to assess site quality, which assigns an expected value to a property based on criteria arrayed into a causal diagram. We then use stochastic dynamic programming to determine whether an organization should acquire or reject a site placed on the public market. Our framework for assessing sites and making land acquisition decisions represents a compromise between the use of generic spatial design criteria and more intensive computational tools, like spatially-explicit population models. There is certainly a loss of precision by using site quality as a surrogate for more direct measures of persistence. However, we believe this simpliﬁcation is defensible when sufﬁcient data, expertise, or other resources are lacking.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = aug,
	year = {2012},
	pages = {178--186},
	file = {1-s2.0-S0006320712001577-main.pdf:/Users/bill/D/Zotero/storage/LTLQHG3Y/1-s2.0-S0006320712001577-main.pdf:application/pdf},
}

@article{schapaughAccountingParametricUncertainty2013a,
	title = {Accounting for parametric uncertainty in {Markov} decision processes},
	volume = {254},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380013000306},
	doi = {10.1016/j.ecolmodel.2013.01.003},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = apr,
	year = {2013},
	pages = {15--21},
	file = {Accounting-for-parametric-uncertainty-in.pdf:/Users/bill/D/Zotero/storage/FVZ3MJCW/Accounting-for-parametric-uncertainty-in.pdf:application/pdf},
}

@article{burgFindingSmoothestPath2010a,
	title = {Finding the {Smoothest} {Path} to {Success}: {Model} {Complexity} and the {Consideration} of {Nonlinear} {Patterns} in {Nest}-{Survival} {Data}},
	volume = {112},
	issn = {0010-5422, 1938-5129},
	shorttitle = {Finding the {Smoothest} {Path} to {Success}},
	url = {https://academic.oup.com/condor/article/112/3/421-431/5152584},
	doi = {10.1525/cond.2010.090053},
	abstract = {Quantifying patterns of nest survival is a ﬁrst step toward understanding why birds decide when and where to breed. Most studies of nest survival have relied on generalized linear models (GLM) to explore these patterns. However, GLMs require assumptions about the models’ structure that might preclude ﬁnding nonlinear patterns in survival data. Generalized additive models (GAM) provide a ﬂexible alternative to GLMs for estimating linear and nonlinear patterns in data. Here we present a comparison of GLMs and GAMs for explaining variation in nest-survival data. We used two different model-selection criteria, the Bayes (BIC) and Akaike (AIC) information criteria, to select among simple and complex models. Our study was focused on the analysis of Redwinged Blackbird (Agelaius phoeniceus) nests in the Rainwater Basin wetlands of south-central Nebraska. Under BIC, our quadratic model of nest age had the most support, and the model predicted a concave pattern of daily nest survival. We found more model-selection uncertainty under AIC and found support for additive models with ordinal effects of both day and age. These models predicted much more temporal variation than did the linear models. Following our analysis, we discuss some of the advantages and disadvantages of GAMs. Despite the possible limitations of GAMs, our results suggest that they provide an efﬁcient and ﬂexible way to demonstrate nonlinear patterns in nest-survival data.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {The Condor},
	author = {Burg, Max Post van der and Powell, Larkin A. and Tyre, Andrew J.},
	month = aug,
	year = {2010},
	pages = {421--431},
	file = {cond.2010.090053.pdf:/Users/bill/D/Zotero/storage/D2KIJ64B/cond.2010.090053.pdf:application/pdf},
}

@article{hefleyFavorableTeamScores2011a,
	title = {Favorable {Team} {Scores} {Under} the {Team}-{Based} {Learning} {Paradigm}: {A} {Statistical} {Artifact}?},
	language = {en},
	author = {Hefley, Trevor},
	year = {2011},
	pages = {11},
	file = {Favorable team scores.pdf:/Users/bill/D/Zotero/storage/YU2NMRS5/Favorable team scores.pdf:application/pdf},
}

@article{postvanderburgRoleBudgetSufficiency2014b,
	title = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management: {Robust} {Species} {Management}},
	volume = {78},
	issn = {0022541X},
	shorttitle = {On the role of budget sufficiency, cost efficiency, and uncertainty in species management},
	url = {http://doi.wiley.com/10.1002/jwmg.638},
	doi = {10.1002/jwmg.638},
	abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used ﬁeld data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efﬁcient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufﬁcient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufﬁcient budget. Below this budget, the costefﬁcient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufﬁcient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one’s budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. Ó 2013 The Wildlife Society.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {The Journal of Wildlife Management},
	author = {Post van der Burg, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
	month = jan,
	year = {2014},
	pages = {153--163},
	file = {fulltext(1).pdf:/Users/bill/D/Zotero/storage/ADQMHC2Q/fulltext(1).pdf:application/pdf},
}

@article{powellTurningStudentsProblema,
	title = {Turning {Students} into {Problem} {Solvers}},
	language = {en},
	author = {Powell, Larkin A and Tyre, Andrew J and Conroy, Michael J and Peterson, James T and Williams, B Ken},
	pages = {4},
	file = {fulltext(3).pdf:/Users/bill/D/Zotero/storage/DGRD6E6Z/fulltext(3).pdf:application/pdf},
}

@article{baaschEvaluationThreeStatistical2010a,
	title = {An evaluation of three statistical methods used to model resource selection},
	volume = {221},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380009007194},
	doi = {10.1016/j.ecolmodel.2009.10.033},
	abstract = {The performance of statistical methods for modeling resource selection by animals is difﬁcult to evaluate with ﬁeld data because true selection patterns are unknown. Simulated data based on a known probability distribution, though, can be used to evaluate statistical methods. Models should estimate true selection patterns if they are to be useful in analyzing and interpreting ﬁeld data. We used simulation techniques to evaluate the effectiveness of three statistical methods used in modeling resource selection. We generated 25 use locations per animal and included 10, 20, 40, or 80 animals in samples of use locations. To simulate species of different mobility, we generated use locations at four levels according to a known probability distribution across DeSoto National Wildlife Refuge (DNWR) in eastern Nebraska and western Iowa, USA. We either generated 5 random locations per use location or 10,000 random locations (total) within 4 predetermined areas around use locations to determine how the deﬁnition of availability and the number of random locations affected results. We analyzed simulated data using discrete choice, logisticregression, and a maximum entropy method (Maxent). We used a simple linear regression of estimated and known probability distributions and area under receiver operating characteristic curves (AUC) to evaluate the performance of each method. Each statistical method was affected differently by number of animals and random locations used in analyses, level at which selection of resources occurred, and area considered available. Discrete-choice modeling resulted in precise and accurate estimates of the true probability distribution when the area in which use locations were generated was ≥ the area deﬁned to be available. Logistic-regression models were unbiased and precise when the area in which use locations were generated and the area deﬁned to be available were the same size; the ﬁt of these models improved with increased numbers of random locations. Maxent resulted in unbiased and precise estimates of the known probability distribution when the area in which use locations were generated was small (homerange level) and the area deﬁned to be available was large (study area). Based on AUC analyses, all models estimated the selection distribution better than random chance. Results from AUC analyses, however, often contradicted results of the linear regression method used to evaluate model performance. Discretechoice modeling was best able to estimate the known selection distribution in our study area regardless of sample size or number of random locations used in the analyses, but we recommend further studies using simulated data over different landscapes and different resource metrics to conﬁrm our results. Our study offers an approach and guidance for others interested in assessing the utility of techniques for modeling resource selection in their study area.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Baasch, David M. and Tyre, Andrew J. and Millspaugh, Joshua J. and Hygnstrom, Scott E. and Vercauteren, Kurt C.},
	month = feb,
	year = {2010},
	pages = {565--574},
	file = {fulltext(4).pdf:/Users/bill/D/Zotero/storage/2QPAHRIA/fulltext(4).pdf:application/pdf},
}

@article{moilanenPlanningRobustReserve2006b,
	title = {Planning for robust reserve networks using uncertainty analysis},
	volume = {199},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380006003127},
	doi = {10.1016/j.ecolmodel.2006.07.004},
	abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence–absence in sites, or on species-speciﬁc distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data—erroneous species presence–absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modiﬁcations to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and Ben-Haim, Yakov},
	month = nov,
	year = {2006},
	pages = {115--124},
	file = {fulltext(5).pdf:/Users/bill/D/Zotero/storage/K54VZ54G/fulltext(5).pdf:application/pdf},
}

@article{hefleyNondetectionSamplingBias2013a,
	title = {Nondetection sampling bias in marked presence‐only data},
	volume = {3},
	issn = {2045-7758, 2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ece3.887},
	doi = {10.1002/ece3.887},
	language = {en},
	number = {16},
	urldate = {2020-11-04},
	journal = {Ecology and Evolution},
	author = {Hefley, Trevor J. and Tyre, Andrew J. and Baasch, David M. and Blankenship, Erin E.},
	month = dec,
	year = {2013},
	pages = {5225--5236},
	file = {Hefley_et_al-2013-Ecology_and_Evolution.pdf:/Users/bill/D/Zotero/storage/LZ8EUGJD/Hefley_et_al-2013-Ecology_and_Evolution.pdf:application/pdf},
}

@article{hefleyCorrectionLocationErrors2014a,
	title = {Correction of location errors for presence-only species distribution models},
	volume = {5},
	issn = {2041210X},
	url = {http://doi.wiley.com/10.1111/2041-210X.12144},
	doi = {10.1111/2041-210X.12144},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Hefley, Trevor J. and Baasch, David M. and Tyre, Andrew J. and Blankenship, Erin E.},
	editor = {Warton, David},
	month = mar,
	year = {2014},
	pages = {207--214},
	file = {Hefley_et_al-2014-Methods_in_Ecology_and_Evolution.pdf:/Users/bill/D/Zotero/storage/2ETWSJXQ/Hefley_et_al-2014-Methods_in_Ecology_and_Evolution.pdf:application/pdf},
}

@article{hefleyFittingPopulationGrowth2013a,
	title = {Fitting population growth models in the presence of measurement and detection error},
	volume = {263},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380013002536},
	doi = {10.1016/j.ecolmodel.2013.05.003},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Hefley, Trevor J. and Tyre, Andrew J. and Blankenship, Erin E.},
	month = aug,
	year = {2013},
	pages = {244--250},
	file = {Hefley_etal_2013_fitting_popn_growth_models_error.pdf:/Users/bill/D/Zotero/storage/BP2FQYR9/Hefley_etal_2013_fitting_popn_growth_models_error.pdf:application/pdf},
}

@article{hoffmanUseSimulatedData2010a,
	title = {Use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence},
	volume = {33},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/j.1600-0587.2009.05495.x},
	doi = {10.1111/j.1600-0587.2009.05495.x},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Ecography},
	author = {Hoffman, Justin D. and Aguilar-Amuchastegui, Naikoa and Tyre, Andrew J.},
	month = apr,
	year = {2010},
	pages = {656--666},
	file = {hoffman aguilar-amuchastegui TYRE 2010 - use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence - SDM - SIMULATION - VIRTUAL SPECIES - GUPPY - ANNO.pdf:/Users/bill/D/Zotero/storage/NSJAFTBP/hoffman aguilar-amuchastegui TYRE 2010 - use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence - SDM - SIMULATION - VIRTUAL SPECIES - GUPPY - ANNO.pdf:application/pdf},
}

@article{tyreIdentifyingLandscapeScale2006a,
	title = {Identifying landscape scale patterns from individual scale processes},
	volume = {199},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380006002882},
	doi = {10.1016/j.ecolmodel.2005.12.001},
	abstract = {Extrapolating across scales is a critical problem in ecology. Explicit mechanistic models of ecological systems provide a bridge from measurements of processes at small and short scales to larger scales; spatial patterns at large scales can be used to test the outcomes of these models. However, it is necessary to identify patterns that are not dependent on initial conditions, because small scale initial conditions will not normally be measured at large scales. We examined one possible pattern that could meet these conditions, the relationship between mean and variance in abundance of a parasitic tick in an individual based model of a lizard tick interaction. We scaled discrepancies between the observed and simulated patterns with a transformation of the variance–covariance matrix of the observed pattern to objectively identify patterns that are “close”.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Michael Bull, C.},
	month = dec,
	year = {2006},
	pages = {442--450},
	file = {Identifying landscape scale patterns from individual scale proces.pdf:/Users/bill/D/Zotero/storage/Y5K2LKVF/Identifying landscape scale patterns from individual scale proces.pdf:application/pdf},
}

@article{tyreIdentifyingMechanisticModels2007a,
	title = {Identifying mechanistic models of spatial behaviour using pattern-based modelling: {An} example from lizard home ranges},
	volume = {208},
	issn = {03043800},
	shorttitle = {Identifying mechanistic models of spatial behaviour using pattern-based modelling},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380007003109},
	doi = {10.1016/j.ecolmodel.2007.06.004},
	abstract = {Landscape and population level patterns form through the aggregation of responses of individual organisms to heterogeneity. Spatial organization within a population can range from random overlap of individual home ranges, to completely exclusive territories, with most populations falling somewhere between these two extremes. A fundamental question in behavioral ecology concerns the factors that influence the degree of spatial overlap of home ranges, and the processes that determine how likely it is that an individual will access resources over its home range. However, traditional experimental methods are not always practical or possible. Pattern-based modeling is an alternative, non-intrusive technique for explaining observed patterns. We explored behavioral mechanisms for home range overlap in a Scincid lizard, Tiliqua rugosa, by constructing a spatially explicit individual based model. We tested two mechanisms, one that used refuge sites randomly and one that included a behavioral component. The random use model, the fixed total range model, incorporated all refuge sites within a circle of radius h. The behavioral model, the variable total range model, probabilistically incorporated refuge sites based on nearest neighbor distances and use by conspecifics. Comparisons between the simulated patterns and the observed patterns of range overlap provided evidence that the variable total range model was a better approximation of lizard space use than the fixed total range model. Pattern-based modeling showed substantial promise as a means for identifying behavioral mechanisms underlying observed patterns.},
	language = {en},
	number = {2-4},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Tyre, Andrew and Kerr, Gregory D. and Tenhumberg, Brigitte and Bull, C. Michael},
	month = nov,
	year = {2007},
	pages = {307--316},
	file = {Identifying mechanistic models of spatial behavior using pattern-.pdf:/Users/bill/D/Zotero/storage/R5BXNR5G/Identifying mechanistic models of spatial behavior using pattern-.pdf:application/pdf},
}

@article{tyreIMPROVINGPRECISIONREDUCING2003a,
	title = {{IMPROVING} {PRECISION} {AND} {REDUCING} {BIAS} {IN} {BIOLOGICAL} {SURVEYS}: {ESTIMATING} {FALSE}-{NEGATIVE} {ERROR} {RATES}},
	volume = {13},
	issn = {1051-0761},
	shorttitle = {{IMPROVING} {PRECISION} {AND} {REDUCING} {BIAS} {IN} {BIOLOGICAL} {SURVEYS}},
	url = {http://doi.wiley.com/10.1890/02-5078},
	doi = {10.1890/02-5078},
	abstract = {The use of presence/absence data in wildlife management and biological surveys is widespread. There is a growing interest in quantifying the sources of error associated with these data. We show that false-negative errors (failure to record a species when in fact it is present) can have a signiﬁcant impact on statistical estimation of habitat models using simulated data. Then we introduce an extension of logistic modeling, the zero-inﬂated binomial (ZIB) model that permits the estimation of the rate of false-negative errors and the correction of estimates of the probability of occurrence for false-negative errors by using repeated visits to the same site. Our simulations show that even relatively low rates of false negatives bias statistical estimates of habitat effects. The method with three repeated visits eliminates the bias, but estimates are relatively imprecise. Six repeated visits improve precision of estimates to levels comparable to that achieved with conventional statistics in the absence of false-negative errors. In general, when error rates are Յ50\% greater efﬁciency is gained by adding more sites, whereas when error rates are Ͼ50\% it is better to increase the number of repeated visits. We highlight the ﬂexibility of the method with three case studies, clearly demonstrating the effect of false-negative errors for a range of commonly used survey methods.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Ecological Applications},
	author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Field, Scott A. and Niejalke, Darren and Parris, Kirsten and Possingham, Hugh P.},
	month = dec,
	year = {2003},
	pages = {1790--1801},
	file = {IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS_ ESTI.pdf:/Users/bill/D/Zotero/storage/CYVYIBNS/IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS_ ESTI.pdf:application/pdf},
}

@article{toonenIfLarvaeWere2007a,
	title = {If larvae were smart: a simple model for optimal settlement behavior of competent larvae},
	volume = {349},
	issn = {0171-8630, 1616-1599},
	shorttitle = {If larvae were smart},
	url = {http://www.int-res.com/abstracts/meps/v349/p43-61/},
	doi = {10.3354/meps06963},
	abstract = {Much research has been done on larval settlement cues. Rather than having simple fixed responses to constant environmental stimuli, it seems likely that settlement decisions made by individual larvae should vary depending on the individual and the conditions under which it encounters that cue. Here, we present a simple stochastic dynamic programming model that explores the conditions under which larvae may maximize their lifetime fitness by accepting lower quality habitat rather than continuing to search for superior habitat. Our model predicts that there is a relatively narrow range of parameter values over which larval selectivity among habitat types changes dramatically from 1 (larvae accept only optimal substrata) to 0 (indiscriminant settlement). This narrow range coincides with our best estimate of parameter values gleaned from empirical studies, and the model output matches data for the polychaete worm Hydroides dianthus remarkably well. The relative availability of habitats and the total time available to search for high quality habitat (i.e. the ability to delay metamorphosis) had the greatest effects on larval selectivity. In contrast, intuitive factors, including larval energetics and mortality, showed little effect on larval habitat preference, but could still alter the proportion of larvae settling in different habitats by reducing search time. Our model predicts that a given larva may behave differently depending on where it falls in the optimality decision matrix at the instant in which it locates substrata. This model provides a conceptual framework in which to conduct future studies involving variability in settlement decisions among individual larvae, and in which to consider the selective forces driving the evolution of specific larval settlement cues. Our results suggest that a combination of the maximum search period and the relative frequency and quality of optimal habitat likely exert the greatest influence on the evolution of larval selectivity in the field.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Marine Ecology Progress Series},
	author = {Toonen, Rj and Tyre, Aj},
	month = nov,
	year = {2007},
	pages = {43--61},
	file = {m349p043.pdf:/Users/bill/D/Zotero/storage/CGQQ2YUT/m349p043.pdf:application/pdf},
}

@article{michaelsHowIndeterminismShapes2012a,
	title = {How indeterminism shapes ecologists’ contributions to managing socio-ecological systems: {Indeterminism} in {SES}},
	volume = {5},
	issn = {1755263X},
	shorttitle = {How indeterminism shapes ecologists’ contributions to managing socio-ecological systems},
	url = {http://doi.wiley.com/10.1111/j.1755-263X.2012.00241.x},
	doi = {10.1111/j.1755-263X.2012.00241.x},
	abstract = {To make a difference in policy making about socio-ecological systems, ecologists must grasp when decision makers are amenable to acting on ecological expertise and when they are not. To enable them to do so we present a matrix for classifying a socio-ecological system by the extent of what we don’t know about its natural components and the social interactions that affects them. We use four examples, Midcontinent Mallards, Laysan Ducks, Pallid Sturgeon, and Rocky Mountain Grey Wolves to illustrate how the combination of natural and social source of indeterminism matters. Where social indeterminism is high, ecologists can expand the range of possible science-based options decision makers might consider even while recognizing societal-based concerns rather than science will dominate decision making. In contrast, where natural indeterminism is low, ecologists can offer reasonably accurate predictions that may well serve as inputs into decision making. Depending on the combination of natural and social indeterminism characterizing a particular circumstance, ecologists have different roles to play in informing socio-ecological system management.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Conservation Letters},
	author = {Michaels, Sarah and Tyre, Andrew J.},
	month = aug,
	year = {2012},
	pages = {289--295},
	file = {Michaels_et_al-2012-Conservation_Letters.pdf:/Users/bill/D/Zotero/storage/993PGBL2/Michaels_et_al-2012-Conservation_Letters.pdf:application/pdf},
}

@article{fieldMinimizingCostEnvironmental2004a,
	title = {Minimizing the cost of environmental management decisions by optimizing statistical thresholds},
	volume = {7},
	issn = {1461-023X, 1461-0248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2004.00625.x},
	doi = {10.1111/j.1461-0248.2004.00625.x},
	abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate (a) of 0.05. We derive optimal a-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on a ¼ 0.05 carries an expected cost over \$5 million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the speciesÕ value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional a-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical Ôburden of proofÕ in environmental decision-making when the cost of Type II errors is relatively high.},
	language = {en},
	number = {8},
	urldate = {2020-11-04},
	journal = {Ecology Letters},
	author = {Field, Scott A. and Tyre, Andrew J. and Jonzen, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
	month = aug,
	year = {2004},
	pages = {669--675},
	file = {Minimizing_the_c.pdf:/Users/bill/D/Zotero/storage/XDKYNIAX/Minimizing_the_c.pdf:application/pdf},
}

@article{quinnApplicationDetectabilityUse2011a,
	title = {Application of detectability in the use of indicator species: {A} case study with birds},
	volume = {11},
	issn = {1470160X},
	shorttitle = {Application of detectability in the use of indicator species},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1470160X11000562},
	doi = {10.1016/j.ecolind.2011.03.003},
	abstract = {The use of indicator species is popular in ecological monitoring and management. In recent years, new methods to improve the quality and application of indicator data have been proposed and developed. Here we propose the use of detection probability in the selection and application of indicator species. We evaluated environmental and observer factors believed to affect detection of potential species. Observer effects were the most evident factor and may necessitate the greatest consideration in the use of indicator species. Our results call attention to the fact that raw counts are far from accurate and that the use of detection probability can and should be incorporated into sampling protocols, species selection, and the allocation of effort for projects that use indicator species as part of monitoring and management programs.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Ecological Indicators},
	author = {Quinn, John E. and Brandle, James R. and Johnson, Ron J. and Tyre, Andrew J.},
	month = sep,
	year = {2011},
	pages = {1413--1418},
	file = {Quinn et al 2011 Application of detectability.pdf:/Users/bill/D/Zotero/storage/4QEJIX4U/Quinn et al 2011 Application of detectability.pdf:application/pdf},
}

@article{schapaughMaximizingNewQuantity2014a,
	title = {Maximizing a new quantity in sequential reserve selection},
	volume = {41},
	issn = {0376-8929, 1469-4387},
	url = {https://www.cambridge.org/core/product/identifier/S0376892913000544/type/journal_article},
	doi = {10.1017/S0376892913000544},
	abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. Numerous empirical studies support the notion that deﬁning and measuring objectives in terms of species richness (where the value of a site is equal to the number of species it contains, or contributes to an existing reserve network) can be inadequate for maintaining biodiversity in the longterm. An existing site-assessment framework that implicitly maximized the persistence probability of multiple species was integrated with a dynamic optimization model. The problem of sequential reserve selection as a Markov decision process was combined with stochastic dynamic programming to ﬁnd the optimal solution. The approach represents a compromise between representation-based approaches (maximizing occurrences) and more complex tools, like spatially-explicit population models. The method, the inherent problems and interesting conclusions are illustrated with a land acquisition case study on the central Platte River.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Environmental Conservation},
	author = {Schapaugh, Adam W. and Tyre, Andrew J.},
	month = jun,
	year = {2014},
	pages = {198--205},
	file = {schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf:/Users/bill/D/Zotero/storage/K32NK5J7/schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf:application/pdf},
}

@article{tyreINFERRINGPROCESSPATTERN2001a,
	title = {{INFERRING} {PROCESS} {FROM} {PATTERN}: {CAN} {TERRITORY} {OCCUPANCY} {PROVIDE} {INFORMATION} {ABOUT} {LIFE} {HISTORY} {PARAMETERS}?},
	volume = {11},
	abstract = {A signiﬁcant problem in wildlife management is identifying ‘‘good’’ habitat for species within the short time frames demanded by policy makers. Statistical models of the response of species presence/absence to predictor variables are one solution, widely known as habitat modeling. We use a ‘‘virtual ecologist’’ to test logistic regression as a means of developing habitat models within a spatially explicit, individual-based simulation that allows habitat quality to inﬂuence either fecundity or survival with a continuous scale. The basic question is how good are logistic regression models of habitat quality at identifying habitat where birth rates are high and death rates low (i.e., ‘‘source’’ habitat)? We ﬁnd that, even when all the important variables are perfectly measured, and there is no error in surveying the species of interest, demographic stochasticity and the limiting effect of localized dispersal generally prevent an explanation of much more than half of the variation in territory occupancy as a function of habitat quality. This is true regardless of whether fecundity or survival is inﬂuenced by habitat quality. In addition, habitat models only detect a signiﬁcant effect of habitat on territory occupancy when habitat quality is spatially autocorrelated. We ﬁnd that habitat models based on logistic regression really measure the ability of the species to reach and colonize areas, not birth or death rates.},
	language = {en},
	number = {6},
	journal = {Ecological Applications},
	author = {Tyre, Andrew J and Possingham, Hugh P and Lindenmayer, David B},
	year = {2001},
	pages = {16},
	file = {tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf:/Users/bill/D/Zotero/storage/IATG5RNT/tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf:application/pdf},
}

@article{salomonPopulationViabilityEcological2006,
	title = {Population viability, ecological processes and biodiversity: {Valuing} sites for reserve selection},
	volume = {128},
	issn = {00063207},
	shorttitle = {Population viability, ecological processes and biodiversity},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320705003794},
	doi = {10.1016/j.biocon.2005.09.018},
	abstract = {No-take reserves constitute one tool to improve conservation of marine ecosystems, yet criteria for their placement, size, and arrangement remain uncertain. Representation of biodiversity is necessary in reserve planning, but will ultimately fail for conservation unless factors affecting species’ persistence are also incorporated. This study presents an empirical example of the divergent relationships among multiple metrics used to quantify a site’s conservation value, including those that address representation (habitat type, species richness, species diversity), and others that address ecological processes and viability (density and reproductive capacity of a keystone species, in this case, the black chiton, Katharina tunicata). We characterized 10 rocky intertidal sites across two habitats in Barkley Sound, British Columbia, Canada, according to these site metrics. High-richness and high-production sites for K. tunicata were present in both habitat types, but high richness and high-production sites did not overlap. Across sites, species richness ranged from 29 to 46, and adult K. tunicata varied from 6 to 22 individuals mÀ2. Adult density was negatively correlated with species richness, a pattern that likely occurs due to post-recruitment growth and survival because no correlation was evident with non-reproductive juveniles. Sites with high adult density also contributed disproportionately greater potential reproductive output (PRO), deﬁned by total gonad mass. PRO varied by a factor of ﬁve across sites and was also negatively correlated with species richness. Compromise or relative weighting would be necessary to select valuable sites for conservation because of inherent contradictions among some reserve selection criteria. We suspect that this inconsistency among site metrics will occur more generally in other ecosystems and emphasize the importance of population viability of strongly interacting species.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Salomon, Anne K. and Ruesink, Jennifer L. and DeWreede, Robert E.},
	month = feb,
	year = {2006},
	pages = {79--92},
	file = {1 - Salomon, Ruesink, Dewreede - 2006 - Population viability, ecological processes and biodiversity Valuing sites for reserve selection - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/WXND6WXG/1 - Salomon, Ruesink, Dewreede - 2006 - Population viability, ecological processes and biodiversity Valuing sites for reserve selection - Biological Conservation.pdf:application/pdf},
}

@article{ferraroMoneyNothingCall2006,
	title = {Money for {Nothing}? {A} {Call} for {Empirical} {Evaluation} of {Biodiversity} {Conservation} {Investments}},
	volume = {4},
	issn = {1545-7885},
	shorttitle = {Money for {Nothing}?},
	url = {https://dx.plos.org/10.1371/journal.pbio.0040105},
	doi = {10.1371/journal.pbio.0040105},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {PLoS Biology},
	author = {Ferraro, Paul J and Pattanayak, Subhrendu K},
	editor = {Mace, Georgina},
	month = apr,
	year = {2006},
	pages = {e105},
	file = {2 - ferraro et al 2006 - money for nothing - a call for empirical evaluation of biodiversity conservation investments.PDF:/Users/bill/D/Zotero/storage/UCTXA5RB/2 - ferraro et al 2006 - money for nothing - a call for empirical evaluation of biodiversity conservation investments.PDF:application/pdf},
}

@article{andamMeasuringEffectivenessProtected2008,
	title = {Measuring the effectiveness of protected area networks in reducing deforestation},
	volume = {105},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0800437105},
	doi = {10.1073/pnas.0800437105},
	language = {en},
	number = {42},
	urldate = {2020-11-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Andam, K. S. and Ferraro, P. J. and Pfaff, A. and Sanchez-Azofeifa, G. A. and Robalino, J. A.},
	month = oct,
	year = {2008},
	pages = {16089--16094},
	file = {3 - andam et al 2008 - measuring the effectiveness of protected area networks in reducing deforestation .pdf:/Users/bill/D/Zotero/storage/BMSAU7I3/3 - andam et al 2008 - measuring the effectiveness of protected area networks in reducing deforestation .pdf:application/pdf},
}

@article{gaveauEvaluatingWhetherProtected2009,
	title = {Evaluating whether protected areas reduce tropical deforestation in {Sumatra}},
	volume = {36},
	issn = {03050270, 13652699},
	url = {http://doi.wiley.com/10.1111/j.1365-2699.2009.02147.x},
	doi = {10.1111/j.1365-2699.2009.02147.x},
	abstract = {Aim This study determines whether the establishment of tropical protected areas (PAs) has led to a reduction in deforestation within their boundaries or whether deforestation has been displaced to adjacent unprotected areas: a process termed neighbourhood leakage.},
	language = {en},
	number = {11},
	urldate = {2020-11-04},
	journal = {Journal of Biogeography},
	author = {Gaveau, David L. A. and Epting, Justin and Lyne, Owen and Linkie, Matthew and Kumara, Indra and Kanninen, Markku and Leader-Williams, Nigel},
	month = nov,
	year = {2009},
	pages = {2165--2175},
	file = {4 - gaveau et al 2009 - Evaluating whether protected areas reduce tropical deforestation in sumatra.pdf:/Users/bill/D/Zotero/storage/Y3W9ZXXC/4 - gaveau et al 2009 - Evaluating whether protected areas reduce tropical deforestation in sumatra.pdf:application/pdf},
}

@article{allisonMARINERESERVESARE1998,
	title = {{MARINE} {RESERVES} {ARE} {NECESSARY} {BUT} {NOT} {SUFFICIENT} {FOR} {MARINE} {CONSERVATION}},
	volume = {8},
	issn = {1051-0761},
	url = {http://doi.wiley.com/10.1890/1051-0761(1998)8[S79:MRANBN]2.0.CO;2},
	doi = {10.1890/1051-0761(1998)8[S79:MRANBN]2.0.CO;2},
	abstract = {The intensity of human pressure on marine systems has led to a push for stronger marine conservation efforts. Recently, marine reserves have become one highly advocated form of marine conservation, and the number of newly designated reserves has increased dramatically. Reserves will be essential for conservation efforts because they can provide unique protection for critical areas, they can provide a spatial escape for intensely exploited species, and they can potentially act as buffers against some management miscalculations and unforeseen or unusual conditions. Reserve design and effectiveness can be dramatically improved by better use of existing scientiﬁc understanding. Reserves are insufﬁcient protection alone, however, because they are not isolated from all critical impacts. Communities residing within marine reserves are strongly inﬂuenced by the highly variable conditions of the water masses that continuously ﬂow through them. To a much greater degree than in terrestrial systems, the scales of fundamental processes, such as population replenishment, are often much larger than reserves can encompass. Further, they offer no protection from some important threats, such as contamination by chemicals. Therefore, without adequate protection of species and ecosystems outside reserves, effectiveness of reserves will be severely compromised. We outline conditions under which reserves are likely to be effective, provide some guidelines for increasing their conservation potential, and suggest some research priorities to ﬁll critical information gaps. We strongly support vastly increasing the number and size of marine reserves; at the same time, strong conservation efforts outside reserves must complement this effort. To date, most reserve design and site selection have involved little scientiﬁc justiﬁcation. They must begin to do so to increase the likelihood of attaining conservation objectives.},
	language = {en},
	number = {sp1},
	urldate = {2020-11-04},
	journal = {Ecological Applications},
	author = {Allison, Gary W. and Lubchenco, Jane and Carr, Mark H.},
	month = feb,
	year = {1998},
	pages = {S79--S92},
	file = {5 - allison lubchenko carr 1998 - marine reserves are necessary but not sufficient for marine conservation.pdf:/Users/bill/D/Zotero/storage/VTGGBGDI/5 - allison lubchenko carr 1998 - marine reserves are necessary but not sufficient for marine conservation.pdf:application/pdf},
}

@article{botsfordPRINCIPLESDESIGNMARINE2003,
	title = {{PRINCIPLES} {FOR} {THE} {DESIGN} {OF} {MARINE} {RESERVES}},
	volume = {13},
	issn = {1051-0761},
	url = {http://doi.wiley.com/10.1890/1051-0761(2003)013[0025:PFTDOM]2.0.CO;2},
	doi = {10.1890/1051-0761(2003)013[0025:PFTDOM]2.0.CO;2},
	abstract = {The theory underlying the design of marine reserves, whether the goal is to preserve biodiversity or to manage ﬁsheries, is still in its infancy. For both of these goals, there is a need for general principles on which to base marine reserve design, and because of the paucity of empirical experience, these principles must be based on models. However, most of the theoretical studies to date have been speciﬁc to a single situation, with few attempts to deduce general principles. Here we attempt to distill existing results into general principles useful to designers of marine reserves. To answer the question of how ﬁshery management using reserves compares to conventional management, we provide two principles: (1) the effect of reserves on yield per recruit is similar to increasing the age of ﬁrst capture, and (2) the effect of reserves on yield is similar to reducing effort. Another two principles answer the question of how to design reserve conﬁgurations so that species with movement in various stages will be sustainable: (3) higher juvenile and adult movement lowers sustainability of reserves for biodiversity, but an intermediate level of adult movement is required for reserves for ﬁshery management, and (4) longer larval dispersal distance requires larger reserves for sustainability. These principles provide general guidelines for design, and attention to them will allow more rapid progress in future modeling studies. Whether populations or communities will persist under any speciﬁc reserve design is uncertain, and we suggest ways of dealing with that uncertainty.},
	language = {en},
	number = {sp1},
	urldate = {2020-11-04},
	journal = {Ecological Applications},
	author = {Botsford, Louis W. and Micheli, Fiorenza and Hastings, Alan},
	month = feb,
	year = {2003},
	pages = {25--31},
	file = {6 - botsford et al 2003 - principles for the design of marine reserves.pdf:/Users/bill/D/Zotero/storage/Y9FA7FT7/6 - botsford et al 2003 - principles for the design of marine reserves.pdf:application/pdf},
}

@article{fleishmanEMPIRICALVALIDATIONMETHOD2001,
	title = {{EMPIRICAL} {VALIDATION} {OF} {A} {METHOD} {FOR} {UMBRELLA} {SPECIES} {SELECTION}},
	volume = {11},
	abstract = {Empirical validation that putative umbrella species protect many co-occurring species is rare. Using 10 sets of data, representing two taxonomic groups and three ecoregions, we tested the effectiveness of a recently developed index for selection of umbrella species. We also tested whether species identiﬁed with the index were more effective umbrellas than species selected at random, evaluated whether sample size and intensity affect selection of umbrella species, and examined whether the index could identify cross-taxonomic umbrellas in a single ecoregion. Conserving all locations with at least one umbrella species would protect the vast majority of each assemblage. A more realistic scenario, conservation of subsets of locations with relatively high numbers of umbrella species, generally would protect Ն0.75 of each assemblage. Randomly selected sets of species often required that more locations be designated for protection than did sets selected using the umbrella index. The umbrella index tended to identify fewer locations that offered an equivalent level of species protection. Sampling intensity affected which species were identiﬁed as umbrellas, but not the proportion of species that would be protected. Umbrella species were no more effective than randomly selected species for cross-taxonomic applications; nonetheless, neither group was signiﬁcantly less effective than same-taxon umbrellas. Particularly when selection of protected areas is not highly constrained, it may indeed be feasible to identify effective umbrella species. Unqualiﬁed utility of the umbrella index or umbrella species concept, however, is not supported by this study.},
	language = {en},
	number = {5},
	journal = {Ecological Applications},
	author = {Fleishman, Erica and Blair, Robert B and Murphy, Dennis D},
	year = {2001},
	pages = {13},
	file = {7 - fleishman et al 2001 - EMPIRICAL VALIDATION OF A METHOD FOR UMBRELLA SPECIES SELECTION.pdf:/Users/bill/D/Zotero/storage/V4SATZ8X/7 - fleishman et al 2001 - EMPIRICAL VALIDATION OF A METHOD FOR UMBRELLA SPECIES SELECTION.pdf:application/pdf},
}

@article{pullinConservationManagersUse2004,
	title = {Do conservation managers use scientific evidence to support their decision-making?},
	volume = {119},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632070300449X},
	doi = {10.1016/j.biocon.2003.11.007},
	abstract = {Conservation involves making decisions on appropriate action from a wide range of options. For conservation to be eﬀective, decision-makers need to know what actions do and do not work. Ideally, decisions should be based on eﬀectiveness as demonstrated by scientiﬁc experiment or systematic review of evidence. Can decision-makers get this kind of information? We undertook a formal assessment of the extent to which scientiﬁc evidence is being used in conservation practice by conducting a survey of management plans and their compilers from major conservation organizations within the UK. Data collected suggest that the majority of conservation actions remain experience-based and rely heavily on traditional land management practices because, many management interventions remain unevaluated and, although some evidence exists, much is not readily accessible in a suitable form. We argue that nature conservation along with other ﬁelds of applied ecology, should exploit the concept of evidence-based practice developed and used in medicine and public health that aims to provide the best available evidence to the decision-maker(s) on the likely outcomes of alternative actions. Through critical evaluation, we present the challenges and beneﬁts of adopting evidence based practice from the decision-makers point of view and identify the process to be followed to make it work.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Pullin, Andrew S and Knight, Teri M and Stone, David A and Charman, Kevin},
	month = sep,
	year = {2004},
	pages = {245--252},
	file = {8 - pullin et al 2004 - Do conservation managers use scientific evidence to support their decision-making.pdf:/Users/bill/D/Zotero/storage/7LDMFU93/8 - pullin et al 2004 - Do conservation managers use scientific evidence to support their decision-making.pdf:application/pdf},
}

@article{banApplyingEmpiricalEstimates2014,
	title = {Applying empirical estimates of marine protected area effectiveness to assess conservation plans in {British} {Columbia}, {Canada}},
	volume = {180},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320714003541},
	doi = {10.1016/j.biocon.2014.09.037},
	abstract = {While efforts to meet international commitments to counter biodiversity declines by establishing networks of marine protected areas (MPAs) continue, assessments of MPAs rarely take into account measures of effectiveness of different categories of protection, or other design principles (size, spacing, governance considerations). We carried out a meta-analysis of ecological effectiveness of IUCN Categories I–II (no-take), IV and VI (MPAs) compared to unprotected areas. We then applied our ecological effectiveness estimates – the added beneﬁt of marine protection over and above conventional ﬁsheries management – to a gap analysis of existing MPAs, and MPAs proposed by four indigenous groups on the Central Coast of British Columbia, Canada. Additionally, we assessed representation, size, spacing, and governance considerations against MPA design criteria outlined in the literature. We found signiﬁcant differences in response ratios for IUCN Categories IV and VI MPAs compared to no-take reserves and areas open to ﬁshing, although variability in responses was high. By rescaling the predicted ecological effectiveness ratios (including conﬁdence estimates), we found that, compared to no-take reserves (biodiversity conservation effectiveness 100\%) and open ﬁshing areas (0\% additional biodiversity contribution over and above conventional ﬁsheries management), IUCN Category IV had a predicted effectiveness score of 60\%, ranging between 34\% and 89\% (95\% lower and upper conﬁdence intervals, respectively), and IUCN Category VI had a predicted effectiveness score of 24\% (ranging between À12\% and 72\% for the 95\% lower and upper conﬁdence intervals, respectively). We found that the existing MPAs did poorly when compared against most MPA design criteria, whereas the proposed MPA network achieved many of the best practices identiﬁed in the literature, and could achieve all if some additional sites were added. By using the Central Coast of British Columbia as a case study, we demonstrated a method for applying empirically-based ecological effectiveness estimates to an assessment of MPA design principles for an existing and proposed network of MPAs.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Ban, Natalie C. and McDougall, Chris and Beck, Martina and Salomon, Anne K. and Cripps, Ken},
	month = dec,
	year = {2014},
	pages = {134--148},
	file = {9 - ban et al 2014 - Applying empirical estimates of marine protected area effectiveness to assess conservation plans in British Columbia, Canada.pdf:/Users/bill/D/Zotero/storage/BP6Y4W7M/9 - ban et al 2014 - Applying empirical estimates of marine protected area effectiveness to assess conservation plans in British Columbia, Canada.pdf:application/pdf},
}

@article{ferraroCounterfactualThinkingImpact2009,
	title = {Counterfactual thinking and impact evaluation in environmental policy},
	volume = {2009},
	issn = {10976736, 1534875X},
	url = {http://doi.wiley.com/10.1002/ev.297},
	doi = {10.1002/ev.297},
	abstract = {Impact evaluations assess the degree to which changes in outcomes can be attributed to an intervention rather than to other factors. Such attribution requires knowing what outcomes would have looked like in the absence of the intervention. This counterfactual world can be inferred only indirectly through evaluation designs that control for confounding factors. Some have argued that environmental policy is different from other social policy fields, and thus attempting to establish causality through identification of counterfactual outcomes is quixotic. This chapter argues that elucidating causal relationships through counterfactual thinking and experimental or quasi-experimental designs is absolutely critical in environmental policy, and that many opportunities for doing so exist. Without more widespread application of such approaches, little progress will be made on building the evidence base in environmental policy. © Wiley Periodicals, Inc.},
	language = {en},
	number = {122},
	urldate = {2020-11-04},
	journal = {New Directions for Evaluation},
	author = {Ferraro, Paul J.},
	month = mar,
	year = {2009},
	pages = {75--84},
	file = {10 - ferraro 2009 - Counterfactual Thinking and Impact Evaluation in Environmental Policy.pdf:/Users/bill/D/Zotero/storage/6BIG2AJC/10 - ferraro 2009 - Counterfactual Thinking and Impact Evaluation in Environmental Policy.pdf:application/pdf},
}

@article{hansenHindsightMarineProtected2011,
	title = {Hindsight in marine protected area selection: {A} comparison of ecological representation arising from opportunistic and systematic approaches},
	volume = {144},
	issn = {00063207},
	shorttitle = {Hindsight in marine protected area selection},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320711001236},
	doi = {10.1016/j.biocon.2011.04.002},
	abstract = {Systematic approaches to site selection for marine protected areas (MPAs) are often favored over opportunistic approaches as a means to meet conservation objectives efﬁciently. In this study, we compared analytically the conservation value of these two approaches. We locate this study in Danajon Bank, central Philippines, where many MPAs were established opportunistically based on community preference, with few if any contributions from biophysical data. We began by identifying the biophysical data that would have been available when the ﬁrst MPA was created in Danajon Bank (1995). We next used these data with the reserve selection software Marxan to identify MPAs that covered the same area as is protected under the current set of MPAs (0.32\% of the total study area) and that would protect the greatest number of conservation targets at the lowest cost. We ﬁnally compared the conservation value of the current MPAs to the value of those selected by Marxan. Because of the dearth of biophysical data available in 1995 and the small area currently under protection, Marxan identiﬁed multiple conﬁgurations of MPAs that would protect the same percentage of conservation targets, with little differentiation among sites. Further, we discovered that the costs of obtaining and analyzing these data to be used for conservation planning would have been large relative to resources typically available to conservation planners in developing countries. Finally, we found that the current set of MPAs protected more ecological features than would be expected by chance, although not as many as could be protected using a systematic approach. Our results suggest that an opportunistic approach can be a valuable component of conservation planning, especially when biophysical data are sparse and community acceptance is a critical factor affecting the success of an MPA.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Hansen, Gretchen J.A. and Ban, Natalie C. and Jones, Michael L. and Kaufman, Les and Panes, Hazel M. and Yasué, Maï and Vincent, Amanda C.J.},
	month = jun,
	year = {2011},
	pages = {1866--1875},
	file = {11 - hanson et al 2011 - Hindsight in marine protected area selection - A comparison of ecological representation arising from opportunistic and systematic approaches.pdf:/Users/bill/D/Zotero/storage/CZIQ4937/11 - hanson et al 2011 - Hindsight in marine protected area selection - A comparison of ecological representation arising from opportunistic and systematic approaches.pdf:application/pdf},
}

@article{pullinDoingMoreGood2009,
	title = {Doing more good than harm – {Building} an evidence-base for conservation and environmental management},
	volume = {142},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320709000421},
	doi = {10.1016/j.biocon.2009.01.010},
	abstract = {The problems of environmental change and biodiversity loss have entered the mainstream political agenda. Given the call from an increasingly inﬂuential environmental lobby for government and wider society to make both ﬁnancial and personal sacriﬁces to address these problems, it seems likely that conservation biologists and environmental managers will be asked tough questions of the general form ‘are conservation interventions effective?’ and, ‘are they doing more good than harm?’ Science constantly advances and must remain open to challenge, but managers and policy formers require an interim product (an evidence-base) to underpin their current decision-making. The health services have been using the objective and transparent methodology of systematic review to summarise the evidence-base relating to the effectiveness of interventions. Environmental management has, up until now, had no formal shared evidence-base of this kind. Reviewing recent developments in evidence-based practice, this paper introduces a ‘systematic review’ section for this journal and argues that constructing an evidence-based framework for environment management is possible, the challenge is scaling it up to engage the global scientiﬁc community. We draw on the history of evidence-based healthcare, but also on the differences between healthcare and conservation, to set out the challenges in creating a Collaboration for Environmental Evidence that develops a library of systematic reviews on the effectiveness of conservation and environmental interventions.},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Pullin, Andrew S. and Knight, Teri M.},
	month = may,
	year = {2009},
	pages = {931--934},
	file = {12 - pullin knight 2009 - Doing more good than harm – Building an evidence-base for conservation and environmental management.pdf:/Users/bill/D/Zotero/storage/H5G8EU92/12 - pullin knight 2009 - Doing more good than harm – Building an evidence-base for conservation and environmental management.pdf:application/pdf},
}

@article{zamora-marinRoleProtectedAreas,
	title = {The role of protected areas in representing aquatic biodiversity: a test using α, β and γ diversity of water beetles from the {Segura} {River} {Basin} ({SE} {Spain})},
	abstract = {The role of protected areas in representing aquatic biodiversity: a test using α, β and γ diversity of water beetles from the Segura River Basin (SE Spain) Networks of protected areas represent one of the main strategies to reduce the rapid loss of biodiversity. However, most of these protected areas have been designed by considering only charismatic groups of vertebrates and plants, most linked to terrestrial environments. Thus, little is known about how well protected areas perform in representing aquatic biodiversity. This study analyses the suitability of national and European protected area networks (Natural Protected Areas and Natura 2000) in representing such biodiversity. For this purpose, we studied the different components of diversity (α, β and γ) using water beetles from the Segura River Basin as surrogates of overall macroinvertebrate biodiversity. Our results revealed no signiﬁcant differences in α-diversity between protected and non-protected areas. Similarly, we did not ﬁnd signiﬁcant differences in β -diversity components (species replacement and nestedness, i.e., differences in among-site richness without species replacement) between protected and non-protected areas. The species replacement contributed more than nestedness to explain overall β -diversity changes. Finally, we found that the γ-diversity component was signiﬁcantly higher in both protected areas, when compared to an equivalent number of randomly selected locations.},
	language = {en},
	author = {Zamora-Marín, J M and Gutiérrez-Cánovas, C and Abellán, P and Millán, A},
	pages = {14},
	file = {13 - zamora-marin et al - the role of protected areas in representing aquatic biodiversity - a test using alpha beta and gamma diversity of water beetles form the segura river basin SE Spain.pdf:/Users/bill/D/Zotero/storage/YWG2BYBJ/13 - zamora-marin et al - the role of protected areas in representing aquatic biodiversity - a test using alpha beta and gamma diversity of water beetles form the segura river basin SE Spain.pdf:application/pdf},
}

@article{cookConservationDarkInformation2010,
	title = {Conservation in the dark? {The} information used to support management decisions},
	volume = {8},
	issn = {1540-9295},
	shorttitle = {Conservation in the dark?},
	url = {http://doi.wiley.com/10.1890/090020},
	doi = {10.1890/090020},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Frontiers in Ecology and the Environment},
	author = {Cook, Carly N and Hockings, Marc and Carter, RW (Bill)},
	month = may,
	year = {2010},
	pages = {181--186},
	file = {14 - cook et al 2010 - Conservation in the dark - The information used to support management decisions.pdf:/Users/bill/D/Zotero/storage/9CMKJBLA/14 - cook et al 2010 - Conservation in the dark - The information used to support management decisions.pdf:application/pdf},
}

@article{cowlingExpertAlgorithmComparison2003,
	title = {The expert or the algorithm?—comparison of priority conservation areas in the {Cape} {Floristic} {Region} identified by park managers and reserve selection software},
	volume = {112},
	issn = {00063207},
	shorttitle = {The expert or the algorithm?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632070200397X},
	doi = {10.1016/S0006-3207(02)00397-X},
	abstract = {Expert-based and systematic, algorithm-based approaches to identifying priority areas for conservation are sometimes posited as alternatives. While both approaches have pros and cons, the systematic approach does have the advantage of providing a regionwide assessment of the options for achieving explicit conservation targets. A distinct advantage of the expert-driven approach is its incorporation of expert knowledge on biodiversity persistence and pragmatic management and implementation issues not normally included in biodiversity feature-site data matrices. Given the widespread application of both approaches, surprisingly little research has been undertaken to evaluate their conservation planning outcomes. Here we compare priority conservation areas in South Africa’s Cape Floristic Region identiﬁed by park managers and reserve-selection software. Managers identiﬁed 29 areas (a wishlist) that together, comprised 31\% of the planning domain and had 40\% of its area under some form of conservation management. This wishlist was assessed for the extent to which it achieved targets for biodiversity pattern and process over and above the existing conservation system, and its incorporation of priority areas identiﬁed in terms of conservation value and vulnerability to processes that threaten biodiversity. Overall, the wishlist reﬂected a desire by managers to improve management eﬃciency and facilitate rapid implementation by expanding existing, largely montane reserves into low-priority areas where land tenure is sympathetic to conservation. Consequently, it was not very eﬀective and eﬃcient in achieving pattern and process targets, and it excluded large areas of vulnerable and inadequately conserved lowland habitat—the areas currently in most need of conservation action. Further, it provided no basis for scheduling implementation or for exploring alternative areas to achieve the same goals, unlike systematic approaches. Nonetheless, the manager’s wishlist did include many highly innovative and feasible projects that make important contributions to the conservation of the region’s biodiversity. Rather than emphasize the dichotomy between expert and systematic approaches, conservation planners should devise ways of integrating them. In particular, priority areas identiﬁed by experts should be carefully considered against the backdrop of the outcomes of systematic conservation planning.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-04},
	journal = {Biological Conservation},
	author = {Cowling, R.M and Pressey, R.L and Sims-Castley, R and le Roux, A and Baard, E and Burgers, C.J and Palmer, G},
	month = jul,
	year = {2003},
	pages = {147--167},
	file = {15 - cowling et al 2003 - The expert or the algorithm - comparison of priority conservation areas in the Cape Floristic Region identified by park managers and reserve selection software.pdf:/Users/bill/D/Zotero/storage/8AF7JKGA/15 - cowling et al 2003 - The expert or the algorithm - comparison of priority conservation areas in the Cape Floristic Region identified by park managers and reserve selection software.pdf:application/pdf},
}

@incollection{kotthoffAutoWEKAAutomaticModel2019,
	address = {Cham},
	title = {Auto-{WEKA}: {Automatic} {Model} {Selection} and {Hyperparameter} {Optimization} in {WEKA}},
	isbn = {978-3-030-05317-8 978-3-030-05318-5},
	shorttitle = {Auto-{WEKA}},
	url = {http://link.springer.com/10.1007/978-3-030-05318-5_4},
	abstract = {WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often ﬁnd it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA’s learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Automated {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_4},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {81--95},
	file = {16-261(1).pdf:/Users/bill/D/Zotero/storage/K5PA6SBK/16-261(1).pdf:application/pdf},
}

@article{gaoIncentivizingEvaluationLimited2016,
	title = {Incentivizing {Evaluation} via {Limited} {Access} to {Ground} {Truth}: {Peer}-{Prediction} {Makes} {Things} {Worse}},
	shorttitle = {Incentivizing {Evaluation} via {Limited} {Access} to {Ground} {Truth}},
	url = {http://arxiv.org/abs/1606.07042},
	abstract = {In many settings, an eﬀective way of evaluating objects of interest is to collect evaluations from dispersed individuals and to aggregate these evaluations together. Some examples are categorizing online content and evaluating student assignments via peer grading. For this data science problem, one challenge is to motivate participants to conduct such evaluations carefully and to report them honestly, particularly when doing so is costly. Existing approaches, notably peer-prediction mechanisms, can incentivize truth telling in equilibrium. However, they also give rise to equilibria in which agents do not pay the costs required to evaluate accurately, and hence fail to elicit useful information. We show that this problem is unavoidable whenever agents are able to coordinate using low-cost signals about the items being evaluated (e.g., text labels or pictures). We then consider ways of circumventing this problem by comparing agents’ reports to ground truth, which is available in practice when there exist trusted evaluators—such as teaching assistants in the peer grading scenario—who can perform a limited number of unbiased (but noisy) evaluations. Of course, when such ground truth is available, a simpler approach is also possible: rewarding each agent based on agreement with ground truth with some probability, and unconditionally rewarding the agent otherwise. Surprisingly, we show that the simpler mechanism achieves stronger incentive guarantees given less access to ground truth than a large set of peer-prediction mechanisms.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1606.07042 [cs]},
	author = {Gao, Alice and Wright, James R. and Leyton-Brown, Kevin},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.07042},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {1606.07042.pdf:/Users/bill/D/Zotero/storage/GWLRBNYH/1606.07042.pdf:application/pdf},
}

@article{wrightFormalSeparationStrategic2020,
	title = {A {Formal} {Separation} {Between} {Strategic} and {Nonstrategic} {Behavior}},
	url = {http://arxiv.org/abs/1812.11571},
	abstract = {It is common to make a distinction between “strategic” behavior and other forms of intentional but “nonstrategic” behavior: typically, that strategic agents model other agents while nonstrategic agents do not. However, a crisp boundary between these concepts has proven elusive. This problem is pervasive throughout the game theoretic literature on bounded rationality. It is particularly critical in parts of the behavioral game theory literature that make an explicit distinction between the behavior of “nonstrategic” level-0 agents and “strategic” higher-level agents (e.g., the level-k and cognitive hierarchy models). The literature gives no clear guidance on how the rationality of nonstrategic agents must be bounded, instead typically just singling out speciﬁc decision rules and informally asserting them to be nonstrategic (e.g., truthfully revealing private information; randomizing uniformly). In this work, we propose a new, formal characterization of nonstrategic behavior. Our main contribution is to show that it satisﬁes two properties: (1) it is general enough to capture all purportedly “nonstrategic” decision rules of which we are aware; (2) behavior that obeys our characterization is distinct from strategic behavior in a precise sense.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1812.11571 [cs]},
	author = {Wright, James R. and Leyton-Brown, Kevin},
	month = aug,
	year = {2020},
	note = {arXiv: 1812.11571},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {1812.11571.pdf:/Users/bill/D/Zotero/storage/52YCTYW8/1812.11571.pdf:application/pdf},
}

@article{kleinbergProcrastinatingConfidenceNearOptimal2019,
	title = {Procrastinating with {Confidence}: {Near}-{Optimal}, {Anytime}, {Adaptive} {Algorithm} {Configuration}},
	shorttitle = {Procrastinating with {Confidence}},
	url = {http://arxiv.org/abs/1902.05454},
	abstract = {Algorithm conﬁguration methods optimize the performance of a parameterized heuristic algorithm on a given distribution of problem instances. Recent work introduced an algorithm conﬁguration procedure (“Structured Procrastination”) that provably achieves near optimal performance with high probability and with nearly minimal runtime in the worst case. It also offers an anytime property: it keeps tightening its optimality guarantees the longer it is run. Unfortunately, Structured Procrastination is not adaptive to characteristics of the parameterized algorithm: it treats every input like the worst case. Follow-up work (“LeapsAndBounds”) achieves adaptivity but trades away the anytime property. This paper introduces a new algorithm, “Structured Procrastination with Conﬁdence”, that preserves the near-optimality and anytime properties of Structured Procrastination while adding adaptivity. In particular, the new algorithm will perform dramatically faster in settings where many algorithm conﬁgurations perform poorly. We show empirically both that such settings arise frequently in practice and that the anytime property is useful for ﬁnding good conﬁgurations quickly.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1902.05454 [cs]},
	author = {Kleinberg, Robert and Leyton-Brown, Kevin and Lucier, Brendan and Graham, Devon},
	month = nov,
	year = {2019},
	note = {arXiv: 1902.05454},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {1902.05454.pdf:/Users/bill/D/Zotero/storage/NHLXV8DC/1902.05454.pdf:application/pdf},
}

@article{frechetteUsingShapleyValue,
	title = {Using the {Shapley} {Value} to {Analyze} {Algorithm} {Portfolios}},
	abstract = {Algorithms for NP-complete problems often have different strengths and weaknesses, and thus algorithm portfolios often outperform individual algorithms. It is surprisingly difﬁcult to quantify a component algorithm’s contribution to such a portfolio. Reporting a component’s standalone performance wrongly rewards near-clones while penalizing algorithms that have small but distinct areas of strength. Measuring a component’s marginal contribution to an existing portfolio is better, but penalizes sets of strongly correlated algorithms, thereby obscuring situations in which it is essential to have at least one algorithm from such a set. This paper argues for analyzing component algorithm contributions via a measure drawn from coalitional game theory—the Shapley value—and yields insight into a research community’s progress over time. We conclude with an application of the analysis we advocate to SAT competitions, yielding novel insights into the behaviour of algorithm portfolios, their components, and the state of SAT solving technology.},
	language = {en},
	author = {Frechette, Alexandre and Kotthoff, Lars and Michalak, Tomasz and Rahwan, Talal and Hoos, Holger H and Leyton-Brown, Kevin},
	pages = {9},
	file = {2016-AAAI-portfolio-shapley-extended.pdf:/Users/bill/D/Zotero/storage/C3IRPU28/2016-AAAI-portfolio-shapley-extended.pdf:application/pdf},
}

@article{frechetteUsingShapleyValuea,
	title = {Using the {Shapley} {Value} to {Analyze} {Algorithm} {Portfolios}},
	abstract = {Algorithms for NP-complete problems often have different strengths and weaknesses, and thus algorithm portfolios often outperform individual algorithms. It is surprisingly difﬁcult to quantify a component algorithm’s contribution to such a portfolio. Reporting a component’s standalone performance wrongly rewards near-clones while penalizing algorithms that have small but distinct areas of strength. Measuring a component’s marginal contribution to an existing portfolio is better, but penalizes sets of strongly correlated algorithms, thereby obscuring situations in which it is essential to have at least one algorithm from such a set. This paper argues for analyzing component algorithm contributions via a measure drawn from coalitional game theory—the Shapley value—and yields insight into a research community’s progress over time. We conclude with an application of the analysis we advocate to SAT competitions, yielding novel insights into the behaviour of algorithm portfolios, their components, and the state of SAT solving technology.},
	language = {en},
	author = {Frechette, Alexandre and Kotthoff, Lars and Michalak, Tomasz and Rahwan, Talal and Hoos, Holger H and Leyton-Brown, Kevin},
	pages = {7},
	file = {2016-AAAI-portfolio-shapley.pdf:/Users/bill/D/Zotero/storage/N4DMNUBP/2016-AAAI-portfolio-shapley.pdf:application/pdf},
}

@article{bischlASlibBenchmarkLibrary2016,
	title = {{ASlib}: {A} benchmark library for algorithm selection},
	volume = {237},
	issn = {00043702},
	shorttitle = {{ASlib}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370216300388},
	doi = {10.1016/j.artint.2016.04.003},
	abstract = {The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it diﬃcult to share and compare diﬀerent approaches eﬀectively, as is done in other, more established ﬁelds. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of diﬀerent scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve signiﬁcant performance improvements across a broad range of problems and algorithms.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Artificial Intelligence},
	author = {Bischl, Bernd and Kerschke, Pascal and Kotthoff, Lars and Lindauer, Marius and Malitsky, Yuri and Fréchette, Alexandre and Hoos, Holger and Hutter, Frank and Leyton-Brown, Kevin and Tierney, Kevin and Vanschoren, Joaquin},
	month = aug,
	year = {2016},
	pages = {41--58},
	file = {2016-AIJ-ASlib.pdf:/Users/bill/D/Zotero/storage/GN2Z4WWN/2016-AIJ-ASlib.pdf:application/pdf},
}

@article{gaoIncentivizingEvaluationLimited2016a,
	title = {Incentivizing {Evaluation} via {Limited} {Access} to {Ground} {Truth}: {Peer}-{Prediction} {Makes} {Things} {Worse}},
	shorttitle = {Incentivizing {Evaluation} via {Limited} {Access} to {Ground} {Truth}},
	url = {http://arxiv.org/abs/1606.07042},
	abstract = {In many settings, an eﬀective way of evaluating objects of interest is to collect evaluations from dispersed individuals and to aggregate these evaluations together. Some examples are categorizing online content and evaluating student assignments via peer grading. For this data science problem, one challenge is to motivate participants to conduct such evaluations carefully and to report them honestly, particularly when doing so is costly. Existing approaches, notably peer-prediction mechanisms, can incentivize truth telling in equilibrium. However, they also give rise to equilibria in which agents do not pay the costs required to evaluate accurately, and hence fail to elicit useful information. We show that this problem is unavoidable whenever agents are able to coordinate using low-cost signals about the items being evaluated (e.g., text labels or pictures). We then consider ways of circumventing this problem by comparing agents’ reports to ground truth, which is available in practice when there exist trusted evaluators—such as teaching assistants in the peer grading scenario—who can perform a limited number of unbiased (but noisy) evaluations. Of course, when such ground truth is available, a simpler approach is also possible: rewarding each agent based on agreement with ground truth with some probability, and unconditionally rewarding the agent otherwise. Surprisingly, we show that the simpler mechanism achieves stronger incentive guarantees given less access to ground truth than a large set of peer-prediction mechanisms.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1606.07042 [cs]},
	author = {Gao, Alice and Wright, James R. and Leyton-Brown, Kevin},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.07042},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {2016-ECDS-PeerPrediction.pdf:/Users/bill/D/Zotero/storage/MYA36WLL/2016-ECDS-PeerPrediction.pdf:application/pdf},
}

@article{cameronBiasAlgorithmPortfolio,
	title = {Bias in {Algorithm} {Portfolio} {Performance} {Evaluation}},
	abstract = {A Virtual Best Solver (VBS) is a hypothetical algorithm that selects the best solver from a given portfolio of alternatives on a per-instance basis. The VBS idealizes performance when all solvers in a portfolio are run in parallel, and also gives a valuable bound on the performance of portfolio-based algorithm selectors. Typically, VBS performance is measured by running every solver in a portfolio once on a given instance and reporting the best performance over all solvers. Here, we argue that doing so results in a ﬂawed measure that is biased to reporting better performance when a randomized solver is present in an algorithm portfolio. Speciﬁcally, this ﬂawed notion of VBS tends to show performance better than that achievable by a perfect selector that for each given instance runs the solver with the best expected running time. We report results from an empirical study using solvers and instances submitted to several SAT competitions, in which we observe signiﬁcant bias on many random instances and some combinatorial instances. We also show that the bias increases with the number of randomized solvers and decreases as we average solver performance over many independent runs per instance. We propose an alternative VBS performance measure by (1) empirically obtaining the solver with best expected performance for each instance and (2) taking bootstrap samples for this solver on every instance, to obtain a conﬁdence interval on VBS performance. Our ﬁndings shed new light on widely studied algorithm selection benchmarks and help explain performance gaps observed between VBS and state-of-the-art algorithm selection approaches.},
	language = {en},
	author = {Cameron, Chris and Hoos, Holger H and Leyton-Brown, Kevin},
	pages = {8},
	file = {2016-IJCAI-VBS.pdf:/Users/bill/D/Zotero/storage/EDV9FHPF/2016-IJCAI-VBS.pdf:application/pdf},
}

@incollection{xuQuantifyingSimilarityAlgorithm2016,
	address = {Cham},
	title = {Quantifying the {Similarity} of {Algorithm} {Configurations}},
	volume = {10079},
	isbn = {978-3-319-50348-6 978-3-319-50349-3},
	url = {http://link.springer.com/10.1007/978-3-319-50349-3_14},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Xu, Lin and KhudaBukhsh, Ashiqur R. and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Festa, Paola and Sellmann, Meinolf and Vanschoren, Joaquin},
	year = {2016},
	doi = {10.1007/978-3-319-50349-3_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {203--217},
	file = {2016-LION-Visualizing-Configurations.pdf:/Users/bill/D/Zotero/storage/TAZJBSSZ/2016-LION-Visualizing-Configurations.pdf:application/pdf},
}

@incollection{kotthoffAutoWEKAAutomaticModel2019a,
	address = {Cham},
	title = {Auto-{WEKA}: {Automatic} {Model} {Selection} and {Hyperparameter} {Optimization} in {WEKA}},
	isbn = {978-3-030-05317-8 978-3-030-05318-5},
	shorttitle = {Auto-{WEKA}},
	url = {http://link.springer.com/10.1007/978-3-030-05318-5_4},
	abstract = {WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often ﬁnd it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA’s learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Automated {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_4},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {81--95},
	file = {2016-MLOSS-AutoWeka2.pdf:/Users/bill/D/Zotero/storage/W4LDV3LE/2016-MLOSS-AutoWeka2.pdf:application/pdf},
}

@article{hartfordDeepLearningPredicting,
	title = {Deep {Learning} for {Predicting} {Human} {Strategic} {Behavior}},
	abstract = {Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant’s cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance signiﬁcantly outperforms that of the previous state of the art, which relies on expert-constructed features.},
	language = {en},
	author = {Hartford, Jason and Wright, James R and Leyton-Brown, Kevin},
	pages = {9},
	file = {2016-NIPS-Gamenet.pdf:/Users/bill/D/Zotero/storage/TFCGJK3B/2016-NIPS-Gamenet.pdf:application/pdf},
}

@article{hartfordAbstractDeepCounterfactual,
	title = {Abstract: {Deep} {Counterfactual} {Prediction} using {Instrumental} {Variables}},
	language = {en},
	author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
	pages = {2},
	file = {2016-NIPSWS-Causenet.pdf:/Users/bill/D/Zotero/storage/XAIDA9RM/2016-NIPSWS-Causenet.pdf:application/pdf},
}

@article{wrightPredictingHumanBehavior2017,
	title = {Predicting human behavior in unrepeated, simultaneous-move games},
	volume = {106},
	issn = {08998256},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0899825617301574},
	doi = {10.1016/j.geb.2017.09.009},
	language = {en},
	urldate = {2020-11-04},
	journal = {Games and Economic Behavior},
	author = {Wright, James R. and Leyton-Brown, Kevin},
	month = nov,
	year = {2017},
	pages = {16--37},
	file = {2017-GEB-PredictingHumanBehavior.pdf:/Users/bill/D/Zotero/storage/F45ZEPQS/2017-GEB-PredictingHumanBehavior.pdf:application/pdf},
}

@article{hartfordDeepIVFlexible,
	title = {Deep {IV}: {A} {Flexible} {Approach} for {Counterfactual} {Prediction}},
	abstract = {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs)—sources of treatment randomization that are conditionally independent from the outcomes. Our IV speciﬁcation resolves into two prediction tasks that can be solved with deep neural nets: a ﬁrst-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework1 allows us to take advantage of oﬀ-the-shelf supervised learning techniques to estimate causal eﬀects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.},
	language = {en},
	author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
	pages = {10},
	file = {2017-ICML-DeepIV.pdf:/Users/bill/D/Zotero/storage/2XDZNCB3/2017-ICML-DeepIV.pdf:application/pdf},
}

@inproceedings{kleinbergEfficiencyProcrastinationApproximately2017,
	address = {Melbourne, Australia},
	title = {Efficiency {Through} {Procrastination}: {Approximately} {Optimal} {Algorithm} {Configuration} with {Runtime} {Guarantees}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Efficiency {Through} {Procrastination}},
	url = {https://www.ijcai.org/proceedings/2017/281},
	doi = {10.24963/ijcai.2017/281},
	abstract = {Algorithm conﬁguration methods have achieved much practical success, but to date have not been backed by meaningful performance guarantees. We address this gap with a new algorithm conﬁguration framework, Structured Procrastination. With high probability and nearly as quickly as possible in the worst case, our framework ﬁnds an algorithm conﬁguration that provably achieves near optimal performance. Further, its running time requirements asymptotically dominate those of existing methods.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kleinberg, Robert and Leyton-Brown, Kevin and Lucier, Brendan},
	month = aug,
	year = {2017},
	pages = {2023--2031},
	file = {2017-StructuredProcrastination.pdf:/Users/bill/D/Zotero/storage/LFE3DLN3/2017-StructuredProcrastination.pdf:application/pdf},
}

@inproceedings{newmanDesigningEvolvingElectronic2018,
	address = {Menlo Park and San Jose, CA, USA},
	title = {Designing and {Evolving} an {Electronic} {Agricultural} {Marketplace} in {Uganda}},
	isbn = {978-1-4503-5816-3},
	url = {http://dl.acm.org/citation.cfm?doid=3209811.3209862},
	doi = {10.1145/3209811.3209862},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 1st {ACM} {SIGCAS} {Conference} on {Computing} and {Sustainable} {Societies} ({COMPASS}) - {COMPASS} '18},
	publisher = {ACM Press},
	author = {Newman, Neil and Bergquist, Lauren Falcao and Immorlica, Nicole and Leyton-Brown, Kevin and Lucier, Brendan and McIntosh, Craig and Quinn, John and Ssekibuule, Richard},
	year = {2018},
	pages = {1--11},
	file = {2018-electronic-agricultural-marketplace.pdf:/Users/bill/D/Zotero/storage/ESUM4YGE/2018-electronic-agricultural-marketplace.pdf:application/pdf},
}

@article{lundyAllocationSocialGood,
	title = {Allocation for {Social} {Good}},
	language = {en},
	author = {Lundy, Taylor},
	pages = {24},
	file = {2019-EC-Slides-AllocationForSocialGood.pdf:/Users/bill/D/Zotero/storage/KHRMLDYZ/2019-EC-Slides-AllocationForSocialGood.pdf:application/pdf},
}

@article{leyton-brownLearningSpaceAlgorithm,
	title = {Learning in the {Space} of {Algorithm} {Designs}},
	language = {en},
	author = {Leyton-Brown, Kevin and Hutter, Frank},
	pages = {201},
	file = {2019-ICML-Tutorial_on_Algorithm_Configuration.pdf:/Users/bill/D/Zotero/storage/J2992FSN/2019-ICML-Tutorial_on_Algorithm_Configuration.pdf:application/pdf},
}

@article{wrightLevel0ModelsPredicting2019,
	title = {Level-0 {Models} for {Predicting} {Human} {Behavior} in {Games}},
	volume = {64},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/11361},
	doi = {10.1613/jair.1.11361},
	abstract = {Behavioral game theory seeks to describe the way actual people (as compared to idealized, “rational” agents) act in strategic situations. Our own recent work has identiﬁed iterative models, such as quantal cognitive hierarchy, as the state of the art for predicting human play in unrepeated, simultaneous-move games. Iterative models predict that agents reason iteratively about their opponents, building up from a speciﬁcation of nonstrategic behavior called level-0. A modeler is in principle free to choose any description of level-0 behavior that makes sense for a given setting. However, in practice almost all existing work speciﬁes this behavior as a uniform distribution over actions. In most games it is not plausible that even nonstrategic agents would choose an action uniformly at random, nor that other agents would expect them to do so. A more accurate model for level-0 behavior has the potential to dramatically improve predictions of human behavior, since a substantial fraction of agents may play level-0 strategies directly, and furthermore since iterative models ground all higher-level strategies in responses to the level-0 strategy. Our work considers models of the way in which level-0 agents construct a probability distribution over actions, given an arbitrary game. We considered a large space of alternatives and, in the end, recommend a model that achieved excellent performance across the board: a linear weighting of four binary features, each of which is general in the sense that it can be computed from any normal form game. Adding real-valued variants of the same four features yielded further improvements in performance, albeit with a corresponding increase in the number of parameters needing to be estimated. We evaluated the eﬀects of combining these new level-0 models with several iterative models and observed large improvements in predictive accuracy.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Journal of Artificial Intelligence Research},
	author = {Wright, James R. and Leyton-Brown, Kevin},
	month = feb,
	year = {2019},
	pages = {357--383},
	file = {11361-Article (PDF)-21052-1-10-20190219.pdf:/Users/bill/D/Zotero/storage/4RIQGLDR/11361-Article (PDF)-21052-1-10-20190219.pdf:application/pdf},
}

@article{neuteboomVariationRankAbundance2005,
	title = {Variation in rank abundance replicate samples and impact of clustering},
	volume = {53},
	issn = {15735214},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1573521405800053},
	doi = {10.1016/S1573-5214(05)80005-3},
	abstract = {Calculating a single-sample rank abundance curve by using the negative-binomial distribution provides a way to investigate the variability within rank abundance replicate samples and yields a measure of the degree of heterogeneity of the sampled community. The calculation of the single-sample rank abundance curve is used in combination with the negative-binomial rank abundance curve-fit model to analyse the principal effect of clustering on the species-individual (S-N) curve and the species-area curve. With the usual plotting of S against log N or log area, assuming that N is proportional to area, S-N curves and species-area curves are the same curves with only a shifted horizontal axis. Clustering results in a lower recorded number of species in a sample and stretches the S-N curve and species-area curve over the horizontal axis to the right. In contrast to what is suggested in the literature, we surmise that the effect of clustering on both curves will gradually fade away with increasing sample size. Since the slopes of the curves are not constant, they cannot be used as species diversity indices or site discriminant. S-N curves and species-area curves cannot be extrapolated.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {NJAS - Wageningen Journal of Life Sciences},
	author = {Neuteboom, J.H. and Struik, P.C},
	year = {2005},
	pages = {199--222},
	file = {1-s2.0-S1573521405800053-main.pdf:/Users/bill/D/Zotero/storage/59F236BD/1-s2.0-S1573521405800053-main.pdf:application/pdf},
}

@article{matthewsREVIEWSpeciesAbundance2015,
	title = {{REVIEW}: {On} the species abundance distribution in applied ecology and biodiversity management},
	volume = {52},
	issn = {00218901},
	shorttitle = {{REVIEW}},
	url = {http://doi.wiley.com/10.1111/1365-2664.12380},
	doi = {10.1111/1365-2664.12380},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Journal of Applied Ecology},
	author = {Matthews, Thomas J. and Whittaker, Robert J.},
	editor = {Fuller, Richard},
	month = apr,
	year = {2015},
	pages = {443--454},
	file = {matthews whittaker 2015 - On the species abundance distribution in applied ecology and biodiversity management - RANK ABUNDANCE DISTRIBUTION - BDPG.pdf:/Users/bill/D/Zotero/storage/TEDMEWZN/matthews whittaker 2015 - On the species abundance distribution in applied ecology and biodiversity management - RANK ABUNDANCE DISTRIBUTION - BDPG.pdf:application/pdf},
}

@article{mcgillSpeciesAbundanceDistributions2007,
	title = {Species abundance distributions: moving beyond single prediction theories to integration within an ecological framework},
	volume = {10},
	issn = {1461-023X, 1461-0248},
	shorttitle = {Species abundance distributions},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2007.01094.x},
	doi = {10.1111/j.1461-0248.2007.01094.x},
	abstract = {Species abundance distributions (SADs) follow one of ecologyÕs oldest and most universal laws – every community shows a hollow curve or hyperbolic shape on a histogram with many rare species and just a few common species. Here, we review theoretical, empirical and statistical developments in the study of SADs. Several key points emerge. (i) Literally dozens of models have been proposed to explain the hollow curve. Unfortunately, very few models are ever rejected, primarily because few theories make any predictions beyond the hollow-curve SAD itself. (ii) Interesting work has been performed both empirically and theoretically, which goes beyond the hollow-curve prediction to provide a rich variety of information about how SADs behave. These include the study of SADs along environmental gradients and theories that integrate SADs with other biodiversity patterns. Central to this body of work is an effort to move beyond treating the SAD in isolation and to integrate the SAD into its ecological context to enable making many predictions. (iii) Moving forward will entail understanding how sampling and scale affect SADs and developing statistical tools for describing and comparing SADs. We are optimistic that SADs can provide signiﬁcant insights into basic and applied ecological science.},
	language = {en},
	number = {10},
	urldate = {2020-11-04},
	journal = {Ecology Letters},
	author = {McGill, Brian J. and Etienne, Rampal S. and Gray, John S. and Alonso, David and Anderson, Marti J. and Benecha, Habtamu Kassa and Dornelas, Maria and Enquist, Brian J. and Green, Jessica L. and He, Fangliang and Hurlbert, Allen H. and Magurran, Anne E. and Marquet, Pablo A. and Maurer, Brian A. and Ostling, Annette and Soykan, Candan U. and Ugland, Karl I. and White, Ethan P.},
	month = oct,
	year = {2007},
	pages = {995--1015},
	file = {mcgill et al 2007 - species abundance distributions - moving beyond single prediction theories to integration within an ecological framework - BDPG.pdf:/Users/bill/D/Zotero/storage/2YJEV94V/mcgill et al 2007 - species abundance distributions - moving beyond single prediction theories to integration within an ecological framework - BDPG.pdf:application/pdf},
}

@article{neuteboomSilicoSamplingReveals2005,
	title = {In silico sampling reveals the effect of clustering and shows that the log-normal rank abundance curve is an artefact},
	volume = {53},
	issn = {15735214},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1573521405800065},
	doi = {10.1016/S1573-5214(05)80006-5},
	abstract = {The impact of clustering on rank abundance, species-individual (5-N) and species-area curves was investigated using a computer programme for in silico sampling. In a rank abundance curve the abundances of species are plotted on log-scale against species sequence. In an 5-N curve the number of species (5) is plotted against the log of the total number of individuals (N) in the sample, in a species-area curve 5 is plotted against log-area. The results from in silico sampling confirm the general shape of 5-N and speciesarea curves for communities with clustering, i.e., a curve that starts with a smaller slope but that later is temporarily steeper than the curve expected for Poisson-distributed species. Extrapolation of 5-N and species-area curves could therefore be misleading. The output furthermore shows that sigmoid rank abundance curves (curves of the type of a log-normal or broken stick) can be an artefact of the standard procedure of first sorting the species in sequence of abundance in combination with clustering in the low abundant and rare species. This makes the usual explanation given to the log-normal rank abundance curve dubious. An extension of the negative-binomial rank abundance curve-fit model is discussed to make it suitable for also fitting sigmoid rank abundance curves.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {NJAS - Wageningen Journal of Life Sciences},
	author = {Neuteboom, J.H. and Struik, P.C.},
	year = {2005},
	pages = {223--245},
	file = {neuteboom struik 2005 - in silica sampling reveals the effect of clustering and shows that the log-normal rank abundance curve is an artefact - BDPG.pdf:/Users/bill/D/Zotero/storage/WUXW8RUG/neuteboom struik 2005 - in silica sampling reveals the effect of clustering and shows that the log-normal rank abundance curve is an artefact - BDPG.pdf:application/pdf},
}

@article{wilsonMethodsFittingDominance1991,
	title = {Methods for fitting dominance/diversity curves},
	volume = {2},
	issn = {11009233, 16541103},
	url = {http://doi.wiley.com/10.2307/3235896},
	doi = {10.2307/3235896},
	abstract = {Dominance/diversity curves, displaying the relative abundances of the species within a community, have often been constructed from field data. Several ecological and statistical models of dominance/diversity have been proposed, to explain the curves. Yet, rarely have curves of different models been fitted to field data.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Journal of Vegetation Science},
	author = {Wilson, J. Bastow},
	month = feb,
	year = {1991},
	pages = {35--46},
	file = {wilson 1991 - Methods for fitting dominance-diversity curves - RANK ABUNDANCE DISTRIBUTION - LOGNORMAL - BDPG.pdf:/Users/bill/D/Zotero/storage/EDSPGMZM/wilson 1991 - Methods for fitting dominance-diversity curves - RANK ABUNDANCE DISTRIBUTION - LOGNORMAL - BDPG.pdf:application/pdf},
}

@article{lariviereImpactFactorMatthew2009,
	title = {The impact factor's {Matthew} {Effect}: {A} natural experiment in bibliometrics},
	issn = {15322882, 15322890},
	shorttitle = {The impact factor's {Matthew} {Effect}},
	url = {http://doi.wiley.com/10.1002/asi.21232},
	doi = {10.1002/asi.21232},
	abstract = {Since the publication of Robert K. Merton’s theory of cumulative advantage in science (Matthew Effect), several empirical studies have tried to measure its presence at the level of papers, individual researchers, institutions or countries. However, these studies seldom control for the intrinsic “quality” of papers or of researchers—“better” (however defined) papers or researchers could receive higher citation rates because they are indeed of better quality. Using an original method for controlling the intrinsic value of papers—identical duplicate papers published in different journals with different impact factors—this paper shows that the journal in which papers are published have a strong influence on their citation rates, as duplicate papers published in high impact journals obtain, on average, twice as much citations as their identical counterparts published in journals with lower impact factors. The intrinsic value of a paper is thus not the only reason a given paper gets cited or not; there is a specific Matthew effect attached to journals and this gives to paper published there an added value over and above their intrinsic quality.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Larivière, Vincent and Gingras, Yves},
	year = {2009},
	pages = {n/a--n/a},
	file = {0908.3177.pdf:/Users/bill/D/Zotero/storage/85RVWB2G/0908.3177.pdf:application/pdf},
}

@article{bolMatthewEffectScience2018,
	title = {The {Matthew} effect in science funding},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1719557115},
	doi = {10.1073/pnas.1719557115},
	abstract = {A classic thesis is that scientific achievement exhibits a “Matthew effect”: Scientists who have previously been successful are more likely to succeed again, producing increasing distinction. We investigate to what extent the Matthew effect drives the allocation of research funds. To this end, we assembled a dataset containing all review scores and funding decisions of grant proposals submitted by recent PhDs in a €2 billion granting program. Analyses of review scores reveal that early funding success introduces a growing rift, with winners just above the funding threshold accumulating more than twice as much research funding (€180,000) during the following eight years as nonwinners just below it. We find no evidence that winners’ improved funding chances in subsequent competitions are due to achievements enabled by the preceding grant, which suggests that early funding itself is an asset for acquiring later funding. Surprisingly, however, the emergent funding gap is partly created by applicants, who, after failing to win one grant, apply for another grant less often.},
	language = {en},
	number = {19},
	urldate = {2020-11-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bol, Thijs and de Vaan, Mathijs and van de Rijt, Arnout},
	month = may,
	year = {2018},
	pages = {4887--4890},
	file = {1719557115.full.pdf:/Users/bill/D/Zotero/storage/K8IN2XKJ/1719557115.full.pdf:application/pdf},
}

@article{mcrtonMatthewEffectM0,
	title = {The {Matthew} {Effect} m0 {Science}},
	language = {en},
	author = {Mcrton, K},
	pages = {8},
	file = {matthew1.pdf:/Users/bill/D/Zotero/storage/JECYNPF9/matthew1.pdf:application/pdf},
}

@article{yinQuantifyingDynamicsFailure2019,
	title = {Quantifying the dynamics of failure across science, startups and security},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1725-y},
	doi = {10.1038/s41586-019-1725-y},
	language = {en},
	number = {7781},
	urldate = {2020-11-04},
	journal = {Nature},
	author = {Yin, Yian and Wang, Yang and Evans, James A. and Wang, Dashun},
	month = nov,
	year = {2019},
	pages = {190--194},
	file = {yin et al 2019 - quantifying the dynamics of failure across science startups and security.pdf:/Users/bill/D/Zotero/storage/QA3SS5RG/yin et al 2019 - quantifying the dynamics of failure across science startups and security.pdf:application/pdf},
}

@article{ingberSimulatedAnnealingPractice1993,
	title = {Simulated annealing: {Practice} versus theory},
	volume = {18},
	issn = {08957177},
	shorttitle = {Simulated annealing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/089571779390204C},
	doi = {10.1016/0895-7177(93)90204-C},
	abstract = {Lester Ingber Lester Ingber Research, P.O.B. 857, McLean, VA 22101 ingber@alumni.caltech.edu Simulated annealing (SA) presents an optimization technique with several striking positive and negative features. Perhaps its most salient feature, statistically promising to deliver an optimal solution, in current practice is often spurned to use instead modiﬁed faster algorithms, “simulated quenching” (SQ). Using the author’s Adaptive Simulated Annealing (ASA) code, some examples are given which demonstrate how SQ can be much faster than SA without sacriﬁcing accuracy.},
	language = {en},
	number = {11},
	urldate = {2020-11-04},
	journal = {Mathematical and Computer Modelling},
	author = {Ingber, L.},
	month = dec,
	year = {1993},
	pages = {29--57},
	file = {ingber 1993 - simulated annealing - practice versus theory.pdf:/Users/bill/D/Zotero/storage/VN74QFZZ/ingber 1993 - simulated annealing - practice versus theory.pdf:application/pdf},
}

@article{nishimoriComparativeStudyPerformance2015,
	title = {Comparative {Study} of the {Performance} of {Quantum} {Annealing} and {Simulated} {Annealing}},
	volume = {91},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/1409.6386},
	doi = {10.1103/PhysRevE.91.012104},
	abstract = {Relations of simulated annealing and quantum annealing are studied by a mapping from the transition matrix of classical Markovian dynamics of the Ising model to a quantum Hamiltonian and vice versa. It is shown that these two operators, the transition matrix and the Hamiltonian, share the eigenvalue spectrum. Thus, if simulated annealing with slow temperature change does not encounter a diﬃculty caused by an exponentially long relaxation time at a ﬁrst-order phase transition, the same is true for the corresponding process of quantum annealing in the adiabatic limit. One of the important diﬀerences between the classical-to-quantum mapping and the converse quantum-to-classical mapping is that the Markovian dynamics of a short-range Ising model is mapped to a short-range quantum system, but the converse mapping from a short-range quantum system to a classical one results in long-range interactions. This leads to a diﬀerence in eﬃciencies that simulated annealing can be eﬃciently simulated by quantum annealing but the converse is not necessarily true. We conclude that quantum annealing is easier to implement and is more ﬂexible than simulated annealing. We also point out that the present mapping can be extended to accommodate explicit time dependence of temperature, which is used to justify the quantum-mechanical analysis of simulated annealing by Somma, Batista, and Ortiz. Additionally, an alternative method to solve the non-equilibrium dynamics of the one-dimensional Ising model is provided through the classical-to-quantum mapping.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Physical Review E},
	author = {Nishimori, Hidetoshi and Tsuda, Junichi and Knysh, Sergey},
	month = jan,
	year = {2015},
	note = {arXiv: 1409.6386},
	keywords = {Condensed Matter - Statistical Mechanics, Quantum Physics},
	pages = {012104},
	annote = {Comment: 19 pages},
	file = {Nishimori Tsuda 2014 - Comparative Study of the Performance of Quantum Annealing and Simulated Annealing.pdf:/Users/bill/D/Zotero/storage/HIH7HZL5/Nishimori Tsuda 2014 - Comparative Study of the Performance of Quantum Annealing and Simulated Annealing.pdf:application/pdf},
}

@article{ruppeinerEnsembleApproachSimulated1991,
	title = {Ensemble approach to simulated annealing},
	volume = {1},
	issn = {1155-4304, 1286-4862},
	url = {http://www.edpsciences.org/10.1051/jp1:1991146},
	doi = {10.1051/jp1:1991146},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Journal de Physique I},
	author = {Ruppeiner, George and Pedersen, Jacob M�rch and Salamon, Peter},
	month = apr,
	year = {1991},
	pages = {455--470},
	file = {ruppeiner et al 1991 - ensemble approach to simulated annealing.pdf:/Users/bill/D/Zotero/storage/LQZQ2DBD/ruppeiner et al 1991 - ensemble approach to simulated annealing.pdf:application/pdf},
}

@article{zhangInfluenceSegmentationFeature1995,
	title = {Influence of segmentation over feature measurement},
	volume = {16},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016786559400083F},
	doi = {10.1016/0167-8655(94)00083-F},
	abstract = {The influence ofimage segmentation over the measurement accuracy of object features is studied. Seven features are examined under different image conditions by using a common segmentation procedure. The results indicate that accurate measures of image properties profoundly depend on the quality of image segmentation.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Pattern Recognition Letters},
	author = {Zhang, Yu Jin},
	month = feb,
	year = {1995},
	keywords = {EFs, Ultimate Measurement Accuracy, Segmentation},
	pages = {201--206},
	file = {zhang 1995 - influence of segmentation over feature measurement - ULTIMATE MEASUREMENT ACCURACY - SEGMENTATION - EF.pdf:/Users/bill/D/Zotero/storage/J2W4HWVS/zhang 1995 - influence of segmentation over feature measurement - ULTIMATE MEASUREMENT ACCURACY - SEGMENTATION - EF.pdf:application/pdf},
}

@article{laiTrophicOverlapbasedMeasure2015,
	title = {A trophic overlap-based measure for species uniqueness in ecological networks},
	volume = {299},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380014006206},
	doi = {10.1016/j.ecolmodel.2014.12.014},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Lai, Shu-mei and Liu, Wei-chung and Jordán, Ferenc},
	month = mar,
	year = {2015},
	pages = {95--101},
	file = {1-s2.0-S0304380014006206-main.pdf:/Users/bill/D/Zotero/storage/WACEYM4R/1-s2.0-S0304380014006206-main.pdf:application/pdf},
}

@article{laiCorrigendumTrophicOverlapbased2016,
	title = {Corrigendum to “{A} trophic overlap-based measure for species uniqueness in ecological networks” [{Ecol}. {Model}. 299 ({March}) (2015) 95–101]},
	volume = {336},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380016301004},
	doi = {10.1016/j.ecolmodel.2016.04.003},
	language = {en},
	urldate = {2020-11-04},
	journal = {Ecological Modelling},
	author = {Lai, Shu-mei and Liu, Wei-chung and Jordán, Ferenc},
	month = sep,
	year = {2016},
	pages = {82},
	file = {1-s2.0-S0304380016301004-main.pdf:/Users/bill/D/Zotero/storage/WNF3IK2F/1-s2.0-S0304380016301004-main.pdf:application/pdf},
}

@article{fodorLinkingBiodiversityMutualistic2013,
	title = {Linking biodiversity to mutualistic networks – woody species and ectomycorrhizal fungi},
	abstract = {Mutualistic interactions are currently mapped by bipartite networks with particular architecture and properties. The mycorrhizae connect the trees and permit them to share resources, therefore relaxing the competition. Ectomycorrhizal macrofungi associated with woody species (Quercus robur, Q. cerris, Q. petraea, Tilia tomentosa, Carpinus betulus, Corylus avellana, and Q. pubescens) growing in a temperate, broadleaved mixed forest, from a hilly area near the city of Cluj–Napoca, central Romania were included in a bipartite mutualistic network. Community structure was investigated using several network metrics, modularity and nestedness algorithms in conjunction with C-score index cluster analysis and nonmetric multidimensional scaling (the Kulczynski similarity was index used as most appropriate metric selected by minimal stress criterion). The results indicate that the network presents high asymmetry (hosts are outnumbered by mycobionts at a great extent), high connectance, low modularity, and high nestedness, competition playing a secondary role in community assemblage (non significant difference between simulated and observed Cscore). The nestedness pattern is non-random and is comparable to previously published results for other similar interactions containing plants. In the proposed network, woody species function exclusively as generalists. Modularity analysis is a finer tool were identifying species roles than centrality measures, however, the two types of algorithms permit the separation of species according to their roles as for example connectors (generalist species) and ultraperipheral species (specialists). Supergeneralist woody species function as hubs for the diverse ectomycorrhizal community while supergeneralist ectomycorrhizal fungi glue the hubs into a coherent aggregate.},
	language = {en},
	author = {Fodor, E},
	year = {2013},
	pages = {26},
	file = {44-140-1-SM.pdf:/Users/bill/D/Zotero/storage/N6ENFQN6/44-140-1-SM.pdf:application/pdf},
}

@techreport{poisotSpeciesWhyEcological2014,
	type = {preprint},
	title = {Beyond species: why ecological interaction networks vary through space and time},
	shorttitle = {Beyond species},
	url = {http://biorxiv.org/lookup/doi/10.1101/001677},
	abstract = {Community ecology is tasked with the considerable challenge of predicting the structure, and properties, of emerging ecosystems. It requires the ability to understand how and why species interact, as this will allow the development of mechanism-based predictive models, and as such to better characterize how ecological mechanisms act locally on the existence of inter-specific interactions. Here we argue that the current conceptualization of species interaction networks is ill-suited for this task. Instead, we propose that future research must start to account for the intrinsic variability of species interactions, then scale up from here onto complex networks. This can be accomplished simply by recognizing that there exists intra-specific variability, in traits or properties related to the establishment of species interactions. By shifting the scale towards population-based processes, we show that this new approach will improve our predictive ability and mechanistic understanding of how species interact over large spatial or temporal scales.},
	language = {en},
	urldate = {2020-11-04},
	institution = {Ecology},
	author = {Poisot, Timothée and Stouffer, Daniel B and Gravel, Dominique},
	month = jan,
	year = {2014},
	doi = {10.1101/001677},
	file = {001677.full.pdf:/Users/bill/D/Zotero/storage/LMN9E5XR/001677.full.pdf:application/pdf},
}

@article{kaiser-bunburyIntegratingNetworkEcology2015,
	title = {Integrating network ecology with applied conservation: a synthesis and guide to implementation},
	volume = {7},
	issn = {2041-2851},
	shorttitle = {Integrating network ecology with applied conservation},
	url = {https://academic.oup.com/aobpla/article-lookup/doi/10.1093/aobpla/plv076},
	doi = {10.1093/aobpla/plv076},
	abstract = {Ecological networks are a useful tool to study the complexity of biotic interactions at a community level. Advances in the understanding of network patterns encourage the application of a network approach in other disciplines than theoretical ecology, such as biodiversity conservation. So far, however, practical applications have been meagre. Here we present a framework for network analysis to be harnessed to advance conservation management by using plant – pollinator networks and islands as model systems. Conservation practitioners require indicators to monitor and assess management effectiveness and validate overall conservation goals. By distinguishing between two network attributes, the ‘diversity’ and ‘distribution’ of interactions, on three hierarchical levels (species, guild/ group and network) we identify seven quantitative metrics to describe changes in network patterns that have implications for conservation. Diversity metrics are partner diversity, vulnerability/generality, interaction diversity and interaction evenness, and distribution metrics are the specialization indices d′ and H′2, and modularity. Distribution metrics account for sampling bias and may therefore be suitable indicators to detect human-induced changes to plant –pollinator communities, thus indirectly assessing the structural and functional robustness and integrity of ecosystems. We propose an implementation pathway that outlines the stages that are required to successfully embed a network approach in biodiversity conservation. Most importantly, only if conservation action and study design are aligned by practitioners and ecologists through joint experiments, are the ﬁndings of a conservation network approach equally beneﬁcial for advancing adaptive management and ecological network theory. We list potential obstacles to the framework, highlight the shortfall in empirical, mostly experimental, network data and discuss possible solutions.},
	language = {en},
	urldate = {2020-11-04},
	journal = {AoB Plants},
	author = {Kaiser-Bunbury, Christopher N. and Blüthgen, Nico},
	year = {2015},
	pages = {plv076},
	file = {AoB PLANTS-2015-Kaiser-Bunbury-aobpla_plv076.pdf:/Users/bill/D/Zotero/storage/EHX9WEFW/AoB PLANTS-2015-Kaiser-Bunbury-aobpla_plv076.pdf:application/pdf},
}

@article{bluthgenWHATINTERACTIONNETWORK2008,
	title = {{WHAT} {DO} {INTERACTION} {NETWORK} {METRICS} {TELL} {US} {ABOUT} {SPECIALIZATION} {AND} {BIOLOGICAL} {TRAITS}},
	volume = {89},
	issn = {0012-9658},
	url = {http://doi.wiley.com/10.1890/07-2121.1},
	doi = {10.1890/07-2121.1},
	abstract = {The structure of ecological interaction networks is often interpreted as a product of meaningful ecological and evolutionary mechanisms that shape the degree of specialization in community associations. However, here we show that both unweighted network metrics (connectance, nestedness, and degree distribution) and weighted network metrics (interaction evenness, interaction strength asymmetry) are strongly constrained and biased by the number of observations. Rarely observed species are inevitably regarded as ‘‘specialists,’’ irrespective of their actual associations, leading to biased estimates of specialization. Consequently, a skewed distribution of species observation records (such as the lognormal), combined with a relatively low sampling density typical for ecological data, already generates a ‘‘nested’’ and poorly ‘‘connected’’ network with ‘‘asymmetric interaction strengths’’ when interactions are neutral. This is conﬁrmed by null model simulations of bipartite networks, assuming that partners associate randomly in the absence of any specialization and any variation in the correspondence of biological traits between associated species (trait matching). Variation in the skewness of the frequency distribution fundamentally changes the outcome of network metrics. Therefore, interpretation of network metrics in terms of fundamental specialization and trait matching requires an appropriate control for such severe constraints imposed by information deﬁcits. When using an alternative approach that controls for these effects, most natural networks of mutualistic or antagonistic systems show a signiﬁcantly higher degree of reciprocal specialization (exclusiveness) than expected under neutral conditions. A higher exclusiveness is coherent with a tighter coevolution and suggests a lower ecological redundancy than implied by nested networks.},
	language = {en},
	number = {12},
	urldate = {2020-11-04},
	journal = {Ecology},
	author = {Blüthgen, Nico and Fründ, Jochen and Vázquez, Diego P. and Menzel, Florian},
	month = dec,
	year = {2008},
	pages = {3387--3399},
	file = {bluethgen_et_al_2008.pdf:/Users/bill/D/Zotero/storage/CNNXBMER/bluethgen_et_al_2008.pdf:application/pdf},
}

@article{chagnonThesePresenteeAu,
	title = {thèse présentée au {Département} de biologie en vue de l’obtention du grade de docteur ès sciences ({Ph}.{D}.)},
	language = {fr},
	author = {Chagnon, Pierre-Luc},
	pages = {256},
	file = {Chagnon_Pierre_Luc_PhD_2015.pdf:/Users/bill/D/Zotero/storage/2F3IDDCT/Chagnon_Pierre_Luc_PhD_2015.pdf:application/pdf},
}

@article{amitContributionBiodiversityEcosystem2011,
	title = {Contribution of biodiversity to ecosystem functioning: a non-equilibrium thermodynamic perspective: {Contribution} of biodiversity to ecosystem functioning: a non-equilibrium thermodynamic perspective},
	volume = {3},
	issn = {1674-6767},
	shorttitle = {Contribution of biodiversity to ecosystem functioning},
	url = {http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1227.2011.00071},
	doi = {10.3724/SP.J.1227.2011.00071},
	abstract = {Ecosystem stays far from thermodynamic equilibrium. Through the interactions among biotic and abiotic components, and encompassing physical environments, ecosystem forms a dissipative structure that allows it to dissipate energy continuously and thereby remains functional over time. Biotic regulation of energy and material fluxes in and out of the ecosystem allows it to maintain a homeostatic state which corresponds to a self-organized state emerged in a non-equilibrium thermodynamic system. While the associated self-organizational processes approach to homeostatic state, entropy (a measure of irreversibility) degrades and dissipation of energy increases. We propose here that at a homeostatic state of ecosystem, biodiversity which includes both phenotypic and functional diversity, attains optimal values. As long as biodiversity remains within its optimal range, the corresponding homeostatic state is maintained. However, while embedded environmental conditions fluctuate along the gradient of accelerating changes, phenotypic diversity and functional diversity contribute inversely to the associated self-organizing processes. Furthermore, an increase or decrease in biodiversity outside of its optimal range makes the ecosystem vulnerable to transition into a different state.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Journal of Arid Land},
	author = {Amit, Chakraborty and B Larry, Li},
	month = feb,
	year = {2011},
	pages = {71--74},
	file = {Contribution_of_biodiversity_to_ecosystem_function.pdf:/Users/bill/D/Zotero/storage/NK5EAG5F/Contribution_of_biodiversity_to_ecosystem_function.pdf:application/pdf},
}

@article{dormannMethodDetectingModules2014,
	title = {A method for detecting modules in quantitative bipartite networks},
	volume = {5},
	issn = {2041210X},
	url = {http://doi.wiley.com/10.1111/2041-210X.12139},
	doi = {10.1111/2041-210X.12139},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Dormann, Carsten F. and Strauss, Rouven},
	editor = {Peres-Neto, Pedro},
	month = jan,
	year = {2014},
	pages = {90--98},
	file = {Dormann_et_al-2014-Methods_in_Ecology_and_Evolution.pdf:/Users/bill/D/Zotero/storage/ER9JL476/Dormann_et_al-2014-Methods_in_Ecology_and_Evolution.pdf:application/pdf},
}

@article{rivera-hutinelEffectsSamplingCompleteness2012,
	title = {Effects of sampling completeness on the structure of plant–pollinator networks},
	volume = {93},
	issn = {0012-9658},
	url = {http://doi.wiley.com/10.1890/11-1803.1},
	doi = {10.1890/11-1803.1},
	abstract = {Plant–animal interaction networks provide important information on community organization. One of the most critical assumptions of network analysis is that the observed interaction patterns constitute an adequate sample of the set of interactions present in plant–animal communities. In spite of its importance, few studies have evaluated this assumption, and in consequence, there is no consensus on the sensitivity of network metrics to sampling methodological shortcomings. In this study we examined how variation in sampling completeness inﬂuences the estimation of six network metrics frequently used in the literature (connectance, nestedness, modularity, robustness to species loss, path length, and centralization). We analyzed data of 186 ﬂowering plants and 336 pollinator species in 10 networks from a forest-fragmented system in central Chile. Using species-based accumulation curves, we estimated the deviation of network metrics in undersampled communities with respect to exhaustively sampled communities and the effect of network size and sampling evenness on network metrics. Our results indicate that: (1) most metrics were affected by sampling completeness but differed in their sensitivity to sampling effort; (2) nestedness, modularity, and robustness to species loss were less inﬂuenced by insufﬁcient sampling than connectance, path length, and centralization; (3) robustness was mildly inﬂuenced by sampling evenness. These results caution studies that summarize information from databases with high, or unknown, heterogeneity in sampling effort per species and should stimulate researchers to report sampling intensity to standardize its effects in the search for broad patterns in plant–pollinator networks.},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {Ecology},
	author = {Rivera-Hutinel, A. and Bustamante, R. O. and Marín, V. H. and Medel, R.},
	month = jul,
	year = {2012},
	pages = {1593--1603},
	file = {Effects_of_sampling_completeness_on_the_structure_.pdf:/Users/bill/D/Zotero/storage/T3PZKRTH/Effects_of_sampling_completeness_on_the_structure_.pdf:application/pdf},
}

@article{macleodHOWSPECIESABUNDANCE,
	title = {{HOW} {DO} {SPECIES} {ABUNDANCE} {DISTRIBUTIONS} {INFLUENCE} {PLANT} – {POLLINATOR} {NETWORKS}?},
	language = {en},
	author = {MacLEOD, MOLLY KATHERINE},
	pages = {94},
	file = {ETD-2015-6873.pdf:/Users/bill/D/Zotero/storage/4JDA5E25/ETD-2015-6873.pdf:application/pdf},
}

@article{garciaBirdsEcologicalNetworks2016,
	title = {Birds in {Ecological} {Networks}: {Insights} from {Bird}-{Plant} {Mutualistic} {Interactions}},
	volume = {63},
	issn = {0570-7358, 2341-0825},
	shorttitle = {Birds in {Ecological} {Networks}},
	url = {http://www.bioone.org/doi/10.13157/arla.63.1.2016.rp7},
	doi = {10.13157/arla.63.1.2016.rp7},
	abstract = {SUMMAry.—research in ecological networks has developed impressively in recent years. A significant part of this growth has been achieved using networks to represent the complexity of mutualistic interactions between species of birds and plants, such as pollination and seed dispersal. Bird-plant networks are built from matrices whose cells account for the field-sampled magnitudes of interaction (e.g. the number of plant fruits consumed by birds) in bird-plant species pairs. the comparative study of mutualistic networks evidences three general patterns in network structure: they are highly heterogeneous (many species having just a few interactions, but a few species being highly connected), nested (with specialists interacting with subsets of species with which generalists interact) and composed of weak and asymmetric relationships between birds and plants. this type of structure emerges from a set of ecological and evolutionary mechanisms accounting for the probabilistic role of species abundances and the deterministic role of species traits, often constrained by species phylogenies. Although bearing structural generalities, bird-plant networks are variable in space and time at very different scales: from habitat to latitudinal and biogeographical gradients, and from seasonal to inter-annual contrasts. they are also highly sensitive to human impact, being especially affected by habitat loss and fragmentation, defaunation and biological invasions. further research on bird-plant mutualistic networks should: 1) apply wide conceptual frameworks which integrate the mechanisms of interaction and the responses of species to environmental gradients, 2) enlarge the ecological scale of networks across interaction types and animal groups, and 3) account for the ultimate functional (i.e. demographic) effects of trophic interactions.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Ardeola},
	author = {García, Daniel},
	month = jun,
	year = {2016},
	pages = {151--180},
	file = {Garcia2016_Ardeola.pdf:/Users/bill/D/Zotero/storage/RQNM4V48/Garcia2016_Ardeola.pdf:application/pdf},
}

@article{gibsonSamplingMethodInfluences2011,
	title = {Sampling method influences the structure of plant-pollinator networks},
	volume = {120},
	issn = {00301299},
	url = {http://doi.wiley.com/10.1111/j.1600-0706.2010.18927.x},
	doi = {10.1111/j.1600-0706.2010.18927.x},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Oikos},
	author = {Gibson, Rachel H. and Knott, Ben and Eberlein, Tim and Memmott, Jane},
	month = jun,
	year = {2011},
	pages = {822--831},
	file = {Gibson et al. Oikos 2011.pdf:/Users/bill/D/Zotero/storage/6QGGKE53/Gibson et al. Oikos 2011.pdf:application/pdf},
}

@article{joppaNestednessEcologicalNetworks,
	title = {On nestedness in ecological networks},
	abstract = {Questions: Are interaction patterns in species interaction networks different from what one expects by chance alone? In particular, are these networks nested – a pattern where resources taken by more specialized consumers form a proper subset of those taken by more generalized consumers? Organisms: Fifty-nine and 42 networks of mutualistic and host–parasitoid interactions, respectively. Analytical methods: For each network, the observed degree of nestedness is compared with the distribution of nestedness values derived from a collection of 1000 random networks. Those networks with nestedness values lower than 95\% of all random values are considered ‘unusually nested’. The analysis considers two different metrics of nestedness and five different network randomization algorithms, each of which differs in the ecological assumptions imposed.},
	language = {en},
	author = {Joppa, Lucas N and Montoya, José M and Solé, Richard and Sanderson, Jim and Pimm, Stuart L},
	pages = {12},
	file = {Montoya_evo_nes.pdf:/Users/bill/D/Zotero/storage/8WM4AQ9M/Montoya_evo_nes.pdf:application/pdf},
}

@article{staniczenkoGhostNestednessEcological2013,
	title = {The ghost of nestedness in ecological networks},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/ncomms2422},
	doi = {10.1038/ncomms2422},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Nature Communications},
	author = {Staniczenko, Phillip P. A. and Kopp, Jason C. and Allesina, Stefano},
	month = jun,
	year = {2013},
	pages = {1391},
	file = {ncomms2422-s1.pdf:/Users/bill/D/Zotero/storage/9LUFU7C8/ncomms2422-s1.pdf:application/pdf},
}

@article{frundSamplingBiasChallenge2016,
	title = {Sampling bias is a challenge for quantifying specialization and network structure: lessons from a quantitative niche model},
	volume = {125},
	issn = {00301299},
	shorttitle = {Sampling bias is a challenge for quantifying specialization and network structure},
	url = {http://doi.wiley.com/10.1111/oik.02256},
	doi = {10.1111/oik.02256},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Oikos},
	author = {Fründ, Jochen and McCann, Kevin S. and Williams, Neal M.},
	month = apr,
	year = {2016},
	pages = {502--513},
	file = {OIK_Frund et al proofs.pdf:/Users/bill/D/Zotero/storage/8EYNL3G5/OIK_Frund et al proofs.pdf:application/pdf},
}

@article{dormannIndicesGraphsNull2009,
	title = {Indices, {Graphs} and {Null} {Models}: {Analyzing} {Bipartite} {Ecological} {Networks}},
	volume = {2},
	issn = {18742130},
	shorttitle = {Indices, {Graphs} and {Null} {Models}},
	url = {http://benthamopen.com/ABSTRACT/TOECOLJ-2-1-7},
	doi = {10.2174/1874213000902010007},
	abstract = {Many analyses of ecological networks in recent years have introduced new indices to describe network properties. As a consequence, tens of indices are available to address similar questions, differing in specific detail, sensitivity in detecting the property in question, and robustness with respect to network size and sampling intensity. Furthermore, some indices merely reflect the number of species participating in a network, but not their interrelationship, requiring a null model approach. Here we introduce a new, free software calculating a large spectrum of network indices, visualizing bipartite networks and generating null models. We use this tool to explore the sensitivity of 26 network indices to network dimensions, sampling intensity and singleton observations. Based on observed data, we investigate the interrelationship of these indices, and show that they are highly correlated, and heavily influenced by network dimensions and connectance. Finally, we re-evaluate five common hypotheses about network properties, comparing 19 pollination networks with three differently complex null models: 1. The number of links per species (“degree”) follow (truncated) power law distributions. 2. Generalist pollinators interact with specialist plants, and vice versa (dependence asymmetry). 3. Ecological networks are nested. 4. Pollinators display complementarity, owing to specialization within the network. 5. Plant-pollinator networks are more robust to extinction than random networks. Our results indicate that while some hypotheses hold up against our null models, others are to a large extent understandable on the basis of network size, rather than ecological interrelationships. In particular, null model pattern of dependence asymmetry and robustness to extinction are opposite to what current network paradigms suggest. Our analysis, and the tools we provide, enables ecologists to readily contrast their findings with null model expectations for many different questions, thus separating statistical inevitability from ecological process.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {The Open Ecology Journal},
	author = {Dormann, Carsten F. and Frund, Jochen and Bluthgen, Nico and Gruber, Bernd},
	month = feb,
	year = {2009},
	pages = {7--24},
	file = {OpenEcolJ2009,2_7-24_open.pdf:/Users/bill/D/Zotero/storage/BHF2UX5H/OpenEcolJ2009,2_7-24_open.pdf:application/pdf},
}

@incollection{vacherLearningEcologicalNetworks2016,
	title = {Learning {Ecological} {Networks} from {Next}-{Generation} {Sequencing} {Data}},
	volume = {54},
	isbn = {978-0-08-100978-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065250415000331},
	abstract = {Species diversity, and the various interactions that occur between species, supports ecosystems functioning and benefit human societies. Monitoring the response of species interactions to human alterations of the environment is thus crucial for preserving ecosystems. Ecological networks are now the standard method for representing and simultaneously analyzing all the interactions between species. However, deciphering such networks requires considerable time and resources to observe and sample the organisms, to identify them at the species level and to characterize their interactions. Next-generation sequencing (NGS) techniques, combined with network learning and modelling, can help alleviate these constraints. They are essential for observing cryptic interactions involving microbial species, as well as short-term interactions such as those between predator and prey. Here, we present three case studies, in which species associations or interactions have been revealed with NGS. We then review several currently available statistical and machine-learning approaches that could be used for reconstructing networks of direct interactions between species, based on the NGS co-occurrence data. Future developments of these methods may allow us to discover and monitor species interactions cost-effectively, under various environmental conditions and within a replicated experimental design framework.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Advances in {Ecological} {Research}},
	publisher = {Elsevier},
	author = {Vacher, Corinne and Tamaddoni-Nezhad, Alireza and Kamenova, Stefaniya and Peyrard, Nathalie and Moalic, Yann and Sabbadin, Régis and Schwaller, Loïc and Chiquet, Julien and Smith, M. Alex and Vallance, Jessica and Fievet, Virgil and Jakuschkin, Boris and Bohan, David A.},
	year = {2016},
	doi = {10.1016/bs.aecr.2015.10.004},
	pages = {1--39},
	file = {Vacher_AER_2016.pdf:/Users/bill/D/Zotero/storage/AQGPU6HB/Vacher_AER_2016.pdf:application/pdf},
}

@inproceedings{kangHowExtractMeaningful2013a,
	address = {Singapore, Singapore},
	title = {How to extract meaningful shapes from noisy time-series subsequences?},
	isbn = {978-1-4673-5895-8},
	url = {http://ieeexplore.ieee.org/document/6597219/},
	doi = {10.1109/CIDM.2013.6597219},
	abstract = {A method for extracting and classifying shapes from noisy time series is proposed. The method consists of two steps. The ﬁrst step is to perform a noise test on each subsequence extracted from the series using a sliding window. All the subsequences recognised as noise are removed from further analysis, and the shapes are extracted from the remaining nonnoise subsequences. The second step is to cluster these extracted shapes. Although extracted from subsequences, these shapes form a non-overlapping set of time series subsequences and are hence amenable to meaningful clustering. The method is primarily designed for extracting and classifying shapes from very noisy real-world time series. Tests using artiﬁcial data with different levels of white noise and the red noise, and the real-world atmospheric turbulence data naturally characterised by strong red noise show that the method is able to correctly extract and cluster shapes from artiﬁcial data and that it has great potential for locating shapes in very noisy real-world time series.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2013 {IEEE} {Symposium} on {Computational} {Intelligence} and {Data} {Mining} ({CIDM})},
	publisher = {IEEE},
	author = {Kang, Yanfei and Smith-Miles, Kate and Belusic, Danijel},
	month = apr,
	year = {2013},
	keywords = {Curve Shapes Christy},
	pages = {65--72},
	file = {kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf:/Users/bill/D/Zotero/storage/6EQV8WTY/kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf:application/pdf},
}

@article{caruanaOverfittingNeuralNets,
	title = {Overﬁtting in {Neural} {Nets}: {Backpropagation}, {Conjugate} {Gradient}, and {Early} {Stopping}},
	abstract = {The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overﬁtting can vary signiﬁcantly in different regions of the model. Excess capacity allows better ﬁt to regions of high non-linearity, and backprop often avoids overﬁtting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overﬁts regions of low non-linearity when learning to ﬁt regions of high non-linearity.},
	language = {en},
	author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
	pages = {7},
	file = {10.1.1.21.1952.pdf:/Users/bill/D/Zotero/storage/M2E8UMYE/10.1.1.21.1952.pdf:application/pdf},
}

@article{caruanaHighPrecisionInformation,
	title = {High {Precision} {Information} {Extraction}},
	abstract = {Most fully automatic information extraction systems achieve less than 100\% extraction precision and recall. On real applications these parameters typically vary between 50\% to 95\%, depending on the extraction method and source data.},
	language = {en},
	author = {Caruana, Rich and Hodor, Paul G and Rosenberg, John},
	pages = {7},
	file = {10.1.1.37.6062.pdf:/Users/bill/D/Zotero/storage/8DMNJE3J/10.1.1.37.6062.pdf:application/pdf},
}

@article{caruanaAlgorithmsApplicationsMultitask,
	title = {Algorithms and {Applications} for {Multitask} {Learning}},
	abstract = {Multitask Learning is an inductive transfer method that improves generalization by using domain information implicit in the training signals of related tasks as an inductive bias. It does this by learning multiple tasks in parallel using a shared representation. Multitask transfer in connectionist nets has already been proven. But questions remain about how often training data for useful extra tasks will be available, and if multitask transfer will work in other learning methods. This paper argues that many real world problems present opportunities for multitask learning if they are not rst overly sanitized. We present eight prototypical applications of multitask transfer where the training signals for related tasks are available and can be leveraged. We also outline algorithms for multitask transfer in decision trees and k-nearest neighbor. We conclude that multitask transfer has broad utility.},
	language = {en},
	author = {Caruana, Rich},
	pages = {9},
	file = {10.1.1.52.975.pdf:/Users/bill/D/Zotero/storage/S9CJ4TW2/10.1.1.52.975.pdf:application/pdf},
}

@article{caruanaLearningManyRelated,
	title = {Learning {Many} {Related} {Tasks} at the {Same} {Time} {With} {Backpropagation}},
	abstract = {Hinton 6] proposed that generalization in arti cial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work 1] shows that the outputs of a backprop net can be used as inputs through which domainspeci c information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify ve mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.},
	language = {en},
	author = {Caruana, Rich},
	pages = {8},
	file = {10.1.1.54.6346.pdf:/Users/bill/D/Zotero/storage/QYLMK9ME/10.1.1.54.6346.pdf:application/pdf},
}

@article{caruanaUtshinagtFWeaotrukrBeEStetleerctaisonExtotrFaiOndutIpnuptusts,
	title = {Utshinagt {FWeaotrukrBe} {eStetleerctaisonExtotrFaiOndutIpnuptusts}},
	abstract = {In supervised learning there is usually a clear distinction between inputs and outputs {\textbar} inputs are what you measure, outputs are what you predict from those measurements. The distinction between inputs and outputs is not this simple. Previously, we demonstrated that on synthetic problems some input features are more useful when used as extra outputs than when used as inputs 6]. This paper shows the same e ect on a real problem, and presents a means of determining what features can be used as extra outputs. We show that the feature selection method devised by Koller and Sahami 11] can be used to select features to use as extra outputs, and that using some features as as extra outputs instead of as inputs yields better performance on the DNA splice-junction domain.},
	language = {en},
	author = {Caruana, Rich and de Sa, Virginia R},
	pages = {6},
	file = {10.1.1.57.4668.pdf:/Users/bill/D/Zotero/storage/WIL4SDY2/10.1.1.57.4668.pdf:application/pdf},
}

@inproceedings{sorokinaDetectingInterpretingVariable2009,
	address = {Miami, FL, USA},
	title = {Detecting and {Interpreting} {Variable} {Interactions} in {Observational} {Ornithology} {Data}},
	isbn = {978-1-4244-5384-9},
	url = {http://ieeexplore.ieee.org/document/5360526/},
	doi = {10.1109/ICDMW.2009.84},
	abstract = {In this paper we demonstrate a practical approach to interaction detection on real data describing the abundance of diﬀerent species of birds in the prairies east of the southern Rocky Mountains. This data is very noisy - predictive models built from this data perform only slightly better than baseline. Previous approaches for interaction detection, including recently proposed algorithm based on Additive Groves, might not work ideally on such noisy data for a number of reasons. We describe the issues that appear when working with such data sets and suggest solutions to them. We further demonstrate that with our improvements to the interaction detection algorithm it is possible to detect interactions between important features and the response function, even when the data is this noisy. In the end, we show and interpret the results of our analysis for several bird species.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2009 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops}},
	publisher = {IEEE},
	author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Hochachka, Wesley and Kelling, Steve},
	month = dec,
	year = {2009},
	pages = {64--69},
	file = {10.1.1.205.4865.pdf:/Users/bill/D/Zotero/storage/WNFX9V29/10.1.1.205.4865.pdf:application/pdf},
}

@inproceedings{louAccurateIntelligibleModels2013,
	address = {Chicago, Illinois, USA},
	title = {Accurate intelligible models with pairwise interactions},
	isbn = {978-1-4503-2174-7},
	url = {http://dl.acm.org/citation.cfm?doid=2487575.2487579},
	doi = {10.1145/2487575.2487579},
	abstract = {Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is signiﬁcantly less than more complex models that permit interactions.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '13},
	publisher = {ACM Press},
	author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
	year = {2013},
	pages = {623},
	file = {10.1.1.352.7682.pdf:/Users/bill/D/Zotero/storage/9VIWNAF8/10.1.1.352.7682.pdf:application/pdf},
}

@inproceedings{louIntelligibleModelsClassification2012,
	address = {Beijing, China},
	title = {Intelligible models for classification and regression},
	isbn = {978-1-4503-1462-6},
	url = {http://dl.acm.org/citation.cfm?doid=2339530.2339556},
	doi = {10.1145/2339530.2339556},
	abstract = {Complex models for regression and classiﬁcation have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '12},
	publisher = {ACM Press},
	author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes},
	year = {2012},
	pages = {150},
	file = {10.1.1.433.8241.pdf:/Users/bill/D/Zotero/storage/XIUZ7VCV/10.1.1.433.8241.pdf:application/pdf},
}

@inproceedings{caruanaEmpiricalEvaluationSupervised2008,
	address = {Helsinki, Finland},
	title = {An empirical evaluation of supervised learning in high dimensions},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390169},
	doi = {10.1145/1390156.1390169},
	abstract = {In this paper we perform an empirical evaluation of supervised learning on highdimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the eﬀect of increasing dimensionality on the performance of the learning algorithms. Our ﬁndings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
	year = {2008},
	pages = {96--103},
	file = {10.1.1.442.6328.pdf:/Users/bill/D/Zotero/storage/I57AGA5M/10.1.1.442.6328.pdf:application/pdf},
}

@inproceedings{nguyenClassificationPartialLabels2008,
	address = {Las Vegas, Nevada, USA},
	title = {Classification with partial labels},
	isbn = {978-1-60558-193-4},
	url = {http://dl.acm.org/citation.cfm?doid=1401890.1401958},
	doi = {10.1145/1401890.1401958},
	abstract = {In this paper, we address the problem of learning when some cases are fully labeled while other cases are only partially labeled, in the form of partial labels. Partial labels are represented as a set of possible labels for each training example, one of which is the correct label. We introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework. The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm regularized empirical risk using hinge loss. We also present an eﬃcient algorithm for classiﬁcation in the presence of partial labels. Experiments with diﬀerent data sets show that partial label information improves the performance of classiﬁcation when there is traditional fully-labeled data, and also yields reasonable performance in the absence of any fully labeled data.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceeding of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} 08},
	publisher = {ACM Press},
	author = {Nguyen, Nam and Caruana, Rich},
	year = {2008},
	pages = {551},
	file = {10.1.1.448.9323.pdf:/Users/bill/D/Zotero/storage/8AU75BX2/10.1.1.448.9323.pdf:application/pdf},
}

@incollection{munsonFeatureSelectionBiasVariance2009,
	address = {Berlin, Heidelberg},
	title = {On {Feature} {Selection}, {Bias}-{Variance}, and {Bagging}},
	volume = {5782},
	isbn = {978-3-642-04173-0 978-3-642-04174-7},
	url = {http://link.springer.com/10.1007/978-3-642-04174-7_10},
	abstract = {We examine the mechanism by which feature selection improves the accuracy of supervised learning. An empirical bias/variance analysis as feature selection progresses indicates that the most accurate feature set corresponds to the best bias-variance trade-oﬀ point for the learning algorithm. Often, this is not the point separating relevant from irrelevant features, but where increasing variance outweighs the gains from adding more (weakly) relevant features. In other words, feature selection can be viewed as a variance reduction method that trades oﬀ the beneﬁts of decreased variance (from the reduction in dimensionality) with the harm of increased bias (from eliminating some of the relevant features). If a variance reduction method like bagging is used, more (weakly) relevant features can be exploited and the most accurate feature set is usually larger. In many cases, the best performance is obtained by using all available features.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Munson, M. Arthur and Caruana, Rich},
	editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
	year = {2009},
	doi = {10.1007/978-3-642-04174-7_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {144--159},
	file = {54eaa0310cf2f7aa4d57f900.pdf:/Users/bill/D/Zotero/storage/PB2E9N27/54eaa0310cf2f7aa4d57f900.pdf:application/pdf},
}

@inproceedings{sorokinaDetectingStatisticalInteractions2008,
	address = {Helsinki, Finland},
	title = {Detecting statistical interactions with additive groves of trees},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390282},
	doi = {10.1145/1390156.1390282},
	abstract = {Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their eﬀects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
	year = {2008},
	pages = {1000--1007},
	file = {2008-ICML-Interactions.pdf:/Users/bill/D/Zotero/storage/5QYDL5JB/2008-ICML-Interactions.pdf:application/pdf},
}

@inproceedings{ganjisaffarDistributedTuningMachine2011,
	address = {San Diego, California},
	title = {Distributed tuning of machine learning algorithms using {MapReduce} {Clusters}},
	isbn = {978-1-4503-0844-1},
	url = {http://portal.acm.org/citation.cfm?doid=2002945.2002947},
	doi = {10.1145/2002945.2002947},
	abstract = {Obtaining the best accuracy in machine learning usually requires carefully tuning learning algorithm parameters for each problem. Parameter optimization is computationally challenging for learning methods with many hyperparameters. In this paper we show that MapReduce Clusters are particularly well suited for parallel parameter optimization. We use MapReduce to optimize regularization parameters for boosted trees and random forests on several text problems: three retrieval ranking problems and a Wikipedia vandalism problem. We show how model accuracy improves as a function of the percent of parameter space explored, that accuracy can be hurt by exploring parameter space too aggressively, and that there can be signiﬁcant interaction between parameters that appear to be independent. Our results suggest that MapReduce is a two-edged sword: it makes parameter optimization feasible on a massive scale that would have been unimaginable just a few years ago, but also creates a new opportunity for overﬁtting that can reduce accuracy and lead to inferior learning parameters.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the {Third} {Workshop} on {Large} {Scale} {Data} {Mining} {Theory} and {Applications} - {LDMTA} '11},
	publisher = {ACM Press},
	author = {Ganjisaffar, Yasser and Debeauvais, Thomas and Javanmardi, Sara and Caruana, Rich and Lopes, Cristina Videira},
	year = {2011},
	pages = {1--8},
	file = {2011-overtuning.pdf:/Users/bill/D/Zotero/storage/TDGX2QF3/2011-overtuning.pdf:application/pdf},
}

@article{hochachkaDataMiningDiscoveryPattern2007,
	title = {Data-{Mining} {Discovery} of {Pattern} and {Process} in {Ecological} {Systems}},
	volume = {71},
	issn = {0022-541X, 1937-2817},
	url = {http://www.bioone.org/perlserv/?request=get-abstract&doi=10.2193%2F2006-503},
	doi = {10.2193/2006-503},
	language = {en},
	number = {7},
	urldate = {2020-11-04},
	journal = {Journal of Wildlife Management},
	author = {Hochachka, Wesley M. and Caruana, Rich and Fink, Daniel and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Kelling, Steve},
	year = {2007},
	pages = {2427},
	file = {0046352320e6e55cbe000000.pdf:/Users/bill/D/Zotero/storage/FFG6KRWT/0046352320e6e55cbe000000.pdf:application/pdf},
}

@book{sugiyamaMachineLearningNonStationary2012,
	title = {Machine {Learning} in {Non}-{Stationary} {Environments}: {Introduction} to {Covariate} {Shift} {Adaptation}},
	isbn = {978-0-262-01709-1},
	shorttitle = {Machine {Learning} in {Non}-{Stationary} {Environments}},
	url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262017091.001.0001/upso-9780262017091},
	abstract = {Most active learning methods avoid model selection by training models of one type (SVMs, boosted trees, etc.) using one pre-deﬁned set of model hyperparameters. We propose an algorithm that actively samples data to simultaneously train a set of candidate models (different model types and/or different hyperparameters) and also select the best model from this set. The algorithm actively samples points for training that are most likely to improve the accuracy of the more promising candidate models, and also samples points for model selection—all samples count against the same labeling budget. This exposes a natural trade-off between the focused active sampling that is most effective for training models, and the unbiased sampling that is better for model selection. We empirically demonstrate on six test problems that this algorithm is nearly as effective as an active learning oracle that knows the optimal model in advance.},
	language = {en},
	urldate = {2020-11-04},
	publisher = {The MIT Press},
	author = {Sugiyama, Masashi and Kawanabe, Motoaki},
	month = mar,
	year = {2012},
	doi = {10.7551/mitpress/9780262017091.001.0001},
	file = {AAAI2014.pdf:/Users/bill/D/Zotero/storage/QBD3GZ7M/AAAI2014.pdf:application/pdf},
}

@article{jimmyDeepNetsReally,
	title = {Do {Deep} {Nets} {Really} {Need} to be {Deep}?},
	abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.},
	language = {en},
	author = {Jimmy, Lei and Caruana, Rich},
	pages = {9},
	file = {ba caruana 2014 - do deep nets really need to be deep - GUPPY.pdf:/Users/bill/D/Zotero/storage/5TB4NK2A/ba caruana 2014 - do deep nets really need to be deep - GUPPY.pdf:application/pdf},
}

@inproceedings{ganjisaffarBaggingGradientboostedTrees2011,
	address = {Beijing, China},
	title = {Bagging gradient-boosted trees for high precision, low variance ranking models},
	isbn = {978-1-4503-0757-4},
	url = {http://portal.acm.org/citation.cfm?doid=2009916.2009932},
	doi = {10.1145/2009916.2009932},
	abstract = {Recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks. In Learning-to-rank, boosted models such as RankBoost and LambdaMART have been shown to be among the best performing learning methods based on evaluations on public data sets. In this paper, we show how the combination of bagging as a variance reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking models. We perform thousands of parameter tuning experiments for LambdaMART to achieve a high precision boosting model. Then we show that a bagged ensemble of such LambdaMART boosted models results in higher accuracy ranking models while also reducing variance as much as 50\%. We report our results on three public learning-to-rank data sets using four metrics. Bagged LamdbaMART outperforms all previously reported results on ten of the twelve comparisons, and bagged LambdaMART outperforms non-bagged LambdaMART on all twelve comparisons. For example, wrapping bagging around LambdaMART increases NDCG@1 from 0.4137 to 0.4200 on the MQ2007 data set; the best prior results in the literature for this data set is 0.4134 by RankBoost.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on {Research} and development in {Information} - {SIGIR} '11},
	publisher = {ACM Press},
	author = {Ganjisaffar, Yasser and Caruana, Rich and Lopes, Cristina Videira},
	year = {2011},
	pages = {85},
	file = {bagging_lmbamart_jforests.pdf:/Users/bill/D/Zotero/storage/5AJIZJB6/bagging_lmbamart_jforests.pdf:application/pdf},
}

@incollection{balujaRemovingGeneticsStandard1995,
	title = {Removing the {Genetics} from the {Standard} {Genetic} {Algorithm}},
	isbn = {978-1-55860-377-6},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603776500141},
	abstract = {We present an abstraction of the genetic algorithm (GA), termed population-based incremental learning (PBIL), that explicitly maintains the statistics contained in a GA’s population, but which abstracts away the crossover operator and redeﬁnes the role of the population. This results in PBIL being simpler, both computationally and theoretically, than the GA. Empirical results reported elsewhere show that PBIL is faster and more effective than the GA on a large set of commonly used benchmark problems. Here we present results on a problem custom designed to beneﬁt both from the GA’s crossover operator and from its use of a population. The results show that PBIL performs as well as, or better than, GAs carefully tuned to do well on this problem. This suggests that even on problems custom designed for GAs, much of the power of the GA may derive from the statistics maintained implicitly in its population, and not from the population itself nor from the crossover operator.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning} {Proceedings} 1995},
	publisher = {Elsevier},
	author = {Baluja, Shumeet and Caruana, Rich},
	year = {1995},
	doi = {10.1016/B978-1-55860-377-6.50014-1},
	pages = {38--46},
	file = {baluja caruana 1995 - removing the genetics from the standard genetic algorithm.pdf:/Users/bill/D/Zotero/storage/QEM3AD9M/baluja caruana 1995 - removing the genetics from the standard genetic algorithm.pdf:application/pdf},
}

@article{caruanaMiningCitizenScience,
	title = {Mining {Citizen} {Science} {Data} to {Predict} {Prevalence} of {Wild} {Bird} {Species}},
	abstract = {The Cornell Laboratory of Ornithology’s mission is to interpret and conserve the earth’s biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and potentially are very noisy. The ecologists are interested in identifying which features have the strongest eﬀect on the distribution and abundance of bird species as well as describing the forms of these relationships. We show how data mining can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.},
	language = {en},
	author = {Caruana, Rich and Elhawary, Mohamed and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Fink, Daniel and Hochachka, Wesley M and Kelling, Steve},
	pages = {7},
	file = {BirdMining.pdf:/Users/bill/D/Zotero/storage/254LDBPR/BirdMining.pdf:application/pdf},
}

@inproceedings{niculescu-mizilPredictingGoodProbabilities2005,
	address = {Bonn, Germany},
	title = {Predicting good probabilities with supervised learning},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102430},
	doi = {10.1145/1102351.1102430},
	abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year = {2005},
	pages = {625--632},
	file = {calibration.icml05.crc.rev3.pdf:/Users/bill/D/Zotero/storage/YRXHYUDM/calibration.icml05.crc.rev3.pdf:application/pdf},
}

@article{caruanaLearningImbalancedData,
	title = {Learning from {Imbalanced} {Data}: {Rank} {Metrics} and {Extra} {Tasks}},
	abstract = {Imbalanceddata creates two problemsfor machinelearning. First, evenif the training set is large, the samplesize of smallerclasses maybe small. Learningaccurate modelsfromsmall samples is hard.Multitasklearningis onewayto learn moreaccurate modelsfromsmall samplesthat is particularlywellsuited to imbalanceddata. Asecondproblemwhenlearning fromimbalanceddata is that the usualerror metrics(e.g., accuracyor squarederror) causelearningto pay moreattention to large classes than to smallclasses. This problemcan be mitigatedby careful selection of the error metric. Wefind rankbasederror metrics often performbetter whenan importantclass is under-represented.},
	language = {en},
	author = {Caruana, Rich},
	pages = {7},
	file = {caruana 2000 - learning from imbalanced data - rank metrics and extra tasks - GUPPY.pdf:/Users/bill/D/Zotero/storage/5JT5HRCZ/caruana 2000 - learning from imbalanced data - rank metrics and extra tasks - GUPPY.pdf:application/pdf},
}

@article{caruanaPromotingPoorFeatures,
	title = {Promoting {Poor} {Features} to {Supervisors}: {Some} {Inputs} {Work} {Better} as {Outputs}},
	abstract = {In supervised learning there is usually a clear distinction between inputs and outputs {\textbar} inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classi cation problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.},
	language = {en},
	author = {Caruana, Rich and de Sa, Virginia R},
	pages = {7},
	file = {caruana et al 1997 - promoting poor features to supervisors - some inputs work better as outpus - MULTITASK.pdf:/Users/bill/D/Zotero/storage/FIFIEYT6/caruana et al 1997 - promoting poor features to supervisors - some inputs work better as outpus - MULTITASK.pdf:application/pdf},
}

@inproceedings{caruanaGettingMostOut2006,
	address = {Hong Kong, China},
	title = {Getting the {Most} {Out} of {Ensemble} {Selection}},
	url = {http://ieeexplore.ieee.org/document/4053111/},
	doi = {10.1109/ICDM.2006.76},
	abstract = {We investigate four previously unexplored aspects of ensemble selection, a procedure for building ensembles of classiﬁers. First we test whether adjusting model predictions to put them on a canonical scale makes the ensembles more effective. Second, we explore the performance of ensemble selection when different amounts of data are available for ensemble hillclimbing. Third, we quantify the beneﬁt of ensemble selection’s ability to optimize to arbitrary metrics. Fourth, we study the performance impact of pruning the number of models available for ensemble selection. Based on our results we present improved ensemble selection methods that double the beneﬁt of the original method.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Sixth {International} {Conference} on {Data} {Mining} ({ICDM}'06)},
	publisher = {IEEE},
	author = {Caruana, Rich and Munson, Art and Niculescu-Mizil, Alexandru},
	month = dec,
	year = {2006},
	note = {ISSN: 1550-4786},
	pages = {828--833},
	file = {caruana et al 2006 or 2007 - Getting the Most Out of Ensemble Selection- GUPPY - ENSEMBLES.pdf:/Users/bill/D/Zotero/storage/ZM9AUJAF/caruana et al 2006 or 2007 - Getting the Most Out of Ensemble Selection- GUPPY - ENSEMBLES.pdf:application/pdf},
}

@article{caruanaBenefittingVariablesThat,
	title = {Beneﬁtting from the {Variables} that {Variable} {Selection} {Discards}},
	abstract = {In supervised learning variable selection is used to ﬁnd a subset of the available inputs that accurately predict the output. This paper shows that some of the variables that variable selection discards can beneﬁcially be used as extra outputs for inductive transfer. Using discarded input variables as extra outputs forces the model to learn mappings from the variables that were selected as inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available to the model that is being trained on the main output, often resulting in improved performance on that main output.},
	language = {en},
	author = {Caruana, Rich},
	pages = {20},
	file = {CaruanaS03.pdf:/Users/bill/D/Zotero/storage/86PXUIE7/CaruanaS03.pdf:application/pdf},
}

@article{guyonUnsupervisedTransferLearning,
	title = {Unsupervised and {Transfer} {Learning}},
	language = {en},
	author = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel and Talbot, Nicola},
	pages = {324},
	file = {CiML-v7-book.pdf:/Users/bill/D/Zotero/storage/24NT83PH/CiML-v7-book.pdf:application/pdf},
}

@article{caruanaHowUsefulRelevance,
	title = {How {Useful} is {Relevance}?},
	abstract = {Eliminating irrelevant attributes prior to induction boosts the performanceof manylearning algorithms. Relevance,however,is no guarantee of usefulness to a particular learner. Wetest two methods of finding relevant attributes, FOCUS and RELIEF,to see howthe attributes they select performwith ID3/C4.5on two learning problems from a calendar scheduling domain. A more direct attribute selection procedure,hillclimbing in attribute space, finds superior attribute sets.},
	language = {en},
	author = {Caruana, Rich and Freitag, Dayne},
	pages = {5},
	file = {FS94-02-007.pdf:/Users/bill/D/Zotero/storage/T3STMZBX/FS94-02-007.pdf:application/pdf},
}

@incollection{sorokinaAdditiveGrovesRegression2007,
	address = {Berlin, Heidelberg},
	title = {Additive {Groves} of {Regression} {Trees}},
	volume = {4701},
	isbn = {978-3-540-74957-8 978-3-540-74958-5},
	url = {http://link.springer.com/10.1007/978-3-540-74958-5_31},
	abstract = {We present a new regression algorithm called Additive Groves and show empirically that it is superior in performance to a number of other established regression methods. A single Grove is an additive model containing a small number of large trees. Trees added to a Grove are trained on the residual error of other trees already in the model. We begin the training process with a single small tree and gradually increase both the number of trees in the Grove and their size. This procedure ensures that the resulting model captures the additive structure of the response. A single Grove may still overﬁt to the training set, so we further decrease the variance of the ﬁnal predictions with bagging. We show that in addition to exhibiting superior performance on a suite of regression test problems, Additive Groves are very resistant to overﬁtting.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning}: {ECML} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek},
	editor = {Kok, Joost N. and Koronacki, Jacek and Mantaras, Raomon Lopez de and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-74958-5_31},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {323--334},
	file = {groves.pdf:/Users/bill/D/Zotero/storage/8366VP5Y/groves.pdf:application/pdf},
}

@article{niculescu-mizilLearningStructureRelated,
	title = {Learning the {Structure} of {Related} {Tasks}},
	abstract = {We consider the problem of learning Bayes Net structures for related tasks. We present a formalism for learning related Bayes Net structures that takes advantage of the similarity between tasks by biasing toward learning similar structures for each task. Heuristic search is used to ﬁnd a high scoring set of structures (one for each task), where the score for a set of structures is computed in a principled way. Experiments on synthetic problems generated from the ALARM and INSURANCE networks show that learning the structures for related tasks using the proposed method yields better results than learning the structures independently.},
	language = {en},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	pages = {10},
	file = {MizilCaruana.pdf:/Users/bill/D/Zotero/storage/55QH5L5Q/MizilCaruana.pdf:application/pdf},
}

@article{caruanaCaseBasedExplanationNonCaseBased,
	title = {Case-{Based} {Explanation} of {Non}-{Case}-{Based} {Learning} {Methods}},
	language = {en},
	author = {Caruana, Rich and Kangarloo, Hooshang and David, John and Dionisio, N and Sinha, Usha and Johnson, David},
	pages = {4},
	file = {procamiasymp00004-0249.pdf:/Users/bill/D/Zotero/storage/WR8UEHVK/procamiasymp00004-0249.pdf:application/pdf},
}

@article{shaparenkoIdentifyingTemporalPatterns,
	title = {Identifying {Temporal} {Patterns} and {Key} {Players} in {Document} {Collections}},
	abstract = {This paper considers the problem of analyzing the development of a document collection over time without requiring meaningful citation data. Given a collection of timestamped documents, we formulate and explore the following two questions. First, what are the main topics and how do these topics develop over time? Second, to gain insight into the dynamics driving this development, what are the documents and who are the authors that are most inﬂuential in this process? Unlike prior work in citation analysis, we propose methods addressing these questions without requiring the availability of citation data. The methods use only the text of the documents as input. Consequentially, they are applicable to a much wider range of document collections (email, blogs, etc.), most of which lack meaningful citation data. We evaluate our methods on the proceedings of the Neural Information Processing Systems (NIPS) conference. Even with the preliminary methods that we implemented, the results show that the methods are effective and that addressing the questions based on the text alone is feasible. In fact, the text-based methods sometimes even identify inﬂuential papers that are missed by citation analysis.},
	language = {en},
	author = {Shaparenko, Benyah and Caruana, Rich and Gehrke, Johannes and Joachims, Thorsten},
	pages = {10},
	file = {shaparenko_etal_05a.pdf:/Users/bill/D/Zotero/storage/JP5DUR3R/shaparenko_etal_05a.pdf:application/pdf},
}

@incollection{skalakClassifierLossMetric2007,
	address = {Berlin, Heidelberg},
	title = {Classifier {Loss} {Under} {Metric} {Uncertainty}},
	volume = {4701},
	isbn = {978-3-540-74957-8 978-3-540-74958-5},
	url = {http://link.springer.com/10.1007/978-3-540-74958-5_30},
	abstract = {Classiﬁers that are deployed in the ﬁeld can be used and evaluated in ways that were not anticipated when the model was trained. The ultimate evaluation metric may not have been known to the modeler at training time, additional performance criteria may have been added, the evaluation metric may have changed over time, or the real-world evaluation procedure may have been impossible to simulate. But unforeseen ways of measuring model utility can degrade performance. Our objective is to provide experimental support for modelers who face potential “cross-metric” performance deterioration. First, to identify model-selection metrics that lead to stronger cross-metric performance, we characterize the expected loss where the selection metric is held ﬁxed and the evaluation metric is varied. Second, we show that the number of data points evaluated by a selection metric has a substantial effect on the optimal evaluation. In trying to address both these issues, we hypothesize that whether classiﬁers are calibrated to output probabilities may inﬂuence these issues. In our consideration of the role of calibration, we show that our experiments demonstrate that cross-entropy is the highestperforming selection metric where little data is available for selection. With these experiments, modelers may be in a better position to choose selection metrics that are robust where it is uncertain what evaluation metric will be applied.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning}: {ECML} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Skalak, David B. and Niculescu-Mizil, Alexandru and Caruana, Rich},
	editor = {Kok, Joost N. and Koronacki, Jacek and Mantaras, Raomon Lopez de and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-74958-5_30},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {310--322},
	file = {skalak ... ccaruana 2007 - classifier loss under metric uncertainty - GUPPY - UNCERTAINTY - ENSEMBLE.pdf:/Users/bill/D/Zotero/storage/EVYMTSAG/skalak ... ccaruana 2007 - classifier loss under metric uncertainty - GUPPY - UNCERTAINTY - ENSEMBLE.pdf:application/pdf},
}

@article{ipekEfficientArchitecturalDesign2008,
	title = {Efficient architectural design space exploration via predictive modeling},
	volume = {4},
	issn = {1544-3566, 1544-3973},
	url = {https://dl.acm.org/doi/10.1145/1328195.1328196},
	doi = {10.1145/1328195.1328196},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Ipek, Engin and McKee, Sally A. and Singh, Karan and Caruana, Rich and Supinski, Bronis R. de and Schulz, Martin},
	month = jan,
	year = {2008},
	pages = {1--34},
	file = {taco08.pdf:/Users/bill/D/Zotero/storage/ZJMVE686/taco08.pdf:application/pdf},
}

@article{munsonClusterEnsemblesNetwork,
	title = {Cluster {Ensembles} for {Network} {Anomaly} {Detection}},
	abstract = {Cluster ensembles aim to ﬁnd better, more natural clusterings by combining multiple clusterings. We apply ensemble clustering to anomaly detection, hypothesizing that multiple views of the data will improve the detection of attacks. Each clustering rates how anomalous a point is; ratings are combined by averaging or taking either the minimum, the maximum, or median score. The evaluation shows that taking the median prediction from the cluster ensemble results in better performance than single clusterings. Surprisingly, averaging the individual predictions a) leads to worse performance than that of individual clusterings, and b) performs identically to taking the minimum prediction from the ensemble. This counter-intuitive result stems from asymmetric prediction distributions.},
	language = {en},
	author = {Munson, Art and Caruana, Rich},
	pages = {9},
	file = {TR2006-2047.pdf:/Users/bill/D/Zotero/storage/SV9W36KE/TR2006-2047.pdf:application/pdf},
}

@article{caruanaObtainingCalibratedProbabilities,
	title = {Obtaining {Calibrated} {Probabilities} from {Boosting}},
	abstract = {Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss instead of the usual exponential loss. Experiments show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, signiﬁcantly improve the probabilities predicted by both boosted stumps and boosted trees. After calibration, boosted full decision trees predict better probabilities than other learning methods such as SVMs, neural nets, bagged decision trees, and KNNs, even after these methods are calibrated.},
	language = {en},
	author = {Caruana, Alexandru Niculescu-Mizil Rich},
	pages = {6},
	file = {WS07-05-006.pdf:/Users/bill/D/Zotero/storage/UCFVCUN6/WS07-05-006.pdf:application/pdf},
}

@article{chenSpatialAutocorrelationApproaches2016,
	title = {Spatial {Autocorrelation} {Approaches} to {Testing} {Residuals} from {Least} {Squares} {Regression}},
	volume = {11},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0146865},
	doi = {10.1371/journal.pone.0146865},
	abstract = {In statistics, the Durbin-Watson test is always employed to detect the presence of serial correlation of residuals from a least squares regression analysis. However, the Durbin-Watson statistic is only suitable for ordered time or spatial series. If the variables comprise cross-sectional data coming from spatial random sampling, the Durbin-Watson will be ineffectual because the value of Durbin-Watson’s statistic depends on the sequences of data point arrangement. Based on the ideas from spatial autocorrelation, this paper presents two new statistics for testing serial correlation of residuals from least squares regression based on spatial samples. By analogy with the new form of Moran’s index, an autocorrelation coefficient is defined with a standardized residual vector and a normalized spatial weight matrix. Then on the analogy of the Durbin-Watson statistic, a serial correlation index is constructed. As a case, the two statistics are applied to the spatial sample of 29 China’s regions. These results show that the new spatial autocorrelation model can be used to test the serial correlation of residuals from regression analysis. In practice, the new statistics can make up for the deficiency of the Durbin-Watson test.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {PLOS ONE},
	author = {Chen, Yanguang},
	editor = {Schumann, Guy J-P.},
	month = jan,
	year = {2016},
	pages = {e0146865},
	file = {1503.04407.pdf:/Users/bill/D/Zotero/storage/X96U3K4K/1503.04407.pdf:application/pdf},
}

@article{paredesLectureMakingRegression,
	title = {Lecture 0 - {Making} {Regression} {Sense}   (from {Mostly} {Harmless} {Econometrics})},
	language = {en},
	author = {Paredes, Dusan},
	pages = {34},
	file = {d51cf1_2eef13b1432afc1f6b6d30d29c18ed63.pdf:/Users/bill/D/Zotero/storage/WQ443PL6/d51cf1_2eef13b1432afc1f6b6d30d29c18ed63.pdf:application/pdf},
}

@article{paredesLectureIntroductionSpatial,
	title = {Lecture 1 - {Introduction} to the {Spatial} {Dependence}   (from {LeSage} and {Peace}, 2009)},
	language = {en},
	author = {Paredes, Dusan},
	pages = {29},
	file = {d51cf1_106e5f17e7191445064c6ef65e8054f2.pdf:/Users/bill/D/Zotero/storage/EQYPNTXP/d51cf1_106e5f17e7191445064c6ef65e8054f2.pdf:application/pdf},
}

@article{paredesLectureExploratorySpatial,
	title = {Lecture 3 - {Exploratory} {Spatial} {Data} {Analysis}},
	language = {en},
	author = {Paredes, Dusan},
	pages = {33},
	file = {d51cf1_4551851a74ebef487541c485b31c8de9.pdf:/Users/bill/D/Zotero/storage/ZR6JILIQ/d51cf1_4551851a74ebef487541c485b31c8de9.pdf:application/pdf},
}

@article{paredesLectureMotivatingInterpreting,
	title = {Lecture 2 - {Motivating} and {Interpreting} {Spatial} {Econometric} {Models}  (from {LeSage} and {Peace}, 2009)},
	language = {en},
	author = {Paredes, Dusan},
	pages = {26},
	file = {d51cf1_a47ea74e132b7ceff52032ce20779bda.pdf:/Users/bill/D/Zotero/storage/W8KF2KGI/d51cf1_a47ea74e132b7ceff52032ce20779bda.pdf:application/pdf},
}

@article{elhorstAppliedSpatialEconometrics2010,
	title = {Applied {Spatial} {Econometrics}: {Raising} the {Bar}},
	volume = {5},
	issn = {1742-1772, 1742-1780},
	shorttitle = {Applied {Spatial} {Econometrics}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/17421770903541772},
	doi = {10.1080/17421770903541772},
	abstract = {This paper places the key issues and implications of the new ‘introductory’ book on spatial econometrics by James LeSage \& Kelley Pace (2009) in a broader perspective: the argument in favour of the spatial Durbin model, the use of indirect effects as a more valid basis for testing whether spatial spillovers are significant, the use of Bayesian posterior model probabilities to determine which spatial weights matrix best describes the data, and the book’s contribution to the literature on spatiotemporal models. The main conclusion is that the state of the art of applied spatial econometrics has taken a step change with the publication of this book.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Spatial Economic Analysis},
	author = {Elhorst, J. Paul},
	month = mar,
	year = {2010},
	pages = {9--28},
	file = {Elhorst_SEA2010.pdf:/Users/bill/D/Zotero/storage/FVFZPUK7/Elhorst_SEA2010.pdf:application/pdf},
}

@article{franzeseTestingSpatialAutoregressiveLag,
	title = {Testing for {Spatial}-{Autoregressive} {Lag} versus ({Unobserved}) {Spatially} {Correlated} {Error}-{Components}},
	language = {en},
	author = {Franzese, Robert J and Hays, Jude C},
	pages = {26},
	file = {Franzese_Hays_IOWA14.pdf:/Users/bill/D/Zotero/storage/W5ZQTDRZ/Franzese_Hays_IOWA14.pdf:application/pdf},
}

@book{goodchildSpatialAutocorrelation1986,
	address = {Norwich},
	series = {{CATMOG}},
	title = {Spatial autocorrelation},
	isbn = {978-0-86094-223-8},
	language = {en},
	number = {47},
	publisher = {Geo Books},
	author = {Goodchild, Michael F.},
	year = {1986},
	keywords = {Geography, Spatial analysis (Statistics), Statistical methods},
	file = {goodchild 1988 - spatial-aurocorrelation.pdf:/Users/bill/D/Zotero/storage/XZINC9QR/goodchild 1988 - spatial-aurocorrelation.pdf:application/pdf},
}

@article{lesageIntroductionSpatialEconometrics2009,
	title = {Introduction to {Spatial} {Econometrics}},
	language = {en},
	author = {LeSage, James and Pace, R Kelley},
	year = {2009},
	pages = {331},
	file = {lesage pace 2009 - introduction to spatial econometrics - ECONOMICS - ECONOMETRICS - SPATIAL - BOOK.pdf:/Users/bill/D/Zotero/storage/F6A2VEY5/lesage pace 2009 - introduction to spatial econometrics - ECONOMICS - ECONOMETRICS - SPATIAL - BOOK.pdf:application/pdf},
}

@article{lesageWhatRegionalScientists2014,
	title = {What {Regional} {Scientists} {Need} to {Know} {About} {Spatial} {Econometrics}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2420725},
	doi = {10.2139/ssrn.2420725},
	abstract = {Regional scientists frequently work with regression relationships involving sample data that is spatial in nature. For example, hedonic house-price regressions relate selling prices of houses located at points in space to characteristics of the homes as well as neighborhood characteristics. Migration, commodity, and transportation ﬂow models relate the size ﬂows between origin and destination regions to the distance between origin and destination as well as characteristics of both origin and destination regions. Regional growth regressions relate growth rates of a region to past period ownand nearby-region resource inputs used in production.},
	language = {en},
	urldate = {2020-11-04},
	journal = {SSRN Electronic Journal},
	author = {LeSage, James P.},
	year = {2014},
	file = {Lesage2014.pdf:/Users/bill/D/Zotero/storage/2WHZKUI3/Lesage2014.pdf:application/pdf},
}

@inproceedings{jiangFocalTestBasedSpatialDecision2013,
	address = {Dallas, TX, USA},
	title = {Focal-{Test}-{Based} {Spatial} {Decision} {Tree} {Learning}: {A} {Summary} of {Results}},
	isbn = {978-0-7695-5108-1},
	shorttitle = {Focal-{Test}-{Based} {Spatial} {Decision} {Tree} {Learning}},
	url = {http://ieeexplore.ieee.org/document/6729516/},
	doi = {10.1109/ICDM.2013.96},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Jiang, Zhe and Shekhar, Shashi and Zhou, Xun and Knight, Joseph and Corcoran, Jennifer},
	month = dec,
	year = {2013},
	pages = {320--329},
	file = {Publication_4.pdf:/Users/bill/D/Zotero/storage/XLX7GD46/Publication_4.pdf:application/pdf},
}

@article{vitonNotesSpatialEconometric,
	title = {Notes on {Spatial} {Econometric} {Models}},
	language = {en},
	author = {Viton, Philip A},
	pages = {23},
	file = {spatial.pdf:/Users/bill/D/Zotero/storage/5LHMUULU/spatial.pdf:application/pdf},
}

@article{zhukovAppliedSpatialStatistics,
	title = {Applied {Spatial} {Statistics} in {R}, {Section} 6},
	language = {en},
	author = {Zhukov, Yuri M},
	pages = {56},
	file = {Spatial6.pdf:/Users/bill/D/Zotero/storage/MD4KD5MI/Spatial6.pdf:application/pdf},
}

@article{glassEconomicCaseSpatial,
	title = {The {Economic} {Case} for the {Spatial} {Error} {Model} with an {Application} to {State} {Vehicle} {Usage} in the {U}.{S}.},
	abstract = {LeSage and Pace (2009) make an econometric case for the spatial Durbin model over, among others, the spatial error model. We make an economic case for the spatial error model because it captures spatial dependence more fully (i.e. beyond that which can be attributed to the dependent variables in neighboring units). Also, when faced with the choice between aggregate or disaggregated data the spatial error model or a related model (e.g. the seemingly unrelated spatial error model) should be …tted. This is to ensure that Wald tests of whole sets of coe¢ cients against one another to establish if disaggregation is necessary are not invalidated. To illustrate the economic case which we make we extend the literature on the determinants of vehicle usage by modelling the spatial dependence of state travel for the U.S. over the period 1980 2008. On the basis of Wald tests, aggregate data on state vehicle usage is progressively disaggregated and spatial error models for travel on all twelve types of highway are …tted. In all cases the spatial dependence in the spatial error models is greater than or approximately equal to that in the spatial lag models, con…rming that the former does capture any additional sources of spatial dependence.},
	language = {en},
	author = {Glass, Anthony J and Kenjegalieva, Karligash and Sickles, Robin},
	pages = {31},
	file = {the economic case for the spatial error model.pdf:/Users/bill/D/Zotero/storage/4Z29GQJI/the economic case for the spatial error model.pdf:application/pdf},
}

@article{saltelliWhatWrongEvidence2017,
	title = {What is wrong with evidence based policy, and how can it be improved?},
	volume = {91},
	issn = {00163287},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016328717300472},
	doi = {10.1016/j.futures.2016.11.012},
	language = {en},
	urldate = {2020-11-04},
	journal = {Futures},
	author = {Saltelli, Andrea and Giampietro, Mario},
	month = aug,
	year = {2017},
	pages = {62--71},
	file = {saltelli giampietro 2017 - What is wrong with evidence based policy, and how can it be improved - ANNO - OPTISEVIL - SENSITIVITY - UNCERTAINTY - DISCOUNTING - ECONOMICS - UNKNOWN UNKNOWNS.pdf:/Users/bill/D/Zotero/storage/UDPG98DE/saltelli giampietro 2017 - What is wrong with evidence based policy, and how can it be improved - ANNO - OPTISEVIL - SENSITIVITY - UNCERTAINTY - DISCOUNTING - ECONOMICS - UNKNOWN UNKNOWNS.pdf:application/pdf},
}

@article{duffySimplicityRobustLight2020,
	title = {The simplicity of robust light harvesting},
	volume = {368},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.abc8063},
	doi = {10.1126/science.abc8063},
	language = {en},
	number = {6498},
	urldate = {2020-11-04},
	journal = {Science},
	author = {Duffy, Christopher D. P.},
	month = jun,
	year = {2020},
	pages = {1427--1428},
	file = {duffy 2020 - The simplicity of robustlight harvesting - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/V5W6Y6D4/duffy 2020 - The simplicity of robustlight harvesting - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf:application/pdf},
}

@article{arpQuietingNoisyAntenna2020,
	title = {Quieting a noisy antenna reproduces photosynthetic light-harvesting spectra},
	volume = {368},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aba6630},
	doi = {10.1126/science.aba6630},
	abstract = {Photosynthesis achieves near unity light-harvesting quantum efficiency yet it remains unknown whether there exists a fundamental organizing principle giving rise to robust light harvesting in the presence of dynamic light conditions and noisy physiological environments. Here, we present a noise-canceling network model that relates noisy physiological conditions, power conversion efficiency, and the resulting absorption spectra of photosynthetic organisms. Using light conditions in full solar exposure, light filtered by oxygenic phototrophs, and light filtered under seawater, we derived optimal absorption characteristics for efficient solar power conversion. We show how light-harvesting antennae can be tuned to maximize power conversion efficiency by minimizing excitation noise, thus providing a unified theoretical basis for the observed wavelength dependence of absorption in green plants, purple bacteria, and green sulfur bacteria.},
	language = {en},
	number = {6498},
	urldate = {2020-11-04},
	journal = {Science},
	author = {Arp, Trevor B. and Kistner-Morris, Jed and Aji, Vivek and Cogdell, Richard J. and van Grondelle, Rienk and Gabor, Nathaniel M.},
	month = jun,
	year = {2020},
	pages = {1490--1495},
	file = {arp et al 2020 - Quieting a noisy antenna reproduces photosynthetic light-harvesting spectra - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/I63MWLUC/arp et al 2020 - Quieting a noisy antenna reproduces photosynthetic light-harvesting spectra - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf:application/pdf},
}

@article{ignizio1999eo,
	title = {Illusions of optimality},
	volume = {31},
	issn = {0305-215X, 1029-0273},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03052159908941395},
	doi = {10.1080/03052159908941395},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Engineering Optimization},
	author = {Ignizio, James P.},
	month = aug,
	year = {1999},
	pages = {749--761},
	file = {ignizio 1999 - illusions of optimality - STABILITY - OPTISEVIL - EFs - BDPG - ROBUST OPTIMIZATION - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/ICEVK2AI/ignizio 1999 - illusions of optimality - STABILITY - OPTISEVIL - EFs - BDPG - ROBUST OPTIMIZATION - RESERVE SELECTION.pdf:application/pdf},
}

@article{colditzResilienceFarmAnimals2016,
	title = {Resilience in farm animals: biology, management, breeding and implications for animal welfare},
	volume = {56},
	issn = {1836-0939},
	shorttitle = {Resilience in farm animals},
	url = {http://www.publish.csiro.au/?paper=AN15297},
	doi = {10.1071/AN15297},
	abstract = {A capacity for the animal to recover quickly from the impact of physical and social stressors and disease challenges is likely to improve evolutionary ﬁtness of wild species and welfare and performance of farm animals. Salience and valence of stimuli sensed through neurosensors, chemosensors and immunosensors are perceived and integrated centrally to generate emotions and engage physiological, behavioural, immune, cognitive and morphological responses that defend against noxious challenges. These responses can be reﬁned through experience to provide anticipatory and learned reactions at lower cost than innate less-speciﬁc reactions. Inﬂuences of behaviour type, coping style, and affective state and the relationships between immune responsiveness, disease resistance and resilience are reviewed. We deﬁne resilience as the capacity of animals to cope with short-term perturbations in their environment and return rapidly to their pre-challenge status. It is manifested in response to episodic, sporadic or situation-speciﬁc attributes of the environment and can be optimised via facultative learning by the individual. It is a comparative measure of differences between individuals in the outcomes that follow exposure to potentially adverse situations. In contrast, robustness is the capacity to maintain productivity in a wide range of environments without compromising reproduction, health and wellbeing. Robustness is manifested in response to persistent or cyclical attributes of the environment and is effected via activity of innate regulatory pathways. We suggest that for farm animals, husbandry practices that incorporate physical and social stressors and interactions with humans such as weaning, change of housing, and introduction to the milking parlour can be used to characterise resilience phenotypes. In these settings, resilience is likely to be more readily identiﬁed through the rate of return of variables to pre-challenge or normal status rather than through measuring the activity of diverse stress response and adaptation mechanisms. Our strategy for phenotyping resilience of sheep and cattle during weaning is described. Opportunities are examined to increase resilience through genetic selection and through improved management practices that provide emotional and cognitive enrichment and stress inoculation.},
	language = {en},
	number = {12},
	urldate = {2020-11-04},
	journal = {Animal Production Science},
	author = {Colditz, Ian G. and Hine, Brad C.},
	year = {2016},
	pages = {1961},
	file = {colditz hine 2015 - Resilience in farm animals - biology, management, breeding and implications for animal welfare - VOICELESS - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/44VTJ2KS/colditz hine 2015 - Resilience in farm animals - biology, management, breeding and implications for animal welfare - VOICELESS - OPTISEVIL.pdf:application/pdf},
}

@article{solomonGenomeEditingAnimals2020,
	title = {Genome editing in animals: why {FDA} regulation matters},
	volume = {38},
	issn = {1087-0156, 1546-1696},
	shorttitle = {Genome editing in animals},
	url = {http://www.nature.com/articles/s41587-020-0413-7},
	doi = {10.1038/s41587-020-0413-7},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Nature Biotechnology},
	author = {Solomon, Steven M.},
	month = feb,
	year = {2020},
	pages = {142--143},
	file = {solomon 2020 - Genome editing in animals - why FDA regulation matters - OPTISEVIL - GENETIC ENGINEERING - GMO.pdf:/Users/bill/D/Zotero/storage/N3X45WS7/solomon 2020 - Genome editing in animals - why FDA regulation matters - OPTISEVIL - GENETIC ENGINEERING - GMO.pdf:application/pdf},
}

@article{gibsonHowEfficiencyShapes2019,
	title = {How {Efficiency} {Shapes} {Human} {Language}},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300580},
	doi = {10.1016/j.tics.2019.02.003},
	language = {en},
	number = {5},
	urldate = {2020-11-04},
	journal = {Trends in Cognitive Sciences},
	author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
	month = may,
	year = {2019},
	pages = {389--407},
	file = {gibson et al 2019 - How Efficiency Shapes Human Language - LINGUISTICS - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/V86HSC4R/gibson et al 2019 - How Efficiency Shapes Human Language - LINGUISTICS - OPTISEVIL.pdf:application/pdf},
}

@article{rudinOptimizedScoringSystems,
	title = {Optimized {Scoring} {Systems}: {Towards} {Trust} in {Machine} {Learning} for {Healthcare} and {Criminal} {Justice}},
	language = {en},
	author = {Rudin, Cynthia and Ustun, Berk},
	pages = {58},
	file = {rudin ustun 2018 - Optimized Scoring Systems - Towards Trust in Machine Learning for Healthcare and Criminal Justice - OPTISEVIL - EFs - ML - POST-RMIT.pdf:/Users/bill/D/Zotero/storage/EA5QBEHH/rudin ustun 2018 - Optimized Scoring Systems - Towards Trust in Machine Learning for Healthcare and Criminal Justice - OPTISEVIL - EFs - ML - POST-RMIT.pdf:application/pdf},
}

@article{wolffClassificationDetectionConsequences2011,
	title = {Classification, {Detection} and {Consequences} of {Data} {Error}: {Evidence} from the {Human} {Development} {Index}},
	volume = {121},
	issn = {0013-0133, 1468-0297},
	shorttitle = {Classification, {Detection} and {Consequences} of {Data} {Error}},
	url = {https://academic.oup.com/ej/article/121/553/843-870/5079722},
	doi = {10.1111/j.1468-0297.2010.02408.x},
	language = {en},
	number = {553},
	urldate = {2020-11-04},
	journal = {The Economic Journal},
	author = {Wolff, Hendrik and Chong, Howard and Auffhammer, Maximilian},
	month = jun,
	year = {2011},
	pages = {843--870},
	file = {wolff et al 2011 - classification detection and consequences of data error - evidence from the human development index - ERROR - UNCERTAINTY - INDICES - ECONOMICS - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/X7YDJ56Q/wolff et al 2011 - classification detection and consequences of data error - evidence from the human development index - ERROR - UNCERTAINTY - INDICES - ECONOMICS - OPTISEVIL.pdf:application/pdf},
}

@article{gianettoDynamicStructureCompetition2018,
	title = {Dynamic {Structure} of {Competition} {Networks} in {Affordable} {Care} {Act} {Insurance} {Market}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8276567/},
	doi = {10.1109/ACCESS.2018.2800659},
	abstract = {Stimulating competition is one of the main topics in most health care reform debates, and it has been a central issue in the Affordable Care Act in the United States since 2009. The goal of this paper is to use complex network methods to study dynamic and structure of competition under Affordable Care Act (ACA) and its evolution over time since its beginning until 2017. Using publicly available data, we construct a bipartite network of counties and insurance providers, create associated weighted single-mode networks, and analyze the evolution of network parameters that are related to competition and potential collusion in complex networks. These parameters have been previously tied to dynamics of collaboration and competition in earlier theoretical works. We argue that three parameters, namely network modularity, and eigenvector centrality mean and skewness are appropriate indicators of the overall competition in the insurance market. Based on these parameters, we show that the level of systemic competition among insurers as a function of time is an inverse U-shape trend, and that competition has returned back to what it was at the very beginning of ACA, indicating an undesirable resilience in the national health care system.},
	language = {en},
	urldate = {2020-11-04},
	journal = {IEEE Access},
	author = {Gianetto, David A. and Mosleh, Mohsen and Heydari, Babak},
	year = {2018},
	pages = {12700--12709},
	file = {gianetto et al 2018 - Dynamic Structure of Competition Networks in Affordable Care Act Insurance Market - NETWORKS - ECONOMICS - COMPETITION - EFs - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/KSTBYN38/gianetto et al 2018 - Dynamic Structure of Competition Networks in Affordable Care Act Insurance Market - NETWORKS - ECONOMICS - COMPETITION - EFs - OPTISEVIL.pdf:application/pdf},
}

@article{nishikawaSensitiveDependenceOptimal2017,
	title = {Sensitive {Dependence} of {Optimal} {Network} {Dynamics} on {Network} {Structure}},
	volume = {7},
	issn = {2160-3308},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.7.041044},
	doi = {10.1103/PhysRevX.7.041044},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Physical Review X},
	author = {Nishikawa, Takashi and Sun, Jie and Motter, Adilson E.},
	month = nov,
	year = {2017},
	pages = {041044},
	file = {nishikawa et al 2017 - Sensitive Dependence of Optimal Network Dynamics on Network Structure - OPTISEVIL - NETWORKS.pdf:/Users/bill/D/Zotero/storage/82EH7883/nishikawa et al 2017 - Sensitive Dependence of Optimal Network Dynamics on Network Structure - OPTISEVIL - NETWORKS.pdf:application/pdf},
}

@article{corbett-daviesAlgorithmicDecisionMaking2017,
	title = {Algorithmic decision making and the cost of fairness},
	url = {http://arxiv.org/abs/1701.08230},
	doi = {10.1145/3097983.309809},
	abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classi ed as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past de nitions of fairness, the optimal algorithms that result require detaining defendants above race-speci c risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. e unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally di er, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-o can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1701.08230 [cs, stat]},
	author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	month = jun,
	year = {2017},
	note = {arXiv: 1701.08230},
	keywords = {Statistics - Applications, Computer Science - Computers and Society},
	annote = {Comment: To appear in Proceedings of KDD'17},
	file = {corbett-davies pierson etl al 2017 - Algorithmic decision making and the cost of fairness - FAIRNESS - EFs - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/YAAUXKFQ/corbett-davies pierson etl al 2017 - Algorithmic decision making and the cost of fairness - FAIRNESS - EFs - OPTISEVIL.pdf:application/pdf},
}

@article{doshi-velezRigorousScienceInterpretable2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {selbst powles 2017 - Meaningful information and the right to explanation - EFs - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/4SG2BE2D/selbst powles 2017 - Meaningful information and the right to explanation - EFs - OPTISEVIL.pdf:application/pdf},
}

@article{powlesNewYorkCity,
	title = {New {York} {City}’s {Bold}, {Flawed} {Attempt} to {Make} {Algorithms} {Accountable}},
	language = {en},
	journal = {New York City},
	author = {Powles, Julia},
	pages = {6},
	file = {powles 2017 - New York Citys Bold Flawed Attempt to Make Algorithms Accountable - OPTISEVIL - EFs.pdf:/Users/bill/D/Zotero/storage/FKD8QPR3/powles 2017 - New York Citys Bold Flawed Attempt to Make Algorithms Accountable - OPTISEVIL - EFs.pdf:application/pdf},
}

@article{nielsenPrinciplesOptimalMetabolic2007,
	title = {Principles of optimal metabolic network operation},
	volume = {3},
	issn = {1744-4292, 1744-4292},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1038/msb4100169},
	doi = {10.1038/msb4100169},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Molecular Systems Biology},
	author = {Nielsen, Jens},
	month = jan,
	year = {2007},
	pages = {126},
	file = {nielsen 2007 - Principles of optimal metabolic network operation - OPTISEVIL - EFs - SYSTEMS BIOLOGY.pdf:/Users/bill/D/Zotero/storage/MKAHNM6J/nielsen 2007 - Principles of optimal metabolic network operation - OPTISEVIL - EFs - SYSTEMS BIOLOGY.pdf:application/pdf},
}

@article{langfordBipartisanWayImprove,
	title = {A {Bipartisan} {Way} to {Improve} {Medical} {Care} - {The} {New} {Yorker}},
	language = {en},
	author = {Langford, Bill},
	pages = {5},
	file = {new yorker 2017 - A Bipartisan Way to Improve Medical Care - ECONOMICS - HEALTHCARE - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/QJKFBT45/new yorker 2017 - A Bipartisan Way to Improve Medical Care - ECONOMICS - HEALTHCARE - OPTISEVIL.pdf:application/pdf},
}

@article{marshallTweakMakesNukes2017,
	title = {Tweak makes {U}.{S}. nukes more precise—and deadlier},
	volume = {355},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.355.6331.1252},
	doi = {10.1126/science.355.6331.1252},
	language = {en},
	number = {6331},
	urldate = {2020-11-04},
	journal = {Science},
	author = {Marshall, Eliot},
	month = mar,
	year = {2017},
	pages = {1252--1253},
	file = {marshall 2017 - tweak makes US nukes more precise and deadlier - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/JQKHPRM9/marshall 2017 - tweak makes US nukes more precise and deadlier - OPTISEVIL.pdf:application/pdf},
}

@article{folkeResilienceRepublished2016,
	title = {Resilience ({Republished})},
	volume = {21},
	issn = {1708-3087},
	url = {http://www.ecologyandsociety.org/vol21/iss4/art44/},
	doi = {10.5751/ES-09088-210444},
	abstract = {Resilience thinking in relation to the environment has emerged as a lens of inquiry that serves a platform for interdisciplinary dialogue and collaboration. Resilience is about cultivating the capacity to sustain development in the face of expected and surprising change and diverse pathways of development and potential thresholds between them. The evolution of resilience thinking is coupled to socialecological systems and a truly intertwined human-environment planet. Resilience as persistence, adaptability, and transformability of complex adaptive social-ecological systems is the focus, clarifying the dynamic and forward-looking nature of the concept. Resilience thinking emphasizes that social-ecological systems, from the individual, to community, to society as a whole, are embedded in the biosphere. The biosphere connection is an essential observation if sustainability is to be taken seriously. In the continuous advancement of resilience thinking there are efforts aimed at capturing resilience of social-ecological systems and finding ways for people and institutions to govern social-ecological dynamics for improved human well-being, at the local, across levels and scales, to the global. Consequently, in resilience thinking, development issues for human well-being, for people and planet, are framed in a context of understanding and governing complex social-ecological dynamics for sustainability as part of a dynamic biosphere.},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Ecology and Society},
	author = {Folke, Carl},
	year = {2016},
	pages = {art44},
	file = {folke 2016 - resilience - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/ZWHQAZ4Z/folke 2016 - resilience - OPTISEVIL.pdf:application/pdf},
}

@book{johnsonNonEquilibriumSocialScience2017,
	address = {Cham},
	series = {Understanding {Complex} {Systems}},
	title = {Non-{Equilibrium} {Social} {Science} and {Policy}: {Introduction} and {Essays} on {New} and {Changing} {Paradigms} in {Socio}-{Economic} {Thinking}},
	isbn = {978-3-319-42422-4 978-3-319-42424-8},
	shorttitle = {Non-{Equilibrium} {Social} {Science} and {Policy}},
	url = {http://link.springer.com/10.1007/978-3-319-42424-8},
	language = {en},
	urldate = {2020-11-04},
	publisher = {Springer International Publishing},
	editor = {Johnson, Jeffrey and Nowak, Andrzej and Ormerod, Paul and Rosewell, Bridget and Zhang, Yi-Cheng},
	year = {2017},
	doi = {10.1007/978-3-319-42424-8},
	file = {johnson et al 2017 - non-equilibrium social science and policy - BOOK - OPTISEVIL - ECONOMICS - EQUILIBRIUM - COMPLEXITY.pdf:/Users/bill/D/Zotero/storage/KGTHF2F5/johnson et al 2017 - non-equilibrium social science and policy - BOOK - OPTISEVIL - ECONOMICS - EQUILIBRIUM - COMPLEXITY.pdf:application/pdf},
}

@article{valsanNatureCorporationTale2016,
	title = {The {Nature} of the {Corporation}: {A} {Tale} of {Economic} {Complexity}},
	issn = {1556-5068},
	shorttitle = {The {Nature} of the {Corporation}},
	url = {http://www.ssrn.com/abstract=2710962},
	doi = {10.2139/ssrn.2710962},
	language = {en},
	urldate = {2020-11-04},
	journal = {SSRN Electronic Journal},
	author = {Valsan, Calin},
	year = {2016},
	file = {valsan 2016 - The Nature of the Corporation - A Tale of Economic Complexity - Proof062216 - BOOK - OPTISEVIL - ECONOMICS.PDF:/Users/bill/D/Zotero/storage/4XMYZKDM/valsan 2016 - The Nature of the Corporation - A Tale of Economic Complexity - Proof062216 - BOOK - OPTISEVIL - ECONOMICS.PDF:application/pdf},
}

@article{haugOptionTradersUse2011,
	title = {Option traders use (very) sophisticated heuristics, never the {Black}–{Scholes}–{Merton} formula},
	volume = {77},
	issn = {01672681},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167268110001927},
	doi = {10.1016/j.jebo.2010.09.013},
	abstract = {Option traders use a heuristically derived pricing formula which they adapt by fudging and changing the tails and skewness by varying one parameter, the standard deviation of a Gaussian. Such formula is popularly called “Black–Scholes–Merton” owing to an attributed eponymous discovery (though changing the standard deviation parameter is in contra- diction with it). However, we have historical evidence that: (1) the said Black, Scholes and Merton did not invent any formula, just found an argument to make a well known (and used) formula compatible with the economics establishment, by removing the “risk” parameter through “dynamic hedging”, (2) option traders use (and evidently have used since 1902) sophisticated heuristics and tricks more compatible with the previous versions of the formula of Louis Bachelier and Edward O. Thorp (that allow a broad choice of probability distributions) and removed the risk parameter using put-call parity, (3) option traders did not use the Black–Scholes–Merton formula or similar formulas after 1973 but continued their bottom-up heuristics more robust to the high impact rare event. The paper draws on historical trading methods and 19th and early 20th century references ignored by the finance literature. It is time to stop using the wrong designation for option pricing.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Haug, Espen Gaarder and Taleb, Nassim Nicholas},
	month = feb,
	year = {2011},
	pages = {97--106},
	file = {haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/SGB6ZDGS/haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf:application/pdf},
}

@article{haugOptionTradersUse2011a,
	title = {Option traders use (very) sophisticated heuristics, never the {Black}–{Scholes}–{Merton} formula},
	volume = {77},
	issn = {01672681},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167268110001927},
	doi = {10.1016/j.jebo.2010.09.013},
	abstract = {Option traders use a heuristically derived pricing formula which they adapt by fudging and changing the tails and skewness by varying one parameter, the standard deviation of a Gaussian. Such formula is popularly called “Black–Scholes–Merton” owing to an attributed eponymous discovery (though changing the standard deviation parameter is in contra- diction with it). However, we have historical evidence that: (1) the said Black, Scholes and Merton did not invent any formula, just found an argument to make a well known (and used) formula compatible with the economics establishment, by removing the “risk” parameter through “dynamic hedging”, (2) option traders use (and evidently have used since 1902) sophisticated heuristics and tricks more compatible with the previous versions of the formula of Louis Bachelier and Edward O. Thorp (that allow a broad choice of probability distributions) and removed the risk parameter using put-call parity, (3) option traders did not use the Black–Scholes–Merton formula or similar formulas after 1973 but continued their bottom-up heuristics more robust to the high impact rare event. The paper draws on historical trading methods and 19th and early 20th century references ignored by the finance literature. It is time to stop using the wrong designation for option pricing.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Haug, Espen Gaarder and Taleb, Nassim Nicholas},
	month = feb,
	year = {2011},
	pages = {97--106},
	file = {haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/7M6A6THP/haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf:application/pdf},
}

@article{langfordWhenNotEnough,
	title = {When 2\% is not enough {\textbar} {The} {Economist}},
	language = {en},
	author = {Langford, Bill},
	pages = {3},
	file = {2014 08 27 - When 2 percent is not enough - The Economist - EFs - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/YSZNCYF3/2014 08 27 - When 2 percent is not enough - The Economist - EFs - OPTISEVIL.pdf:application/pdf},
}

@article{langfordTroubleGDPEconomist,
	title = {The trouble with {GDP} {\textbar} {The} {Economist}},
	language = {en},
	author = {Langford, Bill},
	pages = {9},
	file = {economist 2016 - the trouble with gdp - main article - version from Print in magazine - does not include comments - ECONOMICS - EF - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/DYHQC8SR/economist 2016 - the trouble with gdp - main article - version from Print in magazine - does not include comments - ECONOMICS - EF - OPTISEVIL.pdf:application/pdf},
}

@article{huiGameTheoryRisk2016,
	title = {Game theory and risk‐based leveed river system planning with noncooperation},
	volume = {52},
	issn = {0043-1397, 1944-7973},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2015WR017707},
	doi = {10.1002/2015WR017707},
	abstract = {Optimal risk-based levee designs are usually developed for economic efﬁciency. However, in river systems with multiple levees, the planning and maintenance of different levees are controlled by different agencies or groups. For example, along many rivers, levees on opposite riverbanks constitute a simple leveed river system with each levee designed and controlled separately. Collaborative planning of the two levees can be economically optimal for the whole system. Independent and self-interested landholders on opposite riversides often are willing to separately determine their individual optimal levee plans, resulting in a less efﬁcient leveed river system from an overall society-wide perspective (the tragedy of commons). We apply game theory to simple leveed river system planning where landholders on each riverside independently determine their optimal risk-based levee plans. Outcomes from noncooperative games are analyzed and compared with the overall economically optimal outcome, which minimizes net ﬂood cost system-wide. The system-wide economically optimal solution generally transfers residual ﬂood risk to the lower-valued side of the river, but is often impractical without compensating for ﬂood risk transfer to improve outcomes for all individuals involved. Such compensation can be determined and implemented with landholders’ agreements on collaboration to develop an economically optimal plan. By examining iterative multiple-shot noncooperative games with reversible and irreversible decisions, the costs of myopia for the future in making levee planning decisions show the signiﬁcance of considering the externalities and evolution path of dynamic water resource problems to improve decision-making.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Water Resources Research},
	author = {Hui, Rui and Lund, Jay R. and Madani, Kaveh},
	month = jan,
	year = {2016},
	pages = {119--134},
	file = {hui et al 2015 -  Game theory and risk-based leveed river system planning with noncooperation - OPTISEVIL - GAME THEORY.pdf:/Users/bill/D/Zotero/storage/QRF6R87H/hui et al 2015 -  Game theory and risk-based leveed river system planning with noncooperation - OPTISEVIL - GAME THEORY.pdf:application/pdf},
}

@article{kaplanHumanCostsWorkplace,
	title = {The human costs of workplace monitoring},
	language = {en},
	author = {Kaplan, Esther},
	pages = {11},
	file = {kaplan 2015 - THE SPY WHO FIRED ME - The human costs of workplace monitoring - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/PFFHKT9S/kaplan 2015 - THE SPY WHO FIRED ME - The human costs of workplace monitoring - OPTISEVIL.pdf:application/pdf},
}

@article{langfordGreatDivideEconomists,
	title = {The great divide: {Economists} versus the markets {\textbar} {The} {Economist}},
	language = {en},
	author = {Langford, Bill},
	pages = {3},
	file = {The great divide - Economists versus the markets - The Economist - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/W29RR9PR/The great divide - Economists versus the markets - The Economist - OPTISEVIL.pdf:application/pdf},
}

@article{gershensonWhenSlowerFaster2015,
	title = {When slower is faster},
	volume = {21},
	issn = {10762787},
	url = {http://doi.wiley.com/10.1002/cplx.21736},
	doi = {10.1002/cplx.21736},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Complexity},
	author = {Gershenson, Carlos and Helbing, Dirk},
	month = nov,
	year = {2015},
	pages = {9--15},
	file = {gershenson helbing 2015 - when slower is faster - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/YQ749QJ4/gershenson helbing 2015 - when slower is faster - OPTISEVIL.pdf:application/pdf},
}

@article{lutheIntroducingAdaptiveWaves2015,
	title = {Introducing adaptive waves as a concept to inform mental models of resilience},
	volume = {10},
	issn = {1862-4065, 1862-4057},
	url = {http://link.springer.com/10.1007/s11625-015-0316-6},
	doi = {10.1007/s11625-015-0316-6},
	language = {en},
	number = {4},
	urldate = {2020-11-04},
	journal = {Sustainability Science},
	author = {Luthe, Tobias and Wyss, Romano},
	month = oct,
	year = {2015},
	pages = {673--685},
	file = {luthe wyss 2015 - introducing adaptive waves as a concept to inform mental models of resilience - RESILIENCE - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/97WK5W3U/luthe wyss 2015 - introducing adaptive waves as a concept to inform mental models of resilience - RESILIENCE - OPTISEVIL.pdf:application/pdf},
}

@article{hirstSettlersVagrantsMutual2011,
	title = {Settlers, vagrants and mutual indifference: unintended consequences of hot‐desking},
	volume = {24},
	issn = {0953-4814},
	shorttitle = {Settlers, vagrants and mutual indifference},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09534811111175742/full/html},
	doi = {10.1108/09534811111175742},
	abstract = {Purpose – The purpose of this paper is to provide a sociological analysis of emergent sociospatial structures in a hot-desking ofﬁce environment, where space is used exchangeably. It considers hot-desking as part of broader societal shifts in the ownership of space.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Journal of Organizational Change Management},
	author = {Hirst, Alison},
	editor = {Höpfl, Heather},
	month = oct,
	year = {2011},
	pages = {767--788},
	file = {hirst 2011 - Settlers, vagrants and mutual indifference - unintended consequences of hot-desking - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/BEQ3AFZU/hirst 2011 - Settlers, vagrants and mutual indifference - unintended consequences of hot-desking - OPTISEVIL.pdf:application/pdf},
}

@misc{levy-carcienteSimulatingBarterFinancial2005,
	title = {Simulating {Barter} and {Financial} {Economy}: (401442008-001)},
	shorttitle = {Simulating {Barter} and {Financial} {Economy}},
	url = {http://doi.apa.org/get-pe-doi.cfm?doi=10.1037/e401442008-001},
	doi = {10.1037/e401442008-001},
	abstract = {Inspired by Adam Smith and Friedrich Hayek, many economists have postulated the existence of invisible forces that drive economic markets. These market forces interact in complex ways making it difficult to visualize or understand the interactions in every detail. Here I show how these forces can transcend a zero-sum game and become a win-win business interaction, thanks to emergent social synergies triggered by division of labor. Computer simulations with the model Sociodynamica show here the detailed dynamics underlying this phenomenon in a simple virtual economy. In these simulations, independent agents act in an economy exploiting and trading two different goods in a heterogeneous environment. All and each of the various forces and individuals were tracked continuously, allowing to unveil a synergistic effect on economic output produced by the division of labor between agents. Running simulations in a homogeneous environment, for example, eliminated all benefits of division of labor. The simulations showed that the synergies unleashed by division of labor arise if: Economies work in a heterogeneous environment; agents engage in complementary activities whose optimization processes diverge; agents have means to synchronize their activities. This insight, although trivial if viewed a posteriori, improve our understanding of the source and nature of synergies in real economic markets and might render economic and natural sciences more consilient.},
	language = {en},
	urldate = {2020-11-04},
	publisher = {American Psychological Association},
	author = {Levy-Carciente, Sary and Jaffe, Klaus},
	year = {2005},
	file = {jaffe 2015 - Agent based simulations visualize Adam Smith's invisible hand by solving Friedrich Hayek's Economic Calculus - OPTISEVIL - ECONOMICS - AGENT-BASED.pdf:/Users/bill/D/Zotero/storage/VPYI9KGV/jaffe 2015 - Agent based simulations visualize Adam Smith's invisible hand by solving Friedrich Hayek's Economic Calculus - OPTISEVIL - ECONOMICS - AGENT-BASED.pdf:application/pdf},
}

@article{jonesWhyWhatHow2011,
	title = {The {Why}, {What}, and {How} of {Global} {Biodiversity} {Indicators} {Beyond} the 2010 {Target}: {Biodiversity} {Indicators} {Beyond} 2010},
	volume = {25},
	issn = {08888892},
	shorttitle = {The {Why}, {What}, and {How} of {Global} {Biodiversity} {Indicators} {Beyond} the 2010 {Target}},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2010.01605.x},
	doi = {10.1111/j.1523-1739.2010.01605.x},
	abstract = {The 2010 biodiversity target agreed by signatories to the Convention on Biological Diversity directed the attention of conservation professionals toward the development of indicators with which to measure changes in biological diversity at the global scale. We considered why global biodiversity indicators are needed, what characteristics successful global indicators have, and how existing indicators perform. Because monitoring could absorb a large proportion of funds available for conservation, we believe indicators should be linked explicitly to monitoring objectives and decisions about which monitoring schemes deserve funding should be informed by predictions of the value of such schemes to decision making. We suggest that raising awareness among the public and policy makers, auditing management actions, and informing policy choices are the most important global monitoring objectives. Using four well-developed indicators of biological diversity (extent of forests, coverage of protected areas, Living Planet Index, Red List Index) as examples, we analyzed the characteristics needed for indicators to meet these objectives. We recommend that conservation professionals improve on existing indicators by eliminating spatial biases in data availability, fill gaps in information about ecosystems other than forests, and improve understanding of the way indicators respond to policy changes. Monitoring is not an end in itself, and we believe it is vital that the ultimate objectives of global monitoring of biological diversity inform development of new indicators.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Conservation Biology},
	author = {Jones, Julia P. G. and Collen, Ben and Atkinson, Giles and Baxter, Peter W. J. and Bubb, Philip and Illian, Janine B. and Katzner, Todd E. and Keane, Aidan and Loh, Jonathan and Mcdonald-Madden, Eve and Nicholson, Emily and Pereira, Henrique M. and Possingham, Hugh P. and Pullin, Andrew S. and Rodrigues, Ana S. L. and Ruiz-Gutierrez, Viviana and Sommerville, Matthew and Milner-Gulland, E. J.},
	month = jun,
	year = {2011},
	pages = {450--457},
	file = {jones et al 2011 - the why, what, and how of global biodiversity indicators beyond the 2010 target - GUPPY - EFs - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/AY9EDPDC/jones et al 2011 - the why, what, and how of global biodiversity indicators beyond the 2010 target - GUPPY - EFs - OPTISEVIL.pdf:application/pdf},
}

@article{bondRealEffectsFinancial2012,
	title = {The {Real} {Effects} of {Financial} {Markets}},
	volume = {4},
	issn = {1941-1367, 1941-1375},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-financial-110311-101826},
	doi = {10.1146/annurev-financial-110311-101826},
	abstract = {A large amount of activity in the financial sector occurs in secondary financial markets, where securities are traded among investors without capital flowing to firms. The stock market is the archetypal example, which in most developed economies captures a lot of attention and resources. Is the stock market just a sideshow or does it affect real economic activity? In this review, we discuss the potential real effects of financial markets that stem from the informational role of market prices. We review the theoretical literature and show that accounting for the feedback effect from market prices to the real economy significantly changes our understanding of the price formation process, the informativeness of the price, and speculators’ trading behavior. We make two main points. First, we argue that a new definition of price efficiency is needed to account for the extent to which prices reflect information that is useful for the efficiency of real decisions (rather than the extent to which they forecast future cash flows). Second, incorporating the feedback effect into models of financial markets can explain various market phenomena that otherwise seem puzzling. Finally, we review empirical evidence on the real effects of secondary financial markets.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Annual Review of Financial Economics},
	author = {Bond, Philip and Edmans, Alex and Goldstein, Itay},
	month = oct,
	year = {2012},
	pages = {339--360},
	file = {bond et al 2012 - The Real Effects of Financial Markets - FINANCE - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/MU7LLEIX/bond et al 2012 - The Real Effects of Financial Markets - FINANCE - OPTISEVIL.pdf:application/pdf},
}

@article{seoanePhaseTransitionsPareto2015,
	title = {Phase transitions in {Pareto} optimal complex networks},
	volume = {92},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/1505.06937},
	doi = {10.1103/PhysRevE.92.032807},
	abstract = {The organization of interactions in complex systems can be described by networks connecting different units. These graphs are useful representations of the local and global complexity of the underlying systems. The origin of their topological structure can be diverse, resulting from different mechanisms including multiplicative processes and optimization. In spatial networks or in graphs where cost constraints are at work, as it occurs in a plethora of situations from power grids to the wiring of neurons in the brain, optimization plays an important part in shaping their organization. In this paper we study network designs resulting from a Pareto optimization process, where different simultaneous constraints are the targets of selection. We analyze three variations on a problem finding phase transitions of different kinds. Distinct phases are associated to different arrangements of the connections; but the need of drastic topological changes does not determine the presence, nor the nature of the phase transitions encountered. Instead, the functions under optimization do play a determinant role. This reinforces the view that phase transitions do not arise from intrinsic properties of a system alone, but from the interplay of that system with its external constraints.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {Physical Review E},
	author = {Seoane, Luís F. and Solé, Ricard},
	month = sep,
	year = {2015},
	note = {arXiv: 1505.06937},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Physics - Physics and Society},
	pages = {032807},
	annote = {Comment: 14 pages, 7 figures},
	file = {seoane sole 2015 - Phase transitions in Pareto optimal complex networks - NETWORKS - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/VBA4N7GS/seoane sole 2015 - Phase transitions in Pareto optimal complex networks - NETWORKS - OPTISEVIL.pdf:application/pdf},
}

@incollection{akrourPreferenceBasedPolicyLearning2011,
	address = {Berlin, Heidelberg},
	title = {Preference-{Based} {Policy} {Learning}},
	volume = {6911},
	isbn = {978-3-642-23779-9 978-3-642-23780-5},
	url = {http://link.springer.com/10.1007/978-3-642-23780-5_11},
	abstract = {Many machine learning approaches in robotics, based on reinforcement learning, inverse optimal control or direct policy learning, critically rely on robot simulators. This paper investigates a simulatorfree direct policy learning, called Preference-based Policy Learning (PPL). PPL iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies, and the process is iterated until the desired behavior is obtained. PPL requires a good representation of the policy search space be available, enabling one to learn accurate policy return estimates and limiting the human ranking eﬀort needed to yield a good policy. Furthermore, this representation cannot use informed features (e.g., how far the robot is from any target) due to the simulator-free setting. As a second contribution, this paper proposes a representation based on the agnostic exploitation of the robotic log.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	doi = {10.1007/978-3-642-23780-5_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {12--27},
	file = {akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:/Users/bill/D/Zotero/storage/WE7Q6X72/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:application/pdf},
}

@incollection{mouretNoveltyBasedMultiobjectivization2011,
	address = {Berlin, Heidelberg},
	title = {Novelty-{Based} {Multiobjectivization}},
	volume = {341},
	isbn = {978-3-642-18271-6 978-3-642-18272-3},
	url = {http://link.springer.com/10.1007/978-3-642-18272-3_10},
	abstract = {Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the eﬃciency. However, abandoning the eﬃciency objective(s) may be too radical in many contexts. In this paper, a Paretobased multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant “Novelty + Fitness” is better at ﬁne-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {New {Horizons} in {Evolutionary} {Robotics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mouret, Jean-Baptiste},
	editor = {Kacprzyk, Janusz and Doncieux, Stéphane and Bredèche, Nicolas and Mouret, Jean-Baptiste},
	year = {2011},
	doi = {10.1007/978-3-642-18272-3_10},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {139--154},
	file = {mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/F73HNQQG/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:application/pdf},
}

@incollection{akrourPreferenceBasedPolicyLearning2011a,
	address = {Berlin, Heidelberg},
	title = {Preference-{Based} {Policy} {Learning}},
	volume = {6911},
	isbn = {978-3-642-23779-9 978-3-642-23780-5},
	url = {http://link.springer.com/10.1007/978-3-642-23780-5_11},
	abstract = {Many machine learning approaches in robotics, based on reinforcement learning, inverse optimal control or direct policy learning, critically rely on robot simulators. This paper investigates a simulatorfree direct policy learning, called Preference-based Policy Learning (PPL). PPL iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies, and the process is iterated until the desired behavior is obtained. PPL requires a good representation of the policy search space be available, enabling one to learn accurate policy return estimates and limiting the human ranking eﬀort needed to yield a good policy. Furthermore, this representation cannot use informed features (e.g., how far the robot is from any target) due to the simulator-free setting. As a second contribution, this paper proposes a representation based on the agnostic exploitation of the robotic log.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	doi = {10.1007/978-3-642-23780-5_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {12--27},
	file = {akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:/Users/bill/D/Zotero/storage/J4CSIJBN/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf:application/pdf},
}

@incollection{mouretNoveltyBasedMultiobjectivization2011a,
	address = {Berlin, Heidelberg},
	title = {Novelty-{Based} {Multiobjectivization}},
	volume = {341},
	isbn = {978-3-642-18271-6 978-3-642-18272-3},
	url = {http://link.springer.com/10.1007/978-3-642-18272-3_10},
	abstract = {Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the eﬃciency. However, abandoning the eﬃciency objective(s) may be too radical in many contexts. In this paper, a Paretobased multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant “Novelty + Fitness” is better at ﬁne-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {New {Horizons} in {Evolutionary} {Robotics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mouret, Jean-Baptiste},
	editor = {Kacprzyk, Janusz and Doncieux, Stéphane and Bredèche, Nicolas and Mouret, Jean-Baptiste},
	year = {2011},
	doi = {10.1007/978-3-642-18272-3_10},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {139--154},
	file = {mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/8IW8HAUX/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf:application/pdf},
}

@article{lehmanEncouragingReactivityCreate2013,
	title = {Encouraging reactivity to create robust machines},
	volume = {21},
	issn = {1059-7123, 1741-2633},
	url = {http://journals.sagepub.com/doi/10.1177/1059712313487390},
	doi = {10.1177/1059712313487390},
	abstract = {The robustness of animal behavior is unmatched by current machines, which often falter when exposed to unforeseen conditions. While animals are notably reactive to changes in their environment, machines often follow ﬁnely-tuned yet inﬂexible plans. Thus instead of the traditional approach of training such machines over many di↵erent unpredictable scenarios in detailed simulations (which is the most intuitive approach to inducing robustness), this work proposes to train machines to be reactive to their environment. The idea is that robustness may result not from detailed internal models or ﬁnely-tuned control policies but from cautious exploratory behavior. Supporting this hypothesis, robots trained to navigate mazes with a reactive disposition prove more robust than those trained over many trials yet not rewarded for reactive behavior in both simulated tests and when embodied in real robots. The conclusion is that robustness may neither require an accurate model nor ﬁnely calibrated behavior.},
	language = {en},
	number = {6},
	urldate = {2020-11-04},
	journal = {Adaptive Behavior},
	author = {Lehman, Joel and Risi, Sebastian and D’Ambrosio, David and O Stanley, Kenneth},
	month = dec,
	year = {2013},
	pages = {484--500},
	file = {lehman et al 2013 - encouraging reactivity to create robust machines - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/XMTVLLS9/lehman et al 2013 - encouraging reactivity to create robust machines - OPTISEVIL.pdf:application/pdf},
}

@inproceedings{lehmanOpenendednessQuantifyingImpressiveness2012,
	title = {Beyond {Open}-endedness: {Quantifying} {Impressiveness}},
	isbn = {978-0-262-31050-5},
	shorttitle = {Beyond {Open}-endedness},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/978-0-262-31050-5-ch011},
	doi = {10.7551/978-0-262-31050-5-ch011},
	abstract = {This paper seeks to illuminate and quantify a feature of natural evolution that correlates to our sense of its intuitive greatness: Natural evolution evolves impressive artifacts. Within artiﬁcial life, abstractions aiming to capture what makes natural evolution so powerful often focus on the idea of openendedness, which relates to boundless diversity, complexity, or adaptation. However, creative systems that have passed tests of open-endedness raise the possibility that openendedness does not always correlate to impressiveness in artiﬁcial life simulations. In other words, while natural evolution is both open-ended and demonstrates a drive towards evolving impressive artifacts, it may be a mistake to assume the two properties are always linked. Thus to begin to investigate impressiveness independently in artiﬁcial systems, a novel deﬁnition is proposed: Impressive artifacts readily exhibit signiﬁcant design effort. That is, the difﬁculty of creating them is easy to recognize. Two heuristics, rarity and re-creation effort, are derived from this deﬁnition and applied to the products of an open-ended image evolution system. An important result is that that the heuristics intuitively separate different reward schemes and provide evidence for why each evolved picture is or is not impressive. The conclusion is that impressiveness may help to distinguish open-ended systems and their products, and potentially untangles an aspect of natural evolution’s mystique that is masked by its co-occurrence with open-endedness.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Artificial {Life} 13},
	publisher = {MIT Press},
	author = {Lehman, Joel and Stanley, Kenneth O.},
	month = jul,
	year = {2012},
	pages = {75--82},
	file = {lehman stanley 2012 - Beyond Open-endedness - Quantifying Impressiveness - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/PTJR76ZY/lehman stanley 2012 - Beyond Open-endedness - Quantifying Impressiveness - OPTISEVIL.pdf:application/pdf},
}

@inproceedings{lehmanEfficientlyEvolvingPrograms2010,
	address = {Portland, Oregon, USA},
	title = {Efficiently evolving programs through the search for novelty},
	isbn = {978-1-4503-0072-8},
	url = {http://portal.acm.org/citation.cfm?doid=1830483.1830638},
	doi = {10.1145/1830483.1830638},
	abstract = {A signiﬁcant challenge in genetic programming is premature convergence to local optima, which often prevents evolution from solving problems. This paper introduces to genetic programming a method that originated in neuroevolution (i.e. the evolution of artiﬁcial neural networks) that circumvents the problem of deceptive local optima. The main idea is to search only for behavioral novelty instead of for higher ﬁtness values. Although such novelty search abandons following the gradient of the ﬁtness function, if such gradients are deceptive they may actually occlude paths through the search space towards the objective. Because there are only so many ways to behave, the search for behavioral novelty is often computationally feasible and diﬀers signiﬁcantly from random search. Counterintuitively, in both a deceptive maze navigation task and the artiﬁcial ant benchmark task, genetic programming with novelty search, which ignores the objective, outperforms traditional genetic programming that directly searches for optimal behavior. Additionally, novelty search evolves smaller program trees in every variation of the test domains. Novelty search thus appears less susceptible to bloat, another signiﬁcant problem in genetic programming. The conclusion is that novelty search is a viable new tool for eﬃciently solving some deceptive problems in genetic programming.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Proceedings of the 12th annual conference on {Genetic} and evolutionary computation - {GECCO} '10},
	publisher = {ACM Press},
	author = {Lehman, Joel and Stanley, Kenneth O.},
	year = {2010},
	pages = {837},
	file = {lehman stanley 2010 - efficiently evolving programs through the search for novelty - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/ZFPNYXUE/lehman stanley 2010 - efficiently evolving programs through the search for novelty - OPTISEVIL.pdf:application/pdf},
}

@inproceedings{lehmanRewardingReactivityEvolve2012,
	title = {Rewarding {Reactivity} to {Evolve} {Robust} {Controllers} without {Multiple} {Trials} or {Noise}},
	isbn = {978-0-262-31050-5},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/978-0-262-31050-5-ch050},
	doi = {10.7551/978-0-262-31050-5-ch050},
	abstract = {Behaviors evolved in simulation are often not robust to variations of their original training environment. Thus often researchers must train explicitly to encourage such robustness. Traditional methods of training for robustness typically apply multiple non-deterministic evaluations with carefully modeled noisy distributions for sensors and effectors. In practice, such training is often computationally expensive and requires crafting accurate models. Taking inspiration from nature, where animals react appropriately to encountered stimuli, this paper introduces a measure called reactivity, i.e. the tendency to seek and react to changes in environmental input, that is applicable in single deterministic trials and can encourage robustness without exposure to noise. The measure is tested in four different maze navigation tasks, where training with reactivity proves more robust than training without noise, and equally or more robust than training with noise when testing with moderate noise levels. In this way, the results demonstrate the counterintuitive fact that sometimes training with no exposure to noise at all can evolve individuals signiﬁcantly more robust to noise than by explicitly training with noise. The conclusion is that training for reactivity may often be a computationally more efﬁcient means to encouraging robustness in evolved behaviors.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Artificial {Life} 13},
	publisher = {MIT Press},
	author = {Lehman, Joel and Risi, Sebastian and D’Ambrosio, David B. and Stanley, Kenneth O.},
	month = jul,
	year = {2012},
	pages = {379--386},
	file = {lehman et al 2012 - rewarding reactivity to evolve robust controllers without multiple trials or noise - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/M8B4RKRC/lehman et al 2012 - rewarding reactivity to evolve robust controllers without multiple trials or noise - OPTISEVIL.pdf:application/pdf},
}

@article{lehmanAbandoningObjectivesEvolution2011,
	title = {Abandoning {Objectives}: {Evolution} {Through} the {Search} for {Novelty} {Alone}},
	volume = {19},
	issn = {1063-6560, 1530-9304},
	shorttitle = {Abandoning {Objectives}},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/EVCO_a_00025},
	doi = {10.1162/EVCO_a_00025},
	abstract = {In evolutionary computation, the ﬁtness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artiﬁcial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search signiﬁcantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
	language = {en},
	number = {2},
	urldate = {2020-11-04},
	journal = {Evolutionary Computation},
	author = {Lehman, Joel and Stanley, Kenneth O.},
	month = jun,
	year = {2011},
	pages = {189--223},
	file = {lehman stanley 2011 - abandoning objectives - evolution through the search for novelty alone - OPTISEVIL.pdf:/Users/bill/D/Zotero/storage/YRT6C89K/lehman stanley 2011 - abandoning objectives - evolution through the search for novelty alone - OPTISEVIL.pdf:application/pdf},
}

@article{krzakala2009prl,
	title = {Hiding {Quiet} {Solutions} in {Random} {Constraint} {Satisfaction} {Problems}},
	volume = {102},
	doi = {10.1103/PhysRevLett.102.238701},
	number = {23},
	journal = {Physical Review Letters},
	author = {Krzakala, Florent},
	year = {2009},
	file = {Krzakala_2009_Hiding Quiet Solutions in Random Constraint Satisfaction Problems.pdf:/Users/bill/D/Zotero/storage/5JBVII8C/Krzakala_2009_Hiding Quiet Solutions in Random Constraint Satisfaction Problems.pdf:application/pdf},
}

@book{stevens2009,
	address = {New York, NY},
	series = {Use {R}!},
	title = {A {Primer} of {Ecology} with {R}},
	isbn = {978-0-387-89881-0},
	url = {https://doi.org/10.1007/978-0-387-89882-7},
	number = {6991},
	publisher = {Springer},
	author = {Stevens, M. H. H.},
	year = {2009},
	keywords = {bdpg, bdpg\_P1, lognormal},
}

@misc{xuke2014,
	title = {Challenging {Benchmarks} for {Maximum} {Clique}, {Maximum} {Independent} {Set}, {Minimum} {Vertex} {Cover} and {Vertex} {Coloring} - {Generating} {Hard} {Graph} {Problem} {Instances}},
	url = {http://sites.nlsde.buaa.edu.cn/~kexu/benchmarks/graph-benchmarks.htm},
	urldate = {2020-11-17},
	author = {Xu, Ke},
	month = dec,
	year = {2014},
	file = {Challenging Benchmarks for Maximum Clique, Maximum Independent Set, Minimum Vertex Cover and Vertex Coloring - Generating Hard Graph Problem Instances:/Users/bill/D/Zotero/storage/Q8LDEL4I/graph-benchmarks.html:text/html},
}

@inproceedings{cheeseman1991ip1ijcai,
	address = {San Mateo, CA, USA},
	title = {Where the really hard problems are},
	volume = {1},
	booktitle = {{IJCAI}'91: {Proceedings} of the 12th international joint conference on {Artificial} intelligence},
	publisher = {Morgan Kaufmann},
	author = {Cheeseman, Peter},
	year = {1991},
	pages = {331--337},
	file = {cheeseman et al 1991 - where the really hard problems are - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/XV4GR3PC/cheeseman et al 1991 - where the really hard problems are - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{glasser2011iac,
	title = {The fault tolerance of {NP}-hard problems},
	volume = {209},
	issn = {08905401},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0890540110001987},
	doi = {10.1016/j.ic.2010.11.012},
	abstract = {We study the effects of faulty data on NP-hard sets. We consider hard sets for several polynomial time reductions, add corrupt data and then analyze whether the resulting sets are still hard for NP. We explain that our results are related to a weakened deterministic variant of the notion of program self-correction by Blum, Luby, and Rubinfeld. Among other results, we use the Left-Set technique to prove that m-complete sets for NP are nonadaptively weakly deterministically self-correctable while btt-complete sets for NP are weakly deterministically self-correctable. Our results can also be applied to the study of Yesha’s p-closeness. In particular, we strengthen a result by Ogiwara and Fu.},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {Information and Computation},
	author = {Glaßer, Christian and Pavan, A. and Travers, Stephen},
	month = mar,
	year = {2011},
	keywords = {uncertainty, fault tolerance, NP-hard},
	pages = {443--455},
	file = {glasser pavan travers 2011 - the fault tolerance of np-hard problems.pdf:/Users/bill/D/Zotero/storage/9DUUEIZ4/glasser pavan travers 2011 - the fault tolerance of np-hard problems.pdf:application/pdf},
}

@article{achlioptas2005j,
	title = {Hiding {Satisfying} {Assignments}: {Two} are {Better} than {One}},
	volume = {24},
	issn = {1076-9757},
	shorttitle = {Hiding {Satisfying} {Assignments}},
	url = {https://jair.org/index.php/jair/article/view/10429},
	doi = {10.1613/jair.1681},
	abstract = {The evaluation of incomplete satisﬁability solvers depends critically on the availability of hard satisﬁable instances. A plausible source of such instances consists of random kSAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved eﬃciently by practical heuristics. Roughly speaking, for a number of diﬀerent algorithms, A acts as a stronger and stronger attractor as the formula’s density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this “symmetrization” the eﬀects of the two attractors largely cancel out, making it much harder for algorithms to ﬁnd any truth assignment. We give theoretical and experimental evidence supporting this assertion.},
	language = {en},
	urldate = {2020-11-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Achlioptas, D. and Jia, H. and Moore, C.},
	month = nov,
	year = {2005},
	keywords = {bdpg, planted solution},
	pages = {623--639},
	file = {achlioptas et al 2005 - hiding satisfying assignments - two are better than one.pdf:/Users/bill/D/Zotero/storage/YY45JH2R/achlioptas et al 2005 - hiding satisfying assignments - two are better than one.pdf:application/pdf},
}

@techreport{rcoreteam2019,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	institution = {R Foundation for Statistical Computing},
	author = {"R Core Team"},
	year = {2019},
}

@techreport{gurobioptimizationllc2020,
	title = {Gurobi {Optimizer} {Reference} {Manual}},
	url = {url = "http://www.gurobi.com"},
	author = {"Gurobi Optimization, LLC"},
	year = {2020},
}

@techreport{game2008,
	title = {Marxan {User} {Manual}: {For} {Marxan} version 1.8.10},
	institution = {University of Queensland, St. Lucia, Queensland, Australia, and Pacific Marine Analysis and Research Association, Vancouver, British Columbia, Canada},
	author = {Game, E.T. and Grantham, H.S.},
	year = {2008},
}

@article{albers2016po,
	title = {Spatially-{Correlated} {Risk} in {Nature} {Reserve} {Site} {Selection}},
	volume = {11},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0146023},
	doi = {10.1371/journal.pone.0146023},
	language = {en},
	number = {1},
	urldate = {2020-11-29},
	journal = {PLOS ONE},
	author = {Albers, Heidi J. and Busby, Gwenlyn M. and Hamaide, Bertrand and Ando, Amy W. and Polasky, Stephen},
	editor = {Linkov, Igor},
	month = jan,
	year = {2016},
	pages = {e0146023},
	file = {albers et al 2016 - Spatially-Correlated Risk in Nature Reserve Site Selection - GUPPY.pdf:/Users/bill/D/Zotero/storage/5FY8EAR9/albers et al 2016 - Spatially-Correlated Risk in Nature Reserve Site Selection - GUPPY.pdf:application/pdf},
}

@article{taleghan2015jmlr,
	title = {{PAC} {Optimal} {MDP} {Planning} with {Application} to {Invasive} {Species} {Management}},
	volume = {16},
	abstract = {In a simulator-deﬁned MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efﬁcient exploration and an improved conﬁdence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the conﬁdence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved conﬁdence intervals and the new search heuristics yield reductions of between 8\% and 47\% in the number of simulator calls required to reach near-optimal policies.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Taleghan, Majid Alkaee},
	year = {2015},
	pages = {3877--3903},
	file = {taleghan DIETTERICH ... ALBERS 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management.pdf:/Users/bill/D/Zotero/storage/6LKQZHVE/taleghan DIETTERICH ... ALBERS 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management.pdf:application/pdf},
}

@article{selwood2019cl,
	title = {Collaborative conservation planning: {Quantifying} the contribution of expert engagement to identify spatial conservation priorities},
	volume = {12},
	issn = {1755-263X, 1755-263X},
	shorttitle = {Collaborative conservation planning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/conl.12673},
	doi = {10.1111/conl.12673},
	abstract = {The importance of expert input to spatial conservation prioritization outcomes is poorly understood. We quantified the impacts of refinements made during consultation with experts on spatial conservation prioritization of Christmas Island. There was just 0.57 correlation between the spatial conservation priorities before and after consultation, bottom ranked areas being most sensitive to changes. The inclusion of a landscape condition layer was the most significant individual influence. Changes (addition, removal, modification) to biodiversity layers resulted in a combined 0.2 reduction in correlation between initial and final solutions. Representation of rare species in top ranked areas was much greater after expert consultation; representation of widely distributed species changed relatively little. Our results show how different inputs have notably different impacts on the final plan. Understanding these differences helps plan time and resources for expert consultation.},
	language = {en},
	number = {6},
	urldate = {2020-11-29},
	journal = {Conservation Letters},
	author = {Selwood, Katherine E. and Wintle, Brendan A. and Kujala, Heini},
	month = nov,
	year = {2019},
	file = {selwood wintle kujala 2019 - Collaborative conservation planning - Quantifying the contribution of expert engagement to identify spatial conservation priorities - BDPG - RESERVE SELECTION - UNCERTAINTY - ERROR MODEL.pdf:/Users/bill/D/Zotero/storage/4NLDBW6Y/selwood wintle kujala 2019 - Collaborative conservation planning - Quantifying the contribution of expert engagement to identify spatial conservation priorities - BDPG - RESERVE SELECTION - UNCERTAINTY - ERRO.pdf:application/pdf},
}

@article{woolley2017mee,
	title = {Characterising uncertainty in generalised dissimilarity models},
	volume = {8},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12710},
	doi = {10.1111/2041-210X.12710},
	language = {en},
	number = {8},
	urldate = {2020-11-29},
	journal = {Methods in Ecology and Evolution},
	author = {Woolley, Skipton N.C. and Foster, Scott D. and O'Hara, Timothy D. and Wintle, Brendan A. and Dunstan, Piers K.},
	editor = {Hodgson, David},
	month = aug,
	year = {2017},
	keywords = {uncertainty, Bayesian Bootstrap, GDM, Generalized dissimilarity models},
	pages = {985--995},
	file = {WoolleyFosterDunstanOHaraWintleBBGDM_v3.pdf:/Users/bill/D/Zotero/storage/9UMPEI4Q/WoolleyFosterDunstanOHaraWintleBBGDM_v3.pdf:application/pdf},
}

@article{albers2017i,
	title = {Economics in {Systematic} {Conservation} {Planning} for {Lower}-income {Countries}: {A} {Literature} {Review} and {Assessment}},
	volume = {/10},
	issn = {19321473},
	shorttitle = {Economics in {Systematic} {Conservation} {Planning} for {Lower}-income {Countries}},
	url = {http://www.nowpublishers.com/article/Details/IRERE-0085},
	doi = {10.1561/101.00000085},
	abstract = {Lower-income countries contain much of the world’s biodiversity but often lack the institutions and resources for eﬀective biodiversity conservation. Systematic conservation planning (SCP) frameworks provide tools to identify and implement conservation areas eﬀectively and eﬃciently but rarely address issues central to lower-income countries, which limits SCP’s usefulness in these settings. This paper reviews SCP and discusses how to make SCP more relevant in lowerincome countries. Lower-income countries have small conservation budgets, imperfect measures of conservation costs and beneﬁts, and unique institutions that all inﬂuence the siting, management, and implementation of protected area networks. In addition, these aspects of the lower-income country setting inform the reaction of people to a protected area, which determines the conservation eﬀectiveness of the protected areas. Overall, the institutional and socioeconomic settings of lower-income countries create additional layers of complexity that should be incorporated into SCP frameworks at the stage of selecting reserve sites to improve the eﬃciency of conservation policies.},
	language = {en},
	number = {2},
	urldate = {2020-12-06},
	journal = {International Review of Environmental and Resource Economics},
	author = {Albers, H. J. and Maloney, M. and Robinson, E. J. Z.},
	month = may,
	year = {2017},
	pages = {145--182},
	file = {albers et al 2017 - Economics in systematic conservation planning for lower ­income countries - a literature review and assessment - BDPG - COST - SCP - RESERVE SELECTION - UNCERTAINTY - ECONOMICS.pdf:/Users/bill/D/Zotero/storage/AYBRR44S/albers et al 2017 - Economics in systematic conservation planning for lower ­income countries - a literature review and assessment - BDPG - COST - SCP - RESERVE SELECTION - UNCERTAINTY - ECONOMICS.pdf:application/pdf},
}

@article{moilanen2008bc,
	title = {Two paths to a suboptimal solution – once more about optimality in reserve selection},
	volume = {141},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632070800150X},
	doi = {10.1016/j.biocon.2008.04.018},
	abstract = {Several studies have compared the performances of exact algorithms (integer programming) and heuristic methods in the solution of conservation resource allocation problems, with the conclusion that exact methods are always preferable. Here, I summarize a potentially major deﬁciency in how the relationship between exact and heuristic methods has been presented: the above comparisons have all been done using relatively simple (linear) maximum coverage or minimum set models that are by deﬁnition solvable using integer programming. In contrast, heuristic or meta-heuristic algorithms can be applied to less simpliﬁed nonlinear and/or stochastic problems. The focus of this study is two kinds of suboptimality, ﬁrst-stage suboptimality caused by model simpliﬁcation and second-stage suboptimality caused by inexact solution. Evidence from comparisons between integer programming and heuristic solution methods suggests a suboptimality level of around 3\%–10\% for well-chosen heuristics, much depending on the problem and data. There is also largely anecdotal evidence from a few studies that have evaluated results from simpliﬁed conservation resource allocation problems using more complicated (nonlinear) models. These studies have found that dropping components such as habitat loss rates or connectivity effects from the model can lead to suboptimality from 5\% to 50\%. Consequently, I suggest that more attention should be given to two topics, ﬁrst, how the performance of a conservation plan should be evaluated, and second, what are the consequences of simplifying the ideal conservation resource allocation model? Factors that may lead to relatively complicated problem formulations include connectivity and evaluation of long-term persistence, stochastic habitat loss and availability, species interactions, and distributions that shift due to climate change.},
	language = {en},
	number = {7},
	urldate = {2020-12-13},
	journal = {Biological Conservation},
	author = {Moilanen, Atte},
	month = jul,
	year = {2008},
	pages = {1919--1923},
	file = {Moilanen - 2008 - Two paths to a suboptimal solution – once more about optimality in reserve selection - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/8IWAH845/Moilanen - 2008 - Two paths to a suboptimal solution – once more about optimality in reserve selection - Biological Conservation.pdf:application/pdf},
}

@article{moilanen2007bc,
	title = {Landscape {Zonation}, benefit functions and target-based planning: {Unifying} reserve selection strategies},
	volume = {134},
	issn = {00063207},
	shorttitle = {Landscape {Zonation}, benefit functions and target-based planning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320706003879},
	doi = {10.1016/j.biocon.2006.09.008},
	abstract = {The most widespread reserve selection strategy is target-based planning, as speciﬁed under the framework of systematic conservation planning. Targets are given for the representation levels of biodiversity features, and site selection algorithms are employed to either meet the targets with least cost (the minimum set formulation) or to maximize the number of targets met with a given resource (maximum coverage). Beneﬁt functions are another recent approach to reserve selection. In the beneﬁt function framework the objective is to maximize the value of the reserve network, however value is deﬁned. In one beneﬁt function formulation value is a sum over species-speciﬁc values, and species-speciﬁc value is an increasing function of representation. This beneﬁt function approach is computationally convenient, but because it allows free tradeoffs between species, it essentially makes the assumption that species are acting as surrogates, or samples from a larger regional species pool. The Zonation algorithm is a recent computational method that produces a hierarchy of conservation priority through the landscape. This hierarchy is produced via iterative removal of selection units (cells) using the criterion of least marginal loss of conservation value to decide which cell to remove next. The ﬁrst variant of Zonation, here called core-area Zonation, has a characteristic of emphasizing coreareas of all species. Here I separate the Zonation meta-algorithm from the cell removal rule, the deﬁnition of marginal loss of conservation value utilized inside the algorithm. I show how additive beneﬁt functions and target-based planning can be implemented into the Zonation framework via the use of particular kinds of cell removal rules. The core-area, additive beneﬁt function and targeting beneﬁt function variants of Zonation have interesting conceptual differences in how they treat and trade off between species in the planning process.},
	language = {en},
	number = {4},
	urldate = {2020-12-13},
	journal = {Biological Conservation},
	author = {Moilanen, Atte},
	month = feb,
	year = {2007},
	pages = {571--579},
	file = {Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/W6WK3WIZ/Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf:application/pdf},
}

@article{moilanen2001,
	title = {On the use of connecti7ity measures in spatial ecology},
	language = {en},
	author = {Moilanen, Atte and Hanski, Ilkka},
	year = {2001},
	pages = {5},
	file = {Moilanen et al. - 2001 - On the use of connectivity measures in spatial ecology - Oikos.pdf:/Users/bill/D/Zotero/storage/DCQ8T26L/Moilanen et al. - 2001 - On the use of connectivity measures in spatial ecology - Oikos.pdf:application/pdf},
}

@article{moilanen2006cb,
	title = {Uncertainty {Analysis} for {Regional}-{Scale} {Reserve} {Selection}},
	volume = {20},
	issn = {0888-8892, 1523-1739},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2006.00560.x},
	doi = {10.1111/j.1523-1739.2006.00560.x},
	abstract = {Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error. We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, “distribution discounting,” in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.},
	language = {en},
	number = {6},
	urldate = {2020-12-13},
	journal = {Conservation Biology},
	author = {Moilanen, Atte and Wintle, Brendan A. and Elith, Jane and Burgman, Mark},
	month = dec,
	year = {2006},
	pages = {1688--1697},
	file = {Moilanen et al. - 2006 - Uncertainty Analysis for Regional-Scale Reserve Selection - Conservation Biology.pdf:/Users/bill/D/Zotero/storage/CP7NM73V/Moilanen et al. - 2006 - Uncertainty Analysis for Regional-Scale Reserve Selection - Conservation Biology.pdf:application/pdf},
}

@article{moilanen2009bc,
	title = {Assessing replacement cost of conservation areas: {How} does habitat loss influence priorities?},
	volume = {142},
	issn = {00063207},
	shorttitle = {Assessing replacement cost of conservation areas},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632070800445X},
	doi = {10.1016/j.biocon.2008.11.011},
	abstract = {Replacement cost refers to the loss incurred if the ideal set of conservation areas cannot be protected due to compulsory inclusion or exclusion of some area candidates. This cost can be deﬁned either in terms of loss of conservation value or in terms of extra acquisition cost, and it has a clear mathematical deﬁnition as a difference between the value of the unconstrained optimal solution and a constrained suboptimal solution. In this work we for the ﬁrst time show how replacement cost can be calculated in the context of sequential reserve selection, where a reserve network is developed over a longer time period and ongoing habitat loss inﬂuences retention and availability of sites. In case of site exclusion, a question that can be asked is, ‘‘if a site belonging to the ideal (optimal) solution cannot be obtained, what expected loss in reserve network value does this entail by the end of the planning period given that the rest of the solution is re-organized in the most advantageous manner?’’ Heuristically, the proposed method achieves the ambit of combining irreplaceability and vulnerability into one score of site importance. We applied replacement cost analysis to conservation prioritization for wood-inhabiting fungi in Norway, identifying factors that inﬂuence replacement cost and urgency of site acquisition. Among other things we ﬁnd that the reliability of loss rate information is important, because the optimal site acquisition order may be strongly inﬂuenced by underestimated loss rates.},
	language = {en},
	number = {3},
	urldate = {2020-12-13},
	journal = {Biological Conservation},
	author = {Moilanen, Atte and Arponen, Anni and Stokland, Jogeir N. and Cabeza, Mar},
	month = mar,
	year = {2009},
	pages = {575--585},
	file = {Moilanen et al. - 2009 - Assessing replacement cost of conservation areas How does habitat loss influence priorities - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/N9Q7F5MM/Moilanen et al. - 2009 - Assessing replacement cost of conservation areas How does habitat loss influence priorities - Biological Conservation.pdf:application/pdf},
}

@article{moilanen2005cb,
	title = {Variance and {Uncertainty} in the {Expected} {Number} of {Occurrences} in {Reserve} {Selection}},
	volume = {19},
	issn = {0888-8892, 1523-1739},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2005.00203.x},
	doi = {10.1111/j.1523-1739.2005.00203.x},
	abstract = {Reserve selection often concerns the design of reserve networks for the long-term maintenance of biodiversity. We considered uncertainty in the context of three common reserve-selection formulations, the expected number of populations, proportional coverage of land-cover types, and the probability of having at least one population. By uncertainty, we mean variance in the outcome of any probability-based reserve selection formulation. A typical reserve-selection formulation might ask for the least expensive set of sites that contains n populations per species. It is implicit here that this requirement concerns the expected number of populations, which actually is obtained only with a 50\% chance. If the requirement is changed to select the least expensive set of sites that gives n populations per species with a 95\% probability, the number of sites required in the solution increases and the identity of the sites is changed toward sites that have high probabilities of persistence (or occurrence) and low associated binomial variance. Anthropogenic threat is one factor that may cause probabilistic uncertainty in the context of proportional area coverage.},
	language = {en},
	number = {5},
	urldate = {2020-12-13},
	journal = {Conservation Biology},
	author = {Moilanen, Atte and Cabeza, Mar},
	month = oct,
	year = {2005},
	pages = {1663--1667},
	file = {MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf:/Users/bill/D/Zotero/storage/83YU9U5X/MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf:application/pdf},
}

@article{moilanen2002,
	title = {{SIMPLE} {CONNECTIVITY} {MEASURES} {IN} {SPATIAL} {ECOLOGY}},
	volume = {83},
	abstract = {Connectivity is a fundamental concept that is widely utilized in spatial ecology. The majority of connectivity measures used in the recent ecological literature only consider the nearest neighbor patch/population, or patches within a limited neighborhood of the focal patch (a buffer). Meta-analysis suggests that studies using nearest neighbor connectivity measures are much less likely to ﬁnd statistically signiﬁcant effects of connectivity than studies that use more complex measures. Here we compare simple connectivity measures in their ability to predict colonization events in two large and good-quality empirical data sets. The nearest neighbor distance to an occupied patch is found to be an inferior measure. Buffer measures do much better, but their performance is found to be sensitive to the estimate of the buffer radius. For highly fragmented habitats, the best and most consistent performance is found for a measure that takes into account the size of the focal patch and the sizes of and distances to all potential source populations. When experimenting with reduced data sets, it was discovered that nearest neighbor measures fail to ﬁnd a statistically signiﬁcant effect of connectivity for a large range of data set sizes for which the more complex measures still detect a highly signiﬁcant effect. We conclude that the simplicity of a nearest neighbor measure is not an adequate compensation for poor performance.},
	language = {en},
	number = {4},
	author = {Moilanen, Atte and Nieminen, Marko},
	year = {2002},
	pages = {15},
	file = {Moilanen, Nieminen - 2002 - Simple Connectivity Measures in Spatial Ecology - Ecology.pdf:/Users/bill/D/Zotero/storage/MNFNZX2N/Moilanen, Nieminen - 2002 - Simple Connectivity Measures in Spatial Ecology - Ecology.pdf:application/pdf},
}

@article{moilanen2006bc,
	title = {Uncertainty analysis favours selection of spatially aggregated reserve networks},
	volume = {129},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320705004891},
	doi = {10.1016/j.biocon.2005.11.006},
	abstract = {It has been widely argued that habitat fragmentation is bad for (meta)population persistence and that a high level of fragmentation is a similarly undesirable characteristic for a reserve network. However, modelling the effects of fragmentation for many species is very difﬁcult due to high data demands and uncertainty concerning its effect on particular species. Hence, several reserve selection methods employ qualitative heuristics such as boundary length penalties that aggregate reserve network structures. This aggregation usually comes at a cost because low quality habitats will be included for the sake of increased connectivity. Here a biologically justiﬁed method for designing aggregated reserve networks based on a technique called distribution smoothing is investigated. As with the boundary length penalty, its use incurs an apparent biological cost. However, taking a step further, potential negative effects of fragmentation on individual species are evaluated using a decision-theoretic uncertainty analysis approach. This analysis shows that the aggregated reserve network (based on smoothed distributions) is likely to be biologically more valuable than a more fragmented one (based on habitat model predictions). The method is illustrated with a reserve design case study in the Hunter Valley of south-eastern Australia. The uncertainty analysis method, based on information-gap decision theory, provides a systematic framework for making robust decisions under severe uncertainty, making it particularly well adapted to reserve design problems.},
	language = {en},
	number = {3},
	urldate = {2020-12-13},
	journal = {Biological Conservation},
	author = {Moilanen, Atte and Wintle, Brendan A.},
	month = may,
	year = {2006},
	pages = {427--434},
	file = {Moilanen, Wintle - 2006 - Uncertainty analysis favours selection of spatially aggregated reserve networks - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/MULCV9PZ/Moilanen, Wintle - 2006 - Uncertainty analysis favours selection of spatially aggregated reserve networks - Biological Conservation.pdf:application/pdf},
}

@article{carroll2010gcb,
	title = {Optimizing resiliency of reserve networks to climate change: multispecies conservation planning in the {Pacific} {Northwest}, {USA}},
	volume = {16},
	issn = {13541013, 13652486},
	shorttitle = {Optimizing resiliency of reserve networks to climate change},
	url = {http://doi.wiley.com/10.1111/j.1365-2486.2009.01965.x},
	doi = {10.1111/j.1365-2486.2009.01965.x},
	abstract = {The effectiveness of a system of reserves may be compromised under climate change as species’ habitat shifts to nonreserved areas, a problem that may be compounded when well-studied vertebrate species are used as conservation umbrellas for other taxa. The Northwest Forest Plan was among the ﬁrst efforts to integrate conservation of wideranging focal species and localized endemics into regional conservation planning. We evaluated how effectively the plan’s focal species, the Northern Spotted Owl, acts as an umbrella for localized species under current and projected future climates and how the regional system of reserves can be made more resilient to climate change. We used the program MAXENT to develop distribution models integrating climate data with vegetation variables for the owl and 130 localized species. We used the program ZONATION to identify a system of areas that efﬁciently captures habitat for both the owl and localized species and prioritizes refugial areas of climatic and topographic heterogeneity where current and future habitat for dispersal-limited species is in proximity. We projected future species’ distributions based on an ensemble of contrasting climate models, and incorporating uncertainty between alternate climate projections into the prioritization process. Reserve solutions based on the owl overlap areas of high localized-species richness but poorly capture core areas of localized species’ distribution. Congruence between priority areas across taxa increases when refugial areas are prioritized. Although corearea selection strategies can potentially increase the conservation value and resilience of regional reserve systems, they accentuate contrasts in priority areas between species and over time and should be combined with a broadened taxonomic scope and increased attention to potential effects of climate change. Our results suggest that systems of ﬁxed reserves designed for resilience can increase the likelihood of retaining the biological diversity of forest ecosystems under climate change.},
	language = {en},
	number = {3},
	urldate = {2020-12-13},
	journal = {Global Change Biology},
	author = {Carroll, Carlos and Dunk, Jeffrey R. and Moilanen, Atte},
	month = mar,
	year = {2010},
	pages = {891--904},
	file = {Carroll, Dunk, Moilanen - 2010 - Optimizing resiliency of reserve networks to climate change multispecies conservation planning in the Pacific Northwest, USA - Global Change Biology.pdf:/Users/bill/D/Zotero/storage/I9RVCY5H/Carroll, Dunk, Moilanen - 2010 - Optimizing resiliency of reserve networks to climate change multispecies conservation planning in the Pacific Northwest, USA - Global Change Biology.pdf:application/pdf},
}

@article{cabeza2003cb,
	title = {Site-{Selection} {Algorithms} and {Habitat} {Loss}},
	volume = {17},
	issn = {0888-8892, 1523-1739},
	url = {http://doi.wiley.com/10.1046/j.1523-1739.2003.01421.x},
	doi = {10.1046/j.1523-1739.2003.01421.x},
	abstract = {Site-selection algorithms are used in reserve design to select networks of sites that maximize biodiversity, given some constraints. These algorithms are based on a snapshot of species occurrence, and they typically aim to minimize the area or cost needed to represent all the species once or a few times. Most of these algorithms ignore the question of how well species are likely to persist in the set of selected sites in the long term. Furthermore, the role of the unselected habitat in biodiversity persistence has received no attention in this context. We used a theoretical approach to evaluate the long-term performance of reserve networks in preserving biodiversity by using a model of spatiotemporal population dynamics (a metapopulation model). We compared extinction rates of species in reserve networks in two situations: when all sites remain suitable habitat for the species and, conversely, when the habitat in the unselected sites is lost. We made this comparison to explore the significance of unselected sites for spatial population dynamics and for the continued presence of species in the reserve network. Basic site-selection algorithms are liable to perform badly in terms of biodiversity maintenance because the persistence of species may be strongly dependent on sites not included in the reserve network. Our results support recent calls for the integration of spatial population modeling into reserve network design. Advances in metapopulation theory provide tools that can be used for this purpose.},
	language = {en},
	number = {5},
	urldate = {2020-12-13},
	journal = {Conservation Biology},
	author = {Cabeza, Mar and Moilanen, Atte},
	month = oct,
	year = {2003},
	pages = {1402--1413},
	file = {Cabeza, Moilanen - 2003 - Site-Selection Algorithms and Habitat Loss - October.pdf:/Users/bill/D/Zotero/storage/24LDZHFR/Cabeza, Moilanen - 2003 - Site-Selection Algorithms and Habitat Loss - October.pdf:application/pdf},
}

@article{arponen2008jae,
	title = {A successful community-level strategy for conservation prioritization},
	volume = {45},
	issn = {00218901, 13652664},
	url = {http://doi.wiley.com/10.1111/j.1365-2664.2008.01513.x},
	doi = {10.1111/j.1365-2664.2008.01513.x},
	language = {en},
	number = {5},
	urldate = {2020-12-13},
	journal = {Journal of Applied Ecology},
	author = {Arponen, Anni and Moilanen, Atte and Ferrier, Simon},
	month = oct,
	year = {2008},
	pages = {1436--1445},
	file = {Arponen, Moilanen, Ferrier - 2008 - A successful community-level strategy for conservation prioritization - Journal of Applied Ecology.pdf:/Users/bill/D/Zotero/storage/RI8PVF4Q/Arponen, Moilanen, Ferrier - 2008 - A successful community-level strategy for conservation prioritization - Journal of Applied Ecology.pdf:application/pdf},
}

@article{moilanen2007bca,
	title = {Landscape {Zonation}, benefit functions and target-based planning: {Unifying} reserve selection strategies},
	volume = {134},
	issn = {00063207},
	shorttitle = {Landscape {Zonation}, benefit functions and target-based planning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320706003879},
	doi = {10.1016/j.biocon.2006.09.008},
	abstract = {The most widespread reserve selection strategy is target-based planning, as speciﬁed under the framework of systematic conservation planning. Targets are given for the representation levels of biodiversity features, and site selection algorithms are employed to either meet the targets with least cost (the minimum set formulation) or to maximize the number of targets met with a given resource (maximum coverage). Beneﬁt functions are another recent approach to reserve selection. In the beneﬁt function framework the objective is to maximize the value of the reserve network, however value is deﬁned. In one beneﬁt function formulation value is a sum over species-speciﬁc values, and species-speciﬁc value is an increasing function of representation. This beneﬁt function approach is computationally convenient, but because it allows free tradeoffs between species, it essentially makes the assumption that species are acting as surrogates, or samples from a larger regional species pool. The Zonation algorithm is a recent computational method that produces a hierarchy of conservation priority through the landscape. This hierarchy is produced via iterative removal of selection units (cells) using the criterion of least marginal loss of conservation value to decide which cell to remove next. The ﬁrst variant of Zonation, here called core-area Zonation, has a characteristic of emphasizing coreareas of all species. Here I separate the Zonation meta-algorithm from the cell removal rule, the deﬁnition of marginal loss of conservation value utilized inside the algorithm. I show how additive beneﬁt functions and target-based planning can be implemented into the Zonation framework via the use of particular kinds of cell removal rules. The core-area, additive beneﬁt function and targeting beneﬁt function variants of Zonation have interesting conceptual differences in how they treat and trade off between species in the planning process.},
	language = {en},
	number = {4},
	urldate = {2020-12-13},
	journal = {Biological Conservation},
	author = {Moilanen, Atte},
	month = feb,
	year = {2007},
	pages = {571--579},
	file = {Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf:/Users/bill/D/Zotero/storage/DD3TZ94J/Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf:application/pdf},
}

@article{moilanen2005cba,
	title = {Variance and {Uncertainty} in the {Expected} {Number} of {Occurrences} in {Reserve} {Selection}},
	volume = {19},
	issn = {0888-8892, 1523-1739},
	url = {http://doi.wiley.com/10.1111/j.1523-1739.2005.00203.x},
	doi = {10.1111/j.1523-1739.2005.00203.x},
	abstract = {Reserve selection often concerns the design of reserve networks for the long-term maintenance of biodiversity. We considered uncertainty in the context of three common reserve-selection formulations, the expected number of populations, proportional coverage of land-cover types, and the probability of having at least one population. By uncertainty, we mean variance in the outcome of any probability-based reserve selection formulation. A typical reserve-selection formulation might ask for the least expensive set of sites that contains n populations per species. It is implicit here that this requirement concerns the expected number of populations, which actually is obtained only with a 50\% chance. If the requirement is changed to select the least expensive set of sites that gives n populations per species with a 95\% probability, the number of sites required in the solution increases and the identity of the sites is changed toward sites that have high probabilities of persistence (or occurrence) and low associated binomial variance. Anthropogenic threat is one factor that may cause probabilistic uncertainty in the context of proportional area coverage.},
	language = {en},
	number = {5},
	urldate = {2020-12-13},
	journal = {Conservation Biology},
	author = {Moilanen, Atte and Cabeza, Mar},
	month = oct,
	year = {2005},
	pages = {1663--1667},
	file = {MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf:/Users/bill/D/Zotero/storage/23WNK5V7/MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf:application/pdf},
}

@article{nat,
	title = {Appendix {B} from {A}. {Moilanen}, “{Reserve} {Selection} {Using} {Nonlinear} {Species} {Distribution} {Models}”},
	language = {en},
	author = {Nat, Am},
	pages = {2},
	file = {moilanen 2005 - reserve selection using nonlinear species distribution models - appdx b - amnat.pdf:/Users/bill/D/Zotero/storage/CICSKJRE/moilanen 2005 - reserve selection using nonlinear species distribution models - appdx b - amnat.pdf:application/pdf},
}

@article{haider2018em,
	title = {A robust optimization approach for solving problems in conservation planning},
	volume = {368},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380017304933},
	doi = {10.1016/j.ecolmodel.2017.12.006},
	language = {en},
	urldate = {2020-12-14},
	journal = {Ecological Modelling},
	author = {Haider, Zulqarnain and Charkhgard, Hadi and Kwon, Changhyun},
	month = jan,
	year = {2018},
	pages = {288--297},
	file = {Haider et al 2018 - a robust optimization approach for solving problems in conservation planning - BDPG - RESERVE SELECTION - GUPPY - ANNO.pdf:/Users/bill/D/Zotero/storage/MGYZ7I5Y/Haider et al 2018 - a robust optimization approach for solving problems in conservation planning - BDPG - RESERVE SELECTION - GUPPY - ANNO.pdf:application/pdf},
}

@article{albers2020ere,
	title = {Optimal {Siting}, {Sizing}, and {Enforcement} of {Marine} {Protected} {Areas}},
	volume = {77},
	issn = {0924-6460, 1573-1502},
	url = {http://link.springer.com/10.1007/s10640-020-00472-7},
	doi = {10.1007/s10640-020-00472-7},
	abstract = {The design of protected areas, whether marine or terrestrial, rarely considers how people respond to the imposition of no-take sites with complete or incomplete enforcement. Consequently, these protected areas may fail to achieve their intended goal. We present and solve a spatial bio-economic model in which a manager chooses the optimal location, size, and enforcement level of a marine protected area (MPA). This manager acts as a Stackelberg leader, and her choices consider villagers’ best response to the MPA in a spatial Nash equilibrium of fishing site and effort decisions. Relevant to lower income country settings but general to other settings, we incorporate limited enforcement budgets, distance costs of traveling to fishing sites, and labor allocation to onshore wage opportunities. The optimal MPA varies markedly across alternative manager goals and budget sizes, but always induce changes in villagers’ decisions as a function of distance, dispersal, and wage. We consider MPA managers with ecological conservation goals and with economic goals, and identify the shortcomings of several common manager decision rules, including those focused on: (1) fishery outcomes rather than broader economic goals, (2) fish stocks at MPA sites rather than across the full marinescape, (3) absolute levels rather than additional values, and (4) costless enforcement. Our results demonstrate that such naïve or overly narrow decision rules can lead to inefficient MPA designs that miss economic and conservation opportunities.},
	language = {en},
	number = {1},
	urldate = {2020-12-14},
	journal = {Environmental and Resource Economics},
	author = {Albers, H. J. and Preonas, L. and Capitán, T. and Robinson, E. J. Z. and Madrigal-Ballestero, R.},
	month = sep,
	year = {2020},
	pages = {229--269},
	file = {albers et al 2020 - optimal siting sizing and enforcement of marine protected areas - RESERVE SELECTION - ANNO.pdf:/Users/bill/D/Zotero/storage/S3AXW53A/albers et al 2020 - optimal siting sizing and enforcement of marine protected areas - RESERVE SELECTION - ANNO.pdf:application/pdf},
}

@misc{pappas,
	title = {pappas et al 2009 - {A} {Comparison} of {Heuristic}, {Meta}-{Heuristic} and {Optimal} {Approaches} to the {Selection} of {Conservation} {Area} {Networks} - {BDPG} - {RESERVE} {SELECTION} - {TABU} {SEARCH} - {ANNO}.pdf},
	author = {Pappas, Christopher},
	file = {pappas et al 2009 - A Comparison of Heuristic, Meta-Heuristic and Optimal Approaches to the Selection of Conservation Area Networks - BDPG - RESERVE SELECTION - TABU SEARCH - ANNO.pdf:/Users/bill/D/Zotero/storage/RJI2GVNQ/pappas et al 2009 - A Comparison of Heuristic, Meta-Heuristic and Optimal Approaches to the Selection of Conservation Area Networks - BDPG - RESERVE SELECTION - TABU SEARCH - ANNO.pdf:application/pdf},
}

@article{schobel,
	title = {Mitglieder der {Pru}¨fungskommission},
	language = {en},
	author = {Schobel, Dr Anita},
	pages = {187},
	file = {ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:/Users/bill/D/Zotero/storage/VX27QIFW/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:application/pdf},
}

@article{schobela,
	title = {Mitglieder der {Pru}¨fungskommission},
	language = {en},
	author = {Schobel, Dr Anita},
	pages = {187},
	file = {ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:/Users/bill/D/Zotero/storage/EY93XUFV/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:application/pdf},
}

@phdthesis{ide2014,
	address = {Gottingen},
	title = {Concepts of robustness for uncertain multi-objective optimization},
	school = {Georg-August University},
	author = {Ide, Jonas},
	year = {2014},
	keywords = {bdpg, uncertainty, EFs, optimization, multi-objective, robust},
	annote = {


2 Cumulative Part of the Dissertation 
In this Chapter, we summarize five publications contributing to this thesis’ scientific value. In Chapter 2.1, Ehrgott et al. (2014) (see Addendum A) is summarized. Here, the concept of minmax robustness is extended from single objective optimization problems to multi-objective optimization problems using a different approach than Kuroiwa and Lee (2012). The new approach is investigated closely, and several algorithms for calculating the respective solutions are presented. 
Chapter 2.2 considers the publication Ide and Sch ̈obel (2013) (see Addendum B), where other concepts of robustness for multi-objective optimization problems are in- troduced, namely the concepts of highly, flimsily, and lightly robust efficiency. These concepts are compared with each other as well as with the concept of minmax robustness from Chapter 2.1, and the concept of robustness presented by Kuroiwa and Lee (2012). 
Ide and K ̈obis (2013) (Chapter 2.3, see Addendum C) study the connection between uncertain multi-objective optimization, as introduced by Ehrgott et al. (2014) and set- valued optimization. From this connection, they derive other concepts of efficiency for uncertain multi-objective optimization problems and present algorithms for finding respective solutions. These concepts are extended by Ide et al. (2014) (Chapter 2.4, see Addendum D) to general spaces. Furthermore, the authors extend algorithms for calculating the respective solutions to general spaces and cones, and by this are able to formulate algorithms for solving special classes of set-valued optimization problems. 
Finally, in Ide et al. (2013) (Chapter 2.5, see Addendum E), a real-world application of uncertain multi-objective optimization is presented. The authors describe the modeling process and investigate on the practical value of minmax robust efficient solutions as presented in Ehrgott et al. (2014). 
The connection between the publications is pointed out throughout this chapter, and discussed in detail in Chapter 3. 


},
	file = {ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:/Users/bill/D/Zotero/storage/T9EEQ7SQ/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf:application/pdf},
}

@article{pressey2017bc,
	title = {From displacement activities to evidence-informed decisions in conservation},
	volume = {212},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320717310376},
	doi = {10.1016/j.biocon.2017.06.009},
	abstract = {This paper highlights a disjunction between the basic motivation of conservation planners, policy-makers, and managers, which is to make a positive diﬀerence for biodiversity, and many of our day-to-day activities, which are tangential (at best) to the goal of avoiding biodiversity loss. At the core of this problem is the use of conservation measures (inputs, outputs, and outcomes) that do not explicitly address conservation impact, and thus risk undermining its achievement. These measures are used to formulate policy targets and operational objectives, gauge progress towards them, and identify priorities for action. In particular, the pervasive use of representation of biodiversity features as a sole basis for identifying priorities, and the considerable global eﬀort directed towards increasing protected-area extent and assessing protected-area management eﬀectiveness, exemplify that much conservation decision-making is founded more on belief systems than evidence. Measures such as the extent or representativeness of protected areas risk misdirecting conservation actions towards areas of low impact and misleading decision-makers and the public about conservation progress. To promote more eﬀective, evidence-informed decision-making, analytical evidence can and should be used to test and reﬁne decision-makers' implicit models of the world, focusing on predicting conservation impact - the future diﬀerence made by our future actions - to increase our eﬀectiveness and accountability.},
	language = {en},
	urldate = {2020-12-14},
	journal = {Biological Conservation},
	author = {Pressey, Robert L. and Weeks, Rebecca and Gurney, Georgina G.},
	month = aug,
	year = {2017},
	pages = {337--348},
	file = {pressey et al 2017 - From displacement activities to evidence-informed decisions in conservation - EFs - PREDICTION - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/FMN4GYGR/pressey et al 2017 - From displacement activities to evidence-informed decisions in conservation - EFs - PREDICTION - BDPG - ANNO.pdf:application/pdf},
}

@article{chen2006,
	title = {{COMPARISON} {OF} {METHODS} {FOR} {UNCERTAINTY} {ANALYSIS} {OF} {HYDROLOGIC} {MODELS}},
	language = {en},
	author = {Chen, Changjun and Shrestha, Durga Lal and Perez, Gerald Corzo and Solomatine, Dimitri},
	year = {2006},
	pages = {8},
	file = {Chen,Shrestha,Corzo,Solomatine,ComparisonMethodUncertainty,ProcHI,2006.pdf:/Users/bill/D/Zotero/storage/YSPJFGXM/Chen,Shrestha,Corzo,Solomatine,ComparisonMethodUncertainty,ProcHI,2006.pdf:application/pdf},
}

@article{kayastha,
	title = {{EXPERIMENTS} {WITH} {SEVERAL} {METHODS} {OF} {PARAMETER} {UNCERTAINTY} {ESTIMATION} {IN} {HYDROLOGICAL} {MODELING}},
	language = {en},
	author = {Kayastha, Nagendra and Shrestha, Durga Lal and Solomatine, Dimitri},
	pages = {9},
	file = {Kayastha,Shrestha,Solomatine,ExperimentsSevelMethods,HIC,2010.pdf:/Users/bill/D/Zotero/storage/UW3Z3FUC/Kayastha,Shrestha,Solomatine,ExperimentsSevelMethods,HIC,2010.pdf:application/pdf},
}

@article{pagano2013hp,
	title = {Ensemble dressing for hydrological applications: {ENSEMBLE} {DRESSING} {FOR} {HYDROLOGICAL} {APPLICATIONS}},
	volume = {27},
	issn = {08856087},
	shorttitle = {Ensemble dressing for hydrological applications},
	url = {http://doi.wiley.com/10.1002/hyp.9313},
	doi = {10.1002/hyp.9313},
	language = {en},
	number = {1},
	urldate = {2020-12-15},
	journal = {Hydrological Processes},
	author = {Pagano, Thomas C. and Shrestha, Durga Lal and Wang, Q. J. and Robertson, David and Hapuarachchi, Prasantha},
	month = jan,
	year = {2013},
	pages = {106--116},
	file = {pagano shrestha et al 2012 - Ensemble dressing for hydrological applications - GUPPY - ERROR - ENSEMBLES.pdf:/Users/bill/D/Zotero/storage/YVG7L4PK/pagano shrestha et al 2012 - Ensemble dressing for hydrological applications - GUPPY - ERROR - ENSEMBLES.pdf:application/pdf},
}

@article{shrestha2006nc,
	title = {Experiments with {AdaBoost}.{RT}, an {Improved} {Boosting} {Scheme} for {Regression}},
	volume = {18},
	issn = {0899-7667, 1530-888X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1678},
	doi = {10.1162/neco.2006.18.7.1678},
	abstract = {The application of boosting technique to regression problems has received relatively little attention in contrast to research aimed at classification problems. This letter describes a new boosting algorithm, AdaBoost.RT, for regression problems. Its idea is in filtering out the examples with the relative estimation error that is higher than the preset threshold value, and then following the AdaBoost procedure. Thus, it requires selecting the suboptimal value of the error threshold to demarcate examples as poorly or well predicted. Some experimental results using the M5 model tree as a weak learning machine for several benchmark data sets are reported. The results are compared to other boosting methods, bagging, artificial neural networks, and a single M5 model tree. The preliminary empirical comparisons show higher performance of AdaBoost.RT for most of the considered data sets.},
	language = {en},
	number = {7},
	urldate = {2020-12-15},
	journal = {Neural Computation},
	author = {Shrestha, D. L. and Solomatine, D. P.},
	month = jul,
	year = {2006},
	pages = {1678--1710},
	file = {Shrestha and Solomatine 2006 - Experiments with AdaBoost.RT - an Improved Boosting Scheme for Regression.pdf:/Users/bill/D/Zotero/storage/MZTB7GV7/Shrestha and Solomatine 2006 - Experiments with AdaBoost.RT - an Improved Boosting Scheme for Regression.pdf:application/pdf},
}

@article{shrestha2006nn,
	title = {Machine learning approaches for estimation of prediction interval for the model output},
	volume = {19},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608006000153},
	doi = {10.1016/j.neunet.2006.01.012},
	abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and ﬁnally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artiﬁcial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
	language = {en},
	number = {2},
	urldate = {2020-12-15},
	journal = {Neural Networks},
	author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
	month = mar,
	year = {2006},
	pages = {225--235},
	file = {Shrestha and Solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - GUPPY - UNCERTAINTY - ML - PROBLEM DIFFICULTY - BIODIVPROBGEN.pdf:/Users/bill/D/Zotero/storage/6LVLY9JB/Shrestha and Solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - GUPPY - UNCERTAINTY - ML - PROBLEM DIFFICULTY - BIODIVPROBGEN.pdf:application/pdf},
}

@article{shrestha2006,
	title = {{ASSESSING} {MODEL} {PREDICTION} {LIMITS} {USING} {FUZZY} {CLUSTERING} {AND} {MACHINE} {LEARNING}},
	language = {en},
	author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
	year = {2006},
	pages = {8},
	file = {shrestha et al 2006 - assessing model prediction limits using fuzzy clustering and machine learning.pdf:/Users/bill/D/Zotero/storage/Y8YZ75CJ/shrestha et al 2006 - assessing model prediction limits using fuzzy clustering and machine learning.pdf:application/pdf},
}

@article{shrestha2009hess,
	title = {A novel approach to parameter uncertainty analysis of hydrological models using neural networks},
	abstract = {In this study, a methodology has been developed to emulate a time consuming Monte Carlo (MC) simulation by using an Artiﬁcial Neural Network (ANN) for the assessment of model parametric uncertainty. First, MC simulation of a given process model is run. Then an ANN is trained to approximate the functional relationships between the input variables of the process model and the synthetic uncertainty descriptors estimated from the MC realizations. The trained ANN model encapsulates the underlying characteristics of the parameter uncertainty and can be used to predict uncertainty descriptors for the new data vectors. This approach was validated by comparing the uncertainty descriptors in the veriﬁcation data set with those obtained by the MC simulation. The method is applied to estimate the parameter uncertainty of a lumped conceptual hydrological model, HBV, for the Brue catchment in the United Kingdom. The results are quite promising as the prediction intervals estimated by the ANN are reasonably accurate. The proposed techniques could be useful in real time applications when it is not practicable to run a large number of simulations for complex hydrological models and when the forecast lead time is very short.},
	language = {en},
	journal = {Hydrol. Earth Syst. Sci.},
	author = {Shrestha, D L and Kayastha, N and Solomatine, D P},
	year = {2009},
	pages = {14},
	file = {shrestha et al 2009  A novel approach to parameter uncertainty analysis of hydrological models using neural networks.pdf:/Users/bill/D/Zotero/storage/PRZ27GB5/shrestha et al 2009  A novel approach to parameter uncertainty analysis of hydrological models using neural networks.pdf:application/pdf},
}

@article{shrestha2014jh,
	title = {Encapsulation of parametric uncertainty statistics by various predictive machine learning models: {MLUE} method},
	volume = {16},
	issn = {1464-7141, 1465-1734},
	shorttitle = {Encapsulation of parametric uncertainty statistics by various predictive machine learning models},
	url = {https://iwaponline.com/jh/article/16/1/95/17/Encapsulation-of-parametric-uncertainty-statistics},
	doi = {10.2166/hydro.2013.242},
	abstract = {Monte Carlo simulation-based uncertainty analysis techniques have been applied successfully in hydrology for quantiﬁcation of the model output uncertainty. They are ﬂexible, conceptually simple and straightforward, but provide only average measures of uncertainty based on past data. However, if one needs to estimate uncertainty of a model in a particular hydro-meteorological situation in real time application of complex models, Monte Carlo simulation becomes impractical because of the large number of model runs required. This paper presents a novel approach to encapsulating and predicting parameter uncertainty of hydrological models using machine learning techniques.},
	language = {en},
	number = {1},
	urldate = {2020-12-15},
	journal = {Journal of Hydroinformatics},
	author = {Shrestha, Durga L. and Kayastha, Nagendra and Solomatine, Dimitri and Price, Roland},
	month = jan,
	year = {2014},
	pages = {95--113},
	file = {shrestha et al 2014 - Encapsulation of parametric uncertainty statistics by various predictive machine learning models - MLUE method - GUPPY - ERROR.pdf:/Users/bill/D/Zotero/storage/PHQ2UQK7/shrestha et al 2014 - Encapsulation of parametric uncertainty statistics by various predictive machine learning models - MLUE method - GUPPY - ERROR.pdf:application/pdf},
}

@article{shrestha2006nna,
	title = {Machine learning approaches for estimation of prediction interval for the model output},
	volume = {19},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608006000153},
	doi = {10.1016/j.neunet.2006.01.012},
	abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and ﬁnally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artiﬁcial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
	language = {en},
	number = {2},
	urldate = {2020-12-15},
	journal = {Neural Networks},
	author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
	month = mar,
	year = {2006},
	pages = {225--235},
	file = {shrestha solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - PREDICTION INTERVALS - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/2Z4N3366/shrestha solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - PREDICTION INTERVALS - BDPG - GUPPY.pdf:application/pdf},
}

@article{shrestha,
	title = {{PREDICTING} {HYDROLOGICAL} {MODELS} {UNCERTAINTY}: {USE} {OF} {MACHINE} {LEARNING}},
	abstract = {This paper presents a methodology for assessing total model uncertainty using machine learning techniques. Historical model errors are assumed to be indicator of total model uncertainty. The model uncertainty is measured in the form of the model errors quantiles or prediction intervals (PIs) and such expression of uncertainty comprises all sources of uncertainty (e.g. model structure, model parameters, input data and output data etc.) without attempting to separate the contribution given by the individual sources of uncertainties. The method consists of partition of the model input data into different clusters. The data belonging to the same cluster have similar values of model errors (at least mean and variance). This is done by building a data matrix by combining (some of the) historical model inputs and corresponding model errors; partitioning this calibration data using clustering techniques such as crisp cluster or fuzzy clustering. PIs are constructed for each cluster by constructing empirical distribution of the model errors. The estimation of PIs for unseen test (or validation) data can be done by i) “eager” supervised classification, ii) instance-based (prototype) learning, and iii) supervised regression method. In classification method classifiers are built from the cluster labels and input data matrix and this classifier classifies the unseen input data. Estimation of PIs for the given input data consists of query of lookup table between cluster labels and PIs. In instance-based learning instead of building classifier, distance function is used to identify the cluster for the given validation input data, and represent it by its prototype (typically, its center). In regression method, PIs to each input in calibration data set are computed. Two regression models that estimate upper and lower PIs independently are trained from the input data matrix. The trained regression models are applied to estimate PIs in the unseen validation data set. The third approach was applied by Shrestha and Solomatine (2006) to estimate uncertainty of river flows. This paper presents the instance based approach to estimate the total model uncertainty of simulated river flows by HBV model of the case study of Brue catchment in United Kingdom.},
	language = {en},
	author = {Shrestha, Durga Lal and Solomatine, Dimitri},
	pages = {11},
	file = {shrestha solomatine 2007 - PREDICTING HYDROLOGICAL MODELS UNCERTAINTY - USE OF MACHINE LEARNING - ProcIAHR2007.pdf:/Users/bill/D/Zotero/storage/8TVUS36I/shrestha solomatine 2007 - PREDICTING HYDROLOGICAL MODELS UNCERTAINTY - USE OF MACHINE LEARNING - ProcIAHR2007.pdf:application/pdf},
}

@article{shrestha2006a,
	title = {{ASSESSING} {MODEL} {PREDICTION} {LIMITS} {USING} {FUZZY} {CLUSTERING} {AND} {MACHINE} {LEARNING}},
	language = {en},
	author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
	year = {2006},
	pages = {8},
	file = {Shrestha_and_Solomatine,_2006c.pdf:/Users/bill/D/Zotero/storage/KRD2NNJJ/Shrestha_and_Solomatine,_2006c.pdf:application/pdf},
}

@article{shresthaa,
	title = {{ENCAPSULATION} {OF} {MONTE}-{CARLO} {UNCERTAINTY} {ANALYSIS} {RESULTS} {IN} {A} {PREDICTIVE} {MACHINE} {LEARNING}},
	abstract = {Monte Carlo (MC) simulation is widely used to quantify the parameter uncertainty of hydrological and other models because of its flexibility and robustness. However, MC simulation is not always practical for real time flow forecasting when computationally intensive models are used. Here we present an approach for assessment of model parametric uncertainty using machine learning techniques to replicate time consuming MC simulation. In this approach, firstly MC simulation is performed by sampling parameters form the given probability distribution. Secondly, the uncertainty descriptors such as quantiles or prediction intervals are estimated from the realizations of MC simulation. Thirdly, the machine learning models are trained to approximate the functional relationships between the input variables and the synthetic uncertainty descriptors estimated from the realizations. The trained models encapsulate the underlying dynamics of the parameter uncertainty and can be used to predict uncertainty descriptors for the new data vectors. This approach was validated by comparing the uncertainty descriptors in the verification data set with those obtained by MC simulation. The method is applied to estimate parameter uncertainty of lumped conceptual hydrological model, HBV, for the Brue catchment in UK. The results are quite promising as the prediction intervals estimated by the machine learning techniques are reasonably accurate. The proposed techniques could be useful in real time application to replicate MC simulation when it is not practicable to run large number of simulations for time consuming hydrological models and when the forecast lead time is very short.},
	language = {en},
	author = {Shrestha, Durga Lal and Kayastha, Nagendra and Solomatine, Dimitri},
	pages = {10},
	file = {Shrestha,Kayastha,Solomatine,MonteCarloSimulation,ProcHI,2009.pdf:/Users/bill/D/Zotero/storage/N7QWG9EU/Shrestha,Kayastha,Solomatine,MonteCarloSimulation,ProcHI,2009.pdf:application/pdf},
}

@article{shrestha2006b,
	title = {{ASSESSING} {MODEL} {PREDICTION} {LIMITS} {USING} {FUZZY} {CLUSTERING} {AND} {MACHINE} {LEARNING}},
	language = {en},
	author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
	year = {2006},
	pages = {8},
	file = {Shrestha,Rodriguez,Price,Solomatine,AssesingModelPrediction,ProcHI,2006.pdf:/Users/bill/D/Zotero/storage/KPJK522H/Shrestha,Rodriguez,Price,Solomatine,AssesingModelPrediction,ProcHI,2006.pdf:application/pdf},
}

@article{shrestha2006nnb,
	title = {Machine learning approaches for estimation of prediction interval for the model output},
	volume = {19},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608006000153},
	doi = {10.1016/j.neunet.2006.01.012},
	abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and ﬁnally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artiﬁcial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
	language = {en},
	number = {2},
	urldate = {2020-12-15},
	journal = {Neural Networks},
	author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
	month = mar,
	year = {2006},
	pages = {225--235},
	file = {Shrestha,Sol,MachLearnEstimUncertanty,NNJ,2006.pdf:/Users/bill/D/Zotero/storage/3GWPIKHF/Shrestha,Sol,MachLearnEstimUncertanty,NNJ,2006.pdf:application/pdf},
}

@inproceedings{shrestha2005p2iijcnn2,
	address = {MOntreal, QC, Canada},
	title = {Estimation of prediction intervals for the model outputs using machine learning},
	volume = {5},
	isbn = {978-0-7803-9048-5},
	url = {http://ieeexplore.ieee.org/document/1556351/},
	doi = {10.1109/IJCNN.2005.1556351},
	abstract = {A new method for estimating prediction intervals for a model output using machine learning is presented. In it, first the prediction intervals for insample data using clustering techniques to identify the distinguishable regions in input space with similar distributions of model errors are constructed. Then regression model is built for in-sample data using computed prediction intervals as targets, and, finally, this model is applied to estimate the prediction intervals for out-of-sample data. The method was tested on artificial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction intervals. A new method for evaluating performance for estimating prediction intervals is proposed as well.},
	language = {en},
	urldate = {2020-12-15},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	publisher = {IEEE},
	author = {Shrestha, D.L. and Solomatine, D.P.},
	year = {2005},
	pages = {2700--2705},
	file = {Shrestha,Solomatine,Estimation,PI,ProcIJCNN,2005.pdf:/Users/bill/D/Zotero/storage/EMYK7GIG/Shrestha,Solomatine,Estimation,PI,ProcIJCNN,2005.pdf:application/pdf},
}

@article{shresthab,
	title = {A {Novel} {Method} to {Estimate} the {Model} {Uncertainty} {Based} on the {Model} {Errors}},
	abstract = {This paper presents a novel method for estimating “total” predictive uncertainty using machine learning techniques. By the term “total” we mean that all sources of uncertainty are taken into account, including that of the input and observed data, model parameters and structure, without attempting to separate the contribution given by these different sources. We assume that the model error, which is mismatch between the observed and modelled value reflects all sources of uncertainty. Fuzzy c-means clustering was employed to cluster the input space into different zones or clusters assuming that the all the examples those belong to the particular cluster have similar model errors. The prediction interval is constructed for each cluster on the basis of empirical distributions of the historical model errors associated with all examples of the particular cluster. Prediction interval for the individual example is derived from cluster based prediction interval according to their membership grades in each cluster. Linear or non-linear regression model is then built in calibration data that approximates an underlying functional relationship between an input vector and the computed prediction intervals. Finally, this model is applied to estimate the prediction intervals in verification data. The method was tested on hydrologic datasets using various machine learning techniques. Preliminary results show that the method has certain advantage if compared to other methods.},
	language = {en},
	author = {Shrestha, Durga Lal and Solomatine, Dimitri P},
	pages = {6},
	file = {Shrestha,Solomatine,NovelMethodEstimation,ProcIEMS,2006.pdf:/Users/bill/D/Zotero/storage/ZNZPREBF/Shrestha,Solomatine,NovelMethodEstimation,ProcIEMS,2006.pdf:application/pdf},
}

@article{solomatine2009wrr,
	title = {A novel method to estimate model uncertainty using machine learning techniques: {NOVEL} {METHOD} {TO} {ESTIMATE} {UNCERTAINTY}},
	volume = {45},
	issn = {00431397},
	shorttitle = {A novel method to estimate model uncertainty using machine learning techniques},
	url = {http://doi.wiley.com/10.1029/2008WR006839},
	doi = {10.1029/2008WR006839},
	language = {en},
	number = {12},
	urldate = {2020-12-15},
	journal = {Water Resources Research},
	author = {Solomatine, Dimitri P. and Shrestha, Durga Lal},
	month = dec,
	year = {2009},
	file = {Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf:/Users/bill/D/Zotero/storage/YSYIHXLW/Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf:application/pdf},
}

@article{solomatine2009wrra,
	title = {A novel method to estimate model uncertainty using machine learning techniques: {NOVEL} {METHOD} {TO} {ESTIMATE} {UNCERTAINTY}},
	volume = {45},
	issn = {00431397},
	shorttitle = {A novel method to estimate model uncertainty using machine learning techniques},
	url = {http://doi.wiley.com/10.1029/2008WR006839},
	doi = {10.1029/2008WR006839},
	language = {en},
	number = {12},
	urldate = {2020-12-15},
	journal = {Water Resources Research},
	author = {Solomatine, Dimitri P. and Shrestha, Durga Lal},
	month = dec,
	year = {2009},
	file = {Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf.pdf:/Users/bill/D/Zotero/storage/XEPHWCUM/Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf.pdf:application/pdf},
}

@incollection{solomatine2013eaonn,
	address = {Berlin, Heidelberg},
	title = {Learning {Errors} of {Environmental} {Mathematical} {Models}},
	volume = {383},
	isbn = {978-3-642-41012-3 978-3-642-41013-0},
	url = {http://link.springer.com/10.1007/978-3-642-41013-0_48},
	abstract = {In solving civil engineering problems the use of various models for forecasting environmental variables (for example, water levels in a river during flooding) is a must. Mathematical models of environmental processes inevitably contain errors (even if models are calibrated on accurate data) which can be represented as realizations of a stochastic process. Parameters of this process vary in time and cannot be reliably estimated without making (unrealistic) assumptions. However the model errors depend on various factors characterizing environmental conditions (for example, for extreme events errors are typically higher), and such dependencies can be reconstructed based on data. We present a unifying approach allowing for building machine learning models (in particular ANN and Local weighted regression) able to predict such errors as well as the properties of their distributions. Examples in modelling hydrological processes are considered.},
	language = {en},
	urldate = {2020-12-15},
	booktitle = {Engineering {Applications} of {Neural} {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Solomatine, Dimitri and Kuzmin, Vadim and Shrestha, Durga Lal},
	editor = {Iliadis, Lazaros and Papadopoulos, Harris and Jayne, Chrisina},
	year = {2013},
	doi = {10.1007/978-3-642-41013-0_48},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {466--473},
	file = {solomatine kuzmin shrestha 2013 - learning errors of environmental mathematical models - GUPPY - ERROR.pdf:/Users/bill/D/Zotero/storage/FA7SLMRH/solomatine kuzmin shrestha 2013 - learning errors of environmental mathematical models - GUPPY - ERROR.pdf:application/pdf},
}

@techreport{drira2019,
	type = {preprint},
	title = {Are trade-offs between flexibility and efficiency in systematic conservation planning avoidable ?},
	url = {http://biorxiv.org/lookup/doi/10.1101/775072},
	abstract = {Abstract
          Species distribution models (SDMs) have been proposed as a way to provide robust inference about species-specific sites suitabilities, and have been increasingly used in systematic conservation planning (SCP) applications. However, despite the fact that the use of SDMs in SCP may raise some potential issues, conservation studies have overlooked to assess the implications of SDMs uncertainties. The integration of these uncertainties in conservation solutions requires the development of a reserve-selection approach based on a suitable optimization algorithm. A large body of research has shown that exact optimization algorithms give very precise control over the gap to optimality of conservation solutions. However, their major shortcoming is that they generate a single binary and indivisible solution. Therefore, they provide no flexibility in the implementation of conservation solutions by stakeholders. On the other hand, heuristic decision-support systems provide large amounts of sub-optimal solutions, and therefore more flexibility. This flexibility arises from the availability of many alternative and sub-optimal conservation solutions. The two principles of efficiency and flexibility are implicitly linked in conservation applications, with the most mathematically efficient solutions being inflexible and the flexible solutions provided by heuristics suffering sub-optimality. In order to avoid the trade-offs between flexibility and efficiency in systematic conservation planning, we propose in this paper a new reserve-selection framework based on mathematical programming optimization combined with a post-selection of SDM outputs. This approach leads to a reserve-selection framework that might provide flexibility while simultaneously addressing efficiency and representativeness of conservation solutions and the adequacy of conservation targets. To exemplify the approach we a nalyzed an experimental design crossing pre- and post-selection of SDM outputs versus heuristics and exact mathematical optimizations. We used the Mediterranean Sea as a biogeographical template for our analyses, integrating the outputs of 8 SDM techniques for 438 fishes species.},
	language = {en},
	urldate = {2020-12-16},
	institution = {Ecology},
	author = {Drira, Sabrine and Lasram, Frida Ben Rais and Hattab, Tarek and Shin, Yunne Jai and Jenhani, Amel Ben Rejeb and Guilhaumon, François},
	month = sep,
	year = {2019},
	doi = {10.1101/775072},
	file = {drira et al 2019 - Are trade-offs between flexibility and efficiency in systematic conservationplanning avoidable - BDPG - ENSEMBLES.pdf:/Users/bill/D/Zotero/storage/LMT8HP42/drira et al 2019 - Are trade-offs between flexibility and efficiency in systematic conservationplanning avoidable - BDPG - ENSEMBLES.pdf:application/pdf},
}

@inproceedings{gent1994,
	title = {How not to do it},
	abstract = {We give some dos and don'ts for those analysing algorithms experimentally. We illustrate these with many examples from our own research. Where we have not followed these maxims, we have suffered as a result.},
	language = {en},
	author = {Gent, Ian P. and Walsh, Toby},
	year = {1994},
	keywords = {benchmarking},
	pages = {5},
	file = {gent walsh 1994 - how not to do it - BENCHMARKING - TESTING.pdf:/Users/bill/D/Zotero/storage/JI4GITIX/gent walsh 1994 - how not to do it - BENCHMARKING - TESTING.pdf:application/pdf},
}

@techreport{gent1997,
	title = {How not to do it},
	url = {https://www.cse.unsw.edu.au/~tw/hownotto.pdf},
	abstract = {We give some dos and don'ts for those analysing algorithms experimentally.  We illustrate these with many examples from our own research on the study of algorithms for NP-complete problems such as satisfiability and constraint satisfaction.  Where we have not followed these maxims, we have suffered as a result.},
	number = {97.27},
	institution = {University of Leeds},
	author = {Gent, Ian P. and Grant, Stuart A. and Macintyre, Ewan and Prosser, Patrick and Shaw, Paul and Smith, Barbara M. and Walsh, Toby},
	year = {1997},
	keywords = {benchmarking},
	file = {gent et al 1997 - how not to do it - REPORT - BENCHMARKING - TESTING.pdf:/Users/bill/D/Zotero/storage/N3GVCE34/gent et al 1997 - how not to do it - REPORT - BENCHMARKING - TESTING.pdf:application/pdf},
}

@misc{diaz2019,
	title = {{IPBES} (2019): {Summary} for policymakers of the global assessment report on biodiversity and ecosystem services of the {Intergovernmental} {Science}-{Policy} {Platform} on {Biodiversity} and {Ecosystem} {Services}},
	url = {https://doi.org/10.5281/zenodo.3553579},
	language = {en},
	publisher = {IPBES secretariat, Bonn, Germany},
	editor = {Díaz, S. and Settele, J. and Brondízio, E.S. and Ngo, H.T. and Guèze, M. and Agard, J. and Arneth, A. and Balvanera, P. and Brauman, K.A. and Butchart, S.H.M. and Chan, K.M.A. and Garibaldi, L.A. and Ichii, K. and Liu, J. and Subramanian, S.M. and Midgley, G.F. and Miloslavich, P. and Molnár, Z. and Obura, D. and Pfaff, A. and Polasky, S. and Purvis, A. and Razzaque, J. and Reyers, B. and Chowdhury, R. Roy and Shin, Y.J. and Visseren-Hamakers, I.J. and Willis, K.J. and Zayas, C.N.},
	year = {2019},
	annote = {SUGGESTED CITATION (from the report):IPBES (2019): Summary for policymakers of the global assessment report on biodiversity and ecosystem services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. S. Díaz, J. Settele, E. S. Brondízio E.S., H. T. Ngo, M. Guèze, J.Agard, A. Arneth, P. Balvanera, K. A. Brauman, S. H. M. Butchart, K. M. A. Chan, L. A. Garibaldi, K. Ichii, J. Liu, S. M. Subramanian,G. F. Midgley, P. Miloslavich, Z. Molnár, D. Obura, A. Pfaff, S. Polasky, A. Purvis, J. Razzaque, B. Reyers, R. Roy Chowdhury, Y. J. Shin,I. J. Visseren-Hamakers, K. J. Willis, and C. N. Zayas (eds.). IPBES secretariat, Bonn, Germany. 56 pages.},
	file = {IPBES 2019 - Summary for policymakers of the global assessment report on biodiversity and ecosystem services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services - BDPG.pdf:/Users/bill/D/Zotero/storage/CY9BX4SF/IPBES 2019 - Summary for policymakers of the global assessment report on biodiversity and ecosystem services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services - BDPG.pdf:application/pdf},
}

@book{garey1979,
	series = {Series of {Books} in the {Mathematical} {Sciences}},
	title = {Computers and {Intractability}: {A} {Guide} to the {Theory} of {NP}-{Completeness}},
	publisher = {W.H. Freeman},
	author = {Garey, Michael R. and Johnson, David S.},
	year = {1979},
	file = {Garey_Johnson_1979_Computers and Intractability.pdf:/Users/bill/D/Zotero/storage/T67PFSQW/Garey_Johnson_1979_Computers and Intractability.pdf:application/pdf},
}

@book{cormen2009,
	title = {Introduction to {Algorithms} (3rd ed.)},
	isbn = {0-262-03384-4},
	publisher = {MIT Press and McGraw-Hill},
	author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
	year = {2009},
}

@article{margules2000n,
	title = {Systematic conservation planning},
	volume = {405},
	doi = {https://doi.org/10.1038/35012251},
	language = {en},
	journal = {Nature},
	author = {Margules, C R and Pressey, R L},
	year = {2000},
	pages = {243--253},
	file = {margules pressey 2000 - systematic conservation planning - RESERVE SELECTION - SCP - BDPG.pdf:/Users/bill/D/Zotero/storage/QY49C6AN/margules pressey 2000 - systematic conservation planning - RESERVE SELECTION - SCP - BDPG.pdf:application/pdf},
}

@incollection{ball2009scpqmact,
	address = {Oxford, UK},
	title = {Marxan and relatives: software for spatial conservation prioritisation},
	abstract = {chap. 14. - no place to put this in zotero record

also, not sure if this citation is completely correct.  copied from beyer ILP paper's bibliography and entered here by hand as best I could},
	booktitle = {Spatial {Conservation} {Prioritisation}: {Quantitative} {Methods} and {Computational} {Tools}},
	publisher = {Oxford University Press},
	author = {Ball, Ian R and Possingham, Hugh P and Watts, Matthew E.},
	editor = {Moilanen, Atte and Wilson, Kerrie and Possingham, Hugh},
	year = {2009},
	pages = {185--195},
}

@article{watts2009em&s,
	title = {Marxan with {Zones}: {Software} for optimal conservation based land- and sea-use zoning},
	volume = {24},
	issn = {13648152},
	shorttitle = {Marxan with {Zones}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815209001418},
	doi = {10.1016/j.envsoft.2009.06.005},
	abstract = {Marxan is the most widely used conservation planning software in the world and is designed for solving complex conservation planning problems in landscapes and seascapes. In this paper we describe a substantial extension of Marxan called Marxan with Zones, a decision support tool that provides landuse zoning options in geographical regions for biodiversity conservation. We describe new functions designed to enhance the original Marxan software and expand on its utility as a decision support tool. The major new element in the decision problem is allowing any parcel of land or sea to be allocated to a speciﬁc zone, not just reserved or unreserved. Each zone then has the option of its own actions, objectives and constraints, with the ﬂexibility to deﬁne the contribution of each zone to achieve targets for pre-speciﬁed features (e.g. species or habitats). The objective is to minimize the total cost of implementing the zoning plan while ensuring a variety of conservation and land-use objectives are achieved. We outline the capabilities, limitations and additional data requirements of this new software and perform a comparison with the original version of Marxan. We feature a number of case studies to demonstrate the functionality of the software and highlight its ﬂexibility to address a range of complex spatial planning problems. These studies demonstrate the design of multiple-use marine parks in both Western Australia and California, and the zoning of forest use in East Kalimantan.},
	language = {en},
	number = {12},
	urldate = {2021-02-10},
	journal = {Environmental Modelling \& Software},
	author = {Watts, Matthew E. and Ball, Ian R. and Stewart, Romola S. and Klein, Carissa J. and Wilson, Kerrie and Steinback, Charles and Lourival, Reinaldo and Kircher, Lindsay and Possingham, Hugh P.},
	month = dec,
	year = {2009},
	pages = {1513--1521},
	file = {Watts et al. - 2009 - Marxan with Zones Software for optimal conservation based land- and sea-use zoning - Environmental Modelling & Software.PDF:/Users/bill/D/Zotero/storage/BFM7GDQ6/Watts et al. - 2009 - Marxan with Zones Software for optimal conservation based land- and sea-use zoning - Environmental Modelling & Software.PDF:application/pdf},
}

@article{pressey2002pipgeae,
	title = {Classics in physical geography revisited},
	volume = {26},
	issn = {0309-1333, 1477-0296},
	url = {http://journals.sagepub.com/doi/10.1191/0309133302pp347xx},
	doi = {10.1191/0309133302pp347xx},
	language = {en},
	number = {3},
	urldate = {2021-02-12},
	journal = {Progress in Physical Geography: Earth and Environment},
	author = {Pressey, R. L.},
	month = sep,
	year = {2002},
	keywords = {reserve selection},
	pages = {434--441},
	file = {pressey 2002 - the first reserve selection algorithm - ANNO - BDPG - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/B4YCH27Z/pressey 2002 - the first reserve selection algorithm - ANNO - BDPG - RESERVE SELECTION.pdf:application/pdf},
}

@article{cai2013j,
	title = {{NuMVC}: {An} {Efficient} {Local} {Search} {Algorithm} for {Minimum} {Vertex} {Cover}},
	volume = {46},
	issn = {1076-9757},
	shorttitle = {{NuMVC}},
	url = {https://jair.org/index.php/jair/article/view/10812},
	doi = {10.1613/jair.3907},
	abstract = {The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial optimization problem of great importance in both theory and application. Local search has proved successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local search algorithms. First, they select a pair of vertices to exchange simultaneously, which is timeconsuming. Secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. The strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. These two strategies are used in designing a new MVC local search algorithm, which is referred to as NuMVC.},
	language = {en},
	urldate = {2021-02-13},
	journal = {Journal of Artificial Intelligence Research},
	author = {Cai, S. and Su, K. and Luo, C. and Sattar, A.},
	month = apr,
	year = {2013},
	keywords = {bdpg, benchmarking, minimum vertex cover},
	pages = {687--716},
	annote = {This paper is extremely useful for background on minimum vertex cover benchmarks and approximation. They use 2 benchmark sets: BHOSLIB (which is Xu's) and DIMACS (which is derived from some real problems).},
	file = {cai et al 2013 - NuMVC - An Efficient Local Search Algorithm for Minimum Vertex Cover - BDPG - XU - BENCHMARKS.pdf:/Users/bill/D/Zotero/storage/3VFFX8Y3/cai et al 2013 - NuMVC - An Efficient Local Search Algorithm for Minimum Vertex Cover - BDPG - XU - BENCHMARKS.pdf:application/pdf},
}

@incollection{gomes2006foai,
	title = {Randomness and {Structure}},
	volume = {2},
	isbn = {978-0-444-52726-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574652606800222},
	language = {en},
	urldate = {2021-02-13},
	booktitle = {Foundations of {Artificial} {Intelligence}},
	publisher = {Elsevier},
	author = {Gomes, Carla and Walsh, Toby},
	year = {2006},
	doi = {10.1016/S1574-6526(06)80022-2},
	pages = {639--664},
	file = {gomes walsh 2006 - randomness and structure - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/RBS9KZRQ/gomes walsh 2006 - randomness and structure - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{fan2011ai,
	title = {On the phase transitions of random k-constraint satisfaction problems},
	volume = {175},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210001906},
	doi = {10.1016/j.artint.2010.11.004},
	abstract = {Constraint satisfaction has received increasing attention over the years. Intense research has focused on solving all kinds of constraint satisfaction problems (CSPs). In this paper, ﬁrst we propose a random CSP model, named k-CSP, that guarantees the existence of phase transitions under certain circumstances. The exact location of the phase transition is quantiﬁed and experimental results are provided to illustrate the performance of the proposed model. Second, we revise the model k-CSP to a random linear CSP by incorporating certain linear structure to constraint relations. We also prove the existence of the phase transition and exhibit its exact location for this random linear CSP model.},
	language = {en},
	number = {3-4},
	urldate = {2021-02-13},
	journal = {Artificial Intelligence},
	author = {Fan, Yun and Shen, Jing},
	month = mar,
	year = {2011},
	keywords = {problem difficulty, bdpg, CSP, constraint satisfaction problems, problem generator, phase transition},
	pages = {914--927},
	annote = {Good explanation of the background and history behind Xu's model RB, phase transitions, and constraint satisfaction problems.  They also provide another problem generator of their own.},
	file = {fan shen 2011 - on the phase transitions of random k-constraint satisfaction problems - BDPG - CSP.pdf:/Users/bill/D/Zotero/storage/59WBZLE3/fan shen 2011 - on the phase transitions of random k-constraint satisfaction problems - BDPG - CSP.pdf:application/pdf},
}

@article{gao2015scis,
	title = {Experimental analyses on phase transitions in compiling satisfiability problems},
	volume = {58},
	issn = {1674-733X, 1869-1919},
	url = {http://link.springer.com/10.1007/s11432-014-5154-0},
	doi = {10.1007/s11432-014-5154-0},
	abstract = {In the past decade, a kind of well-known phenomena in many complex combinatorial problems such as satisﬁability problem, called phase transition, have been widely studied. In this paper, the phase transition phenomena are investigated during compiling k-satisﬁability problems into tractable languages with empirical methods. Ordered binary decision diagram and deterministic-decomposable negation normal form are selected as the tractable target languages for the compilation. Via intensive experiments, it can be concluded that an easy–hard–easy pattern exists during the compilations, which is only related to the ratio of the number of clauses to that of variables if we set k to a ﬁxed value, rather than to the target languages. Moreover, it can be concluded that the space exhausted during the compilations grows exponentially with the number of variables growing, whereas there is also a phase transition separating the polynomial-increment region from the exponential-increment region. Additionally, it can be observed that there is a phase transition of prime implicants around peak points of the easy–hard–easy pattern and the ratios of random instances whose average lengths of prime implicants are larger than the threshold 0.5 change sharply. From these analyses, it can be concluded that prime implicant length and solution interchangeability are crucial impacts on sizes of compilation results.},
	language = {en},
	number = {3},
	urldate = {2021-02-13},
	journal = {Science China Information Sciences},
	author = {Gao, Jian and Wang, JiaNan and Yin, MingHao},
	month = mar,
	year = {2015},
	keywords = {problem difficulty, bdpg, phase transition, csp},
	pages = {1--11},
	annote = {Good background on Xu, phase transition, etc.},
	file = {Gao et al. - 2015 - Experimental analyses on phase transitions in comp.pdf:/Users/bill/D/Zotero/storage/49KY6Y7J/Gao et al. - 2015 - Experimental analyses on phase transitions in comp.pdf:application/pdf},
}

@article{li2013m&m,
	title = {Variable-{Centered} {Consistency} in {Model} {RB}},
	volume = {23},
	issn = {0924-6495, 1572-8641},
	url = {http://link.springer.com/10.1007/s11023-012-9270-6},
	doi = {10.1007/s11023-012-9270-6},
	abstract = {Model RB is a model of random constraint satisfaction problems, which exhibits exact satisﬁability phase transition and many hard instances, both experimentally and theoretically. Benchmarks based on Model RB have been successfully used by various international algorithm competitions and many research papers. In a previous work, Xu and Li deﬁned two notions called i-constraint assignment tuple and ﬂawed i-constraint assignment tuple to show an exponential resolution complexity of Model RB. These two notions are similar to some kind of consistency in constraint satisfaction problems, but seem different from all kinds of consistency so far known in literatures. In this paper, we explicitly deﬁne this kind of consistency, called variable-centered consistency, and show an upper bound on a parameter in Model RB, such that up to this bound the typical instances of Model RB are variable-centered consistent.},
	language = {en},
	number = {1},
	urldate = {2021-02-13},
	journal = {Minds and Machines},
	author = {Li, Liang and Liu, Tian and Xu, Ke},
	month = mar,
	year = {2013},
	keywords = {problem difficulty, bdpg, CSP, phase transition, model RB},
	pages = {95--103},
	file = {li liu xu 2013 - Variable-Centered Consistency in Model RB - BDPG.pdf:/Users/bill/D/Zotero/storage/SIESVTI9/li liu xu 2013 - Variable-Centered Consistency in Model RB - BDPG.pdf:application/pdf},
}

@article{shen2016jco,
	title = {Bounding the scaling window of random constraint satisfaction problems},
	volume = {31},
	issn = {1382-6905, 1573-2886},
	url = {http://link.springer.com/10.1007/s10878-014-9789-y},
	doi = {10.1007/s10878-014-9789-y},
	abstract = {The model k-CSP is a random CSP model with moderately growing arity k of constraints. By incorporating certain linear structure, k-CSP is revised to a random linear CSP, named k-hyper-F-linear CSP. It had been shown theoretically that the two models exhibit exact satisﬁability phase transitions when the constraint density r is varied accordingly. In this paper, we use ﬁnite-size scaling analysis to characterize the threshold behaviors of the two models with ﬁnite problem size n. A series of experimental studies are carried out to illustrate the scaling window of the model k-CSP.},
	language = {en},
	number = {2},
	urldate = {2021-02-13},
	journal = {Journal of Combinatorial Optimization},
	author = {Shen, Jing and Ren, Yaofeng},
	month = feb,
	year = {2016},
	keywords = {problem difficulty, bdpg, phase transition, csp, model RB},
	pages = {786--801},
	file = {shen ren 2016 - Bounding the scaling window of random constraintsatisfaction problems - MODEL RB - XU - BDPG - PHASE TRANSITION - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/45UWC8UG/shen ren 2016 - Bounding the scaling window of random constraintsatisfaction problems - MODEL RB - XU - BDPG - PHASE TRANSITION - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{moreno-scott2016cian,
	title = {Experimental {Matching} of {Instances} to {Heuristics} for {Constraint} {Satisfaction} {Problems}},
	volume = {2016},
	issn = {1687-5265, 1687-5273},
	url = {http://www.hindawi.com/journals/cin/2016/7349070/},
	doi = {10.1155/2016/7349070},
	abstract = {Constraint satisfaction problems are of special interest for the artificial intelligence and operations research community due to their many applications. Although heuristics involved in solving these problems have largely been studied in the past, little is known about the relation between instances and the respective performance of the heuristics used to solve them. This paper focuses on both the exploration of the instance space to identify relations between instances and good performing heuristics and how to use such relations to improve the search. Firstly, the document describes a methodology to explore the instance space of constraint satisfaction problems and evaluate the corresponding performance of six variable ordering heuristics for such instances in order to find regions on the instance space where some heuristics outperform the others. Analyzing such regions favors the understanding of how these heuristics work and contribute to their improvement. Secondly, we use the information gathered from the first stage to predict the most suitable heuristic to use according to the features of the instance currently being solved. This approach proved to be competitive when compared against the heuristics applied in isolation on both randomly generated and structured instances of constraint satisfaction problems.},
	language = {en},
	urldate = {2021-02-13},
	journal = {Computational Intelligence and Neuroscience},
	author = {Moreno-Scott, Jorge Humberto and Ortiz-Bayliss, José Carlos and Terashima-Marín, Hugo and Conant-Pablos, Santiago Enrique},
	year = {2016},
	keywords = {problem difficulty, bdpg, phase transition, csp, problem difficulty prediction},
	pages = {1--15},
	file = {moreno-scott et al 2016 - experimental matching of instances to heuristics for constraint satisfaction problems - BDPG - PROBLEM DIFFICULTY PREDICTION.pdf:/Users/bill/D/Zotero/storage/PWDJQS45/moreno-scott et al 2016 - experimental matching of instances to heuristics for constraint satisfaction problems - BDPG - PROBLEM DIFFICULTY PREDICTION.pdf:application/pdf},
}

@article{boussemart,
	title = {Description and {Representation} of the {Problems} selected for the {First} {International} {Constraint} {Satisfaction} {Solver} {Competition}},
	abstract = {In this paper, we present the problems that have been selected for the ﬁrst international competition of CSP solvers. First, we introduce a succinct description of each problem and then, we present the two formats that have been used to represent the CSP instances.},
	language = {en},
	author = {Boussemart, Frederic and Hemery, Fred and Lecoutre, Christophe},
	keywords = {bdpg, benchmarking, csp},
	pages = {20},
	file = {boussemart et al 2006 - Description and Representation of the Problemsselected for the First International ConstraintSatisfaction Solver Competition - BDPG.pdf:/Users/bill/D/Zotero/storage/AVYIAATI/boussemart et al 2006 - Description and Representation of the Problemsselected for the First International ConstraintSatisfaction Solver Competition - BDPG.pdf:application/pdf},
}

@incollection{gilmour2006acoasi,
	address = {Berlin, Heidelberg},
	title = {Kernelization as {Heuristic} {Structure} for the {Vertex} {Cover} {Problem}},
	volume = {4150},
	isbn = {978-3-540-38482-3 978-3-540-38483-0},
	url = {http://link.springer.com/10.1007/11839088_45},
	abstract = {For solving combinatorial optimisation problems, exact methods accurately exploit the structure of the problem but are tractable only up to a certain size; approximation or heuristic methods are tractable for very large problems but may possibly be led into a bad solution. A question that arises is, From where can we obtain knowledge of the problem structure via exact methods that can be exploited on large-scale problems by heuristic methods? We present a framework that allows the exploitation of existing techniques and resources to integrate such structural knowledge into the Ant Colony System metaheuristic, where the structure is determined through the notion of kernelization from the ﬁeld of parameterized complexity. We give experimental results using vertex cover as the problem instance, and show that knowledge of this type of structure improves performance beyond previously deﬁned ACS algorithms.},
	language = {en},
	urldate = {2021-02-14},
	booktitle = {Ant {Colony} {Optimization} and {Swarm} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gilmour, Stephen and Dras, Mark},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dorigo, Marco and Gambardella, Luca Maria and Birattari, Mauro and Martinoli, Alcherio and Poli, Riccardo and Stützle, Thomas},
	year = {2006},
	doi = {10.1007/11839088_45},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {bdpg, vertex cover, kernelization, parameterized complexity, learning to predict performance, ant colony optimization},
	pages = {452--459},
	file = {gilmour dras 2006 - Kernelization as Heuristic Structure for the Vertex Cover Problem - BDPG.pdf:/Users/bill/D/Zotero/storage/I6JGZ3M9/gilmour dras 2006 - Kernelization as Heuristic Structure for the Vertex Cover Problem - BDPG.pdf:application/pdf},
}

@article{fellows2002enitcs,
	title = {Parameterized {Complexity}},
	volume = {61},
	issn = {15710661},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1571066104003019},
	doi = {10.1016/S1571-0661(04)00301-9},
	language = {en},
	urldate = {2021-02-14},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Fellows, Michael R.},
	month = jan,
	year = {2002},
	keywords = {bdpg, parameterized complexity, learning to predict performance},
	pages = {1--19},
	file = {fellows 2002 -parameterized Complexity - The Main Ideas and Connections to Practical Computing - BDPG.pdf:/Users/bill/D/Zotero/storage/J94QBJH7/fellows 2002 -parameterized Complexity - The Main Ideas and Connections to Practical Computing - BDPG.pdf:application/pdf},
}

@article{spence2010ajea,
	title = {sgen1: {A} generator of small but difficult satisfiability benchmarks},
	volume = {15},
	issn = {1084-6654, 1084-6654},
	shorttitle = {sgen1},
	url = {https://dl.acm.org/doi/10.1145/1671970.1671972},
	doi = {10.1145/1671970.1671972},
	language = {en},
	urldate = {2021-02-14},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Spence, Ivor},
	month = mar,
	year = {2010},
	keywords = {problem difficulty, bdpg, problem generator},
	file = {spence 2010 - sgen1 - A Generator of Small but Difficult Satisfiability Benchmarks - PROBLEM GENERATOR - SAT - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/NJVCWTSY/spence 2010 - sgen1 - A Generator of Small but Difficult Satisfiability Benchmarks - PROBLEM GENERATOR - SAT - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{fan2012ai,
	title = {A general model and thresholds for random constraint satisfaction problems},
	volume = {193},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370212001038},
	doi = {10.1016/j.artint.2012.08.003},
	abstract = {In this paper, we study the relation among the parameters in their most general setting that deﬁne a large class of random CSP models d-k-CSP where d is the domain size and k is the length of the constraint scopes. The model d-k-CSP uniﬁes several related models such as the model RB and the model k-CSP. We prove that the model d-k-CSP exhibits exact phase transitions if k ln d increases no slower than the logarithm of the number of variables. A series of experimental studies with interesting observations are carried out to illustrate the solubility phase transition and the hardness of instances around phase transitions.},
	language = {en},
	urldate = {2021-02-14},
	journal = {Artificial Intelligence},
	author = {Fan, Yun and Shen, Jing and Xu, Ke},
	month = dec,
	year = {2012},
	keywords = {bdpg},
	pages = {1--17},
	file = {fan shen xu 2012 - a general model and thresholds for random constraint satisfaction problems - BDPG.pdf:/Users/bill/D/Zotero/storage/7XNJZG2W/fan shen xu 2012 - a general model and thresholds for random constraint satisfaction problems - BDPG.pdf:application/pdf},
}

@incollection{fang2014fia,
	address = {Cham},
	title = {Combining {Edge} {Weight} and {Vertex} {Weight} for {Minimum} {Vertex} {Cover} {Problem}},
	volume = {8497},
	isbn = {978-3-319-08015-4 978-3-319-08016-1},
	url = {http://link.springer.com/10.1007/978-3-319-08016-1_7},
	abstract = {The Minimum Vertex Cover (MVC) problem is an important NP-hard combinatorial optimization problem. Constraint weighting is an eﬀective technique in stochastic local search algorithms for the MVC problem. The edge weight and the vertex weight have been used separately by diﬀerent algorithms. We present a new local search algorithm, namely VEWLS, which integrates the edge weighting scheme with the vertex weighting scheme. To the best of our knowledge, it is the ﬁrst time to combine two weighting schemes for the MVC problem. Experiments over both the DIMACS benchmark and the BHOSLIB benchmark show that VEWLS outperforms NuMVC, the state-of-the-art local search algorithm for MVC, on 73\% and 68\% of the instances, respectively.},
	language = {en},
	urldate = {2021-02-14},
	booktitle = {Frontiers in {Algorithmics}},
	publisher = {Springer International Publishing},
	author = {Fang, Zhiwen and Chu, Yang and Qiao, Kan and Feng, Xu and Xu, Ke},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Chen, Jianer and Hopcroft, John E. and Wang, Jianxin},
	year = {2014},
	doi = {10.1007/978-3-319-08016-1_7},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {bdpg, vertex cover},
	pages = {71--81},
	file = {fang ... xu 2014 - Combining Edge Weight and Vertex Weight for Minimum Vertex Cover Problem - BDPG - VERTEX COVER - XU STUDENT.pdf:/Users/bill/D/Zotero/storage/JMP4W4K6/fang ... xu 2014 - Combining Edge Weight and Vertex Weight for Minimum Vertex Cover Problem - BDPG - VERTEX COVER - XU STUDENT.pdf:application/pdf},
}

@article{yehuda,
	title = {It's {Not} {What} {Machines} {Can} {Learn}, {It}'s {What} {We} {Cannot} {Teach}},
	abstract = {Can deep neural networks learn to solve any task, and in particular problems of high complexity? This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisﬁability. In this work we offer a different perspective on this question. Given the common assumption that NP = coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem. We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased datasets that lead practitioners to over-estimate model accuracy. Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difﬁculty of generating sufﬁciently large and unbiased training sets.},
	language = {en},
	author = {Yehuda, Gal and Gabel, Moshe and Schuster, Assaf},
	keywords = {problem difficulty, bdpg, machine learning, deep learning},
	pages = {11},
	file = {yehuda et al 2020 - Its Not What Machines Can Learn, It’s What We Cannot Teach - BDPG - PROBLEM DIFFICULTY - ML - DEEP LEARNING.pdf:/Users/bill/D/Zotero/storage/VD6AT7PA/yehuda et al 2020 - Its Not What Machines Can Learn, It’s What We Cannot Teach - BDPG - PROBLEM DIFFICULTY - ML - DEEP LEARNING.pdf:application/pdf},
}

@article{schmidhuber2015nn,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep learning in neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2021-02-14},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {deep learning, neural networks, credit assignment},
	pages = {85--117},
	file = {schmidhuber 2015 - Deep Learning In Neural Networks Overview - DEEP LEARNING - NEURAL NETWORKS - CREDIT ASSIGNMENT.pdf:/Users/bill/D/Zotero/storage/L6F3SHVW/schmidhuber 2015 - Deep Learning In Neural Networks Overview - DEEP LEARNING - NEURAL NETWORKS - CREDIT ASSIGNMENT.pdf:application/pdf},
}

@inproceedings{you2019anips3acnips2n2d82vbc,
	address = {Vancouver, Canada},
	title = {{G2SAT}: {Learning} to {Generate} {SAT} {Formulas}},
	abstract = {The Boolean Satisﬁability (SAT) problem is the canonical NP-complete problem and is fundamental to computer science, with a wide array of applications in planning, veriﬁcation, and theorem proving. Developing and evaluating practical SAT solvers relies on extensive empirical testing on a set of real-world benchmark formulas. However, the availability of such real-world SAT formulas is limited. While these benchmark formulas can be augmented with synthetically generated ones, existing approaches for doing so are heavily hand-crafted and fail to simultaneously capture a wide range of characteristics exhibited by real-world SAT instances. In this work, we present G2SAT, the ﬁrst deep generative framework that learns to generate SAT formulas from a given set of input formulas. Our key insight is that SAT formulas can be transformed into latent bipartite graph representations which we model using a specialized deep generative neural network. We show that G2SAT can generate SAT formulas that closely resemble given real-world SAT instances, as measured by both graph metrics and SAT solver behavior. Further, we show that our synthetic SAT formulas could be used to improve SAT solver performance on real-world benchmarks, which opens up new opportunities for the continued development of SAT solvers and a deeper understanding of their performance.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}},
	author = {You, Jiaxuan and Wu, Haoze},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {bdpg, problem generator, machine learning, generative, bipartite network},
	pages = {10552--10563},
	file = {you et al 2019 - G2SAT - Learning to Generate SAT Formulas.pdf:/Users/bill/D/Zotero/storage/GLKQ4MBQ/you et al 2019 - G2SAT - Learning to Generate SAT Formulas.pdf:application/pdf},
}

@incollection{xu2016ioaaoticp,
	address = {Cham},
	title = {A {New} {Solver} for the {Minimum} {Weighted} {Vertex} {Cover} {Problem}},
	volume = {9676},
	isbn = {978-3-319-33953-5 978-3-319-33954-2},
	url = {http://link.springer.com/10.1007/978-3-319-33954-2_28},
	language = {en},
	urldate = {2021-02-16},
	booktitle = {Integration of {AI} and {OR} {Techniques} in {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {Xu, Hong and Kumar, T. K. Satish and Koenig, Sven},
	editor = {Quimper, Claude-Guy},
	year = {2016},
	doi = {10.1007/978-3-319-33954-2_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {392--405},
	file = {xu et al 2016 - HONG XU not KE XU - a new solver for the minimum weighted vertex cover problem - SLIDES - BDPG.pdf:/Users/bill/D/Zotero/storage/2Y9PVRRW/xu et al 2016 - HONG XU not KE XU - a new solver for the minimum weighted vertex cover problem - SLIDES - BDPG.pdf:application/pdf},
}

@incollection{carbonnel2016papocp,
	address = {Cham},
	title = {Propagation via {Kernelization}: {The} {Vertex} {Cover} {Constraint}},
	volume = {9892},
	isbn = {978-3-319-44952-4 978-3-319-44953-1},
	shorttitle = {Propagation via {Kernelization}},
	url = {http://link.springer.com/10.1007/978-3-319-44953-1_10},
	abstract = {The technique of kernelization consists in extracting, from an instance of a problem, an essentially equivalent instance whose size is bounded in a parameter k. Besides being the basis for efﬁcient parameterized algorithms, this method also provides a wealth of information to reason about in the context of constraint programming. We study the use of kernelization for designing propagators through the example of the Vertex Cover constraint. Since the classic kernelization rules often correspond to dominance rather than consistency, we introduce the notion of “loss-less” kernel. While our preliminary experimental results show the potential of the approach, they also show some of its limits. In particular, this method is more effective for vertex covers of large and sparse graphs, as they tend to have, relatively, smaller kernels.},
	language = {en},
	urldate = {2021-02-16},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {Carbonnel, Clément and Hebrard, Emmanuel},
	editor = {Rueher, Michel},
	year = {2016},
	doi = {10.1007/978-3-319-44953-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {147--156},
	file = {carbonnel hebrel 2017 - propagation via kernelization - the vertex cover constraint - BDPG.pdf:/Users/bill/D/Zotero/storage/MFRTTKWX/carbonnel hebrel 2017 - propagation via kernelization - the vertex cover constraint - BDPG.pdf:application/pdf},
}

@article{mammola2021s,
	title = {Impact of the reference list features on the number of citations},
	volume = {126},
	issn = {0138-9130, 1588-2861},
	url = {http://link.springer.com/10.1007/s11192-020-03759-0},
	doi = {10.1007/s11192-020-03759-0},
	abstract = {Many believe that the quality of a scientific publication is as good as the science it cites. However, quantifications of how features of reference lists affect citations remain sparse. We examined seven numerical characteristics of reference lists of 50,878 research articles published in 17 ecological journals between 1997 and 2017. Over this period, significant changes occurred in reference lists’ features. On average, more recent papers have longer reference lists and cite more high Impact Factor papers and fewer non-journal publications. We also show that highly cited articles across the ecological literature have longer reference lists, cite more recent and impactful references, and include more self-citations. Conversely, the proportion of ‘classic’ papers and non-journal publications cited, as well as the temporal span of the reference list, have no significant influence on articles’ citations. From this analysis, we distill a recipe for crafting impactful reference lists, at least in ecology.},
	language = {en},
	number = {1},
	urldate = {2021-02-16},
	journal = {Scientometrics},
	author = {Mammola, Stefano and Fontaneto, Diego and Martínez, Alejandro and Chichorro, Filipe},
	month = jan,
	year = {2021},
	pages = {785--799},
	file = {mammola et al 2021 - Impact of the reference list features on the number of citations - ECOLOGY - BDPG.pdf:/Users/bill/D/Zotero/storage/RBRM5WAP/mammola et al 2021 - Impact of the reference list features on the number of citations - ECOLOGY - BDPG.pdf:application/pdf},
}

@article{doubleday2017tie&e,
	title = {Publishing with {Objective} {Charisma}: {Breaking} {Science}’s {Paradox}},
	volume = {32},
	issn = {01695347},
	shorttitle = {Publishing with {Objective} {Charisma}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534717301593},
	doi = {10.1016/j.tree.2017.06.011},
	language = {en},
	number = {11},
	urldate = {2021-02-16},
	journal = {Trends in Ecology \& Evolution},
	author = {Doubleday, Zoë A. and Connell, Sean D.},
	month = nov,
	year = {2017},
	pages = {803--805},
	file = {doubleday connell 2017 - Publishing with Objective  Charisma - Breaking  Sciences Paradox - SCIENTIFIC WRITING - BDPG.pdf:/Users/bill/D/Zotero/storage/879QSPP8/doubleday connell 2017 - Publishing with Objective  Charisma - Breaking  Sciences Paradox - SCIENTIFIC WRITING - BDPG.pdf:application/pdf},
}

@article{franca2019b,
	title = {Writing {Papers} to {Be} {Memorable}, {Even} {When} {They} {Are} {Not} {Really} {Read}},
	volume = {41},
	issn = {0265-9247, 1521-1878},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bies.201900035},
	doi = {10.1002/bies.201900035},
	language = {en},
	number = {5},
	urldate = {2021-02-16},
	journal = {BioEssays},
	author = {França, Thiago F. A. and Monserrat, José M.},
	month = may,
	year = {2019},
	pages = {1900035},
	file = {franca montserrat 2019 - Writing Papers to Be Memorable, Even When They Are Not Really Read - SCIENTIFIC WRITING - BDPG.pdf:/Users/bill/D/Zotero/storage/7DRUEPNJ/franca montserrat 2019 - Writing Papers to Be Memorable, Even When They Are Not Really Read - SCIENTIFIC WRITING - BDPG.pdf:application/pdf},
}

@article{freeling2019pnasu,
	title = {Opinion: {How} can we boost the impact of publications? {Try} better writing},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Opinion},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1819937116},
	doi = {10.1073/pnas.1819937116},
	language = {en},
	number = {2},
	urldate = {2021-02-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Freeling, Benjamin and Doubleday, Zoë A. and Connell, Sean D.},
	month = jan,
	year = {2019},
	pages = {341--343},
	file = {freeling doubleday connell 2019 - How can we boost the impact of publications - Try better writing - SCIENTIFIC WRITING - BDPG.pdf:/Users/bill/D/Zotero/storage/SD5VTHWN/freeling doubleday connell 2019 - How can we boost the impact of publications - Try better writing - SCIENTIFIC WRITING - BDPG.pdf:application/pdf},
}

@article{thomson2020bc,
	title = {Spatial conservation action planning in heterogeneous landscapes},
	volume = {250},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632072030793X},
	doi = {10.1016/j.biocon.2020.108735},
	abstract = {A key challenge in conservation is the efficient allocation of limited resources to maximise benefits for biodi­ versity. Decision-support tools that account for landscape heterogeneity are needed to identify spatially-explicit actions that will achieve the greatest biodiversity benefits with available resources. We developed a raster-based, landscape-scale, spatial conservation action planning tool (SCAP) that offers significant advances for prioritising local and regional scale conservation actions in heterogenous landscapes. The SCAP tool was developed for the state of Victoria, Australia, to integrate heterogeneity of landscapes, species distributions, threats, and man­ agement costs and benefits across the state. We used empirical data to derive current and pre-European set­ tlement distributions for 4400 native terrestrial species, and developed spatially explicit models of 19 threats to biodiversity. We coupled structured expert-elicitation techniques with machine learning to map the expected benefits to species, and the implementation costs, of 17 management actions – alone and in combination. We then ranked location-specific actions by their cost-effective contribution to an overall objective of minimizing the risk of species loss in Victoria over the next 50 years, using a modified implementation of the Zonation conservation planning framework. The SCAP tool provides decision makers with a transparent decision-support tool for identifying the cost-effective management actions at scales relevant to management.},
	language = {en},
	urldate = {2021-02-16},
	journal = {Biological Conservation},
	author = {Thomson, Jim and Regan, Tracey J. and Hollings, Tracey and Amos, Nevil and Geary, William L. and Parkes, David and Hauser, Cindy E. and White, Matthew},
	month = oct,
	year = {2020},
	pages = {108735},
	file = {thomson regan ... white 2020 - spatial conservation action planning in heterogeneous landscapes - GUPPY - BDPG - ARI - RESERVE SELECTION - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/WVE28TFB/thomson regan ... white 2020 - spatial conservation action planning in heterogeneous landscapes - GUPPY - BDPG - ARI - RESERVE SELECTION - UNCERTAINTY.pdf:application/pdf},
}

@article{grundel,
	title = {{PROBABILISTIC} {ANALYSIS} {AND} {RESULTS} {OF} {COMBINATORIAL} {PROBLEMS} {WITH} {MILITARY} {APPLICATIONS}},
	language = {en},
	author = {Grundel, Don A},
	pages = {135},
	file = {grundel 2004 - probabilistic analysis and results of combinatorial problems with military applications - THESIS - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/95EITJ55/grundel 2004 - probabilistic analysis and results of combinatorial problems with military applications - THESIS - BDPG - ANNO.pdf:application/pdf},
}

@article{alagador2016mee,
	title = {Climate change, species range shifts and dispersal corridors: an evaluation of spatial conservation models},
	volume = {7},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Climate change, species range shifts and dispersal corridors},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12524},
	doi = {10.1111/2041-210X.12524},
	language = {en},
	number = {7},
	urldate = {2021-02-16},
	journal = {Methods in Ecology and Evolution},
	author = {Alagador, Diogo and Cerdeira, Jorge Orestes and Araújo, Miguel Bastos},
	editor = {Anderson, Barbara},
	month = jul,
	year = {2016},
	pages = {853--866},
	file = {alagador et al 2016 - climate change species range shifts and dispersal corridors - an evaluation of spatial conservation models - BDPG - GUPPY.pdf:/Users/bill/D/Zotero/storage/3J5W8VPT/alagador et al 2016 - climate change species range shifts and dispersal corridors - an evaluation of spatial conservation models - BDPG - GUPPY.pdf:application/pdf},
}

@article{alagador2020mee,
	title = {Revisiting the minimum set cover, the maximal coverage problems and a maximum benefit area selection problem to make climate‐change‐concerned conservation plans effective},
	volume = {11},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13455},
	doi = {10.1111/2041-210X.13455},
	language = {en},
	number = {10},
	urldate = {2021-02-16},
	journal = {Methods in Ecology and Evolution},
	author = {Alagador, Diogo and Cerdeira, Jorge Orestes},
	editor = {Freckleton, Robert},
	month = oct,
	year = {2020},
	pages = {1325--1337},
	file = {flagador cerdeira 2020 - revisiting the minimum set cover the maximal coverage problems and a maximum benefit area selection problem to make climate-change-concerned conservation plans effective - BDPG.pdf:/Users/bill/D/Zotero/storage/HI2PSJAE/flagador cerdeira 2020 - revisiting the minimum set cover the maximal coverage problems and a maximum benefit area selection problem to make climate-change-concerned conservation plans effective - BDPG.pdf:application/pdf},
}

@article{petersen2021esae,
	title = {Species data for understanding biodiversity dynamics: {The} what, where and when of species occurrence data collection},
	volume = {2},
	issn = {2688-8319, 2688-8319},
	shorttitle = {Species data for understanding biodiversity dynamics},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/2688-8319.12048},
	doi = {10.1002/2688-8319.12048},
	language = {en},
	number = {1},
	urldate = {2021-02-16},
	journal = {Ecological Solutions and Evidence},
	author = {Petersen, Tanja K. and Speed, James D. M. and Grøtan, Vidar and Austrheim, Gunnar},
	month = jan,
	year = {2021},
	file = {petersen et al 2021 - Species data for understanding biodiversity dynamics - The what, where and when of species occurrence data collection - GUPPY - BDPG - SDM - DATA BIAS - KOALA. - ANNO.pdf:/Users/bill/D/Zotero/storage/QW7K9CRN/petersen et al 2021 - Species data for understanding biodiversity dynamics - The what, where and when of species occurrence data collection - GUPPY - BDPG - SDM - DATA BIAS - KOALA. - ANNO.pdf:application/pdf},
}

@article{jarvis2020esae,
	title = {Navigating spaces between conservation research and practice: {Are} we making progress?},
	volume = {1},
	issn = {2688-8319, 2688-8319},
	shorttitle = {Navigating spaces between conservation research and practice},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/2688-8319.12028},
	doi = {10.1002/2688-8319.12028},
	language = {en},
	number = {2},
	urldate = {2021-02-16},
	journal = {Ecological Solutions and Evidence},
	author = {Jarvis, Rebecca M. and Borrelle, Stephanie B. and Forsdick, Natalie J. and Pérez‐Hämmerle, Katharina‐Victoria and Dubois, Natalie S. and Griffin, Sean R. and Recalde‐Salas, Angela and Buschke, Falko and Rose, David Christian and Archibald, Carla L. and Gallo, John A. and Mair, Louise and Kadykalo, Andrew N. and Shanahan, Danielle and Prohaska, Bianca K},
	month = dec,
	year = {2020},
	file = {jarvis et al 2020 - Navigating spaces between conservation research and practice - Are we making progress - GUPPY - BDPG.pdf:/Users/bill/D/Zotero/storage/BDKAB52H/jarvis et al 2020 - Navigating spaces between conservation research and practice - Are we making progress - GUPPY - BDPG.pdf:application/pdf},
}

@article{cadotte2020esae,
	title = {Making the applied research that practitioners need and want accessible},
	volume = {1},
	issn = {2688-8319, 2688-8319},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2688-8319.12000},
	doi = {10.1002/2688-8319.12000},
	language = {en},
	number = {1},
	urldate = {2021-02-16},
	journal = {Ecological Solutions and Evidence},
	author = {Cadotte, Marc W. and Jones, Holly P. and Newton, Erika L.},
	month = jul,
	year = {2020},
	file = {cadotte et al 2020 - Making the applied research that practitioners need and want accessible - GUPPY - BDPG - KOALA.pdf:/Users/bill/D/Zotero/storage/WFRYUAPG/cadotte et al 2020 - Making the applied research that practitioners need and want accessible - GUPPY - BDPG - KOALA.pdf:application/pdf},
}

@article{hoeppke2021mee,
	title = {maxnodf: {An} {R} package for fair and fast comparisons of nestedness between networks},
	issn = {2041-210X, 2041-210X},
	shorttitle = {maxnodf},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13545},
	doi = {10.1111/2041-210X.13545},
	language = {en},
	urldate = {2021-02-16},
	journal = {Methods in Ecology and Evolution},
	author = {Hoeppke, Christoph and Simmons, Benno I.},
	editor = {Freckleton, Robert},
	month = jan,
	year = {2021},
	pages = {2041--210X.13545},
	file = {hoeppke simmons 2021 - maxnodf - an R package for fair and fast comparisons of nestedness between networks - NETWORKS - GRAPH THEORY - NESTEDNESS - BDPG.pdf:/Users/bill/D/Zotero/storage/XP5QPTWV/hoeppke simmons 2021 - maxnodf - an R package for fair and fast comparisons of nestedness between networks - NETWORKS - GRAPH THEORY - NESTEDNESS - BDPG.pdf:application/pdf},
}

@article{wilkinson2020mee,
	title = {Defining and evaluating predictions of joint species distribution models},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13518},
	doi = {10.1111/2041-210X.13518},
	language = {en},
	urldate = {2021-02-16},
	journal = {Methods in Ecology and Evolution},
	author = {Wilkinson, David P. and Golding, Nick and Guillera‐Arroita, Gurutzeta and Tingley, Reid and McCarthy, Michael A.},
	editor = {Freckleton, Robert},
	month = nov,
	year = {2020},
	pages = {2041--210X.13518},
	file = {wilkinson ... mccarthy 2020 - defining and evaluating predictions of joint species distribution models - BDPG - GUPPY - SDMs - EVALUATION - EFs.pdf:/Users/bill/D/Zotero/storage/72LNGFPW/wilkinson ... mccarthy 2020 - defining and evaluating predictions of joint species distribution models - BDPG - GUPPY - SDMs - EVALUATION - EFs.pdf:application/pdf},
}

@incollection{johnson2002dsnnsamfasdic,
	address = {Providence, RI, USA},
	series = {{DIMACS} {Series} in {Discrete} {Mathematics} and {Theoretical} {Computer} {Science}},
	title = {A theoretician’s guide to the experimental analysis of algorithms},
	volume = {59},
	booktitle = {Data {Structures}, {Near} {Neighbor} {Searches}, and {Methodology}: {Fifth} and {Sixth} {DIMACS} {Implementation} {Challenges}},
	publisher = {American Mathematical Society},
	author = {Johnson, David S.},
	editor = {Goldwasser, Michael H. and Johnson, David S. and McGeogh, Catherine C.},
	year = {2002},
	pages = {215--250},
	file = {johnson 2001 - A Theoreticians Guide to the Exp erimental Analysis of Algorithms - BDPG.pdf:/Users/bill/D/Zotero/storage/6YCZLK3Z/johnson 2001 - A Theoreticians Guide to the Exp erimental Analysis of Algorithms - BDPG.pdf:application/pdf},
}

@article{hogg1996ai,
	title = {Phase transitions and the search problem},
	volume = {81},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370295000445},
	doi = {10.1016/0004-3702(95)00044-5},
	abstract = {We describe how techniques that were originally developed in statistical mechanics can be applied to search problems that arise commonly in artificial intelligence. This approach is useful for understanding the typical behavior of classes of problems. In particular, these techniques predict that abrupt changes in computational cost, analogous to physical phase transitions, should occur universally, as heuristic effectiveness or search space topology is varied. We also present a number of open questions raised by these studies.},
	language = {en},
	number = {1-2},
	urldate = {2021-03-16},
	journal = {Artificial Intelligence},
	author = {Hogg, Tad and Huberman, Bernardo A. and Williams, Colin P.},
	month = mar,
	year = {1996},
	pages = {1--15},
	file = {hogg huberman williams 1996 - Phase transitions and the search problem  - BDPG - SEARCH - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/ELNSAH98/hogg huberman williams 1996 - Phase transitions and the search problem  - BDPG - SEARCH - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{huberman1987ai,
	title = {Phase transitions in artificial intelligence systems},
	volume = {33},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370287900336},
	doi = {10.1016/0004-3702(87)90033-6},
	abstract = {We predict that large-scale artificial intelligence systems and cognitive models will undergo sudden phase transitions from disjointed parts into coherent structures as their topological connectivity increases beyond a critical value. These situations, ranging from production systems to semantic net computations, are characterized by event horizons in space-time that determine the range of causal connections between processes. At transition, these event horizons undergo explosive changes in size. This phenomenon, analogous to phase transitions in nature, provides a new paradigm with which to analyze the behavior of large-scale computation and determine its generic features.},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {Artificial Intelligence},
	author = {Huberman, Bernardo A. and Hogg, Tad},
	month = oct,
	year = {1987},
	pages = {155--171},
	file = {huberman hogg 1987 - Phase Transitions in Artificial Intelligence Systems - BDPG - PROBLEM DIFFICULTY - ANNO.pdf:/Users/bill/D/Zotero/storage/5ZUFE543/huberman hogg 1987 - Phase Transitions in Artificial Intelligence Systems - BDPG - PROBLEM DIFFICULTY - ANNO.pdf:application/pdf},
}

@article{flouvat2010jiis,
	title = {A new classification of datasets for frequent itemsets},
	volume = {34},
	issn = {0925-9902, 1573-7675},
	url = {http://link.springer.com/10.1007/s10844-008-0077-0},
	doi = {10.1007/s10844-008-0077-0},
	abstract = {The discovery of frequent patterns is a famous problem in data mining. While plenty of algorithms have been proposed during the last decade, only a few contributions have tried to understand the inﬂuence of datasets on the algorithms behavior. Being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question. In this setting, we describe a thorough experimental study of datasets with respect to frequent itemsets. We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations: frequent closed, frequent free and frequent essential itemsets. For each of them, we also study the distribution of their positive and negative borders whenever possible. The main outcome of these experiments is a new classiﬁcation of datasets invariant w.r.t. minsup variations and robust to explain efﬁciency of several implementations.},
	language = {en},
	number = {1},
	urldate = {2021-03-18},
	journal = {Journal of Intelligent Information Systems},
	author = {Flouvat, Frédéric and De Marchi, Fabien and Petit, Jean-Marc},
	month = feb,
	year = {2010},
	keywords = {bdpg, classification of datasets},
	pages = {1--19},
	file = {flouvat et al 2010 - A new classification of datasets for frequent itemsets - BDPG.pdf:/Users/bill/D/Zotero/storage/NJFX855L/flouvat et al 2010 - A new classification of datasets for frequent itemsets - BDPG.pdf:application/pdf},
}

@techreport{sanchis1989,
	address = {University of Rochester},
	type = {Dissertation},
	title = {Language instance generation and test case construction for {NP}-hard problems},
	number = {TR 296},
	institution = {Computer Science Dept, University of Rochester},
	author = {Sanchis, Laura A.},
	month = may,
	year = {1989},
	file = {sanchis 1989 - language instance generation and test case construction for np-hard problems - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/S5EUPCT3/sanchis 1989 - language instance generation and test case construction for np-hard problems - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{krishnamurthy1987itc,
	title = {Constructing {Test} {Cases} for {Partitioning} {Heuristics}},
	volume = {C-36},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/5009543/},
	doi = {10.1109/TC.1987.5009543},
	language = {en},
	number = {9},
	urldate = {2021-03-20},
	journal = {IEEE Transactions on Computers},
	author = {Krishnamurthy, Balakrishnan},
	month = sep,
	year = {1987},
	pages = {1112--1114},
	file = {krishnamurthy 1987 - constructing test cases for partitioning heuristics - BDPG.pdf:/Users/bill/D/Zotero/storage/TX4L29U6/krishnamurthy 1987 - constructing test cases for partitioning heuristics - BDPG.pdf:application/pdf},
}

@article{smith2006ms,
	title = {The {Optimizer}’s {Curse}: {Skepticism} and {Postdecision} {Surprise} in {Decision} {Analysis}},
	volume = {52},
	issn = {0025-1909, 1526-5501},
	shorttitle = {The {Optimizer}’s {Curse}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451},
	doi = {10.1287/mnsc.1050.0451},
	language = {en},
	number = {3},
	urldate = {2021-03-21},
	journal = {Management Science},
	author = {Smith, James E. and Winkler, Robert L.},
	month = mar,
	year = {2006},
	keywords = {guppy, bdpg, uncertainty, optimization, surprise},
	pages = {311--322},
	file = {smith et al 2006 - the optimizers curse - skepticism and postdecision surprise in decision analysis.talks about negative surprise - BDPG.pdf:/Users/bill/D/Zotero/storage/KZTXU87Y/smith et al 2006 - the optimizers curse - skepticism and postdecision surprise in decision analysis.talks about negative surprise - BDPG.pdf:application/pdf},
}

@book{tsang1993,
	address = {London},
	series = {Computation in cognitive science},
	title = {Foundations of constraint satisfaction},
	isbn = {978-0-12-701610-8},
	language = {en},
	publisher = {Academic Press},
	author = {Tsang, Edward},
	year = {1993},
	note = {OCLC: 636781070},
	file = {tsang 1993 - Foundations of Constraint Satisfaction - CSP - BDPG_.pdf:/Users/bill/D/Zotero/storage/UD9UPBAM/tsang 1993 - Foundations of Constraint Satisfaction - CSP - BDPG_.pdf:application/pdf},
}

@article{gent2001c,
	title = {Random {Constraint} {Satisfaction}: {Flaws} and {Structure}},
	volume = {6},
	issn = {13837133},
	url = {http://link.springer.com/10.1023/A:1011454308633},
	doi = {10.1023/A:1011454308633},
	abstract = {A recent theoretical result by Achlioptas et al. shows that many models of random binary constraint satisfaction problems become trivially insoluble as problem size increases. This insolubility is partly due to the presence of ‘ﬂawed variables,’ variables whose values are all ‘ﬂawed’ (or unsupported). In this paper, we analyse how seriously existing work has been affected. We survey the literature to identify experimental studies that use models and parameters that may have been affected by ﬂaws. We then estimate theoretically and measure experimentally the size at which ﬂawed variables can be expected to occur. To eliminate ﬂawed values and variables in the models currently used, we introduce a ‘ﬂawless’ generator which puts a limited amount of structure into the conﬂict matrix. We prove that such ﬂawless problems are not trivially insoluble for constraint tightnesses up to 1/2. We also prove that the standard models B and C do not suffer from ﬂaws when the constraint tightness is less than the reciprocal of domain size. We consider introducing types of structure into the constraint graph which are rare in random graphs and present experimental results with such structured graphs.},
	language = {en},
	number = {4},
	journal = {Constraints},
	author = {Gent, Ian P and Macintyre, Ewan and Prosser, Patrick and Smith, Barbara M. and Walsh, Toby},
	year = {2001},
	pages = {345--372},
	file = {gent et al 2001 - random constraint satisfaction - flaws and structure - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/F2IG2MVG/gent et al 2001 - random constraint satisfaction - flaws and structure - BDPG - ANNO.pdf:application/pdf},
}

@article{smith2001tcs,
	title = {Constructing an asymptotic phase transition in random binary constraint satisfaction problems},
	volume = {265},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397501001669},
	doi = {10.1016/S0304-3975(01)00166-9},
	abstract = {The standard models used to generate random binary constraint satisfaction problems are described. At the problem sizes studied experimentally, a phase transition is seen as the constraint tightness is varied. However, Achlioptas et al. showed that if the problem size (number of variables) increases while the remaining parameters are kept constant, asymptotically almost all instances are unsatisÿable. In this paper, an alternative scheme for one of the standard models is proposed in which both the number of values in each variable’s domain and the average degree of the constraint graph are increased with problem size. It is shown that with this scheme there is asymptotically a range of values of the constraint tightness in which instances are trivially satisÿable with probability at least 0.5 and a range in which instances are almost all unsatisÿable; hence there is a crossover point at some value of the constraint tightness between these two ranges. This scheme is compared to a similar scheme due to Xu and Li. c 2001 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2021-03-23},
	journal = {Theoretical Computer Science},
	author = {Smith, Barbara M.},
	month = aug,
	year = {2001},
	pages = {265--283},
	file = {smith 2001 - Constructing an asymptotic phase transition in random binary constraint satisfaction problems - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/JYM9UBIA/smith 2001 - Constructing an asymptotic phase transition in random binary constraint satisfaction problems - BDPG - ANNO.pdf:application/pdf},
}

@article{borrett2001c,
	title = {A {Context} for {Constraint} {Satisfaction} {Problem} {Formulation} {Selection}},
	volume = {6},
	issn = {1572-9354},
	url = {https://doi.org/10.1023/A:1011432307724},
	doi = {10.1023/A:1011432307724},
	abstract = {Much research effort has been applied to ﬁnding effective ways for solving constraint satisfaction problems. However, the most fundamental aspect of constraint satisfaction problem solving, problem formulation, has received much less attention. This is important because the selection of an appropriate formulation can have dramatic effects on the efﬁciency of any constraint satisfaction problem solving algorithm.},
	language = {en},
	number = {4},
	journal = {Constraints},
	author = {Borrett, James E and Tsang, Edward P K},
	year = {2001},
	keywords = {bdpg, EFs, constraint satisfaction problems, csp},
	pages = {299--327},
	file = {borrett tsang 2001 - a context for constraint satisfaction problem formulation selection - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/YJDG4ESI/borrett tsang 2001 - a context for constraint satisfaction problem formulation selection - BDPG - ANNO.pdf:application/pdf},
}

@article{xu2000j,
	title = {Exact {Phase} {Transitions} in {Random} {Constraint} {Satisfaction} {Problems}},
	volume = {12},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10251},
	doi = {10.1613/jair.696},
	abstract = {In this paper we propose a new type of random CSP model,    called Model RB, which is a revision to the standard Model B. It is    proved that phase transitions from a region where almost all problems    are satisfiable to a region where almost all problems are    unsatisfiable do exist for Model RB as the number of variables    approaches infinity.  Moreover, the critical values at which the phase    transitions occur are also known exactly. By relating the hardness of    Model RB to Model B, it is shown that there exist a lot of hard    instances in Model RB.},
	urldate = {2021-03-23},
	journal = {Journal of Artificial Intelligence Research},
	author = {Xu, K. and Li, W.},
	month = mar,
	year = {2000},
	pages = {93--103},
	file = {xu li 2000 - exact phase transitions in random constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/XCZ2GDSW/xu li 2000 - exact phase transitions in random constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf:application/pdf},
}

@article{mccreesh2018j,
	title = {When {Subgraph} {Isomorphism} is {Really} {Hard}, and {Why} {This} {Matters} for {Graph} {Databases}},
	volume = {61},
	issn = {1076-9757},
	url = {http://jair.org/index.php/jair/article/view/11187},
	doi = {10.1613/jair.5768},
	abstract = {The subgraph isomorphism problem involves deciding whether a copy of a pattern graph occurs inside a larger target graph. The non-induced version allows extra edges in the target, whilst the induced version does not. Although both variants are NP-complete, algorithms inspired by constraint programming can operate comfortably on many real-world problem instances with thousands of vertices. However, they cannot handle arbitrary instances of this size. We show how to generate “really hard” random instances for subgraph isomorphism problems, which are computationally challenging with a couple of hundred vertices in the target, and only twenty pattern vertices. For the non-induced version of the problem, these instances lie on a satisﬁable / unsatisﬁable phase transition, whose location we can predict; for the induced variant, much richer behaviour is observed, and constrainedness gives a better measure of diﬃculty than does proximity to a phase transition. These results have practical consequences: we explain why the widely researched “ﬁlter / verify” indexing technique used in graph databases is founded upon a misunderstanding of the empirical hardness of NP-complete problems, and cannot be beneﬁcial when paired with any reasonable subgraph isomorphism algorithm.},
	language = {en},
	urldate = {2021-03-23},
	journal = {Journal of Artificial Intelligence Research},
	author = {McCreesh, Ciaran and Prosser, Patrick and Solnon, Christine and Trimble, James},
	month = mar,
	year = {2018},
	pages = {723--759},
	file = {mccreesh et al 2018 - When Subgraph Isomorphism is Really Hard, and Why This Matters for Graph Databases.pdf:/Users/bill/D/Zotero/storage/695TEGR6/mccreesh et al 2018 - When Subgraph Isomorphism is Really Hard, and Why This Matters for Graph Databases.pdf:application/pdf},
}

@article{dilkina2014amai,
	title = {Tradeoffs in the complexity of backdoors to satisfiability: dynamic sub-solvers and learning during search},
	volume = {70},
	issn = {1012-2443, 1573-7470},
	shorttitle = {Tradeoffs in the complexity of backdoors to satisfiability},
	url = {http://link.springer.com/10.1007/s10472-014-9407-9},
	doi = {10.1007/s10472-014-9407-9},
	language = {en},
	number = {4},
	urldate = {2021-03-23},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Dilkina, Bistra and Gomes, Carla P. and Sabharwal, Ashish},
	month = apr,
	year = {2014},
	pages = {399--431},
	file = {dilkina gomes sabharwal 2014 - Tradeoffs in the complexity of backdoors to satisfiability - dynamic sub-solvers and learning during search - PROBLEM DIFFICULTY - BDPG - CSPs.pdf:/Users/bill/D/Zotero/storage/P7GYPKCW/dilkina gomes sabharwal 2014 - Tradeoffs in the complexity of backdoors to satisfiability - dynamic sub-solvers and learning during search - PROBLEM DIFFICULTY - BDPG - CSPs.pdf:application/pdf},
}

@inproceedings{williams2003p1ijcai,
	title = {Backdoors {To} {Typical} {Case} {Complexity}},
	abstract = {There has been signiﬁcant recent progress in reasoning and constraint processing methods. In areas such as planning and ﬁnite model-checking, current solution techniques can handle combinatorial problems with up to a million variables and ﬁve million constraints. The good scaling behavior of these methods appears to defy what one would expect based on a worst-case complexity analysis. In order to bridge this gap between theory and practice, we propose a new framework for studying the complexity of these techniques on practical problem instances. In particular, our approach incorporates general structural properties observed in practical problem instances into the formal complexity analysis. We introduce a notion of “backdoors”, which are small sets of variables that capture the overall combinatorics of the problem instance. We provide empirical results showing the existence of such backdoors in real-world problems. We then present a series of complexity results that explain the good scaling behavior of current reasoning and constraint methods observed on practical problem instances.},
	language = {en},
	booktitle = {Proceedings of the 18th international joint conference on {Artificial} intelligence},
	author = {Williams, Ryan and Gomes, Carla P and Selman, Bart},
	year = {2003},
	pages = {1173--1178},
	file = {williams gomes selman 2003 - Backdoors To Typical Case Complexity - PROBLEM DIFFICULTY - BDPG - CSPs.pdf:/Users/bill/D/Zotero/storage/MDEGXBKS/williams gomes selman 2003 - Backdoors To Typical Case Complexity - PROBLEM DIFFICULTY - BDPG - CSPs.pdf:application/pdf},
}

@inproceedings{DBLP:conf/dimacs/Sanchis92,
	series = {{DIMACS} series in discrete mathematics and theoretical computer science},
	title = {Test case construction for the vertex cover problem},
	volume = {15},
	url = {https://doi.org/10.1090/dimacs/015/21},
	doi = {10.1090/dimacs/015/21},
	booktitle = {Computational support for discrete mathematics, proceedings of a {DIMACS} workshop, piscataway, new jersey, {USA}, march 12-14, 1992},
	publisher = {DIMACS/AMS},
	author = {Sanchis, Laura A.},
	editor = {Dean, Nathaniel and Shannon, Gregory E.},
	year = {1992},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/dimacs/Sanchis92.bib
tex.timestamp: Tue, 16 Jul 2019 17:45:06 +0200},
	pages = {315--326},
}

@inproceedings{mitchell1992ptncaia,
	address = {San Jose, CA},
	title = {Hard and {Easy} {Distributions} of {SAT} {Problems}},
	abstract = {We report results from large-scale experiments in satisability testing. As has been observed by others, testing the satis ability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satis ability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.},
	language = {en},
	booktitle = {Proceedings of the {Tenth} {National} {Conference} on {Artificial} {Intelligence} ({AAAI}-92)},
	author = {Mitchell, David and Selman, Bart and Levesque, Hector},
	year = {1992},
	pages = {459--465},
	file = {mitchell et al 1992 - Hard and Easy Distributions of SAT Problems - BDPG.pdf:/Users/bill/D/Zotero/storage/PCJ6WPNJ/mitchell et al 1992 - Hard and Easy Distributions of SAT Problems - BDPG.pdf:application/pdf},
}

@inproceedings{gomes1997,
	title = {Problem {Structure} in the {Presence} of {Perturbations}},
	abstract = {Recent progress on search and reasoning procedures has been driven by experimentation on computationally hard problem instances. Hard random problem distributions are an important source of such instances. Challenge problems from the area of nite algebra have also stimulated research on search and reasoning procedures. Nevertheless, the relation of such problems to practical applications is somewhat unclear. Realistic problem instances clearly have more structure than the random problem instances, but, on the other hand, they are not as regular as the structured mathematical problems. We propose a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain. We will show how to obtain interesting search problems in this manner, and how such problems can be used to study the robustness of search control mechanisms. Our experiments demonstrate that the performance of search strategies designed to mimic direct constructive methods degrade surprisingly quickly in the presence of even minor perturbations.},
	language = {en},
	author = {Gomes, Carla P and Selman, Bart},
	year = {1997},
	keywords = {bdpg, test generation},
	pages = {221--226},
	file = {gomes selman 1997 - problem structure in the presence of perturbations - BDPG.pdf:/Users/bill/D/Zotero/storage/47C59SV5/gomes selman 1997 - problem structure in the presence of perturbations - BDPG.pdf:application/pdf},
}

@article{franco1985sn,
	title = {Sensitivity of probabilistic results on algorithms for {NP}-complete problems to input distributions},
	volume = {17},
	issn = {0163-5700},
	url = {https://dl.acm.org/doi/10.1145/382250.382807},
	doi = {10.1145/382250.382807},
	language = {en},
	number = {1},
	urldate = {2021-03-27},
	journal = {ACM SIGACT News},
	author = {Franco, J.},
	month = jun,
	year = {1985},
	keywords = {bdpg},
	pages = {40--59},
	file = {franco 1985 - sensitivity of probabilistic results on algorithms for NP-complete problems to input distributions - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/GDHDY2D8/franco 1985 - sensitivity of probabilistic results on algorithms for NP-complete problems to input distributions - BDPG - ANNO.pdf:application/pdf},
}

@techreport{franco1985,
	address = {Bloomington, IN},
	type = {Technical report},
	title = {On the probabilistic performance of algorithms for the satisfiability problem},
	number = {167},
	institution = {Department of Computer Science, Indiana University},
	author = {Franco, John},
	year = {1985},
	keywords = {bdpg},
	file = {franco 1985 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf:/Users/bill/D/Zotero/storage/CNP56NM9/franco 1985 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf:application/pdf},
}

@article{franco1983dam,
	title = {Probabilistic analysis of the {Davis} {Putnam} procedure for solving the satisfiability problem},
	volume = {5},
	issn = {0166218X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0166218X83900173},
	doi = {10.1016/0166-218X(83)90017-3},
	language = {en},
	number = {1},
	urldate = {2021-03-27},
	journal = {Discrete Applied Mathematics},
	author = {Franco, John and Paull, Marvin},
	month = jan,
	year = {1983},
	pages = {77--87},
	file = {franco paull 1983 - probabilistic analysis of the davis putnam procedure ofr solving the satisfiability problem - BDPG.pdf:/Users/bill/D/Zotero/storage/KIM2V9PG/franco paull 1983 - probabilistic analysis of the davis putnam procedure ofr solving the satisfiability problem - BDPG.pdf:application/pdf},
}

@article{franco1986ipl,
	title = {On the probabilistic performance of algorithms for the satisfiability problem},
	volume = {23},
	issn = {00200190},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0020019086900517},
	doi = {10.1016/0020-0190(86)90051-7},
	language = {en},
	number = {2},
	urldate = {2021-03-27},
	journal = {Information Processing Letters},
	author = {Franco, John},
	month = aug,
	year = {1986},
	pages = {103--106},
	file = {franco 1986 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf:/Users/bill/D/Zotero/storage/LSISED47/franco 1986 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf:application/pdf},
}

@phdthesis{jia2007,
	title = {What makes {NP}-complete problems hard?},
	language = {en},
	school = {University of New Mexico},
	author = {Jia, Haixia},
	year = {2007},
	keywords = {bdpg, planted solution},
	file = {jia 2007 - what makes np-complete problems hard - aka hard instances with hidden solutions - THESIS - BDPG - PLANTED SOLUTIONS.pdf:/Users/bill/D/Zotero/storage/BWY66SLI/jia 2007 - what makes np-complete problems hard - aka hard instances with hidden solutions - THESIS - BDPG - PLANTED SOLUTIONS.pdf:application/pdf},
}

@article{jia2007j,
	title = {Generating {Hard} {Satisfiable} {Formulas} by {Hiding} {Solutions} {Deceptively}},
	volume = {28},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10484},
	doi = {10.1613/jair.2039},
	abstract = {To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisﬁable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisﬁed by A. However, this method tends to produce easy problems, since the majority of literals point toward the “hidden” assignment A. Last year, (Achlioptas, Jia, \& Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time.},
	language = {en},
	urldate = {2021-03-27},
	journal = {Journal of Artificial Intelligence Research},
	author = {Jia, H. and Moore, C. and Strain, D.},
	month = feb,
	year = {2007},
	keywords = {bdpg, planted solution},
	pages = {107--118},
	file = {jia moore strain 2005 - generating hard satisfiable formulas by hiding solutions deceptively - AAAI05-061 - BDPG - PLANTED SOLUTIONS.pdf:/Users/bill/D/Zotero/storage/EP2MSHHA/jia moore strain 2005 - generating hard satisfiable formulas by hiding solutions deceptively - AAAI05-061 - BDPG - PLANTED SOLUTIONS.pdf:application/pdf},
}

@article{haanpaa2006s,
	title = {Hard {Satisfiable} {Clause} {Sets} for {Benchmarking} {Equivalence} {Reasoning} {Techniques}},
	volume = {2},
	issn = {15740617},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SAT190015},
	doi = {10.3233/SAT190015},
	abstract = {A family of satisﬁable benchmark instances in conjunctive normal form is introduced. The instances are constructed by transforming a random regular graph into a system of linear equations followed by clausiﬁcation. Schemes for introducing nonlinearity to the instances are developed, making the instances suitable for benchmarking solvers with equivalence reasoning techniques. An extensive experimental evaluation shows that state-ofthe-art solvers scale exponentially in the instance size. Compared with other well-known families of satisﬁable benchmark instances, the present instances are among the hardest.},
	language = {en},
	number = {1-4},
	urldate = {2021-03-27},
	journal = {Journal on Satisfiability, Boolean Modeling and Computation},
	author = {Haanpää, Harri and Järvisalo, Matti and Kaski, Petteri and Niemelä, Ilkka},
	editor = {Le Berre, Daniel and Simon, Laurent},
	month = mar,
	year = {2006},
	keywords = {bdpg, benchmarking, planted solution},
	pages = {27--46},
	file = {haanpaa et al 2006 - Hard Satisable Clause Sets for Benchmarking Equivalence Reasoning Techniques - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/XREPZ98N/haanpaa et al 2006 - Hard Satisable Clause Sets for Benchmarking Equivalence Reasoning Techniques - BDPG - ANNO.pdf:application/pdf},
}

@article{hartmann2005cpc,
	title = {Phase transition and finite-size scaling in the vertex-cover problem},
	volume = {169},
	issn = {00104655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465505001517},
	doi = {10.1016/j.cpc.2005.03.054},
	abstract = {NP-complete problems play a fundamental role in theoretical computer science. Recently, phase transitions were discovered in such problems, when studying suitably parameterized random ensembles. By applying concepts and methods from statistical physics, it is possible to understand these models and the phase transitions which occur. Here, we consider the vertex-cover problem, one of the six “basic” NP-complete problems. We describe the phase transition and the running time of an exact algorithm around the phase transition. To investigate how this transition resembles a phase transition in physical systems, we apply ﬁnite-size scaling and we study a correlation-length like quantity.},
	language = {en},
	number = {1-3},
	urldate = {2021-03-27},
	journal = {Computer Physics Communications},
	author = {Hartmann, Alexander K. and Barthel, Wolfgang and Weigt, Martin},
	month = jul,
	year = {2005},
	keywords = {bdpg, vertex cover},
	pages = {234--237},
	file = {hartmann et al 2017 - Phase transition and finite-size scaling in the vertex-cover problem - BDPG.pdf:/Users/bill/D/Zotero/storage/BW2NN5N7/hartmann et al 2017 - Phase transition and finite-size scaling in the vertex-cover problem - BDPG.pdf:application/pdf},
}

@article{barthel2002prl,
	title = {Hiding {Solutions} in {Random} {Satisfiability} {Problems}: {A} {Statistical} {Mechanics} {Approach}},
	volume = {88},
	issn = {0031-9007, 1079-7114},
	shorttitle = {Hiding {Solutions} in {Random} {Satisfiability} {Problems}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.188701},
	doi = {10.1103/PhysRevLett.88.188701},
	language = {en},
	number = {18},
	urldate = {2021-03-27},
	journal = {Physical Review Letters},
	author = {Barthel, W. and Hartmann, A. K. and Leone, M. and Ricci-Tersenghi, F. and Weigt, M. and Zecchina, R.},
	month = apr,
	year = {2002},
	keywords = {bdpg, solution hiding, solution planting, statistical mechanics},
	pages = {188701},
	file = {barthel et al 2002 - Hiding Solutions in Random Satisfiability Problems - A Statistical Mechanics Approach - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/39QSZASR/barthel et al 2002 - Hiding Solutions in Random Satisfiability Problems - A Statistical Mechanics Approach - BDPG - ANNO.pdf:application/pdf},
}

@article{church1996bc,
	title = {Reserve selection as a maximal covering location problem},
	volume = {76},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006320795001026},
	doi = {10.1016/0006-3207(95)00102-6},
	language = {en},
	number = {2},
	urldate = {2021-03-28},
	journal = {Biological Conservation},
	author = {Church, Richard L. and Stoms, David M. and Davis, Frank W.},
	year = {1996},
	pages = {105--112},
	file = {1-s2.0-0006320795001026-main.pdf:/Users/bill/D/Zotero/storage/IBRKRYKU/1-s2.0-0006320795001026-main.pdf:application/pdf},
}

@article{smith-miles2011amai,
	title = {Discovering the suitability of optimisation algorithms by learning from evolved instances},
	volume = {61},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-011-9230-5},
	doi = {10.1007/s10472-011-9230-5},
	language = {en},
	number = {2},
	urldate = {2021-03-28},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Smith-Miles, Kate and van Hemert, Jano},
	month = feb,
	year = {2011},
	pages = {87--104},
	file = {smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf:/Users/bill/D/Zotero/storage/IG5EQX24/smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf:application/pdf},
}

@article{achlioptas2021rsa,
	title = {The number of satisfying assignments of random 2‐{SAT} formulas},
	issn = {1042-9832, 1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rsa.20993},
	doi = {10.1002/rsa.20993},
	abstract = {We show that throughout the satisfiable phase the normalized number of satisfying assignments of a random 2-SAT formula converges in probability to an expression predicted by the cavity method from statistical physics. The proof is based on showing that the Belief Propagation algorithm renders the correct marginal probability that a variable is set to “true” under a uniformly random satisfying assignment.},
	language = {en},
	urldate = {2021-03-28},
	journal = {Random Structures \& Algorithms},
	author = {Achlioptas, Dimitris and Coja‐Oghlan, Amin and Hahn‐Klimroth, Max and Lee, Joon and Müller, Noëla and Penschuck, Manuel and Zhou, Guangyan},
	month = jan,
	year = {2021},
	keywords = {bdpg, statistical mechanics, 2-SAT},
	pages = {rsa.20993},
	file = {achlioptas et al 2020 - The number of satisfying assignments of random2-SAT formulas - BDPG - STATISTICAL MECHANICS.pdf:/Users/bill/D/Zotero/storage/8TQEWSHT/achlioptas et al 2020 - The number of satisfying assignments of random2-SAT formulas - BDPG - STATISTICAL MECHANICS.pdf:application/pdf},
}

@incollection{achlioptas1997papocp,
	address = {Berlin, Heidelberg},
	title = {Random constraint satisfaction: {A} more accurate picture},
	volume = {1330},
	isbn = {978-3-540-63753-0 978-3-540-69642-1},
	shorttitle = {Random constraint satisfaction},
	url = {http://link.springer.com/10.1007/BFb0017433},
	abstract = {Recently there has been a great amount of interest in Random Constraint Satisfaction Problems, both from an experimental and a theoretical point of view. Rather intriguingly, experimental results with various models for generating random CSP instances suggest a "threshold-like" behavior and some theoretical work has been done in analyzing these models when the number of variables becomes large (asymptotic). In this paper we prove that the models commonly used for generating random CSP instances do not have an asymptotic threshold. In particular, we prove that as the number of variables becomes large, almost all instances they generate are trivially overconstrained. We then present a new model for random CSP and, in the spirit of random k-SAT, we derive lower and upper bounds for its parameters so that instances are "almost surely" underconstrained and overconstrained, respectively.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}-{CP97}},
	publisher = {Springer Berlin Heidelberg},
	author = {Achlioptas, Dimitris and Kirousis, Lefteris M. and Kranakis, Evangelos and Krizanc, Danny and Molloy, Michael S. O. and Stamatiou, Yannis C.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Smolka, Gert},
	year = {1997},
	doi = {10.1007/BFb0017433},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {107--120},
	file = {achlioptas et al 1997 - Random Constraint Satisfaction - a more accurate picture - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/Q7JFEAX8/achlioptas et al 1997 - Random Constraint Satisfaction - a more accurate picture - BDPG - ANNO.pdf:application/pdf},
}

@article{diaz2019s,
	title = {Pervasive human-driven decline of life on {Earth} points to the need for transformative change},
	volume = {366},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aax3100},
	doi = {10.1126/science.aax3100},
	abstract = {The human impact on life on Earth has increased sharply since the 1970s, driven by the demands of a growing population with rising average per capita income. Nature is currently supplying more materials than ever before, but this has come at the high cost of unprecedented global declines in the extent and integrity of ecosystems, distinctness of local ecological communities, abundance and number of wild species, and the number of local domesticated varieties. Such changes reduce vital benefits that people receive from nature and threaten the quality of life of future generations. Both the benefits of an expanding economy and the costs of reducing nature’s benefits are unequally distributed. The fabric of life on which we all depend—nature and its contributions to people—is unravelling rapidly. Despite the severity of the threats and lack of enough progress in tackling them to date, opportunities exist to change future trajectories through transformative action. Such action must begin immediately, however, and address the root economic, social, and technological causes of nature’s deterioration.},
	language = {en},
	number = {6471},
	urldate = {2021-04-02},
	journal = {Science},
	author = {Díaz, Sandra and Settele, Josef and Brondízio, Eduardo S. and Ngo, Hien T. and Agard, John and Arneth, Almut and Balvanera, Patricia and Brauman, Kate A. and Butchart, Stuart H. M. and Chan, Kai M. A. and Garibaldi, Lucas A. and Ichii, Kazuhito and Liu, Jianguo and Subramanian, Suneetha M. and Midgley, Guy F. and Miloslavich, Patricia and Molnár, Zsolt and Obura, David and Pfaff, Alexander and Polasky, Stephen and Purvis, Andy and Razzaque, Jona and Reyers, Belinda and Chowdhury, Rinku Roy and Shin, Yunne-Jai and Visseren-Hamakers, Ingrid and Willis, Katherine J. and Zayas, Cynthia N.},
	month = dec,
	year = {2019},
	pages = {eaax3100},
	file = {diaz et al 2019 - pervasive human-driven decline of life on earth points to theneed for transformative change - BDPG.pdf:/Users/bill/D/Zotero/storage/YJPUN6SU/diaz et al 2019 - pervasive human-driven decline of life on earth points to theneed for transformative change - BDPG.pdf:application/pdf},
}

@article{1988orl,
	title = {Resolution vs. cutting plane solution of inference problems: some computational experience},
	volume = {7},
	number = {1},
	journal = {Operations Research Letters},
	year = {1988},
	keywords = {bdpg},
	pages = {1--7},
	file = {hooker 1988 - Resolution vs. cutting plane solution of inference problems - some computational experience - BDPG.pdf:/Users/bill/D/Zotero/storage/2EPSYV5X/hooker 1988 - Resolution vs. cutting plane solution of inference problems - some computational experience - BDPG.pdf:application/pdf},
}

@article{goldberg1982ipl,
	title = {Average time analyses of simplified {Davis}-{Putnam} procedures},
	volume = {15},
	issn = {00200190},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0020019083901278},
	doi = {10.1016/0020-0190(83)90127-8},
	language = {en},
	number = {2},
	urldate = {2021-04-07},
	journal = {Information Processing Letters},
	author = {Goldberg, Allen and Purdom, Paul and Brown, Cynthia},
	month = sep,
	year = {1982},
	pages = {213},
	file = {goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - BDPG.pdf:/Users/bill/D/Zotero/storage/V5TEVVJJ/goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - BDPG.pdf:application/pdf},
}

@article{goldberg1983ipl,
	title = {Average time analyses of simplified {Davis}-{Putnam} procedures: {Corrigendum}},
	volume = {16},
	issn = {00200190},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0020019083901278},
	doi = {10.1016/0020-0190(83)90127-8},
	language = {en},
	number = {4},
	urldate = {2021-04-07},
	journal = {Information Processing Letters},
	author = {Goldberg, Allen and Purdom, Paul and Brown, Cynthia},
	month = may,
	year = {1983},
	pages = {213},
	file = {goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - Corrigendum 1983 - BDPG.pdf.pdf:/Users/bill/D/Zotero/storage/BE7BD2DL/goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - Corrigendum 1983 - BDPG.pdf.pdf:application/pdf},
}

@techreport{goldberg1979,
	address = {New York University, NY},
	type = {Courant {Computer} {Science} {Report}},
	title = {On the complexity of the satisfiability problem},
	url = {https://archive.org/details/oncomplexityofsa00gold},
	language = {en},
	number = {\#16},
	institution = {Courant Institute of Mathematical Sciences},
	author = {Goldberg, Allen T},
	year = {1979},
	file = {goldberg 1979 - on the complexity of the satisfiability problem - THESIS - BDPG.pdf:/Users/bill/D/Zotero/storage/YW6BD4JG/goldberg 1979 - on the complexity of the satisfiability problem - THESIS - BDPG.pdf:application/pdf},
}

@article{hooker1994or,
	title = {Needed: {An} {Empirical} {Science} of {Algorithms}},
	volume = {42},
	issn = {0030-364X, 1526-5463},
	shorttitle = {Needed},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.42.2.201},
	doi = {10.1287/opre.42.2.201},
	language = {en},
	number = {2},
	urldate = {2021-04-07},
	journal = {Operations Research},
	author = {Hooker, J. N.},
	month = apr,
	year = {1994},
	pages = {201--212},
	file = {hooker 1994 -  Needed - An Empirical Science of Algorithms - BDPG.pdf:/Users/bill/D/Zotero/storage/6CT8ZX3D/hooker 1994 -  Needed - An Empirical Science of Algorithms - BDPG.pdf:application/pdf},
}

@article{schuster2020p,
	title = {Exact integer linear programming solvers outperform simulated annealing for solving conservation planning problems},
	volume = {8},
	issn = {2167-8359},
	url = {https://peerj.com/articles/9258},
	doi = {10.7717/peerj.9258},
	abstract = {The resources available for conserving biodiversity are limited, and so protected areas need to be established in places that will achieve objectives for minimal cost. Two of the main algorithms for solving systematic conservation planning problems are Simulated Annealing (SA) and exact integer linear programing (EILP) solvers. Using a case study in BC, Canada, we compare the cost-effectiveness and processing times of SA used in Marxan versus EILP using both commercial and open-source algorithms. Plans for expanding protected area systems based on EILP algorithms were 12–30\% cheaper than plans using SA, due to EILP’s ability to ﬁnd optimal solutions as opposed to approximations. The best EILP solver we examined was on average 1,071 times faster than the SA algorithm tested. The performance advantages of EILP solvers were also observed when we aimed for spatially compact solutions by including a boundary penalty. One practical advantage of using EILP over SA is that the analysis does not require calibration, saving even more time. Given the performance of EILP solvers, they can be used to generate conservation plans in real-time during stakeholder meetings and can facilitate rapid sensitivity analysis, and contribute to a more transparent, inclusive, and defensible decision-making process.},
	language = {en},
	urldate = {2021-04-21},
	journal = {PeerJ},
	author = {Schuster, Richard and Hanson, Jeffrey O. and Strimas-Mackey, Matthew and Bennett, Joseph R.},
	month = may,
	year = {2020},
	keywords = {bdpg},
	pages = {e9258},
	file = {schuster et al 2020 - exact integer linear programming solvers outperform simulated annealing for solving conservation planning problems - BDPG.pdf:/Users/bill/D/Zotero/storage/GMEHCEGQ/schuster et al 2020 - exact integer linear programming solvers outperform simulated annealing for solving conservation planning problems - BDPG.pdf:application/pdf},
}

@article{brown2015pnasu,
	title = {Effective conservation requires clear objectives and prioritizing actions, not places or species},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1509189112},
	doi = {10.1073/pnas.1509189112},
	language = {en},
	number = {32},
	urldate = {2021-04-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brown, Christopher J. and Bode, Michael and Venter, Oscar and Barnes, Megan D. and McGowan, Jennifer and Runge, Claire A. and Watson, James E. M. and Possingham, Hugh P.},
	month = aug,
	year = {2015},
	pages = {E4342--E4342},
	file = {brown et al 2015 - effective conservation requires clear objectives and prioritizing actions not places or species - BDPG.pdf:/Users/bill/D/Zotero/storage/QFM7G3G8/brown et al 2015 - effective conservation requires clear objectives and prioritizing actions not places or species - BDPG.pdf:application/pdf},
}

@article{mcgeoch1995ijc,
	title = {Toward an experimental method for algorithm simulation},
	volume = {8},
	number = {1},
	journal = {INFORMS Journal on Computing},
	author = {McGeoch, Catherine},
	year = {1995},
	keywords = {bdpg, optimization, benchmarking},
	file = {McGeoch 1995 - toward an experimental method for algorithm simulation.pdf:/Users/bill/D/Zotero/storage/PL6F926P/McGeoch 1995 - toward an experimental method for algorithm simulation.pdf:application/pdf},
}

@techreport{rice1975,
	address = {West Lafayette, Indiana},
	title = {The algorithm selection problem},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.7199&rep=rep1&type=pdf},
	abstract = {Publisher Summary

The problem of selecting an effective algorithm arises in a wide variety of situations. This chapter starts with a discussion on abstract models: the basic model and associated problems, the model with selection based on features, and the model with variable performance criteria. One objective of this chapter is to explore the applicability of the approximation theory to the algorithm selection problem. There is an intimate relationship here and that the approximation theory forms an appropriate base upon which to develop a theory of algorithm selection methods. The approximation theory currently lacks much of the necessary machinery for the algorithm selection problem. There is a need to develop new results and apply known techniques to these new circumstances. The final pages of this chapter form a sort of appendix, which lists 15 specific open problems and questions in this area. There is a close relationship between the algorithm selection problem and the general optimization theory. This is not surprising since the approximation problem is a special form of the optimization problem. Most realistic algorithm selection problems are of moderate to high dimensionality and thus one should expect them to be quite complex. One consequence of this is that most straightforward approaches (even well-conceived ones) are likely to lead to enormous computations for the best selection. The single most important part of the solution of a selection problem is the appropriate choice of the form for selection mapping. It is here that theories give the least guidance and that the art of problem solving is most crucial.},
	number = {CSD-TR 152},
	institution = {Purdue University},
	author = {Rice, John R.},
	year = {1975},
	note = {published later but can't find a copy yet:
Advances in Computers
Volume 15, 1976, Pages 65-118},
	file = {rice 1975 - the algorithm selection problem - tech report - TR 75-152.pdf:/Users/bill/D/Zotero/storage/J64BYEQR/rice 1975 - the algorithm selection problem - tech report - TR 75-152.pdf:application/pdf},
}

@article{rice1976ac,
	title = {The {Algorithm} {Selection} {Problem}},
	volume = {15},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245808605203},
	doi = {10.1016/S0065-2458(08)60520-3},
	language = {en},
	urldate = {2021-05-08},
	journal = {Advances in Computers},
	author = {Rice, John R.},
	year = {1976},
	pages = {65--118},
	file = {rice 1976 - the algorithm selection problem - BDPG - PROBLEM DIFFICULTY - BENCHMARKING - ALGORITHM SELECTION.pdf:/Users/bill/D/Zotero/storage/PQHUFAVQ/rice 1976 - the algorithm selection problem - BDPG - PROBLEM DIFFICULTY - BENCHMARKING - ALGORITHM SELECTION.pdf:application/pdf},
}

@article{abdulrahman2017,
	title = {An {Overview} of the {Algorithm} {Selection} {Problem}},
	volume = {26},
	abstract = {Users of machine learning algorithms need methods that can help them to identify algorithm or their combinations (workflows) that achieve the potentially best performance. Selecting the best algorithm to solve a given problem has been the subject of many studies over the past four decades. This survey presents an overview of the contributions made in the area of algorithm selection problems. We present different methods for solving the algorithm selection problem identifying some of the future research challenges in this domain.},
	language = {en},
	number = {1},
	author = {Abdulrahman, Salisu Mamman and Adamu, Alhassan and Ibrahim, Yazid Ado and Muhammad, Rilwan},
	year = {2017},
	pages = {11},
	file = {Abdulrahmana et al 2017 - An Overview of the Algorithm Selection Problem - BDPG - ALGORITHM SELECTION.pdf:/Users/bill/D/Zotero/storage/CXFETVRC/Abdulrahmana et al 2017 - An Overview of the Algorithm Selection Problem - BDPG - ALGORITHM SELECTION.pdf:application/pdf},
}

@incollection{hall2010emftaooa,
	address = {Berlin, Heidelberg},
	title = {The {Generation} of {Experimental} {Data} for {Computational} {Testing} in {Optimization}},
	isbn = {978-3-642-02537-2 978-3-642-02538-9},
	url = {http://link.springer.com/10.1007/978-3-642-02538-9_4},
	abstract = {This chapter discusses approaches to generating synthetic data for use in scientiﬁc experiments. In many diverse scientiﬁc ﬁelds, the lack of availability, high cost or inconvenience of the collection of real-world data motivates the generation of synthetic data. In many experiments, the method chosen to generate synthetic data can signiﬁcantly affect the results of an experiment. Unfortunately, the scientiﬁc literature does not contain general protocols for how synthetic data should be generated. The purpose of this chapter is to rectify that deﬁciency. The protocol we propose is based on several generation principles. These principles motivate and organize the data generation process. The principles are operationalized by generation properties. Then, together with information about the features of the application and of the experiment, the properties are used to construct a data generation scheme. Finally, we suggest procedures for validating the synthetic data generated. The usefulness of our protocol is illustrated by a discussion of numerous applications of data generation from the optimization literature. This discussion identiﬁes examples of both good and bad data generation practice as it relates to our protocol.},
	language = {en},
	urldate = {2021-05-08},
	booktitle = {Experimental {Methods} for the {Analysis} of {Optimization} {Algorithms}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	editor = {Bartz-Beielstein, Thomas and Chiarandini, Marco and Paquete, Luís and Preuss, Mike},
	year = {2010},
	doi = {10.1007/978-3-642-02538-9_4},
	pages = {73--101},
	file = {hall posner 2010 - the generation of experimental data for computational testing in optimization - BDPG.pdf:/Users/bill/D/Zotero/storage/X339P69G/hall posner 2010 - the generation of experimental data for computational testing in optimization - BDPG.pdf:application/pdf},
}

@article{smith-miles2021c&or,
	title = {Revisiting where are the hard knapsack problems? via {Instance} {Space} {Analysis}},
	volume = {128},
	issn = {03050548},
	shorttitle = {Revisiting where are the hard knapsack problems?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054820303014},
	doi = {10.1016/j.cor.2020.105184},
	abstract = {In 2005, David Pisinger asked the question “where are the hard knapsack problems?”. Noting that the classical benchmark test instances were limited in diﬃculty due to their selected structure, he proposed a set of new test instances for the 0-1 knapsack problem with characteristics that made them more challenging for dynamic programming and branch-and-bound algorithms. This important work highlighted the inﬂuence of diversity in test instances to draw reliable conclusions about algorithm performance. In this paper, we revisit the question in light of recent methodological advances – in the form of Instance Space Analysis –enabling the strengths and weaknesses of algorithms to be visualised and assessed across the broadest possible space of test instances. We show where the hard instances lie, and objectively assess algorithm performance across the instance space to articulate the strengths and weaknesses of algorithms. Furthermore, we propose a method to ﬁll the instance space with diverse and challenging new test instances with controllable properties to support greater insights into algorithm selection, and drive future algorithmic innovations.},
	language = {en},
	urldate = {2021-05-08},
	journal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Christiansen, Jeffrey and Muñoz, Mario Andrés},
	month = apr,
	year = {2021},
	pages = {105184},
	file = {smith-miles et al 2021 - revisiting where are the hard knapsack problems - via instance space analysis - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/E6TCQ9KW/smith-miles et al 2021 - revisiting where are the hard knapsack problems - via instance space analysis - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{hall2007or,
	title = {Performance {Prediction} and {Preselection} for {Optimization} and {Heuristic} {Solution} {Procedures}},
	volume = {55},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0398},
	doi = {10.1287/opre.1070.0398},
	language = {en},
	number = {4},
	urldate = {2021-05-08},
	journal = {Operations Research},
	author = {Hall, Nicholas G. and Posner, Marc E.},
	month = aug,
	year = {2007},
	pages = {703--716},
	file = {hall posner 2007 - Performance Prediction and Preselection for Optimization and Heuristic Solution Procedures - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/SH7ZJR2N/hall posner 2007 - Performance Prediction and Preselection for Optimization and Heuristic Solution Procedures - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{beiranvand2017oe,
	title = {Best practices for comparing optimization algorithms},
	volume = {18},
	issn = {1389-4420, 1573-2924},
	url = {http://arxiv.org/abs/1709.08242},
	doi = {10.1007/s11081-017-9366-1},
	abstract = {The ﬁnal publication is available at Springer via http://dx.doi.org/10.1007/s11081017-9366-1 Comparing, or benchmarking, of optimization algorithms is a complicated task that involves many subtle considerations to yield a fair and unbiased evaluation. In this paper, we systematically review the benchmarking process of optimization algorithms, and discuss the challenges of fair comparison. We provide suggestions for each step of the comparison process and highlight the pitfalls to avoid when evaluating the performance of optimization algorithms. We also discuss various methods of reporting the benchmarking results. Finally, some suggestions for future research are presented to improve the current benchmarking process.},
	language = {en},
	number = {4},
	urldate = {2021-05-08},
	journal = {Optimization and Engineering},
	author = {Beiranvand, Vahid and Hare, Warren and Lucet, Yves},
	month = dec,
	year = {2017},
	note = {arXiv: 1709.08242},
	keywords = {Mathematics - Optimization and Control, 65K05, F.2.1, G.1.6},
	pages = {815--848},
	annote = {Comment: Optim Eng (2017)},
	file = {beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING.pdf:/Users/bill/D/Zotero/storage/EN2GCNIH/beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING.pdf:application/pdf},
}

@incollection{mcgeoch2002hogo,
	title = {Experimental analysis of algorithms},
	volume = {2},
	language = {en},
	booktitle = {Handbook of global optimization},
	author = {McGeoch, Catherine C},
	year = {2002},
	pages = {489--513},
	file = {mcgeogh 2002 - experimental analysis of algorithms - BOOK CHAPTER - BDPG.pdf:/Users/bill/D/Zotero/storage/XBJ57FMA/mcgeogh 2002 - experimental analysis of algorithms - BOOK CHAPTER - BDPG.pdf:application/pdf},
}

@article{sluban2015n,
	title = {Relating ensemble diversity and performance: {A} study in class noise detection},
	volume = {160},
	issn = {09252312},
	shorttitle = {Relating ensemble diversity and performance},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231215001265},
	doi = {10.1016/j.neucom.2014.10.086},
	abstract = {The advantage of ensemble methods over single methods is their ability to correct the errors of individual ensemble members and thereby improve the overall ensemble performance. This paper explores the relation between ensemble diversity and noise detection performance in the context of ensemble-based class noise detection by studying different diversity measures on a range of heterogeneous noise detection ensembles. In the empirical analysis the majority and the consensus ensemble voting schemes are studied. It is shown that increased diversity of ensembles using the majority voting scheme does not lead to better noise detection performance and may even degrade the performance of heterogeneous noise detection ensembles. On the other hand, for consensus-based noise detection ensembles the results show that more diverse ensembles achieve higher precision of class noise detection, whereas less diverse ensembles lead to higher recall of noise detection and higher F-scores. \& 2015 Elsevier B.V. All rights reserved.},
	language = {en},
	urldate = {2021-05-08},
	journal = {Neurocomputing},
	author = {Sluban, Borut and Lavrač, Nada},
	month = jul,
	year = {2015},
	keywords = {bdpg, uncertainty, ensembles, ensemble diversity, noise, label noise},
	pages = {120--131},
	file = {sluban lavrac 2015 - Relating ensemble diversity and performance - A study in class noise detection - BDPG - UNCERTAINTY - NOISE - ENSEMBLE DIVERSITY.pdf:/Users/bill/D/Zotero/storage/HXDTIG7D/sluban lavrac 2015 - Relating ensemble diversity and performance - A study in class noise detection - BDPG - UNCERTAINTY - NOISE - ENSEMBLE DIVERSITY.pdf:application/pdf},
}

@misc{chang2021,
	title = {shiny: {Web} {Application} {Framework} for {R}},
	url = {https://CRAN.R-project.org/package=shiny},
	author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Sievert, Carson and Schloerke, Barret and Xie, Yihui and Allen, Jeff and McPherson, Jonathan and Dipert, Alan and Borges, Barbara},
	year = {2021},
}

@article{sierra-altamiranda2020em,
	title = {Spatial conservation planning under uncertainty using modern portfolio theory and {Nash} bargaining solution},
	volume = {423},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380020300880},
	doi = {10.1016/j.ecolmodel.2020.109016},
	abstract = {In recent years, researchers from interdisciplinary teams involving ecologists, economists and operations researchers collaborated to provide decision support tools to address the challenges of preserving biodiversity by optimizing the design of reserves. The goal of this paper is to further advance this area of research and provide new solutions to solve complex Spatial Conservation Planning (SCP) problems under uncertainty that consider risk preferences of decision makers. Our approach employs modern portfolio theory to address uncertainties in SCP problems, and involves two conflicting objectives: maximizing return and minimizing risk. We apply concepts from game theory such as the Nash bargaining solution to directly compute a desirable Pareto-optimal solution for the proposed bi-objective optimization formulation in natural resource management problems. We demonstrate with numerical examples that by directly computing a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. We show that our approach (implementable with commercial solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature. Optimal solutions for problems with less than 400 parcels can be computed within a minute. Near optimal solutions (within at most 0.2\% gap from an optimal solution) for high-dimensional problems involving up to 800 parcels can be computed within 8 h on a standard computer. We have presented a new approach to solve SCP optimization problems while considering uncertainty and risk tolerance of decision makers. Our new approach expands considerably the applicability of such SCP optimization methods to address real conservation problems.},
	language = {en},
	urldate = {2021-11-13},
	journal = {Ecological Modelling},
	author = {Sierra-Altamiranda, Alvaro and Charkhgard, Hadi and Eaton, Mitchell and Martin, Julien and Yurek, Simeon and Udell, Bradley J.},
	month = may,
	year = {2020},
	pages = {109016},
	file = {sierra-altamiranda et al 2020 - Spatial conservation planning under uncertainty using modern portfolio theory and Nash bargaining solution - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/KIJ867QG/sierra-altamiranda et al 2020 - Spatial conservation planning under uncertainty using modern portfolio theory and Nash bargaining solution - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf:application/pdf},
}

@article{zotero-1799,
}

@article{ghasemisaghand2021e,
	title = {{SiteOpt}: an open‐source {R}‐package for site selection and portfolio optimization},
	volume = {44},
	issn = {0906-7590, 1600-0587},
	shorttitle = {{SiteOpt}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.05717},
	doi = {10.1111/ecog.05717},
	language = {en},
	number = {11},
	urldate = {2021-11-13},
	journal = {Ecography},
	author = {Ghasemi Saghand, Payman and Haider, Zulqarnain and Charkhgard, Hadi and Eaton, Mitchell and Martin, Julien and Yurek, Simeon and Udell, Bradley J.},
	month = nov,
	year = {2021},
	pages = {1678--1685},
	file = {Saghand Haider et al 2021 - SiteOpt - an open-source R-package for site selection and  portfolio optimization - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf:/Users/bill/D/Zotero/storage/CVX9AM8Z/Saghand Haider et al 2021 - SiteOpt - an open-source R-package for site selection and  portfolio optimization - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf:application/pdf},
}

@article{hanson2019mee,
	title = {Optimality in prioritizing conservation projects},
	volume = {10},
	issn = {2041-210X, 2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/2041-210X.13264},
	doi = {10.1111/2041-210X.13264},
	language = {en},
	number = {10},
	urldate = {2021-11-14},
	journal = {Methods in Ecology and Evolution},
	author = {Hanson, Jeffrey O. and Schuster, Richard and Strimas‐Mackey, Matthew and Bennett, Joseph R.},
	editor = {Hodgson, Dave},
	month = oct,
	year = {2019},
	pages = {1655--1663},
	file = {hanson ... strimas-mackey ... 2019 - Optimality in prioritizing conservation projects - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/TV2MTFF7/hanson ... strimas-mackey ... 2019 - Optimality in prioritizing conservation projects - BDPG - ANNO.pdf:application/pdf},
}

@article{billionnet2015ema,
	title = {Designing {Robust} {Nature} {Reserves} {Under} {Uncertain} {Survival} {Probabilities}},
	volume = {20},
	issn = {1420-2026, 1573-2967},
	url = {http://link.springer.com/10.1007/s10666-014-9437-z},
	doi = {10.1007/s10666-014-9437-z},
	abstract = {We address the problem of optimal selection of sites to constitute a nature reserve which ensures that a given set of species has fixed survival probabilities. This classic problem has already been considered in the literature of conservation biology. The originality of this article is to consider that the values of the survival probabilities of each species in each potential site may be subject to a certain error while assuming that the number of sites where these probabilities are wrong is limited. We thus define a set of possible survival probability values in each site. We then show how to determine, by solving a relatively simple mixed-integer linear program, an optimal robust reserve, i.e., a reserve which ensures that each species has a certain survival probability whatever the values taken by the survival probabilities in each site, in the set of possible values. The fact of being able to formulate the search for an optimal robust reserve by a mixed-integer linear program provides an easy way to take into account additional constraints on the selection of sites such as, for example, spatial constraints. We report some computational experiments carried out on many hypothetical landscapes to illustrate the concept of robust reserve and show the effectiveness of the approach.},
	language = {en},
	number = {4},
	urldate = {2021-11-29},
	journal = {Environmental Modeling \& Assessment},
	author = {Billionnet, Alain},
	month = aug,
	year = {2015},
	pages = {383--397},
	file = {billionnet 2015 - designing robust nature reserves under uncertain survival probabilities - GUPPY - BDPG - ERROR MODELS - ANNO.pdf:/Users/bill/D/Zotero/storage/EK4IVBKR/billionnet 2015 - designing robust nature reserves under uncertain survival probabilities - GUPPY - BDPG - ERROR MODELS - ANNO.pdf:application/pdf},
}

@article{billionnet2017ema,
	title = {How to {Take} into {Account} {Uncertainty} in {Species} {Extinction} {Probabilities} for {Phylogenetic} {Conservation} {Prioritization}},
	volume = {22},
	issn = {1420-2026, 1573-2967},
	url = {http://link.springer.com/10.1007/s10666-017-9561-7},
	doi = {10.1007/s10666-017-9561-7},
	abstract = {In this article, we are concerned with the general problem of choosing from a set of taxa T a subset S to protect in order to try to contribute to halting biodiversity loss as efficiently as possible given limited resources. The protection of a taxon decreases its extinction probability, and the impact of protecting the taxa of S is measured by the resulting expected phylogenetic diversity (ePD) of the set T. The primary challenge posed by this approach lies in determining the extinction probability of a taxon (protected or unprotected). To deal with this difficulty, the uncertainty about the extinction probabilities can be described through a set of possible scenarios, each corresponding to different extinction probability values for each taxon. We show how to determine an Boptimal robust set{\textasciicircum} of taxa to protect, defined as the set of taxa that minimizes the maximum Bregret,{\textasciicircum} i.e., the maximum relative gap, over all the scenarios, between (1) the ePD of T obtained by protecting the taxa of this set and (2) the ePD of T which would be produced by protecting the subset of taxa optimal for the considered scenario. In our experimental conditions covering 100 cases, this gap is almost always less than 1\%. Consequently, the ePD of T obtained by protecting the taxa of the optimal robust set is not far from the maximum ePD of T that could have been obtained if we had known the true scenario. In other words, a way of escaping (in large part, at least) from the uncertainty related to the extinction probabilities of the taxa consists of choosing to protect those belonging to the optimal robust set. We also compare the optimal robust set to other relevant subsets of T.},
	language = {en},
	number = {6},
	urldate = {2021-11-29},
	journal = {Environmental Modeling \& Assessment},
	author = {Billionnet, Alain},
	month = dec,
	year = {2017},
	pages = {535--548},
	file = {billionnet 2017 - how to take into account uncertinty in species extinction probabilities for phylogenetic conservation prioritization.pdf:/Users/bill/D/Zotero/storage/AY3B6P9C/billionnet 2017 - how to take into account uncertinty in species extinction probabilities for phylogenetic conservation prioritization.pdf:application/pdf},
}

@article{billionnet2018bc,
	title = {Quantifying extinction probabilities of endangered species for phylogenetic conservation prioritization may not be as sensitive as might be feared},
	volume = {27},
	issn = {0960-3115, 1572-9710},
	url = {http://link.springer.com/10.1007/s10531-017-1487-5},
	doi = {10.1007/s10531-017-1487-5},
	abstract = {In this study we are concerned with the general problem of choosing from a set of endangered species T a subset S of k species to protect as a priority. Here, the interest to protect the species of S is assessed by the resulting expected phylogenetic diversity (ePD) of the set T, a widely used criterion for measuring the expected amount of evolutionary history associated with T. We consider that the survival of the protected species is assured and, on the contrary, that there is a risk of extinction for the unprotected species. The problem is easy to solve by a greedy type method if the extinction probabilities of the unprotected species are known but these probabilities are generally not easy to quantify. We show in this note that the choice of the precise values attributed to the extinction probabilities—provided it respects the rank of imperilment of each species—is not as decisive as might be feared for the considered problem. The values of these probabilities have a clear impact on the selection of the species to be protected but a little impact on the resulting ePD. More precisely, if T1 and T2 are the two optimal subsets of species corresponding to two scenarios (two different sets of probabilities) the ePDs of T1 and T2, calculated with the probabilities of the first scenario—or with the probabilities of the second scenario—are not very different.},
	language = {en},
	number = {5},
	urldate = {2021-11-29},
	journal = {Biodiversity and Conservation},
	author = {Billionnet, Alain},
	month = apr,
	year = {2018},
	pages = {1189--1200},
	file = {billionnet 2018 - Quantifying extinction probabilities of endangered species for phylogenetic conservation prioritization may not be as sensitive as might be feared - BDPG.pdf:/Users/bill/D/Zotero/storage/DHZYKREW/billionnet 2018 - Quantifying extinction probabilities of endangered species for phylogenetic conservation prioritization may not be as sensitive as might be feared - BDPG.pdf:application/pdf},
}

@article{billionnet2016ema,
	title = {Designing {Connected} and {Compact} {Nature} {Reserves}},
	volume = {21},
	issn = {1420-2026, 1573-2967},
	url = {http://link.springer.com/10.1007/s10666-015-9465-3},
	doi = {10.1007/s10666-015-9465-3},
	abstract = {It is generally accepted that for many species, the ability to get around a reserve promotes their longterm persistence. Here, we measure the ease with which species can move by two spatial criteria: (i) the connectivity of the reserve, that is to say, the possibility to go through the whole reserve without leaving it, and (ii) the compactness of the reserve, that is to say, the remoteness of the sites in relation to each other, the distance between two sites being measured by the shortest distance to travel to get from one site to another without leaving the reserve. To protect the reserve of external disturbances, we also impose a connectivity constraint for the area outside the reserve. This article presents a method based on integer linear programming to define connected and compact reserves. Computational experiments carried out on artificial instances with 400 sites and 100 species are presented to illustrate the effectiveness of the approach.},
	language = {en},
	number = {2},
	urldate = {2021-11-29},
	journal = {Environmental Modeling \& Assessment},
	author = {Billionnet, Alain},
	month = apr,
	year = {2016},
	pages = {211--219},
	file = {billionnet 2016 - designing connected and compact nature reserves.pdf:/Users/bill/D/Zotero/storage/XTQWPT9U/billionnet 2016 - designing connected and compact nature reserves.pdf:application/pdf},
}

@article{zotero-1814,
}

@article{dietterich2017a,
	title = {Steps {Toward} {Robust} {Artificial} {Intelligence}},
	volume = {38},
	issn = {2371-9621, 0738-4602},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2756},
	doi = {10.1609/aimag.v38i3.2756},
	abstract = {Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world},
	language = {en},
	number = {3},
	urldate = {2021-12-22},
	journal = {AI Magazine},
	author = {Dietterich, Thomas G.},
	month = oct,
	year = {2017},
	pages = {3--24},
	file = {dietterich 2017 - Steps Toward Robust Artificial Intelligence - UNCERTAINTY - AI - ML - ROBUSTNESS - OVERFITTING - REGULARIZATION.pdf:/Users/bill/D/Zotero/storage/5W6C2W4S/dietterich 2017 - Steps Toward Robust Artificial Intelligence - UNCERTAINTY - AI - ML - ROBUSTNESS - OVERFITTING - REGULARIZATION.pdf:application/pdf},
}

@article{zhou2021acm,
	title = {Hiding solutions in model {RB}: {Forced} instances are almost as hard as unforced ones},
	shorttitle = {Hiding solutions in model {RB}},
	url = {http://arxiv.org/abs/2103.06649},
	abstract = {In this paper we study the forced instance spaces of model RB, where one or two arbitrary satisfying assignments have been imposed. We prove rigorously that the expected number of solutions of forced RB instances is asymptotically the same with those of unforced ones. Moreover, the distribution of forced RB instances in the corresponding forced instance space is asymptotically the same with that of unforced RB instances in the unforced instance space. These results imply that the hidden assignments will not lead to easily solvable formulas, and the hardness of solving forced RB instances will be the same with unforced RB instances.},
	language = {en},
	urldate = {2022-02-11},
	journal = {arXiv:2103.06649 [cs, math]},
	author = {Zhou, Guangyan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06649},
	keywords = {Mathematics - Combinatorics, Computer Science - Computational Complexity, 60C05, 68Q87, 68T20},
	annote = {Comment: 27 pages},
	annote = {She has a video of giving the paper at the “Random graphs and statistical inference: New methods and applications” online conference.
https://www.birs.ca/events/2021/5-day-workshops/21w5108/videos/watch/202108120606-Zhou.html
},
	file = {zhou 2021 - Hiding solutions in model RB - Forced instances are almost as hard as unforced ones - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/WCB43232/zhou 2021 - Hiding solutions in model RB - Forced instances are almost as hard as unforced ones - BDPG - ANNO.pdf:application/pdf},
}

@book{james2021,
	address = {New York, NY},
	edition = {Second Edition},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning} with {Applications} in {R}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2022-02-11},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	doi = {10.1007/978-1-4614-7138-7},
	file = {james witten hastie tibshirani 2013-2017 8th printing - An Introduction to Statistical Learning with Applications in R - BOOK.pdf:/Users/bill/D/Zotero/storage/2SM2PB6M/james witten hastie tibshirani 2013-2017 8th printing - An Introduction to Statistical Learning with Applications in R - BOOK.pdf:application/pdf},
}

@book{efron2016,
	title = {Computer {Age} {Statistical} {Inference}},
	language = {en},
	author = {Efron, Bradley and Hastie, Trevor},
	year = {2016},
	file = {efron hastie - computer age statistical inference.pdf:/Users/bill/D/Zotero/storage/8CN688ID/efron hastie - computer age statistical inference.pdf:application/pdf},
}

@book{hastie2015,
	series = {Monographs on {Statistics} and {Applied} {Probability}},
	title = {Statistical learning with sparsity - the lasso and generalizations},
	isbn = {978-1-4987-1216-3},
	url = {https://www.routledge.com/Statistical-Learning-with-Sparsity-The-Lasso-and-Generalizations/Hastie-Tibshirani-Wainwright/p/book/9781498712163},
	abstract = {In this monograph, we have attempted to summarize the actively developing field of statistical learning with sparsity. A sparse statistical model is one having only a small number of nonzero parameters or weights. It represents a classic case of “less is more”: a sparse model can be much easier to estimate and interpret than a dense model. In this age of big data, the number of features measured on a person or object can be large, and might be larger than the number of observations. The sparsity assumption allows us to tackle such problems and extract useful and reproducible patterns from big datasets.},
	language = {en},
	number = {143},
	publisher = {CRC Press},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
	file = {hastie tibshirani wainwright - statistical learning with sparsity - the lasso and generalizations.pdf:/Users/bill/D/Zotero/storage/MTUCSCUM/hastie tibshirani wainwright - statistical learning with sparsity - the lasso and generalizations.pdf:application/pdf},
}

@book{hastie2009,
	edition = {Second},
	series = {Springer {Series} in {Statistics}},
	title = {The elements of statistical learning},
	url = {https://hastie.su.domains/ElemStatLearn/},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	file = {hastie et al 2009 - the elements of statistical learning - 2nd edition.pdf:/Users/bill/D/Zotero/storage/GLIQAQPG/hastie et al 2009 - the elements of statistical learning - 2nd edition.pdf:application/pdf},
}

@misc{torquette2022,
	title = {Characterizing instance hardness in classification and regression problems},
	url = {http://arxiv.org/abs/2212.01897},
	abstract = {Some recent pieces of work in the Machine Learning (ML) literature have demonstrated the usefulness of assessing which observations are hardest to have their label predicted accurately. By identifying such instances, one may inspect whether they have any quality issues that should be addressed. Learning strategies based on the diﬃculty level of the observations can also be devised. This paper presents a set of meta-features that aim at characterizing which instances of a dataset are hardest to have their label predicted accurately and why they are so, aka instance hardness measures. Both classiﬁcation and regression problems are considered. Synthetic datasets with diﬀerent levels of complexity are built and analyzed. A Python package containing all implementations is also provided.},
	language = {en},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Torquette, Gustavo P. and Nunes, Victor S. and Paiva, Pedro Y. A. and Neto, Lourenço B. C. and Lorena, Ana C.},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01897 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {torquette ... lorena 2022 - Characterizing instance hardness in classification and regression problems.pdf:/Users/bill/D/Zotero/storage/GMRIJV2D/torquette ... lorena 2022 - Characterizing instance hardness in classification and regression problems.pdf:application/pdf},
}

@article{luengo2015kis,
	title = {An automatic extraction method of the domains of competence for learning classifiers using data complexity measures},
	volume = {42},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-013-0700-4},
	doi = {10.1007/s10115-013-0700-4},
	abstract = {The constant appearance of algorithms and problems in data mining makes impossible to know in advance whether the model will perform well or poorly until it is applied, which can be costly. It would be useful to have a procedure that indicates, prior to the application of the learning algorithm and without needing a comparison with other methods, whether the outcome will be good or bad using the information available in the data. In this work, we present an automatic extraction method to determine the domains of competence of a classiﬁer using a set of data complexity measures proposed for the task of classiﬁcation. These domains codify the characteristics of the problems that are suitable or not for it, relating the concepts of data geometrical structures that may be difﬁcult and the ﬁnal accuracy obtained by any classiﬁer. In order to do so, this proposal uses 12 metrics of data complexity acting over a large benchmark of datasets in order to analyze the behavior patterns of the method, obtaining intervals of data complexity measures with good or bad performance. As a representative for classiﬁers to analyze the proposal, three classical but different algorithms are used: C4.5, SVM and K-NN. From these intervals, two simple rules that describe the good or bad behaviors of the classiﬁers mentioned each are obtained, allowing the user to characterize the response quality of the methods from a dataset’s complexity. These two rules have been validated using fresh problems, showing that they are general and accurate. Thus, it can be established when the classiﬁer will perform well or poorly prior to its application.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {Knowledge and Information Systems},
	author = {Luengo, Julián and Herrera, Francisco},
	month = jan,
	year = {2015},
	pages = {147--180},
	file = {luengo herrera 2013 - An automatic extraction method of the domains of competence for learning classifiers using data complexity measures - PROBLEM DIFFICULTY - DATA COMPLEXITY - BDPG.pdf:/Users/bill/D/Zotero/storage/XEN7XNGQ/luengo herrera 2013 - An automatic extraction method of the domains of competence for learning classifiers using data complexity measures - PROBLEM DIFFICULTY - DATA COMPLEXITY - BDPG.pdf:application/pdf},
}

@book{basu2006,
	address = {London},
	series = {Advanced information and knowledge processing},
	title = {Data complexity in pattern recognition},
	isbn = {978-1-84628-171-6 978-1-84628-172-3},
	language = {en},
	publisher = {Springer},
	editor = {Basu, Mitra and Ho, Tin Kam},
	year = {2006},
	note = {OCLC: ocm62761587},
	keywords = {Classification, Computational complexity, Pattern perception, Pattern recognition systems},
	file = {basu ho 2006 - Data Complexity in Pattern Recognition - BDPG - PROBLEM DIFFICULTY - DATA COMPLEXITY - EDITED BOOK.pdf:/Users/bill/D/Zotero/storage/JCCMWDYG/basu ho 2006 - Data Complexity in Pattern Recognition - BDPG - PROBLEM DIFFICULTY - DATA COMPLEXITY - EDITED BOOK.pdf:application/pdf},
}

@article{garcia-callejas2016em,
	title = {The effects of model and data complexity on predictions from species distributions models},
	volume = {326},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380015002513},
	doi = {10.1016/j.ecolmodel.2015.06.002},
	language = {en},
	urldate = {2023-01-25},
	journal = {Ecological Modelling},
	author = {García-Callejas, David and Araújo, Miguel B.},
	month = apr,
	year = {2016},
	pages = {4--12},
	file = {Garcia-Callejas Araujo 2015 - The effects of model and data complexity on predictions from species distributions models - SDMs - BDPG - GUPPY - PROBLEM DIFFICULTY - VIRTUAL SPECIES - ANNO.pdf:/Users/bill/D/Zotero/storage/RF84U5G9/Garcia-Callejas Araujo 2015 - The effects of model and data complexity on predictions from species distributions models - SDMs - BDPG - GUPPY - PROBLEM DIFFICULTY - VIRTUAL SPECIES - ANNO.pdf:application/pdf},
}

@article{alagador2022joem,
	title = {Operations research applicability in spatial conservation planning},
	volume = {315},
	issn = {03014797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301479722007459},
	doi = {10.1016/j.jenvman.2022.115172},
	abstract = {A large fraction of the current environmental crisis derives from the large rates of human-driven biodiversity loss. Biodiversity conservation questions human practices towards biodiversity and, therefore, largely conflicts with ordinary societal aspirations. Decisions on the location of protected areas, one of the most convincing conser­ vation tools, reflect such a competitive endeavor. Operations Research (OR) brings a set of analytical models and tools capable of resolving the conflicting interests between ecology and economy. Recent technological advances have boosted the size and variety of data available to planners, thus challenging conventional approaches bounded on optimized solutions. New models and methods are needed to use such a massive amount of data in integrative schemes addressing a large variety of concerns. This study provides an overview on the past, present and future challenges that characterize spatial conservation models supported by OR. We discuss the progress of OR models and methods in spatial conservation planning and how those models may be optimized through sophisticated algorithms and computational tools. Moreover, we anticipate possible panoramas of modern spatial conservation studies supported by OR and we explore possible avenues for the design of optimized interdisci­ plinary collaborative platforms in the era of Big Data, through consortia where distinct players with different motivations and services meet. By enlarging the spatial, temporal, taxonomic and societal horizons of biodi­ versity conservation, planners navigate around multiple socioecological/environmental equilibria and are able to decide on cost-effective strategies to improve biodiversity persistence under complex environments.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Journal of Environmental Management},
	author = {Alagador, Diogo and Cerdeira, Jorge Orestes},
	month = aug,
	year = {2022},
	pages = {115172},
	file = {alagador cerdeira 2022 - operations research appicability in spatial conservation planning - BDPG.pdf:/Users/bill/D/Zotero/storage/UBHBNDND/alagador cerdeira 2022 - operations research appicability in spatial conservation planning - BDPG.pdf:application/pdf},
}

@article{chardon2022dad,
	title = {High resolution species distribution and abundance models cannot predict separate shrub datasets in adjacent {Arctic} fjords},
	volume = {28},
	issn = {1366-9516, 1472-4642},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ddi.13498},
	doi = {10.1111/ddi.13498},
	abstract = {Aim: Improving species distribution models (SDMs) and species abundance models (SAMs) of woody shrubs is critical for predicting biodiversity changes in the Arctic, which is experiencing especially high warming rates. Yet, it remains relatively unexplored if SDMs and SAMs can explain local scale patterns. We aim to identify predictor differences for the distribution versus abundance of two widespread Arctic shrub species with high resolution models and to compare validation approaches to assess the models’ predictive abilities.},
	language = {en},
	number = {5},
	urldate = {2023-01-25},
	journal = {Diversity and Distributions},
	author = {Chardon, Nathalie Isabelle and Nabe‐Nielsen, Jacob and Assmann, Jakob Johan and Dyrholm Jacobsen, Ida Bomholt and Guéguen, Maya and Normand, Signe and Wipf, Sonja},
	editor = {Jarvis, Susan},
	month = may,
	year = {2022},
	pages = {956--975},
	file = {chardon et al 2022 - High resolution species distribution and abundance models cannot predict separate shrub datasets in adjacent Arctic fjords - SDMs - BDPG - EVALUATION - GUPPY - ANNO.pdf:/Users/bill/D/Zotero/storage/77JESCRV/chardon et al 2022 - High resolution species distribution and abundance models cannot predict separate shrub datasets in adjacent Arctic fjords - SDMs - BDPG - EVALUATION - GUPPY - ANNO.pdf:application/pdf},
}

@inproceedings{beery2021asccssc,
	address = {Virtual Event Australia},
	title = {Species {Distribution} {Modeling} for {Machine} {Learning} {Practitioners}: {A} {Review}},
	isbn = {978-1-4503-8453-7},
	shorttitle = {Species {Distribution} {Modeling} for {Machine} {Learning} {Practitioners}},
	url = {https://dl.acm.org/doi/10.1145/3460112.3471966},
	doi = {10.1145/3460112.3471966},
	language = {en},
	urldate = {2023-01-25},
	booktitle = {{ACM} {SIGCAS} {Conference} on {Computing} and {Sustainable} {Societies} ({COMPASS})},
	publisher = {ACM},
	author = {Beery, Sara and Cole, Elijah and Parker, Joseph and Perona, Pietro and Winner, Kevin},
	month = jun,
	year = {2021},
	pages = {329--348},
	file = {beery et al 2021 - Species Distribution Modeling for Machine Learning Practitioners - A Review - BDPG - GUPPY - SDMs - ANNO.pdf:/Users/bill/D/Zotero/storage/DEY6ZZEM/beery et al 2021 - Species Distribution Modeling for Machine Learning Practitioners - A Review - BDPG - GUPPY - SDMs - ANNO.pdf:application/pdf},
}

@article{elith2020bi,
	title = {Presence-only and {Presence}-absence {Data} for {Comparing} {Species} {Distribution} {Modeling} {Methods}},
	volume = {15},
	issn = {1546-9735},
	url = {https://journals.ku.edu/jbi/article/view/13384},
	doi = {10.17161/bi.v15i2.13384},
	abstract = {Species distribution models (SDMs) are widely used to predict and study distributions of species. Many different modeling methods and associated algorithms are used and continue to emerge. It is important to understand how different approaches perform, particularly when applied to species occurrence records that were not gathered in structured surveys (e.g. opportunistic records). This need motivated a large-scale, collaborative effort, published in 2006, that aimed to create objective comparisons of algorithm performance. As a benchmark, and to facilitate future comparisons of approaches, here we publish that dataset: point location records for 226 anonymised species from six regions of the world, with accompanying predictor variables in raster (grid) and point formats. A particularly interesting characteristic of this dataset is that independent presence-absence survey data are available for evaluation alongside the presence-only species occurrence data intended for modeling. The dataset is available on Open Science Framework and as an R package and can be used as a benchmark for modeling approaches and for testing new ways to evaluate the accuracy of SDMs.},
	language = {en},
	number = {2},
	urldate = {2023-01-25},
	journal = {Biodiversity Informatics},
	author = {Elith, Jane and Graham, Catherine and Valavi, Roozbeh and Abegg, Meinrad and Bruce, Caroline and Ford, Andrew and Guisan, Antoine and Hijmans, Robert J. and Huettmann, Falk and Lohmann, Lucia and Loiselle, Bette and Moritz, Craig and Overton, Jake and Peterson, A. Townsend and Phillips, Steven and Richardson, Karen and Williams, Stephen and Wiser, Susan K. and Wohlgemuth, Thomas and Zimmermann, Niklaus E.},
	month = jul,
	year = {2020},
	pages = {69--80},
	file = {elith et al 2020 - presence-only and presence-absence data for comparing species distribution modeling methods - BDPG - GUPPY - SDMs - BENCHMARKING - ANNO.pdf:/Users/bill/D/Zotero/storage/6W8ZN3QQ/elith et al 2020 - presence-only and presence-absence data for comparing species distribution modeling methods - BDPG - GUPPY - SDMs - BENCHMARKING - ANNO.pdf:application/pdf},
}

@article{valavi2022em,
	title = {Predictive performance of presence‐only species distribution models: a benchmark study with reproducible code},
	volume = {92},
	issn = {0012-9615, 1557-7015},
	shorttitle = {Predictive performance of presence‐only species distribution models},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ecm.1486},
	doi = {10.1002/ecm.1486},
	abstract = {Species distribution modeling (SDM) is widely used in ecology and conservation. Currently, the most available data for SDM are species presence-only records (available through digital databases). There have been many studies comparing the performance of alternative algorithms for modeling presence-only data. Among these, a 2006 paper from Elith and colleagues has been particularly influential in the field, partly because they used several novel methods (at the time) on a global data set that included independent presence–absence records for model evaluation. Since its publication, some of the algorithms have been further developed and new ones have emerged. In this paper, we explore patterns in predictive performance across methods, by reanalyzing the same data set (225 species from six different regions) using updated modeling knowledge and practices. We apply well-established methods such as generalized additive models and MaxEnt, alongside others that have received attention more recently, including regularized regressions, point-process weighted regressions, random forests, XGBoost, support vector machines, and the ensemble modeling framework biomod. All the methods we use include background samples (a sample of environments in the landscape) for model fitting. We explore impacts of using weights on the presence and background points in model fitting. We introduce new ways of evaluating models fitted to these data, using the area under the precision-recall gain curve, and focusing on the rank of results. We find that the way models are fitted matters. The top method was an ensemble of tuned individual models. In contrast, ensembles built using the biomod framework with default parameters performed no better than single moderate performing models. Similarly, the second top performing method was a random forest parameterized to deal with many background samples (contrasted to relatively few presence records), which substantially outperformed other random forest implementations. We find that, in general, nonparametric techniques with the capability of controlling for model complexity outperformed traditional regression methods, with MaxEnt and boosted regression trees still among the top performing models. All the data and code with working examples are provided to make this study fully reproducible.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {Ecological Monographs},
	author = {Valavi, Roozbeh and Guillera‐Arroita, Gurutzeta and Lahoz‐Monfort, José J. and Elith, Jane},
	month = feb,
	year = {2022},
	file = {Valavi guillera-arroita lahoz-monfort elith 2022 - Predictive performance of presence‐only species distribution models a benchmark - SDMs - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/IKI5HAW8/Valavi guillera-arroita lahoz-monfort elith 2022 - Predictive performance of presence‐only species distribution models a benchmark - SDMs - BDPG - ANNO.pdf:application/pdf},
}

@article{young2010jss,
	title = {\textbf{tolerance} : {An} \textit{{R}} {Package} for {Estimating} {Tolerance} {Intervals}},
	volume = {36},
	issn = {1548-7660},
	shorttitle = {\textbf{tolerance}},
	url = {http://www.jstatsoft.org/v36/i05/},
	doi = {10.18637/jss.v036.i05},
	abstract = {The tolerance package for R provides a set of functions for estimating and plotting tolerance limits. This package provides a wide-range of functions for estimating discrete and continuous tolerance intervals as well as for estimating regression tolerance intervals. An additional tool of the tolerance package is the plotting capability for the univariate and regression settings as well as for the multivariate normal setting. The tolerance package’s capabilities are illustrated using simulated data sets. Formulas used for the estimation procedures are also presented.},
	language = {en},
	number = {5},
	urldate = {2023-01-25},
	journal = {Journal of Statistical Software},
	author = {Young, Derek S.},
	year = {2010},
	file = {young 2010 - tolerance - an R package for estimating tolerance intervals - BDPG - TOLERANCE INTERVALS.pdf:/Users/bill/D/Zotero/storage/S3PPZPB6/young 2010 - tolerance - an R package for estimating tolerance intervals - BDPG - TOLERANCE INTERVALS.pdf:application/pdf},
}

@article{flouri2017bj,
	title = {Tolerance limits and tolerance intervals for ratios of normal random variables using a bootstrap calibration},
	volume = {59},
	issn = {0323-3847, 1521-4036},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/bimj.201600117},
	doi = {10.1002/bimj.201600117},
	language = {en},
	number = {3},
	urldate = {2023-01-25},
	journal = {Biometrical Journal},
	author = {Flouri, Marilena and Zhai, Shuyan and Mathew, Thomas and Bebu, Ionut},
	month = may,
	year = {2017},
	pages = {550--566},
	file = {flouri et al 2010 - tolerance limits and tolerance intervals for ratios of normal random variabels using a bootstrap calculation - BDPG - TOLERANCE INTERVALS.pdf:/Users/bill/D/Zotero/storage/CQUS973Q/flouri et al 2010 - tolerance limits and tolerance intervals for ratios of normal random variabels using a bootstrap calculation - BDPG - TOLERANCE INTERVALS.pdf:application/pdf},
}

@article{macia2013pr,
	title = {Learner excellence biased by data set selection: {A} case for data characterisation and artificial data sets},
	volume = {46},
	issn = {00313203},
	shorttitle = {Learner excellence biased by data set selection},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320312004281},
	doi = {10.1016/j.patcog.2012.09.022},
	abstract = {The excellence of a given learner is usually claimed through a performance comparison with other learners over a collection of data sets. Too often, researchers are not aware of the impact of their data selection on the results. Their test beds are small, and the selection of the data sets is not supported by any previous data analysis. Conclusions drawn on such test beds cannot be generalised, because particular data characteristics may favour certain learners unnoticeably. This work raises these issues and proposes the characterisation of data sets using complexity measures, which can be helpful for both guiding experimental design and explaining the behaviour of learners.},
	language = {en},
	number = {3},
	urldate = {2023-02-09},
	journal = {Pattern Recognition},
	author = {Macià, Núria and Bernadó-Mansilla, Ester and Orriols-Puig, Albert and Kam Ho, Tin},
	month = mar,
	year = {2013},
	pages = {1054--1066},
	file = {macia et al 2013 - Learner excellence biased by data set selection- A case for data characterisation and artificial data sets - BENCHMARK - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/2GL9RN25/macia et al 2013 - Learner excellence biased by data set selection- A case for data characterisation and artificial data sets - BENCHMARK - BDPG - ANNO.pdf:application/pdf},
}

@article{peterson2022bi,
	title = {{ENM2020}: {A} {Free} {Online} {Course} and {Set} of {Resources} on {Modeling} {Species}' {Niches} and {Distributions}},
	volume = {17},
	issn = {1546-9735},
	shorttitle = {{ENM2020}},
	url = {https://journals.ku.edu/jbi/article/view/15016},
	doi = {10.17161/bi.v17i.15016},
	abstract = {The field of distributional ecology has seen considerable recent attention, particularly surrounding the theory, protocols, and tools for Ecological Niche Modeling (ENM) or Species Distribution Modeling (SDM). Such analyses have grown steadily over the past two decades—including a maturation of relevant theory and key concepts—but methodological consensus has yet to be reached. In response, and following an online course taught in Spanish in 2018, we designed a comprehensive English-language course covering much of the underlying theory and methods currently applied in this broad field. Here, we summarize that course, ENM2020, and provide links by which resources produced for it can be accessed into the future. ENM2020 lasted 43 weeks, with presentations from 52 instructors, who engaged with {\textgreater}2500 participants globally through {\textgreater}14,000 hours of viewing and {\textgreater}90,000 views of instructional video and question-and-answer sessions. Each major topic was introduced by an “Overview” talk, followed by more detailed lectures on subtopics. The hierarchical and modular format of the course permits updates, corrections, or alternative viewpoints, and generally facilitates revision and reuse, including the use of only the Overview lectures for introductory courses. All course materials are free and openly accessible (CC-BY license) to ensure these resources remain available to all interested in distributional ecology.},
	language = {en},
	urldate = {2023-09-25},
	journal = {Biodiversity Informatics},
	author = {Peterson, A. Townsend and Aiello-Lammens, Matthew and Amatulli, Giuseppe and Anderson, Robert and Cobos, Marlon and Diniz-Filho, José Alexandre and Escobar, Luis and Feng, Xiao and Franklin, Janet and Gadelha, Luiz and Georges, Damien and Guéguen, M and Gueta, Tomer and Ingenloff, Kate and Jarvie, Scott and Jiménez, Laura and Karger, Dirk and Kass, Jamie and Kearney, Michael and Loyola, Rafael and Machado-Stredel, Fernando and Martínez-Meyer, Enrique and Merow, Cory and Mondelli, Maria Luiza and Mortara, Sara and Muscarella, Robert and Myers, Corinne and Naimi, Babak and Noesgaard, Daniel and Ondo, Ian and Osorio-Olvera, Luis and Owens, Hannah and Pearson, Richard and Pinilla-Buitrago, Gonzalo and Sánchez-Tapia, Andrea and Saupe, Erin and Thuiller, Wilfried and Varela, Sara and Warren, Dan and Wieczorek, John and Yates, Katherine and Zhu, Gengping and Zuquim, Gabriela and Zurell, Damaris},
	month = mar,
	year = {2022},
	file = {peterson et al 2022 - enm2020 - a free online course and set of resources on modeling species niches and distributions - SDMs.pdf:/Users/bill/D/Zotero/storage/TYWZTWDQ/peterson et al 2022 - enm2020 - a free online course and set of resources on modeling species niches and distributions - SDMs.pdf:application/pdf},
}

@article{canessa2015mee,
	title = {When do we need more data? {A} primer on calculating the value of information for applied ecologists},
	volume = {6},
	issn = {2041-210X, 2041-210X},
	shorttitle = {When do we need more data?},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12423},
	doi = {10.1111/2041-210X.12423},
	abstract = {Summary
            
              
                
                  Applied ecologists continually advocate further research, under the assumption that obtaining more information will lead to better decisions. Value of information (VoI) analysis can be used to quantify how additional information may improve management outcomes: despite its potential, this method is still underused in environmental decision‐making. We provide a primer on how to calculate the VoI and assess whether reducing uncertainty will change a decision. Our aim is to facilitate the application of VoI by managers who are not familiar with decision‐analytic principles and notation, by increasing the technical accessibility of the tool.
                
                
                  
                    Calculating the VoI requires explicit formulation of management objectives and actions. Uncertainty must be clearly structured and its effects on management outcomes evaluated. We present two measures of the VoI. The expected value of
                    perfect
                    information is a calculation of the expected improvement in management outcomes that would result from access to perfect knowledge. The expected value of
                    sample
                    information calculates the improvement in outcomes expected by collecting a given sample of new data.
                  
                
                
                  We guide readers through the calculation of VoI using two case studies: (i) testing for disease when managing a frog species and (ii) learning about demographic rates for the reintroduction of an endangered turtle. We illustrate the use of Bayesian updating to incorporate new information.
                
                
                  The VoI depends on our current knowledge, the quality of the information collected and the expected outcomes of the available management actions. Collecting information can require significant investments of resources; VoI analysis assists managers in deciding whether these investments are justified.},
	language = {en},
	number = {10},
	urldate = {2023-09-29},
	journal = {Methods in Ecology and Evolution},
	author = {Canessa, Stefano and Guillera‐Arroita, Gurutzeta and Lahoz‐Monfort, José J. and Southwell, Darren M. and Armstrong, Doug P. and Chadès, Iadine and Lacy, Robert C. and Converse, Sarah J.},
	editor = {Gimenez, Olivier},
	month = oct,
	year = {2015},
	pages = {1219--1228},
	file = {canessa et al 2015 - when do we need more data - a primer on calculating the value of information for applied ecologists - VOI - VALUE OF INFORMATION.pdf:/Users/bill/D/Zotero/storage/FYR6QP6X/canessa et al 2015 - when do we need more data - a primer on calculating the value of information for applied ecologists - VOI - VALUE OF INFORMATION.pdf:application/pdf},
}

@article{regin,
	title = {Integration of {AI} and {OR} {Techniques} in {Constraint} {Programming} for {Combinatorial} {Optimization} {Problems} : {First} {International} {Conference}, {CPAIOR} 2004, {Nice}, {France}, {April} 20-22, 2004 : {Proceedings}},
	language = {en},
	author = {Régin, Jean-Charles and Rueher, Michel},
	file = {hebrard hnich walsh 2004 - Super solutions in constraint programming - IN Proceedings of the 1st International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) ... 2004, - BDPG - ROBUST - CSP.pdf:/Users/bill/D/Zotero/storage/UUSYIJIZ/hebrard hnich walsh 2004 - Super solutions in constraint programming - IN Proceedings of the 1st International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) ... 2004, .pdf:application/pdf},
}

@phdthesis{hebrard2007,
	title = {Robust solutions for constraint satisfaction and optimisation under uncertainty.},
	url = {http://hdl.handle.net/1959.4/40765},
	abstract = {We develop a framework for ﬁnding robust solutions of constraint programs. Our approach is based on the notion of fault tolerance. We formalise this concept within constraint programming, extend it in several dimensions and introduce some algorithms to ﬁnd robust solutions eﬃciently.},
	language = {en},
	urldate = {2023-10-11},
	school = {UNSW Sydney},
	author = {Hebrard, Emmanuel},
	collaborator = {{UNSW Sydney}},
	year = {2007},
	doi = {10.26190/UNSWORKS/16444},
	keywords = {Constraint programming., Fault-tolerance computing., Logic programming.},
	file = {hebrard 2007 - robust solutions for constraint satisfaction and optimisation under uncertainty - BDPG.pdf:/Users/bill/D/Zotero/storage/LBPVMX7I/hebrard 2007 - robust solutions for constraint satisfaction and optimisation under uncertainty - BDPG.pdf:application/pdf},
}

@article{zhou2022fcs,
	title = {Super solutions of the model {RB}},
	volume = {16},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-021-1189-8},
	doi = {10.1007/s11704-021-1189-8},
	language = {en},
	number = {6},
	urldate = {2023-10-11},
	journal = {Frontiers of Computer Science},
	author = {Zhou, Guangyan and Xu, Wei},
	month = dec,
	year = {2022},
	pages = {166406},
	file = {zhou xu 2022 - Super solutions of the model RB - BDPG.pdf:/Users/bill/D/Zotero/storage/NXY8XER8/zhou xu 2022 - Super solutions of the model RB - BDPG.pdf:application/pdf},
}

@article{sarkar2012ahes,
	title = {Complementarity and the selection of nature reserves: algorithms and the origins of conservation planning, 1980–1995},
	volume = {66},
	issn = {0003-9519, 1432-0657},
	shorttitle = {Complementarity and the selection of nature reserves},
	url = {http://link.springer.com/10.1007/s00407-012-0097-6},
	doi = {10.1007/s00407-012-0097-6},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Archive for History of Exact Sciences},
	author = {Sarkar, Sahotra},
	month = jul,
	year = {2012},
	pages = {397--426},
	file = {sarkar 2012 - Complementarity and the selection of nature reserves -  algorithms and the origins of conservation planning, 1980–1995.pdf:/Users/bill/D/Zotero/storage/JHAZFH3M/sarkar 2012 - Complementarity and the selection of nature reserves -  algorithms and the origins of conservation planning, 1980–1995.pdf:application/pdf},
}

@article{brunel,
	title = {Risk-averse optimisation for the marine reserve site selection: chance constraint by sampling approximation approach},
	abstract = {In response to marine habitats destruction and living population depletion, marine spatial planning (MSP) proposes to regulate uses of the marine environment. Practically, MSP seeks an ocean zoning to meet both ecological and socio-economic objectives eventually aiming at a sustainable development. In particular, an identiﬁed conservation answer to mitigate marine biodiversity erosion consists in the development of a comprehensive network of marine protected areas (MPAs) accompanied with global beneﬁts [1]. The International Union for Conservation of Nature (IUCN) established a 30\% protection target of each marine eco-regions by 2030 reaﬃrming United Nations (UN) commitment of Aichi Target 11. Similarly and more recently, the European Green Deal aims to protect 30\% of European seas. Consequently, conservation science and especially reserve design knows a strong appeal among decision-makers and institutions. Systematic reserve site selection procedures and associated decision support tools (DSTs) are thus a strategic issue.},
	language = {en},
	author = {Brunel, Adrien and Omer, Jérémy and Bertrand, Sophie},
	file = {brunel et al 2022 - Risk-averse optimisation for the marine reserve site selection- chance constraint by sampling approximation approach.pdf:/Users/bill/D/Zotero/storage/DIZ34BXL/brunel et al 2022 - Risk-averse optimisation for the marine reserve site selection- chance constraint by sampling approximation approach.pdf:application/pdf},
}

@article{brunela,
	title = {Uncertainty in reserve site selection : provide a robust solution through risk-averse optimisation},
	abstract = {Marine environment is nowadays frequently seen as tomorrow’s «blue growth» areas . Yet these spaces are already being at the heart of multiple anthropogenic pressures (ﬁshing, aquaculture, shipping routes, recreational activities, renewable energies, etc.). Marine Spatial Planning (MSP) positions itself as a rational decision-making process regulating use of marine spaces and resources in order to reduce tensions between exploitation and ecosystems. Besides, conservation institutions identiﬁed Marine Protected Areas (MPAs) as an essential part of the solution to ensure biodiversity resilience and eventually ecosystem services provision. Indeed, conservation dedicated area are proved to provide biotic communities global beneﬁts [1]. Therefore, in the continuation of United Nations (UN) 10\% target for global ocean protection of the coastal and marine areas in MPAs by 2020, International Union for Conservation of Nature (IUCN) members (governments, non-governmental organisations, agencies) agreed on an even more ambitious protection target of 30\% for each marine eco-region by 2030 against less than 8\% today 1. That is why systematic conservation-based approach such as reserve selection Decision Support Tools (DSTs) (e.g. Marxan) knew a strong appeal among decision makers.},
	language = {en},
	author = {Brunel, Adrien and Omer, Jérémy and Bertrand, Sophie},
	file = {brunel et al 2021 - Uncertainty in reserve site selection - provide a robust solution through risk-averse optimisation.pdf:/Users/bill/D/Zotero/storage/B4HTY6PR/brunel et al 2021 - Uncertainty in reserve site selection - provide a robust solution through risk-averse optimisation.pdf:application/pdf},
}

@article{brunel2022sj,
	title = {Opening the {Black} {Box} of {Decision} {Support} {Tools} in {Marine} {Spatial} {Planning}: {Shedding} {Light} into {Reserve} {Site} {Selection} {Algorithms} for a {Balanced} {Empowerment} of {Stakeholders}},
	issn = {1556-5068},
	shorttitle = {Opening the {Black} {Box} of {Decision} {Support} {Tools} in {Marine} {Spatial} {Planning}},
	url = {https://www.ssrn.com/abstract=4060705},
	doi = {10.2139/ssrn.4060705},
	abstract = {Marine spatial planning (MSP) is positioning itself as a rational decision-making process regulating uses of marine spaces and resources in order to reduce tensions between exploitation and conservation as well as between ocean stakeholders. As global political agendas identiﬁed marine protected areas as a key answer to biodiversity erosion, systematic reserve site selection became a critical component of MSP. Establishing an ocean zoning involves the analysis of large quantities of heterogeneous, multi-sources and spatially explicit data. This often leads to problems too complex to be solved by human intuition only, thus calling for optimisation tools to support the decisions. In that context, our work aims at informing practitioners about stakes, possibilities and limitations of MSP approach through reserve site selection tools. We ﬁrst clarify the reserve site selection framework, especially the underlying mathematics - the problem formulation and the solving method. Then, we highlight potential pitfalls due to input data feeding the reserve-based planning approach. Finally, and more practically, we show to what extent parameters used in reserve selection tools shape the reserve outcome. These elements are explored and illustrated on a real case study, namely the Fernando de Noronha archipelago in the Brazilian tropical Atlantic. This work provides a brief overview of informational challenges brought by decision support tools in marine spatial planning negotiations.},
	language = {en},
	urldate = {2023-10-11},
	journal = {SSRN Electronic Journal},
	author = {Brunel, Adrien and Davret, Juliette and Trouillet, Brice and Bez, Nicolas and Salvetat, Julie and Gicquel, Antoine and Bertrand, Sophie Lanco},
	year = {2022},
	file = {brunel et al - Opening the black box of decision support tools in marine spatial planning- shedding light into reserve site selection algorithms for a balanced empowerment of stakeholders.pdf:/Users/bill/D/Zotero/storage/3V2M56US/brunel et al - Opening the black box of decision support tools in marine spatial planning- shedding light into reserve site selection algorithms for a balanced empowerment of stakeholders.pdf:application/pdf},
}

@book{hooker2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Principles and {Practice} of {Constraint} {Programming}: 24th {International} {Conference}, {CP} 2018, {Lille}, {France}, {August} 27-31, 2018, {Proceedings}},
	volume = {11008},
	isbn = {978-3-319-98333-2 978-3-319-98334-9},
	shorttitle = {Principles and {Practice} of {Constraint} {Programming}},
	url = {http://link.springer.com/10.1007/978-3-319-98334-9},
	language = {en},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	editor = {Hooker, John},
	year = {2018},
	doi = {10.1007/978-3-319-98334-9},
	file = {justeau-alaire et al 2018 - unifying reserve design strategies with graph theory and constraint programming - IN hooker (ed) principles and practice of constraint programming CP 2018 Lille France pp 507-523 - BDPG.pdf:/Users/bill/D/Zotero/storage/CVR3ZBYC/justeau-alaire et al 2018 - unifying reserve design strategies with graph theory and constraint programming - IN hooker (ed) principles and practice of constraint programming CP 2018 Lille France pp 507-523 -.pdf:application/pdf},
}

@article{alagador2022joema,
	title = {Operations research applicability in spatial conservation planning},
	volume = {315},
	issn = {03014797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301479722007459},
	doi = {10.1016/j.jenvman.2022.115172},
	abstract = {A large fraction of the current environmental crisis derives from the large rates of human-driven biodiversity loss. Biodiversity conservation questions human practices towards biodiversity and, therefore, largely conflicts with ordinary societal aspirations. Decisions on the location of protected areas, one of the most convincing conser­ vation tools, reflect such a competitive endeavor. Operations Research (OR) brings a set of analytical models and tools capable of resolving the conflicting interests between ecology and economy. Recent technological advances have boosted the size and variety of data available to planners, thus challenging conventional approaches bounded on optimized solutions. New models and methods are needed to use such a massive amount of data in integrative schemes addressing a large variety of concerns. This study provides an overview on the past, present and future challenges that characterize spatial conservation models supported by OR. We discuss the progress of OR models and methods in spatial conservation planning and how those models may be optimized through sophisticated algorithms and computational tools. Moreover, we anticipate possible panoramas of modern spatial conservation studies supported by OR and we explore possible avenues for the design of optimized interdisci­ plinary collaborative platforms in the era of Big Data, through consortia where distinct players with different motivations and services meet. By enlarging the spatial, temporal, taxonomic and societal horizons of biodi­ versity conservation, planners navigate around multiple socioecological/environmental equilibria and are able to decide on cost-effective strategies to improve biodiversity persistence under complex environments.},
	language = {en},
	urldate = {2023-10-11},
	journal = {Journal of Environmental Management},
	author = {Alagador, Diogo and Cerdeira, Jorge Orestes},
	month = aug,
	year = {2022},
	pages = {115172},
	file = {algador cerdeira 2022 - Operations research applicability in spatial conservation planning.pdf:/Users/bill/D/Zotero/storage/QPYSEQ2D/algador cerdeira 2022 - Operations research applicability in spatial conservation planning.pdf:application/pdf},
}

@article{murray2021tie&e,
	title = {Data {Freshness} in {Ecology} and {Conservation}},
	volume = {36},
	issn = {01695347},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534721000720},
	doi = {10.1016/j.tree.2021.03.005},
	language = {en},
	number = {6},
	urldate = {2023-10-11},
	journal = {Trends in Ecology \& Evolution},
	author = {Murray, Nicholas J. and Kennedy, Emma V. and Álvarez-Romero, Jorge G. and Lyons, Mitchell B.},
	month = jun,
	year = {2021},
	pages = {485--487},
	file = {murray et al 2021 - Data Freshness in Ecology and Conservation.pdf:/Users/bill/D/Zotero/storage/RSZDMBXP/murray et al 2021 - Data Freshness in Ecology and Conservation.pdf:application/pdf},
}

@article{yurek2023e,
	title = {Quantifying uncertainty in coastal salinity regime for biological application using quantile regression},
	volume = {14},
	issn = {2150-8925, 2150-8925},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.4488},
	doi = {10.1002/ecs2.4488},
	abstract = {Abstract
            
              Salinity regimes in coastal ecosystems are highly dynamic and driven by complex geomorphic and hydrological processes. Estuarine biota are generally adapted to salinity fluctuation, but are vulnerable to salinity extremes. Characterizing coastal salinity regime for ecological studies therefore requires representing extremes of salinity ranges at time scales relevant to ecology (e.g., daily, monthly, and seasonally). Here, we propose a framework for modeling coastal salinity with these overall goals: (1) quantify uncertainty in salinity associated with important terrestrial and oceanographic drivers, (2) examine time scales of salinity response to river streamflow events, and (3) predict salinity continuously over space at key time scales. Salinity is modeled as quantile surfaces related to river discharge, tidal dynamics, wind, and spatial location, applied to Suwannee Sound estuary, FL, USA, where salinity has been monitored spatially since 1981. Each quantile level is regressed independently, and together they comprise a distribution of salinity uncertainty across space, with upper and lower quantiles describing salinity extremes. Effects of physical drivers on salinity are compared through four base models with various combinations of tide and wind variables, each including spatial coordinates and a single streamflow metric (in cubic meters per second). Multiple time scales of streamflow are considered by taking means across various periods, from 1 to 12 days, and at various lagged intervals prior to salinity sample, totaling 144 streamflow metrics. We found that the Suwannee coastal salinity regime is dynamic at multiple time scales and varies nonlinearly across space from the river effluence outward. Salinity increases nonlinearly with decreasing river flow rates below 200 m
              3
              /s, most prominently in the lower quantiles of salinity (τ = 0.05–0.25). Wind appears to have a stronger influence on salinity than astronomic tides for this estuary. The regression approach developed here can be applied to any coastal system that has sufficient spatial and temporal monitoring coverage to capture multiple flood and drought events. It is implemented with a simple
              R
              routine, and is less computationally‐intensive than finite difference hydrodynamic modeling. The characterizations of salinity uncertainty developed in these analyses can be directly applied to future studies of fish and wildlife responses to changes in watershed management.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Ecosphere},
	author = {Yurek, Simeon and Allen, Micheal and Eaton, Mitchell J. and Chagaris, David and Reaver, Nathan and Martin, Julien and Frederick, Peter and Dehaven, Mark},
	month = apr,
	year = {2023},
	pages = {e4488},
	file = {yurek et al 2023 - Quantifying uncertainty in coastal salinity regime for biological application using quantile regression.pdf:/Users/bill/D/Zotero/storage/DMX7ASKL/yurek et al 2023 - Quantifying uncertainty in coastal salinity regime for biological application using quantile regression.pdf:application/pdf},
}

@article{raymond2020joae,
	title = {Combining species distribution models and value of information analysis for spatial allocation of conservation resources},
	volume = {57},
	issn = {0021-8901, 1365-2664},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2664.13580},
	doi = {10.1111/1365-2664.13580},
	abstract = {Abstract
            
              
                
                  Managers often have incomplete information to make decisions about threatened species management, and lack the time or funding needed to obtain complete information. Value of information (VOI) analysis can assist managers in deciding whether to manage using current information or monitor to reduce uncertainty before managing. However, VOI analysis has not yet been applied to spatial allocation of monitoring resources across a landscape.
                
                
                  Here, we demonstrate how to make the best use of data from species distribution models (SDMs) and VOI analysis to assess the value of land protection decisions for single and multiple‐species objectives across a heterogeneous landscape. Our method determines the situations where one should monitor before protecting the land, and those where one should act based on current incomplete information. Further, we prioritize land planning units based on cost‐effectiveness (expected number of occurrences protected per dollar spent) and identify properties to target for monitoring or immediate conservation.
                
                
                  In a single species case study, we found that the optimal decision was to act based on current information when the prior probability of detecting an occurrence in a survey was low. When probability of detection was high, it was most effective to monitor the majority of units. In a multi‐species case study, monitoring was only optimal in 50\% of cases, due to high inferred probability of at least one occurrence of a threatened species in many units. When compared to a simulation where units were monitored by default, using VOI to determine which units were monitored or prioritized for immediate conservation led to an increase in the expected number of occurrences protected.
                
                
                  
                    Synthesis and applications
                    . Using a combination of species distribution models and value of information analysis can assist managers in efficiently distributing limited resources for protected area allocation. Our results suggest that if managers can use value of information to monitor more efficiently, it can lead to protecting a greater number of threatened species occurrences.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Journal of Applied Ecology},
	author = {Raymond, Calla V. and McCune, Jenny L. and Rosner‐Katz, Hanna and Chadès, Iadine and Schuster, Richard and Gilbert, Benjamin and Bennett, Joseph R.},
	editor = {Grantham, Hedley},
	month = apr,
	year = {2020},
	pages = {819--830},
	file = {raymond et al 2020 - Combining species distribution models and value of information analysis for spatial allocation of conservation resources - BDPG - VOI - VALUE OF INFORMATION.pdf:/Users/bill/D/Zotero/storage/Z3JDBHB4/raymond et al 2020 - Combining species distribution models and value of information analysis for spatial allocation of conservation resources - BDPG - VOI - VALUE OF INFORMATION.pdf:application/pdf},
}

@article{schuster2023cb,
	title = {Protected area planning to conserve biodiversity in an uncertain future},
	volume = {37},
	issn = {0888-8892, 1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.14048},
	doi = {10.1111/cobi.14048},
	abstract = {Protected areas are a key instrument for conservation. Despite this, they are vulnerable to risks associated with weak governance, land-use intensiﬁcation, and climate change. We used a novel hierarchical optimization approach to identify priority areas for expanding the global protected area system that explicitly accounted for such risks while maximizing protection of all known terrestrial vertebrate species. To incorporate risk categories, we built on the minimum set problem, where the objective is to reach species distribution protection targets while accounting for 1 constraint, such as land cost or area. We expanded this approach to include multiple objectives accounting for risk in the problem formulation by treating each risk layer as a separate objective in the problem formulation. Reducing exposure to these risks required expanding the area of the global protected area system by 1.6\% while still meeting conservation targets. Incorporating risks from weak governance drove the greatest changes in spatial priorities for protection, and incorporating risks from climate change required the largest increase (2.52\%) in global protected area. Conserving wide-ranging species required countries with relatively strong governance to protect more land when they bordered nations with comparatively weak governance. Our results underscore the need for cross-jurisdictional coordination and demonstrate how risk can be efﬁciently incorporated into conservation planning.},
	language = {en},
	number = {3},
	urldate = {2023-10-11},
	journal = {Conservation Biology},
	author = {Schuster, Richard and Buxton, Rachel and Hanson, Jeffrey O. and Binley, Allison D. and Pittman, Jeremy and Tulloch, Vivitskaia and La Sorte, Frank A. and Roehrdanz, Patrick R. and Verburg, Peter H. and Rodewald, Amanda D. and Wilson, Scott and Possingham, Hugh P. and Bennett, Joseph R.},
	month = jun,
	year = {2023},
	pages = {e14048},
	file = {schuster et al 2022 - Protected area planning to conserve biodiversity in an uncertain future - UNCERTAINTY - RISK.pdf:/Users/bill/D/Zotero/storage/CNJF3M2Z/schuster et al 2022 - Protected area planning to conserve biodiversity in an uncertain future - UNCERTAINTY - RISK.pdf:application/pdf},
}

@misc{xu2023,
	title = {Reflections from the {Workshop} on {AI}-{Assisted} {Decision} {Making} for {Conservation}},
	url = {http://arxiv.org/abs/2307.08774},
	abstract = {In this white paper, we synthesize key points made during presentations and discussions from the AI-Assisted Decision Making for Conservation workshop, hosted by the Center for Research on Computation and Society at Harvard University on October 20–21, 2022. We identify key open research questions in resource allocation, planning, and interventions for biodiversity conservation, highlighting conservation challenges that not only require AI solutions, but also require novel methodological advances. In addition to providing a summary of the workshop talks and discussions, we hope this document serves as a callto-action to orient the expansion of algorithmic decision-making approaches to prioritize real-world conservation challenges, through collaborative efforts of ecologists, conservation decision-makers, and AI researchers.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Xu, Lily and Rolf, Esther and Beery, Sara and Bennett, Joseph R. and Berger-Wolf, Tanya and Birch, Tanya and Bondi-Kelly, Elizabeth and Brashares, Justin and Chapman, Melissa and Corso, Anthony and Davies, Andrew and Garg, Nikhil and Gaylard, Angela and Heilmayr, Robert and Kerner, Hannah and Klemmer, Konstantin and Kumar, Vipin and Mackey, Lester and Monteleoni, Claire and Moorcroft, Paul and Palmer, Jonathan and Perrault, Andrew and Thau, David and Tambe, Milind},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Co-authored by participants from the October 2022 workshop: https://crcs.seas.harvard.edu/conservation-workshop},
	file = {xu et al 2023 - Reflections from the Workshop on AI-Assisted Decision Making for Conservation.pdf:/Users/bill/D/Zotero/storage/S6BFRJ6K/xu et al 2023 - Reflections from the Workshop on AI-Assisted Decision Making for Conservation.pdf:application/pdf},
}

@article{muenzel2023cb,
	title = {Comparing spatial conservation prioritization methods with site‐ versus spatial dependency‐based connectivity},
	volume = {37},
	issn = {0888-8892, 1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.14008},
	doi = {10.1111/cobi.14008},
	abstract = {Larval dispersal is an important component of marine reserve networks. Two conceptually different approaches to incorporate dispersal connectivity into spatial planning of these networks exist, and it is an open question as to when either is most appropriate. Candidate reserve sites can be selected individually based on local properties of connectivity or on a spatial dependency-based approach of selecting clusters of strongly connected habitat patches. The ﬁrst acts on individual sites, whereas the second acts on linked pairs of sites. We used a combination of larval dispersal simulations representing different seascapes and case studies of biophysical larval dispersal models in the Coral Triangle region and the province of Southeast Sulawesi, Indonesia, to compare the performance of these 2 methods in the spatial planning software Marxan. We explored the reserve design performance implications of different dispersal distances and patterns based on the equilibrium settlement of larvae in protected and unprotected areas. We further assessed different assumptions about metapopulation contributions from unprotected areas, including the case of 100\% depletion and more moderate scenarios. The spatial dependency method was suitable when dispersal was limited, a high proportion of the area of interest was substantially degraded, or the target amount of habitat protected was low. Conversely, when subpopulations were well connected, the 100\% depletion was relaxed, or more habitat was protected, protecting individual sites with high scores in metrics of connectivity was a better strategy. Spatial dependency methods generally produced more spatially clustered solutions with more beneﬁts inside than outside reserves compared with site-based methods. Therefore, spatial dependency methods potentially provide better results for ecological persistence objectives over enhancing ﬁsheries objectives, and vice versa. Different spatial prioritization methods of using connectivity are appropriate for different contexts, depending on dispersal characteristics, unprotected area contributions, habitat protection targets, and speciﬁc management objectives.},
	language = {en},
	number = {2},
	urldate = {2023-10-11},
	journal = {Conservation Biology},
	author = {Muenzel, Dominic and Critchell, Kay and Cox, Courtney and Campbell, Stuart J. and Jakub, Raymond and Chollett, Iliana and Krueck, Nils and Holstein, Daniel and Treml, Eric A. and Beger, Maria},
	month = apr,
	year = {2023},
	pages = {e14008},
	file = {muenzel et al 2022 - Comparing spatial conservation prioritization methods with site-versus spatial dependency-based connectivity - MARXAN - GRAPH THEORY.pdf:/Users/bill/D/Zotero/storage/ZJAGGWEA/muenzel et al 2022 - Comparing spatial conservation prioritization methods with site-versus spatial dependency-based connectivity - MARXAN - GRAPH THEORY.pdf:application/pdf},
}

@misc{bougeret2022,
	title = {Approximating optimization problems in graphs with locational uncertainty},
	url = {http://arxiv.org/abs/2206.08187},
	abstract = {Many combinatorial optimization problems can be formulated as the search for a subgraph that satisﬁes certain properties and minimizes the total weight. We assume here that the vertices correspond to points in a metric space and can take any position in given uncertainty sets. Then, the cost function to be minimized is the sum of the distances for the worst positions of the vertices in their uncertainty sets. We propose two types of polynomial-time approximation algorithms. The ﬁrst one relies on solving a deterministic counterpart of the problem where the uncertain distances are replaced with maximum pairwise distances. We study in details the resulting approximation ratio, which depends on the structure of the feasible subgraphs and whether the metric space is Ptolemaic or not. The second algorithm is a fully-polynomial time approximation scheme for the special case of s − t paths.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Bougeret, Marin and Omer, Jérémy and Poss, Michael},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08187 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2109.00389},
	file = {bougeret omer poss 2022 - Approximating optimization problems in graphs with locational uncertainty.pdf:/Users/bill/D/Zotero/storage/Q6JDQ782/bougeret omer poss 2022 - Approximating optimization problems in graphs with locational uncertainty.pdf:application/pdf},
}

@article{brunel2023ema,
	title = {Producing a {Diverse} {Set} of {Near}-{Optimal} {Reserve} {Solutions} with {Exact} {Optimisation}},
	volume = {28},
	issn = {1420-2026, 1573-2967},
	url = {https://link.springer.com/10.1007/s10666-022-09862-1},
	doi = {10.1007/s10666-022-09862-1},
	abstract = {Reserves are at the heart of global policies to stop the erosion of biodiversity. Optimisation is increasingly used to identify reserve locations that preserve biodiversity at a minimum cost to human activities. Two classes of algorithms solve this reserve site selection problem: metaheuristic algorithms (such as simulated annealing, commonly implemented in Marxan) and exact optimisation (i.e. integer programming, commonly implemented in PrioritizR). Although exact approaches are now able to solve large-scale problems, metaheuristics are still widely used. One reason is that metaheuristic-based software provides a set of suboptimal reserve solutions instead of a single one. These alternative solutions are usually welcomed by stakeholders as they provide a better basis for negotiations among potentially conflictive objectives. Metaheuristic algorithms use random procedures to explore the space of suboptimal reserve solutions. Therefore, they may produce a large amount of similar, thus uninformative, alternative solutions, which usually calls for a heavy statistical post-processing. Effective methods for generating a diverse set of near-optimal solutions using exact optimisation are lacking. Here we present two new approaches for addressing this issue. Our algorithms explicitly control both the optimality gap and the dissimilarity between alternative reserve solutions. It allows the identification of a parsimonious, yet meaningful set of reserve solutions. The algorithms presented here could potentially increase the uptake of exact optimisation by practitioners. These methods should contribute to less noisy and more efficient discussions in the design of conservation policies.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Environmental Modeling \& Assessment},
	author = {Brunel, Adrien and Omer, Jérémy and Lanco Bertrand, Sophie},
	month = aug,
	year = {2023},
	pages = {619--634},
	file = {brunel omer bertrand 2023 - Producing a Diverse Set of Near‑Optimal Reserve Solutions with Exact Optimisation - BDPG - EFs - DIVERSITY.pdf:/Users/bill/D/Zotero/storage/RDHXX2X9/brunel omer bertrand 2023 - Producing a Diverse Set of Near‑Optimal Reserve Solutions with Exact Optimisation - BDPG - EFs - DIVERSITY.pdf:application/pdf},
}

@article{weerasena2023em,
	title = {A sequential approach to reserve design with compactness and contiguity considerations},
	volume = {478},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380023000091},
	doi = {10.1016/j.ecolmodel.2023.110281},
	abstract = {Biological conservation depends increasingly on the establishment of protected areas that include as many species as possible, and are extensive, compact, and connected. Various optimization models have been developed to address one or more of these goals; this article develops a set of models that can be used sequentially or separately to promote all four. Assuming that species are mapped across a rectangular grid, we first identify core areas that are extensive and compact by maximizing the density of the graph associated with the reserve. This minimizes the number of boundary edges (suitably normalized) and thus reduces opportunities for organisms to leave the core areas, and external threats to enter. We then identify contiguous corridors between these compact areas based on costs and species conservation goals, allowing the judicious replacement of core elements where possible. This suite of optimization models can assist in designing an efficient and effective system of compact protected areas along with connecting corridors.},
	language = {en},
	urldate = {2023-10-11},
	journal = {Ecological Modelling},
	author = {Weerasena, Lakmali and Shier, Douglas and Tonkyn, David and McFeaters, Mark and Collins, Christopher},
	month = apr,
	year = {2023},
	pages = {110281},
	file = {weerasena et al 2023 - A sequential approach to reserve design with compactness and contiguity considerations - BDPG - CONNECTIVITY - UNCERTAINTY.pdf:/Users/bill/D/Zotero/storage/PQZY26HN/weerasena et al 2023 - A sequential approach to reserve design with compactness and contiguity considerations - BDPG - CONNECTIVITY - UNCERTAINTY.pdf:application/pdf},
}

@article{muscatello2021cb,
	title = {How decisions about fitting species distribution models affect conservation outcomes},
	volume = {35},
	issn = {0888-8892, 1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.13669},
	doi = {10.1111/cobi.13669},
	abstract = {Species distribution models (SDMs) are increasingly used in conservation and land-use planning as inputs to describe biodiversity patterns. These models can be built in different ways, and decisions about data preparation, selection of predictor variables, model fitting, and evaluation all alter the resulting predictions. Commonly, the true distribution of species is unknown and independent data to verify which SDM variant to choose are lacking. Such model uncertainty is of concern to planners. We analyzed how 11 routine decisions about model complexity, predictors, bias treatment, and setting thresholds for predicted values altered conservation priority patterns across 25 species. Models were created with MaxEnt and run through Zonation to determine the priority rank of sites. Although all SDM variants performed well (area under the curve {\textgreater}0.7), they produced spatially different predictions for species and different conservation priority solutions. Priorities were most strongly altered by decisions to not address bias or to apply binary thresholds to predicted values; on average 40\% and 35\%, respectively, of all grid cells received an opposite priority ranking. Forcing high model complexity altered conservation solutions less than forcing simplicity (14\% and 24\% of cells with opposite rank values, respectively). Use of fewer species records to build models or choosing alternative bias treatments had intermediate effects (25\% and 23\%, respectively). Depending on modeling choices, priority areas overlapped as little as 10–20\% with the baseline solution, affecting top and bottom priorities differently. Our results demonstrate the extent of modelbased uncertainty and quantify the relative impacts of SDM building decisions. When it is uncertain what the best SDM approach and conservation plan is, solving uncertainty or considering alterative options is most important for those decisions that change plans the most.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Conservation Biology},
	author = {Muscatello, Angela and Elith, Jane and Kujala, Heini},
	month = aug,
	year = {2021},
	pages = {1309--1320},
	file = {muscatello elith kujala 2020 - How decisions about fitting species distribution models affect conservation outcomes - BDPG - SDMs.pdf:/Users/bill/D/Zotero/storage/QCIM2SYG/muscatello elith kujala 2020 - How decisions about fitting species distribution models affect conservation outcomes - BDPG - SDMs.pdf:application/pdf},
}

@misc{goldenberg2019,
	title = {Hardness {Amplification} of {Optimization} {Problems}},
	url = {http://arxiv.org/abs/1908.10248},
	abstract = {In this paper, we prove a general hardness amplification scheme for optimization problems based on the technique of direct products. We say that an optimization problem \${\textbackslash}Pi\$ is direct product feasible if it is possible to efficiently aggregate any \$k\$ instances of \${\textbackslash}Pi\$ and form one large instance of \${\textbackslash}Pi\$ such that given an optimal feasible solution to the larger instance, we can efficiently find optimal feasible solutions to all the \$k\$ smaller instances. Given a direct product feasible optimization problem \${\textbackslash}Pi\$, our hardness amplification theorem may be informally stated as follows: If there is a distribution \${\textbackslash}mathcal\{D\}\$ over instances of \${\textbackslash}Pi\$ of size \$n\$ such that every randomized algorithm running in time \$t(n)\$ fails to solve \${\textbackslash}Pi\$ on \${\textbackslash}frac\{1\}\{{\textbackslash}alpha(n)\}\$ fraction of inputs sampled from \${\textbackslash}mathcal\{D\}\$, then, assuming some relationships on \${\textbackslash}alpha(n)\$ and \$t(n)\$, there is a distribution \${\textbackslash}mathcal\{D\}'\$ over instances of \${\textbackslash}Pi\$ of size \$O(n{\textbackslash}cdot {\textbackslash}alpha(n))\$ such that every randomized algorithm running in time \${\textbackslash}frac\{t(n)\}\{poly({\textbackslash}alpha(n))\}\$ fails to solve \${\textbackslash}Pi\$ on \${\textbackslash}frac\{99\}\{100\}\$ fraction of inputs sampled from \${\textbackslash}mathcal\{D\}'\$. As a consequence of the above theorem, we show hardness amplification of problems in various classes such as NP-hard problems like Max-Clique, Knapsack, and Max-SAT, problems in P such as Longest Common Subsequence, Edit Distance, Matrix Multiplication, and even problems in TFNP such as Factoring and computing Nash equilibrium.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Goldenberg, Elazar and S., Karthik C.},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10248 [cs]},
	keywords = {Computer Science - Computational Complexity},
	file = {goldenberg karthik 2019 - Hardness Amplification of Optimization Problems.pdf:/Users/bill/D/Zotero/storage/UHRLA2HF/goldenberg karthik 2019 - Hardness Amplification of Optimization Problems.pdf:application/pdf},
}

@misc{weiherer2023,
	title = {From {Zero} to {Hero}: {Convincing} with {Extremely} {Complicated} {Math}},
	shorttitle = {From {Zero} to {Hero}},
	url = {http://arxiv.org/abs/2304.00399},
	abstract = {Becoming a (super) hero is almost every kid’s dream. During their sheltered childhood, they do whatever it takes to grow up to be one. Work hard, play hard – all day long. But as they’re getting older, distractions are more and more likely to occur. They’re getting off track. They start discovering what is feared as simple math. Finally, they end up as a researcher, writing boring, non-impressive papers all day long because they only rely on simple mathematics. No toptier conferences, no respect, no groupies. Life’s over.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Weiherer, Maximilian and Egger, Bernhard},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00399 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: SIGBOVIK'23},
	file = {weiherer egger 2023 - From Zero to Hero - Convincing with Extremely Complicated Math.pdf:/Users/bill/D/Zotero/storage/VDF5Q3JC/weiherer egger 2023 - From Zero to Hero - Convincing with Extremely Complicated Math.pdf:application/pdf},
}

@misc{jooken2022,
	title = {Features for the 0-1 knapsack problem based on inclusionwise maximal solutions},
	url = {http://arxiv.org/abs/2211.09665},
	abstract = {Decades of research on the 0-1 knapsack problem led to very efﬁcient algorithms that are able to quickly solve large problem instances to optimality. This prompted researchers to also investigate whether relatively small problem instances exist that are hard for existing solvers and investigate which features characterize their hardness. Previously the authors proposed a new class of hard 0-1 knapsack problem instances and demonstrated that the properties of so-called inclusionwise maximal solutions (IMSs) can be important hardness indicators for this class. In the current paper, we formulate several new computationally challenging problems related to the IMSs of arbitrary 0-1 knapsack problem instances. Based on generalizations of previous work and new structural results about IMSs, we formulate polynomial and pseudopolynomial time algorithms for solving these problems. From this we derive a set of 14 computationally expensive features, which we calculate for two large datasets on a supercomputer in approximately 540 CPU-hours. We show that the proposed features contain important information related to the empirical hardness of a problem instance that was missing in earlier features from the literature by training machine learning models that can accurately predict the empirical hardness of a wide variety of 0-1 knapsack problem instances. Using the instance space analysis methodology, we also show that hard 0-1 knapsack problem instances are clustered together around a relatively dense region of the instance space and several features behave differently in the easy and hard parts of the instance space.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Jooken, Jorik and Leyman, Pieter and De Causmaecker, Patrick},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09665 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms},
	file = {jooken et al 2022 - Features for the 0-1 knapsack problem based on inclusionwise maximal solutions - BDPG.pdf:/Users/bill/D/Zotero/storage/DYTC4QHS/jooken et al 2022 - Features for the 0-1 knapsack problem based on inclusionwise maximal solutions - BDPG.pdf:application/pdf},
}

@inproceedings{dinur2015p2citcs,
	address = {Rehovot Israel},
	title = {The {Computational} {Benefit} of {Correlated} {Instances}},
	isbn = {978-1-4503-3333-7},
	url = {https://dl.acm.org/doi/10.1145/2688073.2688082},
	doi = {10.1145/2688073.2688082},
	abstract = {The starting point of this paper is that instances of computational problems often do not exist in isolation. Rather, multiple and correlated instances of the same problem arise naturally in the real world. The challenge is how to gain computationally from instance correlations when they exist. We will be interested in settings where signiﬁcant computational gain can be made in solving a single primary instance by having access to additional auxiliary instances which are correlated to the primary instance via the solution space.},
	language = {en},
	urldate = {2023-10-11},
	booktitle = {Proceedings of the 2015 {Conference} on {Innovations} in {Theoretical} {Computer} {Science}},
	publisher = {ACM},
	author = {Dinur, Irit and Goldwasser, Shafi and Lin, Huijia},
	month = jan,
	year = {2015},
	pages = {219--228},
	file = {dinur et al 2015 - the computational benefit of correlated instances.pdf:/Users/bill/D/Zotero/storage/HPXDZSTZ/dinur et al 2015 - the computational benefit of correlated instances.pdf:application/pdf},
}

@misc{zhou2021,
	title = {Super {Solutions} of the {Model} {RB}},
	url = {http://arxiv.org/abs/2105.03831},
	abstract = {The concept of super solution is a special type of generalized solutions with certain degree of robustness and stability. In this paper we consider the (1, 1)-super solutions of the model RB. Using the ﬁrst moment method, we establish a “threshold” such that as the constraint density crosses this value, the expected number of (1, 1)-super solutions goes from 0 to inﬁnity.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Zhou, Guangyan and Xu, Wei},
	month = may,
	year = {2021},
	note = {arXiv:2105.03831 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, 68T99, 68Q25},
	annote = {Comment: 8 pages},
	file = {zhou xu 2021 - Super Solutions of the Model RB - BDPG.pdf:/Users/bill/D/Zotero/storage/2GWFI6HW/zhou xu 2021 - Super Solutions of the Model RB - BDPG.pdf:application/pdf},
}

@article{kujala2018mee,
	title = {Spatial characteristics of species distributions as drivers in conservation prioritization},
	volume = {9},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12939},
	doi = {10.1111/2041-210X.12939},
	abstract = {Abstract
            
              
                
                  Spatial prioritization, based on the biogeographical identification of priority areas for conservation actions, is an important aspect of conservation planning. Although the influence of factors such as costs, threats or use of surrogates on the resulting priorities has been studied extensively, relatively little is known about how the spatial characteristics of species distributions drive the spatial pattern of priorities in multi‐species conservation plans.
                
                
                  Using datasets from Australia and Finland, we explore how excluding or including a given species changes spatial priorities in a multi‐species prioritization. We develop three metrics to quantify changes in priorities, and explore how these changes depend on the total number of species used in the prioritization, the spatial characteristics of the given species distribution, and how species share their space with other species used in the prioritization. We randomly selected 12 set of 10 species from each dataset, and explore the influence of each of these species in prioritizations done for a total of 10, 20, 50 or 100 species.
                
                
                  We show that spatial priorities become increasingly stable as the number of species is increased, and that the stability of highest and lowest priority areas behave differently. When less than 50 species were used in a prioritization, intermediately rare species that occupy mostly species‐poor habitats tend to have the greatest influence on priorities, whereas very rare and common species that co‐occur with many other species tend to have a small influence.
                
                
                  Our results present a systematic method to explore the stability of spatial priorities to changes in the species pool used for a conservation plan. Although the analysed two datasets differed in data type, location, scale and species composition, they both showed how using a small number of species leads to unstable spatial solutions, where the choice to include or exclude an individual species can strongly influence the conservation outcome. Our results also show that conservation planners should carefully assess the use of spatial prioritizations for identifying least important areas (e.g. for development) as these can be particularly unstable when the prioritization is based on a small number of species.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Methods in Ecology and Evolution},
	author = {Kujala, Heini and Moilanen, Atte and Gordon, Ascelin},
	editor = {Travis, Justin},
	month = apr,
	year = {2018},
	pages = {1121--1132},
	file = {kujala moilanen gordon 2017 - Spatial characteristics of species distributions as drivers in conservation prioritization - BDPG.pdf:/Users/bill/D/Zotero/storage/AWZYSTQX/kujala moilanen gordon 2017 - Spatial characteristics of species distributions as drivers in conservation prioritization - BDPG.pdf:application/pdf},
}

@article{milner-gulland2017joae,
	title = {Embracing uncertainty in applied ecology},
	volume = {54},
	issn = {0021-8901, 1365-2664},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2664.12887},
	doi = {10.1111/1365-2664.12887},
	abstract = {Summary
            
              
              
                
                  
                    Applied ecologists often face uncertainty that hinders effective decision‐making.
                  
                  
                    Common traps that may catch the unwary are: ignoring uncertainty, acknowledging uncertainty but ploughing on, focussing on trivial uncertainties, believing your models, and unclear objectives.
                  
                  
                    We integrate research insights and examples from a wide range of applied ecological fields to illustrate advances that are generally underused, but could facilitate ecologists’ ability to plan and execute research to support management.
                  
                  
                    Recommended approaches to avoid uncertainty traps are: embracing models, using decision theory, using models more effectively, thinking experimentally, and being realistic about uncertainty.
                  
                  
                    
                      Synthesis and applications
                      . Applied ecologists can become more effective at informing management by using approaches that explicitly take account of uncertainty.
                    
                  
                
              
            
          , 
            Applied ecologists can become more effective at informing management by using approaches that explicitly take account of uncertainty.},
	language = {en},
	number = {6},
	urldate = {2023-10-11},
	journal = {Journal of Applied Ecology},
	author = {Milner‐Gulland, E. J. and Shea, Katriona},
	editor = {Punt, Andre},
	month = dec,
	year = {2017},
	pages = {2063--2068},
	file = {milner-gulland shea 2017 - Embracing uncertainty in applied ecology.pdf:/Users/bill/D/Zotero/storage/ERBUKQ3L/milner-gulland shea 2017 - Embracing uncertainty in applied ecology.pdf:application/pdf},
}

@article{raymond2020joaea,
	title = {Combining species distribution models and value of information analysis for spatial allocation of conservation resources},
	volume = {57},
	issn = {0021-8901, 1365-2664},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2664.13580},
	doi = {10.1111/1365-2664.13580},
	abstract = {Abstract
            
              
                
                  Managers often have incomplete information to make decisions about threatened species management, and lack the time or funding needed to obtain complete information. Value of information (VOI) analysis can assist managers in deciding whether to manage using current information or monitor to reduce uncertainty before managing. However, VOI analysis has not yet been applied to spatial allocation of monitoring resources across a landscape.
                
                
                  Here, we demonstrate how to make the best use of data from species distribution models (SDMs) and VOI analysis to assess the value of land protection decisions for single and multiple‐species objectives across a heterogeneous landscape. Our method determines the situations where one should monitor before protecting the land, and those where one should act based on current incomplete information. Further, we prioritize land planning units based on cost‐effectiveness (expected number of occurrences protected per dollar spent) and identify properties to target for monitoring or immediate conservation.
                
                
                  In a single species case study, we found that the optimal decision was to act based on current information when the prior probability of detecting an occurrence in a survey was low. When probability of detection was high, it was most effective to monitor the majority of units. In a multi‐species case study, monitoring was only optimal in 50\% of cases, due to high inferred probability of at least one occurrence of a threatened species in many units. When compared to a simulation where units were monitored by default, using VOI to determine which units were monitored or prioritized for immediate conservation led to an increase in the expected number of occurrences protected.
                
                
                  
                    Synthesis and applications
                    . Using a combination of species distribution models and value of information analysis can assist managers in efficiently distributing limited resources for protected area allocation. Our results suggest that if managers can use value of information to monitor more efficiently, it can lead to protecting a greater number of threatened species occurrences.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Journal of Applied Ecology},
	author = {Raymond, Calla V. and McCune, Jenny L. and Rosner‐Katz, Hanna and Chadès, Iadine and Schuster, Richard and Gilbert, Benjamin and Bennett, Joseph R.},
	editor = {Grantham, Hedley},
	month = apr,
	year = {2020},
	pages = {819--830},
	file = {raymond  ... chades schuster et al 2020 - Combining species distribution models and value of information analysis for spatial allocation of conservation resources - BDPG - VOI - VALUE OF INFORMATION - SDMs.pdf:/Users/bill/D/Zotero/storage/62BJIUMJ/raymond  ... chades schuster et al 2020 - Combining species distribution models and value of information analysis for spatial allocation of conservation resources - BDPG - VOI - VALUE OF INFORMATION - SDMs.pdf:application/pdf},
}

@article{beel2010josp,
	title = {Academic {Search} {Engine} {Optimization} ({ASEO}): {Optimizing} {Scholarly} {Literature} for {Google} {Scholar} \&amp; {Co}.},
	volume = {41},
	issn = {1710-1166},
	shorttitle = {Academic {Search} {Engine} {Optimization} ({ASEO})},
	url = {http://muse.jhu.edu/content/crossref/journals/journal_of_scholarly_publishing/v041/41.2.beel.html},
	doi = {10.1353/scp.0.0082},
	abstract = {This article introduces and discusses the concept of academic search engine optimization (ASEO). Based on three recently conducted studies, guidelines are provided on how to optimize scholarly literature for academic search engines in general and for Google Scholar in particular. In addition, we briefly discuss the risk of researchers’ illegitimately ‘over-optimizing’ their articles.},
	language = {en},
	number = {2},
	urldate = {2023-10-11},
	journal = {Journal of Scholarly Publishing},
	author = {Beel, Jöran and Gipp, Bela and Eilde, Erik},
	year = {2010},
	pages = {176--190},
	file = {beel et al 2010 - Academic Search Engine Optimization (ASEO) - OptimizingScholarly Literature for Google Scholar and Co - BDPG.pdf:/Users/bill/D/Zotero/storage/QZ8VAN5U/beel et al 2010 - Academic Search Engine Optimization (ASEO) - OptimizingScholarly Literature for Google Scholar and Co - BDPG.pdf:application/pdf},
}

@article{romano2020pcb,
	title = {Ten simple rules for writing a paper about scientific software},
	volume = {16},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1008390},
	doi = {10.1371/journal.pcbi.1008390},
	abstract = {Papers describing software are an important part of computational fields of scientific research. These “software papers” are unique in a number of ways, and they require special consideration to improve their impact on the scientific community and their efficacy at conveying important information. Here, we discuss 10 specific rules for writing software papers, covering some of the different scenarios and publication types that might be encountered, and important questions from which all computational researchers would benefit by asking along the way.},
	language = {en},
	number = {11},
	urldate = {2023-10-11},
	journal = {PLOS Computational Biology},
	author = {Romano, Joseph D. and Moore, Jason H.},
	editor = {Markel, Scott},
	month = nov,
	year = {2020},
	pages = {e1008390},
	file = {romano moore 2020 - Ten simple rules for writing a paper about scientific software - BDPG.pdf:/Users/bill/D/Zotero/storage/5NQXNB3M/romano moore 2020 - Ten simple rules for writing a paper about scientific software - BDPG.pdf:application/pdf},
}

@article{gadelha2021wdm&k,
	title = {A survey of biodiversity informatics: {Concepts}, practices, and challenges},
	volume = {11},
	issn = {1942-4787, 1942-4795},
	shorttitle = {A survey of biodiversity informatics},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1394},
	doi = {10.1002/widm.1394},
	abstract = {The unprecedented size of the human population, along with its associated economic activities, has an ever-increasing impact on global environments. Across the world, countries are concerned about the growing resource consumption and the capacity of ecosystems to provide resources. To effectively conserve biodiversity, it is essential to make indicators and knowledge openly available to decision-makers in ways that they can effectively use them. The development and deployment of tools and techniques to generate these indicators require having access to trustworthy data from biological collections, field surveys and automated sensors, molecular data, and historic academic literature. The transformation of these raw data into synthesized information that is fit for use requires going through many refinement steps. The methodologies and techniques applied to manage and analyze these data constitute an area usually called biodiversity informatics. Biodiversity data follow a life cycle consisting of planning, collection, certification, description, preservation, discovery, integration, and analysis. Researchers, whether producers or consumers of biodiversity data, will likely perform activities related to at least one of these steps. This article explores each stage of the life cycle of biodiversity data, discussing its methodologies, tools, and challenges.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Gadelha, Luiz M. R. and De Siracusa, Pedro C. and Dalcin, Eduardo Couto and Da Silva, Luís Alexandre Estevão and Augusto, Douglas A. and Krempser, Eduardo and Affe, Helen Michelle and Costa, Raquel Lopes and Mondelli, Maria Luiza and Meirelles, Pedro Milet and Thompson, Fabiano and Chame, Marcia and Ziviani, Artur and De Siqueira, Marinez Ferreira},
	month = jan,
	year = {2021},
	pages = {e1394},
	file = {gadelha et al 2020 - A survey of biodiversity informatics - Concepts, practices,and challenges.pdf:/Users/bill/D/Zotero/storage/UCIEC6IH/gadelha et al 2020 - A survey of biodiversity informatics - Concepts, practices,and challenges.pdf:application/pdf},
}

@article{peterson2022bia,
	title = {{ENM2020}: {A} {Free} {Online} {Course} and {Set} of {Resources} on {Modeling} {Species}' {Niches} and {Distributions}},
	volume = {17},
	issn = {1546-9735},
	shorttitle = {{ENM2020}},
	url = {https://journals.ku.edu/jbi/article/view/15016},
	doi = {10.17161/bi.v17i.15016},
	abstract = {The field of distributional ecology has seen considerable recent attention, particularly surrounding the theory, protocols, and tools for Ecological Niche Modeling (ENM) or Species Distribution Modeling (SDM). Such analyses have grown steadily over the past two decades—including a maturation of relevant theory and key concepts—but methodological consensus has yet to be reached. In response, and following an online course taught in Spanish in 2018, we designed a comprehensive English-language course covering much of the underlying theory and methods currently applied in this broad field. Here, we summarize that course, ENM2020, and provide links by which resources produced for it can be accessed into the future. ENM2020 lasted 43 weeks, with presentations from 52 instructors, who engaged with {\textgreater}2500 participants globally through {\textgreater}14,000 hours of viewing and {\textgreater}90,000 views of instructional video and question-and-answer sessions. Each major topic was introduced by an “Overview” talk, followed by more detailed lectures on subtopics. The hierarchical and modular format of the course permits updates, corrections, or alternative viewpoints, and generally facilitates revision and reuse, including the use of only the Overview lectures for introductory courses. All course materials are free and openly accessible (CC-BY license) to ensure these resources remain available to all interested in distributional ecology.},
	language = {en},
	urldate = {2023-10-11},
	journal = {Biodiversity Informatics},
	author = {Peterson, A. Townsend and Aiello-Lammens, Matthew and Amatulli, Giuseppe and Anderson, Robert and Cobos, Marlon and Diniz-Filho, José Alexandre and Escobar, Luis and Feng, Xiao and Franklin, Janet and Gadelha, Luiz and Georges, Damien and Guéguen, M and Gueta, Tomer and Ingenloff, Kate and Jarvie, Scott and Jiménez, Laura and Karger, Dirk and Kass, Jamie and Kearney, Michael and Loyola, Rafael and Machado-Stredel, Fernando and Martínez-Meyer, Enrique and Merow, Cory and Mondelli, Maria Luiza and Mortara, Sara and Muscarella, Robert and Myers, Corinne and Naimi, Babak and Noesgaard, Daniel and Ondo, Ian and Osorio-Olvera, Luis and Owens, Hannah and Pearson, Richard and Pinilla-Buitrago, Gonzalo and Sánchez-Tapia, Andrea and Saupe, Erin and Thuiller, Wilfried and Varela, Sara and Warren, Dan and Wieczorek, John and Yates, Katherine and Zhu, Gengping and Zuquim, Gabriela and Zurell, Damaris},
	month = mar,
	year = {2022},
	file = {peterson et al 2022 - enm2020 - a free online course and set of resources on modeling species niches and distributions - SDMs.pdf:/Users/bill/D/Zotero/storage/B9TW6CDM/peterson et al 2022 - enm2020 - a free online course and set of resources on modeling species niches and distributions - SDMs.pdf:application/pdf},
}

@article{johnson2015s,
	title = {Training {Conservation} {Practitioners} to be {Better} {Decision} {Makers}},
	volume = {7},
	issn = {2071-1050},
	url = {http://www.mdpi.com/2071-1050/7/7/8354},
	doi = {10.3390/su7078354},
	abstract = {Traditional conservation curricula and training typically emphasizes only one part of systematic decision making (i.e., the science), at the expense of preparing conservation practitioners with critical skills in values-setting, working with decision makers and stakeholders, and effective problem framing. In this article we describe how the application of decision science is relevant to conservation problems and suggest how current and future conservation practitioners can be trained to be better decision makers. Though decision-analytic approaches vary considerably, they all involve: (1) properly formulating the decision problem; (2) specifying feasible alternative actions; and (3) selecting criteria for evaluating potential outcomes. Two approaches are available for providing training in decision science, with each serving different needs. Formal education is useful for providing simple, well-defined problems that allow demonstrations of the structure, axioms and general characteristics of a decision-analytic approach. In contrast, practical training can offer complex, realistic decision problems requiring more careful structuring and analysis than those used for formal training purposes. Ultimately, the kinds and degree of training necessary depend on the role conservation practitioners play in a decision-making process. Those attempting to facilitate decision-making processes will need advanced training in both technical aspects of decision science and in facilitation techniques, as well as opportunities to apprentice under decision analysts/consultants. Our primary goal should be an attempt to ingrain a discipline for applying clarity of thought to all decisions.},
	language = {en},
	number = {7},
	urldate = {2023-10-11},
	journal = {Sustainability},
	author = {Johnson, Fred and Eaton, Mitchell and Williams, James and Jensen, Gitte and Madsen, Jesper},
	month = jun,
	year = {2015},
	pages = {8354--8373},
	file = {johnson et al 2015 - training conservation practitioners to be better decision makers.pdf:/Users/bill/D/Zotero/storage/J9PPDYHD/johnson et al 2015 - training conservation practitioners to be better decision makers.pdf:application/pdf},
}

@article{phillips2013e,
	title = {On estimating probability of presence from use–availability or presence–background data},
	volume = {94},
	issn = {0012-9658},
	url = {http://doi.wiley.com/10.1890/12-1520.1},
	doi = {10.1890/12-1520.1},
	abstract = {A fundamental ecological modeling task is to estimate the probability that a species is present in (or uses) a site, conditional on environmental variables. For many species, available data consist of ‘‘presence’’ data (locations where the species [or evidence of it] has been observed), together with ‘‘background’’ data, a random sample of available environmental conditions.},
	language = {en},
	number = {6},
	urldate = {2023-10-11},
	journal = {Ecology},
	author = {Phillips, Steven J. and Elith, Jane},
	month = jun,
	year = {2013},
	pages = {1409--1419},
	file = {Phillips Elith 2013 - On estimating probability of presence form use_availability or presence_background data - SDMs - GUPPY.pdf:/Users/bill/D/Zotero/storage/TL7VZG9D/Phillips Elith 2013 - On estimating probability of presence form use_availability or presence_background data - SDMs - GUPPY.pdf:application/pdf},
}

@article{merow2014e,
	title = {What do we gain from simplicity versus complexity in species distribution models?},
	volume = {37},
	issn = {0906-7590, 1600-0587},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.00845},
	doi = {10.1111/ecog.00845},
	abstract = {Species distribution models (SDMs) are widely used to explain and predict species ranges and environmental niches. They are most commonly constructed by inferring species' occurrence–environment relationships using statistical and machine‐learning methods. The variety of methods that can be used to construct SDMs (e.g. generalized linear/additive models, tree‐based models, maximum entropy, etc.), and the variety of ways that such models can be implemented, permits substantial flexibility in SDM complexity. Building models with an appropriate amount of complexity for the study objectives is critical for robust inference. We characterize complexity as the shape of the inferred occurrence–environment relationships and the number of parameters used to describe them, and search for insights into whether additional complexity is informative or superfluous. By building ‘under fit’ models, having insufficient flexibility to describe observed occurrence–environment relationships, we risk misunderstanding the factors shaping species distributions. By building ‘over fit’ models, with excessive flexibility, we risk inadvertently ascribing pattern to noise or building opaque models. However, model selection can be challenging, especially when comparing models constructed under different modeling approaches. Here we argue for a more pragmatic approach: researchers should constrain the complexity of their models based on study objective, attributes of the data, and an understanding of how these interact with the underlying biological processes. We discuss guidelines for balancing under fitting with over fitting and consequently how complexity affects decisions made during model building. Although some generalities are possible, our discussion reflects differences in opinions that favor simpler versus more complex models. We conclude that combining insights from both simple and complex SDM building approaches best advances our knowledge of current and future species ranges.},
	language = {en},
	number = {12},
	urldate = {2023-10-11},
	journal = {Ecography},
	author = {Merow, Cory and Smith, Mathew J. and Edwards, Thomas C. and Guisan, Antoine and McMahon, Sean M. and Normand, Signe and Thuiller, Wilfried and Wüest, Rafael O. and Zimmermann, Niklaus E. and Elith, Jane},
	month = dec,
	year = {2014},
	pages = {1267--1281},
	file = {merow et al 2014- What do we gain from simplicity versus complexity in species distribution models - SDMs - GUPPY.pdf:/Users/bill/D/Zotero/storage/6JPZEDJF/merow et al 2014- What do we gain from simplicity versus complexity in species distribution models - SDMs - GUPPY.pdf:application/pdf},
}

@article{merow2014ea,
	title = {What do we gain from simplicity versus complexity in species distribution models?},
	volume = {37},
	issn = {0906-7590, 1600-0587},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.00845},
	doi = {10.1111/ecog.00845},
	abstract = {Species distribution models (SDMs) are widely used to explain and predict species ranges and environmental niches. They are most commonly constructed by inferring species' occurrence–environment relationships using statistical and machine‐learning methods. The variety of methods that can be used to construct SDMs (e.g. generalized linear/additive models, tree‐based models, maximum entropy, etc.), and the variety of ways that such models can be implemented, permits substantial flexibility in SDM complexity. Building models with an appropriate amount of complexity for the study objectives is critical for robust inference. We characterize complexity as the shape of the inferred occurrence–environment relationships and the number of parameters used to describe them, and search for insights into whether additional complexity is informative or superfluous. By building ‘under fit’ models, having insufficient flexibility to describe observed occurrence–environment relationships, we risk misunderstanding the factors shaping species distributions. By building ‘over fit’ models, with excessive flexibility, we risk inadvertently ascribing pattern to noise or building opaque models. However, model selection can be challenging, especially when comparing models constructed under different modeling approaches. Here we argue for a more pragmatic approach: researchers should constrain the complexity of their models based on study objective, attributes of the data, and an understanding of how these interact with the underlying biological processes. We discuss guidelines for balancing under fitting with over fitting and consequently how complexity affects decisions made during model building. Although some generalities are possible, our discussion reflects differences in opinions that favor simpler versus more complex models. We conclude that combining insights from both simple and complex SDM building approaches best advances our knowledge of current and future species ranges.},
	language = {en},
	number = {12},
	urldate = {2023-10-11},
	journal = {Ecography},
	author = {Merow, Cory and Smith, Mathew J. and Edwards, Thomas C. and Guisan, Antoine and McMahon, Sean M. and Normand, Signe and Thuiller, Wilfried and Wüest, Rafael O. and Zimmermann, Niklaus E. and Elith, Jane},
	month = dec,
	year = {2014},
	pages = {1267--1281},
	file = {Ecography - 2014 - Merow - What do we gain from simplicity versus complexity in species distribution models - SDMs - GUPPY.pdf:/Users/bill/D/Zotero/storage/CQG9DJN7/Ecography - 2014 - Merow - What do we gain from simplicity versus complexity in species distribution models - SDMs - GUPPY.pdf:application/pdf},
}

@article{guisan2013el,
	title = {Predicting species distributions for conservation decisions},
	volume = {16},
	issn = {1461-023X, 1461-0248},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ele.12189},
	doi = {10.1111/ele.12189},
	abstract = {Species distribution models (SDMs) are increasingly proposed to support conservation decision making. However, evidence of SDMs supporting solutions for on-ground conservation problems is still scarce in the scientiﬁc literature. Here, we show that successful examples exist but are still largely hidden in the grey literature, and thus less accessible for analysis and learning. Furthermore, the decision framework within which SDMs are used is rarely made explicit. Using case studies from biological invasions, identiﬁcation of critical habitats, reserve selection and translocation of endangered species, we propose that SDMs may be tailored to suit a range of decision-making contexts when used within a structured and transparent decision-making process. To construct appropriate SDMs to more effectively guide conservation actions, modellers need to better understand the decision process, and decision makers need to provide feedback to modellers regarding the actual use of SDMs to support conservation decisions. This could be facilitated by individuals or institutions playing the role of ‘translators’ between modellers and decision makers. We encourage species distribution modellers to get involved in real decision-making processes that will beneﬁt from their technical input; this strategy has the potential to better bridge theory and practice, and contribute to improve both scientiﬁc knowledge and conservation outcomes.},
	language = {en},
	number = {12},
	urldate = {2023-10-11},
	journal = {Ecology Letters},
	author = {Guisan, Antoine and Tingley, Reid and Baumgartner, John B. and Naujokaitis‐Lewis, Ilona and Sutcliffe, Patricia R. and Tulloch, Ayesha I. T. and Regan, Tracey J. and Brotons, Lluis and McDonald‐Madden, Eve and Mantyka‐Pringle, Chrystal and Martin, Tara G. and Rhodes, Jonathan R. and Maggini, Ramona and Setterfield, Samantha A. and Elith, Jane and Schwartz, Mark W. and Wintle, Brendan A. and Broennimann, Olivier and Austin, Mike and Ferrier, Simon and Kearney, Michael R. and Possingham, Hugh P. and Buckley, Yvonne M.},
	editor = {Arita, Hector},
	month = dec,
	year = {2013},
	pages = {1424--1435},
	file = {guisan et al 2013 - Predicting species distributions for conservation decisions - SDMs - GUPPY.pdf:/Users/bill/D/Zotero/storage/QHZZ5ZCW/guisan et al 2013 - Predicting species distributions for conservation decisions - SDMs - GUPPY.pdf:application/pdf},
}

@inproceedings{macia2010p1acgec,
	address = {Portland Oregon USA},
	title = {In search of targeted-complexity problems},
	isbn = {978-1-4503-0072-8},
	url = {https://dl.acm.org/doi/10.1145/1830483.1830674},
	doi = {10.1145/1830483.1830674},
	abstract = {Currently available real-world problems do not cover the whole complexity space and, therefore, do not allow us to thoroughly test learner behavior on the border of its domain of competence. Thus, the necessity of developing a more suitable testing scenario arises. With this in mind, data complexity analysis has shown promise in characterizing difﬁculty of classiﬁcation problems through a set of complexity descriptors which used in artiﬁcial data sets generation could supply the required framework to reﬁne and design learners. This paper, then, proposes the use of instance selection based on an evolutionary multiobjective technique to generate data sets that meet speciﬁc characteristics established by such complexity descriptors. These artiﬁcial targeted-complexity problems, which capture the essence of real-world structures, may help to deﬁne a set of benchmarks that contributes to test the properties of learners and to improve them.},
	language = {en},
	urldate = {2023-10-11},
	booktitle = {Proceedings of the 12th annual conference on {Genetic} and evolutionary computation},
	publisher = {ACM},
	author = {Macià, Núria and Orriols-Puig, Albert and Bernadó-Mansilla, Ester},
	month = jul,
	year = {2010},
	pages = {1055--1062},
	file = {macia et al 2010 - In Search of Targeted-Complexity Problems - BDPG - PROBLEM GENERATION.pdf:/Users/bill/D/Zotero/storage/CI2CKDLL/macia et al 2010 - In Search of Targeted-Complexity Problems - BDPG - PROBLEM GENERATION.pdf:application/pdf},
}

@article{foody2011geb,
	title = {Impacts of imperfect reference data on the apparent accuracy of species presence-absence models and their predictions: {Imperfect} reference data},
	volume = {20},
	issn = {1466822X},
	shorttitle = {Impacts of imperfect reference data on the apparent accuracy of species presence-absence models and their predictions},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1466-8238.2010.00605.x},
	doi = {10.1111/j.1466-8238.2010.00605.x},
	abstract = {Aim To explore the impacts of imperfect reference data on the accuracy of species distribution model predictions. The main focus is on impacts of the quality of reference data (labelling accuracy) and, to a lesser degree, data quantity (sample size) on species presence–absence modelling.},
	language = {en},
	number = {3},
	urldate = {2023-10-18},
	journal = {Global Ecology and Biogeography},
	author = {Foody, Giles M.},
	month = may,
	year = {2011},
	pages = {498--508},
	file = {foody 2011 - Impacts of imperfect reference data on the apparent accuracy of species presence–absence models and their predictions - SDMs - UNCERTAINTY - BDPG.pdf:/Users/bill/D/Zotero/storage/7CQW5WGP/foody 2011 - Impacts of imperfect reference data on the apparent accuracy of species presence–absence models and their predictions - SDMs - UNCERTAINTY - BDPG.pdf:application/pdf},
}

@article{beiranvand2017oea,
	title = {Best practices for comparing optimization algorithms},
	volume = {18},
	issn = {1389-4420, 1573-2924},
	url = {http://link.springer.com/10.1007/s11081-017-9366-1},
	doi = {10.1007/s11081-017-9366-1},
	abstract = {Comparing, or benchmarking, of optimization algorithms is a complicated task that involves many subtle considerations to yield a fair and unbiased evaluation. In this paper, we systematically review the benchmarking process of optimization algorithms, and discuss the challenges of fair comparison. We provide suggestions for each step of the comparison process and highlight the pitfalls to avoid when evaluating the performance of optimization algorithms. We also discuss various methods of reporting the benchmarking results. Finally, some suggestions for future research are presented to improve the current benchmarking process.},
	language = {en},
	number = {4},
	urldate = {2023-10-25},
	journal = {Optimization and Engineering},
	author = {Beiranvand, Vahid and Hare, Warren and Lucet, Yves},
	month = dec,
	year = {2017},
	pages = {815--848},
	file = {beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING - ANNO.pdf.pdf:/Users/bill/D/Zotero/storage/9F8VEZBT/beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING - ANNO.pdf.pdf:application/pdf},
}

@article{crowder1979atms,
	title = {On {Reporting} {Computational} {Experiments} with {Mathematical} {Software}},
	volume = {5},
	issn = {0098-3500, 1557-7295},
	url = {https://dl.acm.org/doi/10.1145/355826.355833},
	doi = {10.1145/355826.355833},
	language = {en},
	number = {2},
	urldate = {2023-10-25},
	journal = {ACM Transactions on Mathematical Software},
	author = {Crowder, Harlan and Dembo, Ron S. and Mulvey, John M.},
	month = jun,
	year = {1979},
	pages = {193--203},
	file = {crowder dembo mulvey 1978 - on reporting computational experiments with mathematical software - BDPG.pdf:/Users/bill/D/Zotero/storage/FZ84K58K/crowder dembo mulvey 1978 - on reporting computational experiments with mathematical software - BDPG.pdf:application/pdf},
}

@article{segan2010bc,
	title = {Can we determine conservation priorities without clear objectives?},
	volume = {143},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320709004145},
	doi = {10.1016/j.biocon.2009.09.014},
	language = {en},
	number = {1},
	urldate = {2023-10-27},
	journal = {Biological Conservation},
	author = {Segan, Daniel B. and Carwardine, Josie and Klein, Carissa and Grantham, Hedley and Pressey, Robert L.},
	month = jan,
	year = {2010},
	pages = {2--4},
	file = {segan carwardine klein grantham pressey 2009 - Can we determine conservation priorities without clear objectives - RESERVE SELECTION - EF EFs - ANNO.pdf:/Users/bill/D/Zotero/storage/EDY2N2ST/segan carwardine klein grantham pressey 2009 - Can we determine conservation priorities without clear objectives - RESERVE SELECTION - EF EFs - ANNO.pdf:application/pdf},
}

@article{visconti2010bc,
	title = {Conservation planning with dynamic threats: {The} role of spatial design and priority setting for species’ persistence},
	volume = {143},
	issn = {00063207},
	shorttitle = {Conservation planning with dynamic threats},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320709005266},
	doi = {10.1016/j.biocon.2009.12.018},
	abstract = {Conservation actions frequently need to be scheduled because both funding and implementation capacity are limited. Two approaches to scheduling are possible. Maximizing gain (MaxGain) which attempts to maximize representation with protected areas, or minimizing loss (MinLoss) which attempts to minimize total loss both inside and outside protected areas. Conservation planners also choose between setting priorities based solely on biodiversity pattern and considering surrogates for biodiversity processes such as connectivity. We address both biodiversity processes and habitat loss in a scheduling framework by comparing four different prioritization strategies deﬁned by MaxGain and MinLoss applied to biodiversity patterns and processes to solve the dynamic area selection problem with variable area cost. We compared each strategy by estimating predicted species’ occurrences within a landscape after 20 years of incremental reservation and loss of habitat. By incorporating species-speciﬁc responses to fragmentation, we found that you could improve the performance of conservation strategies. MinLoss was the best approach for conserving both biodiversity pattern and process. However, due to the spatial autocorrelation of habitat loss, reserves selected with this approach tended to become more isolated through time; losing up to 40\% of occurrences of edge-sensitive species. Additionally, because of the positive correlation between threats and land cost, reserve networks designed with this approach contained smaller and fewer reserves compared with networks designed with a MaxGain approach. We suggest a possible way to account for the negative effect of fragmentation by considering both local and neighbourhood vulnerability to habitat loss.},
	language = {en},
	number = {3},
	urldate = {2023-10-27},
	journal = {Biological Conservation},
	author = {Visconti, Piero and Pressey, Robert L. and Segan, Daniel B. and Wintle, Brendan A.},
	month = mar,
	year = {2010},
	pages = {756--767},
	file = {visconti pressey segan wintle 2010 - conservation planning with dynamic threats - the role of spatial design and priority setting for species persistence - GUPPY - AminusBoverAplusB.pdf:/Users/bill/D/Zotero/storage/JIAT9VUR/visconti pressey segan wintle 2010 - conservation planning with dynamic threats - the role of spatial design and priority setting for species persistence - GUPPY - AminusBoverAplusB.pdf:application/pdf},
}

@article{williamson2020n,
	title = {Factors associated with {COVID}-19-related death using {OpenSAFELY}},
	volume = {584},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2521-4},
	doi = {10.1038/s41586-020-2521-4},
	language = {en},
	number = {7821},
	urldate = {2023-10-28},
	journal = {Nature},
	author = {Williamson, Elizabeth J. and Walker, Alex J. and Bhaskaran, Krishnan and Bacon, Seb and Bates, Chris and Morton, Caroline E. and Curtis, Helen J. and Mehrkar, Amir and Evans, David and Inglesby, Peter and Cockburn, Jonathan and McDonald, Helen I. and MacKenna, Brian and Tomlinson, Laurie and Douglas, Ian J. and Rentsch, Christopher T. and Mathur, Rohini and Wong, Angel Y. S. and Grieve, Richard and Harrison, David and Forbes, Harriet and Schultze, Anna and Croker, Richard and Parry, John and Hester, Frank and Harper, Sam and Perera, Rafael and Evans, Stephen J. W. and Smeeth, Liam and Goldacre, Ben},
	month = aug,
	year = {2020},
	pages = {430--436},
	file = {williamson et al 2020 - factors associated with COVID-19-related death using OpenSAFELY - PRIVACY - ANONYMIZING DATA - BDPG.pdf:/Users/bill/D/Zotero/storage/5U5DSAMV/williamson et al 2020 - factors associated with COVID-19-related death using OpenSAFELY - PRIVACY - ANONYMIZING DATA - BDPG.pdf:application/pdf},
}

@incollection{bartz-beielstein2009pot1accogaecclbp,
	title = {The {Future} of {Experimental} {Research}},
	abstract = {In the experimental analysis of metaheuristic methods, two issues are still not sufﬁciently treated. Firstly, the performance of algorithms depends on their parametrizations—and of the parametrizations of the problem instances. However, these dependencies can be seen as means for understanding an algorithm’s behavior. Secondly, the nondeterminism of evolutionary and other metaheuristic methods renders result distributions, not numbers.},
	language = {en},
	booktitle = {Proceedings of the 11th {Annual} {Conference} {Companion} on {Genetic} and {Evolutionary} {Computation} {Conference}: {Late} {Breaking} {Papers}},
	author = {Bartz-Beielstein, Thomas and Preuss, Mike},
	year = {2009},
	pages = {3185--3226},
	file = {bartz-beielstein preuss 2009 - the future of experimental research - BDPG.pdf:/Users/bill/D/Zotero/storage/ZERJGDML/bartz-beielstein preuss 2009 - the future of experimental research - BDPG.pdf:application/pdf},
}

@article{tang2006ml,
	title = {An analysis of diversity measures},
	volume = {65},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-9449-2},
	doi = {10.1007/s10994-006-9449-2},
	abstract = {Diversity among the base classiﬁers is deemed to be important when constructing a classiﬁer ensemble. Numerous algorithms have been proposed to construct a good classiﬁer ensemble by seeking both the accuracy of the base classiﬁers and the diversity among them. However, there is no generally accepted deﬁnition of diversity, and measuring the diversity explicitly is very difﬁcult. Although researchers have designed several experimental studies to compare different diversity measures, usually confusing results were observed. In this paper, we present a theoretical analysis on six existing diversity measures (namely disagreement measure, double fault measure, KW variance, inter-rater agreement, generalized diversity and measure of difﬁculty), show underlying relationships between them, and relate them to the concept of margin, which is more explicitly related to the success of ensemble learning algorithms. We illustrate why confusing experimental results were observed and show that the discussed diversity measures are naturally ineffective. Our analysis provides a deeper understanding of the concept of diversity, and hence can help design better ensemble learning algorithms.},
	language = {en},
	number = {1},
	urldate = {2023-12-09},
	journal = {Machine Learning},
	author = {Tang, E. K. and Suganthan, P. N. and Yao, X.},
	month = oct,
	year = {2006},
	pages = {247--271},
	file = {tang suganthan yao 2006 - an analysis of diversity measures - DIVERSITY - BDPG.pdf:/Users/bill/D/Zotero/storage/65WEKMLP/tang suganthan yao 2006 - an analysis of diversity measures - DIVERSITY - BDPG.pdf:application/pdf},
}

@incollection{brown2010mcs,
	address = {Berlin, Heidelberg},
	title = {“{Good}” and “{Bad}” {Diversity} in {Majority} {Vote} {Ensembles}},
	volume = {5997},
	isbn = {978-3-642-12126-5 978-3-642-12127-2},
	url = {http://link.springer.com/10.1007/978-3-642-12127-2_13},
	abstract = {Although diversity in classiﬁer ensembles is desirable, its relationship with the ensemble accuracy is not straightforward. Here we derive a decomposition of the majority vote error into three terms: average individual accuracy, “good” diversity and “bad diversity”. The good diversity term is taken out of the individual error whereas the bad diversity term is added to it. We relate the two diversity terms to the majority vote limits deﬁned previously (the patterns of success and failure). A simulation study demonstrates how the proposed decomposition can be used to gain insights about majority vote classiﬁer ensembles.},
	language = {en},
	urldate = {2023-12-09},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Brown, Gavin and Kuncheva, Ludmila I.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and El Gayar, Neamat and Kittler, Josef and Roli, Fabio},
	year = {2010},
	doi = {10.1007/978-3-642-12127-2_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {124--133},
	file = {brown kuncheva 2010 - good and bad diversity in majority vote ensembles - ENSEMBLE DIVERSITY - BDPG.pdf:/Users/bill/D/Zotero/storage/AXGBDPE8/brown kuncheva 2010 - good and bad diversity in majority vote ensembles - ENSEMBLE DIVERSITY - BDPG.pdf:application/pdf},
}

@incollection{kuncheva2003praia,
	address = {Berlin, Heidelberg},
	title = {That {Elusive} {Diversity} in {Classifier} {Ensembles}},
	volume = {2652},
	isbn = {978-3-540-40217-6 978-3-540-44871-6},
	url = {http://link.springer.com/10.1007/978-3-540-44871-6_130},
	abstract = {Is “useful diversity” a myth? Many experiments and the little available theory on diversity in classiﬁer ensembles are either inconclusive, too heavily assumption-bound or openly non-supportive of the intuition that diverse classiﬁers fare better than non-divers ones. Although a rough general tendency was conﬁrmed in our previous studies, no prominent link appeared between diversity of the ensemble and its accuracy. Diversity alone is a poor predictor of the ensemble accuracy. But there is no agreed deﬁnition of diversity to start with! Can we borrow a concept of diversity from biology? How can diversity, as far as we can deﬁne and measure it, be used to improve the ensemble? Here we argue that even without a clear-cut deﬁnition and theory behind it, studying diversity may prompt viable heuristic solutions. We look into some ways in which diversity can be used in analyzing, selecting or training the ensemble.},
	language = {en},
	urldate = {2023-12-09},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kuncheva, Ludmila I.},
	editor = {Perales, Francisco José and Campilho, Aurélio J. C. and De La Blanca, Nicolás Pérez and Sanfeliu, Alberto},
	year = {2003},
	doi = {10.1007/978-3-540-44871-6_130},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1126--1138},
	file = {kuncheva 2003 - that elusive diversity in classifier ensembles - ENSEMBLE DIVERSITY - BDPG - ANNO.pdf:/Users/bill/D/Zotero/storage/SUEDJIHZ/kuncheva 2003 - that elusive diversity in classifier ensembles - ENSEMBLE DIVERSITY - BDPG - ANNO.pdf:application/pdf},
}

@article{kuncheva2003ml,
	title = {Measures of {Diversity} in {Classifier} {Ensembles} and {Their} {Relationship} with the {Ensemble} {Accuracy}},
	volume = {51},
	abstract = {Diversity among the members of a team of classiﬁers is deemed to be a key issue in classiﬁer combination. However, measuring diversity is not straightforward because there is no generally accepted formal deﬁnition. We have found and studied ten statistics which can measure diversity among binary classiﬁer outputs (correct or incorrect vote for the class label): four averaged pairwise measures (the Q statistic, the correlation, the disagreement and the double fault) and six non-pairwise measures (the entropy of the votes, the difﬁculty index, the Kohavi-Wolpert variance, the interrater agreement, the generalized diversity, and the coincident failure diversity). Four experiments have been designed to examine the relationship between the accuracy of the team and the measures of diversity, and among the measures themselves. Although there are proven connections between diversity and accuracy in some special cases, our results raise some doubts about the usefulness of diversity measures in building classiﬁer ensembles in real-life pattern recognition problems.},
	language = {en},
	journal = {Machine Learning},
	author = {Kuncheva, Ludmila I and Whitaker, Christopher J., },
	year = {2003},
	pages = {181--207},
	file = {kuncheva whitaker 2013 - Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy - ENSEMBLE DIVERSITY MEASURES - BDPG.PDF:/Users/bill/D/Zotero/storage/RN8TUKG6/kuncheva whitaker 2013 - Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy - ENSEMBLE DIVERSITY MEASURES - BDPG.PDF:application/pdf},
}

@article{shipp2002if,
	title = {Relationships between combination methods and measures of diversity in combining classifiers},
	volume = {3},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253502000519},
	doi = {10.1016/S1566-2535(02)00051-9},
	abstract = {This study looks at the relationships between diﬀerent methods of classiﬁer combination and diﬀerent measures of diversity. We considered 10 combination methods and 10 measures of diversity on two benchmark data sets. The relationship was sought on ensembles of three classiﬁers built on all possible partitions of the respective feature sets into subsets of pre-speciﬁed sizes. The only positive ﬁnding was that the Double-Fault measure of diversity and the measure of diﬃculty both showed reasonable correlation with Majority Vote and Naive Bayes combinations. Since both these measures have an indirect connection to the ensemble accuracy, this result was not unexpected. However, our experiments did not detect a consistent relationship between the other measures of diversity and the 10 combination methods. Ó 2002 Published by Elsevier Science B.V.},
	language = {en},
	number = {2},
	urldate = {2023-12-09},
	journal = {Information Fusion},
	author = {Shipp, Catherine A. and Kuncheva, Ludmila I.},
	month = jun,
	year = {2002},
	pages = {135--148},
	file = {shipp kuncheva 2002 - relationships between combination methods and measures of diversity in combining classifiers - ENSEMBLES - ENSEMBLE DIVERSITY MEASURES - BDPG.pdf:/Users/bill/D/Zotero/storage/VSFCQXGI/shipp kuncheva 2002 - relationships between combination methods and measures of diversity in combining classifiers - ENSEMBLES - ENSEMBLE DIVERSITY MEASURES - BDPG.pdf:application/pdf},
}

@article{zuur2016mee,
	title = {A protocol for conducting and presenting results of regression‐type analyses},
	volume = {7},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12577},
	doi = {10.1111/2041-210X.12577},
	abstract = {Summary
            
              
                
                  Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis of multifaceted interrelated data make obtaining more accurate and meaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming.
                
                
                  We offer a 10‐step protocol to streamline analysis of data that will enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending the model via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature.
                
                
                  Following this protocol will reduce the organization, analysis and presentation of what may be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.},
	language = {en},
	number = {6},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Zuur, Alain F. and Ieno, Elena N.},
	editor = {Freckleton, Robert},
	month = jun,
	year = {2016},
	pages = {636--645},
	file = {zuur ieno 2016 - A protocol for conducting and presenting results of regression‐type analyses - BDPG - GUPPY - ANNO.pdf:/Users/bill/D/Zotero/storage/F7EIHPYW/zuur ieno 2016 - A protocol for conducting and presenting results of regression‐type analyses - BDPG - GUPPY - ANNO.pdf:application/pdf},
}

@article{zuur2010mee,
	title = {A protocol for data exploration to avoid common statistical problems},
	volume = {1},
	issn = {2041210X},
	shorttitle = {A protocol for data exploration to avoid common statistical problems},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x},
	doi = {10.1111/j.2041-210X.2009.00001.x},
	language = {en},
	number = {1},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Zuur, Alain F. and Ieno, Elena N. and Elphick, Chris S.},
	month = mar,
	year = {2010},
	pages = {3--14},
	file = {zuur et al 2010 - A protocol for data exploration to avoid common statistical problems - BDPG - GUPPY - STATISTICS - REGRESSION - DATA EXPLORATION - OVERFITTING - KOALA.pdf:/Users/bill/D/Zotero/storage/44CC9EAB/zuur et al 2010 - A protocol for data exploration to avoid common statistical problems - BDPG - GUPPY - STATISTICS - REGRESSION - DATA EXPLORATION - OVERFITTING - KOALA.pdf:application/pdf},
}

@article{schielzeth2010mee,
	title = {Simple means to improve the interpretability of regression coefficients},
	volume = {1},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	abstract = {Summary
            
              1.
              Linear regression models are an important statistical tool in evolutionary and ecological studies. Unfortunately, these models often yield some uninterpretable estimates and hypothesis tests, especially when models contain interactions or polynomial terms. Furthermore, the standard errors for treatment groups, although often of interest for including in a publication, are not directly available in a standard linear model.
            
            
              2.
              Centring and standardization of input variables are simple means to improve the interpretability of regression coefficients. Further, refitting the model with a slightly modified model structure allows extracting the appropriate standard errors for treatment groups directly from the model.
            
            
              3.
              Centring will make main effects biologically interpretable even when involved in interactions and thus avoids the potential misinterpretation of main effects. This also applies to the estimation of linear effects in the presence of polynomials. Categorical input variables can also be centred and this sometimes assists interpretation.
            
            
              4.
              Standardization (
              z
              ‐transformation) of input variables results in the estimation of standardized slopes or standardized partial regression coefficients. Standardized slopes are comparable in magnitude within models as well as between studies. They have some advantages over partial correlation coefficients and are often the more interesting standardized effect size.
            
            
              5.
              The thoughtful removal of intercepts or main effects allows extracting treatment means or treatment slopes and their appropriate standard errors directly from a linear model. This provides a simple alternative to the more complicated calculation of standard errors from contrasts and main effects.
            
            
              6.
              The simple methods presented here put the focus on parameter estimation (point estimates as well as confidence intervals) rather than on significance thresholds. They allow fitting complex, but meaningful models that can be concisely presented and interpreted. The presented methods can also be applied to generalised linear models (GLM) and linear mixed models.},
	language = {en},
	number = {2},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	month = jun,
	year = {2010},
	pages = {103--113},
	file = {schielzeth 2010 - Simple means to improve the interpretability of regression coefficients - BDPG.pdf:/Users/bill/D/Zotero/storage/XWNKZRPL/schielzeth 2010 - Simple means to improve the interpretability of regression coefficients - BDPG.pdf:application/pdf},
}

@article{ives2022mee,
	title = {Random errors are neither: {On} the interpretation of correlated data},
	volume = {13},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Random errors are neither},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13971},
	doi = {10.1111/2041-210X.13971},
	abstract = {Abstract
            
              
                
                  Many statistical models currently used in ecology and evolution account for covariances among random errors. Here, I address five points: (i) correlated random errors unite many types of statistical models, including spatial, phylogenetic and time‐series models; (ii) random errors are neither unpredictable nor mistakes; (iii) diagnostics for correlated random errors are not useful, but simulations are; (iv) model predictions can be made with random errors; and (v) can random errors be causal?
                
                
                  These five points are illustrated by applying statistical models to analyse simulated spatial, phylogenetic and time‐series data. These three simulation studies are paired with three types of predictions that can be made using information from covariances among random errors: predictions for goodness‐of‐fit, interpolation, and forecasting.
                
                
                  In the simulation studies, models incorporating covariances among random errors improve inference about the relationship between dependent and independent variables. They also imply the existence of unmeasured variables that generate the covariances among random errors. Understanding the covariances among random errors gives information about possible processes underlying the data.
                
                
                  Random errors are caused by something. Therefore, to extract full information from data, covariances among random errors should not just be included in statistical models; they should also be studied in their own right. Data are hard won, and appropriate statistical analyses can make the most of them.},
	language = {en},
	number = {10},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Ives, Anthony R.},
	month = oct,
	year = {2022},
	pages = {2092--2105},
	file = {ives 2022 - Random errors are neither - on the interpretation of correlated data - BDPG.pdf:/Users/bill/D/Zotero/storage/VNDVS4YX/ives 2022 - Random errors are neither - on the interpretation of correlated data - BDPG.pdf:application/pdf},
}

@techreport{caruana2006,
	address = {Ithaca, New York},
	title = {Meta {Clustering}},
	abstract = {Clustering is ill-defined. Unlike supervised learning where labels lead to crisp performance criteria such as accuracy and squared error, clustering quality depends on how the clusters will be used. Devising clustering criteria that capture what users need is difficult. Most clustering algorithms search for one optimal clustering based on a prespecified clustering criterion. Once that clustering has been determined, no further clusterings are examined. Our approach differs in that we search for many alternate reasonable clusterings of the data, and then allow users to select the clustering(s) that best fit their needs. Any reasonable partitioning of the data is potentially useful for some purpose, regardless of whether or not it is optimal according to a specific clustering criterion. Our approach first finds a variety of reasonable clusterings. It then clusters this diverse set of clusterings so that users must only examine a small number of qualitatively different clusterings. In this paper, we present methods for automatically generating a diverse set of alternate clusterings, as well as methods for grouping clusterings into meta clusters. We evaluate meta clustering on four test problems, and then apply meta clustering to two case studies. Surprisingly, clusterings that would be of most interest to users often are not very compact clusterings.},
	number = {TR2006-2049},
	institution = {Cornell University},
	author = {Caruana, Rich and Elhawary, Mohamed and Nguyen, Nam and Smith, Casey},
	year = {2006},
	annote = {Also see the Meta Clustering project page at https://www.cs.cornell.edu/{\textasciitilde}nhnguyen/metaclustering.htm
},
	annote = {I don’t have access to the published version, but it was eventually published as:  R. Caruana, M. Elhawary, N. Nguyen and C. Smith, "Meta Clustering," Sixth International Conference on Data Mining (ICDM'06), Hong Kong, China, 2006, pp. 107-118, doi: 10.1109/ICDM.2006.103.
https://ieeexplore.ieee.org/document/4053039
},
	file = {TR2006-2049.pdf:/Users/bill/D/Zotero/storage/2BRLT4IA/TR2006-2049.pdf:application/pdf},
}

@article{nakagawa2013mee,
	title = {A general and simple method for obtaining \textit{{R}} $^{\textrm{2}}$ from generalized linear mixed‐effects models},
	volume = {4},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x},
	doi = {10.1111/j.2041-210x.2012.00261.x},
	abstract = {Summary
            
              
                
                  
                    The use of both linear and generalized linear mixed‐effects models (
                    LMM
                    s and
                    GLMM
                    s) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (
                    AIC
                    ), are usually presented as model comparison tools for mixed‐effects models.
                  
                
                
                  
                    The presentation of ‘variance explained’ (
                    R
                    2
                    ) as a relevant summarizing statistic of mixed‐effects models, however, is rare, even though
                    R
                    2
                    is routinely reported for linear models (
                    LM
                    s) and also generalized linear models (
                    GLM
                    s).
                    R
                    2
                    has the extremely useful property of providing an absolute value for the goodness‐of‐fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained,
                    R
                    2
                    can also be a quantity of biological interest.
                  
                
                
                  
                    One reason for the under‐appreciation of
                    R
                    2
                    for mixed‐effects models lies in the fact that
                    R
                    2
                    can be defined in a number of ways. Furthermore, most definitions of
                    R
                    2
                    for mixed‐effects have theoretical problems (e.g. decreased or negative
                    R
                    2
                    values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation).
                  
                
                
                  
                    Here, we make a case for the importance of reporting
                    R
                    2
                    for mixed‐effects models. We first provide the common definitions of
                    R
                    2
                    for
                    LM
                    s and
                    GLM
                    s and discuss the key problems associated with calculating
                    R
                    2
                    for mixed‐effects models. We then recommend a general and simple method for calculating two types of
                    R
                    2
                    (marginal and conditional
                    R
                    2
                    ) for both
                    LMM
                    s and
                    GLMM
                    s, which are less susceptible to common problems.
                  
                
                
                  
                    This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed‐effects models. The proposed method has the potential to facilitate the presentation of
                    R
                    2
                    for a wide range of circumstances.},
	language = {en},
	number = {2},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Nakagawa, Shinichi and Schielzeth, Holger},
	editor = {O'Hara, Robert B.},
	month = feb,
	year = {2013},
	pages = {133--142},
	file = {nakagawa schielzeth 2013 - A general and simple method for obtaining R2 from generalized linear mixed‐effects models.pdf:/Users/bill/D/Zotero/storage/TKC8HMJA/nakagawa schielzeth 2013 - A general and simple method for obtaining R2 from generalized linear mixed‐effects models.pdf:application/pdf},
}

@article{schielzeth2013mee,
	title = {Nested by design: model fitting and interpretation in a mixed model era},
	volume = {4},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Nested by design},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00251.x},
	doi = {10.1111/j.2041-210x.2012.00251.x},
	abstract = {Summary
            
              
                
                  Nested data structures are ubiquitous in the study of ecology and evolution, and such structures need to be modelled appropriately. Mixed‐effects models offer a powerful framework to do so. Nested effects can usually be fitted using the syntax for crossed effects in mixed models, provided that the coding reflects implicit nesting. But the experimental design (either nested or crossed) affects the interpretation of the results.
                
                
                  The key difference between nested and crossed effects in mixed models is the estimation and interpretation of the interaction variance. With nested data structures, the interaction variance is pooled with the main effect variance of the nested factor. Crossed designs are required to separate the two components. This difference between nested and crossed data is determined by the experimental design (thus by the nature of data sets) and not by the coding of the statistical model.
                
                
                  Data can be nested by design in the sense that it would have been technically feasible and biologically relevant to collect the data in a crossed design. In such cases, the pooling of the variances needs to be clearly acknowledged. In other situations, it might be impractical or even irrelevant to apply a crossed design. We call such situations naturally nested, a case in which the pooling of the interaction variance will be less of an issue.
                
                
                  The interpretation of results should reflect the fact that the interaction variance inflates the main effect variance when dealing with nested data structures. Whether or not this distinction is critical depends on the research question and the system under study.
                
                
                  
                    We present mixed models as a particularly useful tool for analysing nested designs, and we highlight the value of the estimated random variance as a quantity of biological interest. Important insights can be gained if random‐effect variances are appropriately interpreted. We hope that our paper facilitates the transition from classical
                    anova
                    s to mixed models in dealing with categorical data.},
	language = {en},
	number = {1},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger and Nakagawa, Shinichi},
	editor = {Freckleton, Robert},
	month = jan,
	year = {2013},
	pages = {14--24},
	file = {schielzeth nakagawa 2013 - Nested by design model fitting and interpretation in a mixed model era.pdf:/Users/bill/D/Zotero/storage/BGAG3ZFM/schielzeth nakagawa 2013 - Nested by design model fitting and interpretation in a mixed model era.pdf:application/pdf},
}

@article{schielzeth,
	title = {Robustness of linear mixed‐effects models to violations of distributional assumptions},
	language = {en},
	author = {Schielzeth, Holger and Dingemanse, Niels J and Nakagawa, Shinichi and Westneat, David F and Allegue, Hassen and Teplitsky, Céline and Réale, Denis and Dochtermann, Ned A and Garamszegi, László Zsolt and Araya-Ajoy, Yimen G},
	file = {schielzeth et al 2020 - Robustness of linear mixed‐effects models to violations of distributional assumptions.pdf:/Users/bill/D/Zotero/storage/ZNE7FU2T/schielzeth et al 2020 - Robustness of linear mixed‐effects models to violations of distributional assumptions.pdf:application/pdf},
}

@article{blasco-moreno2019mee,
	title = {What does a zero mean? {Understanding} false, random and structural zeros in ecology},
	volume = {10},
	issn = {2041-210X, 2041-210X},
	shorttitle = {What does a zero mean?},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13185},
	doi = {10.1111/2041-210X.13185},
	abstract = {Abstract
            
              
                
                  Zeros (i.e. events that do not happen) are the source of two common phenomena in count data: overdispersion and zero‐inflation. Zeros have multiple origins in a dataset: false zeros occur due to errors in the experimental design or the observer; structural zeros are related to the ecological or evolutionary restrictions of the system under study; and random zeros are the result of the sampling variability. Identifying the type of zeros and their relation with overdispersion and/or zero inflation is key to select the most appropriate statistical model.
                
                
                  Here we review the different modelling options in relation to the presence of overdispersion and zero inflation, tested through the dispersion and zero inflation indices. We then examine the theory of the zero‐inflated (ZI) models and the use of the score tests to assess overdispersion and zero inflation over a model.
                
                
                  In order to choose an adequate model when analysing count data we suggest the following protocol: Step 1) classify the zeros and minimize the presence of false zeros; Step 2) identify suitable covariates; Step 3) test the data for overdispersion and zero‐inflation and Step 4) choose the most adequate model based on the results of step 3 and use score tests to determine whether more complex models should be implemented.
                
                
                  We applied the recommended protocol on a real dataset on plant–herbivore interactions to evaluate the suitability of six different models (Poisson, NB and their zero‐inflated versions—ZIP, ZINB). Our data were overdispersed and zero‐inflated, and the ZINB was the model with the best fit, as predicted.
                
                
                  Ignoring overdispersion and/or zero inflation during data analyses caused biased estimates of the statistical parameters and serious errors in the interpretation of the results. Our results are a clear example on how the conclusions of an ecological hypothesis can change depending on the model applied. Understanding how zeros arise in count data, for example identifying the potential sources of structural zeros, is essential to select the best statistical design. A good model not only fits the data correctly but also takes into account the idiosyncrasies of the biological system.
                
              
            
          , 
            Resum
            
              
                
                  Els zeros (és a dir, successos que no s'esdevenen) són la font de dos fenòmens comuns en les dades de recompte: la sobredispersió i la zero inflació. L'origen dels zeros pot ser divers: els zeros falsos són el resultat d'errors en el disseny experimental o en l'observador; els zeros estructurals es relacionen amb les restriccions ecològiques o evolutives del sistema d'estudi; i els zeros aleatoris s'esdevenen per la variabilitat en el mostreig. Identificar els tipus de zeros i la seva relació amb la sobredispersió i/o la zero inflació és clau per seleccionar el model estadístic més apropiat.
                
                
                  En aquest article hem revisat les diferents opcions per modelar dades amb sobredispersió i zero inflació, característiques que hem determinat mitjançant els índex de sobredispersió i zero inflació. Hem revisat la teoria dels models zero‐inflats (ZI) i l’ús dels score tests per determinar sobredispersió i zero inflació sobre un model.
                
                
                  Per tal d'escollir el model estadístic més adequat quan analitzem dades de recomptes, suggerim aplicar el següent protocol: Pas 1) classificar els zeros i minimitzar la presència de zeros falsos; Pas 2) identificar les covariables adequades; Pas 3) comprovar si les dades estan sobredispersades o zero inflades; i Pas 4) escollir el model estadístic més adequat en base als resultats obtinguts al pas 3, i aplicar els score tests per determinar si cal implementar altres models més complexes.
                
                
                  Hem aplicat el protocol recomanat en unes dades reals d'interaccions planta‐herbívor per avaluar l'adequació de sis models diferents (Poisson, NB i les seves versions zero‐inflades—ZIP, ZINB). Les dades estaven sobredipersades i zero inflades, i el model ZINB oferia el millor ajust, tal com preveiem.
                
                
                  Quan ignoravem la sobredipsersió i/o la zero inflació en l'anàlisi de les dades l'estima dels paràmetres estadístics resultava esbiaixada, fet que provocava errors seriosos en la interpretació dels resultats. Els nostres resultats són un clar exemple de com les conclusions d'una hipòtesi ecològica poden canviar depenent del model estadístic aplicat. Per seleccionar el millor disseny estadístic és essencial entendre com es generen els zeros, per exemple identificant fonts potencials de zeros estructurals. Un bon model estadístic no només s'ha d'ajustar a les dades correctament sinó que també ha de contemplar les idiosincràsies del sistema biològic.},
	language = {en},
	number = {7},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Blasco‐Moreno, Anabel and Pérez‐Casany, Marta and Puig, Pedro and Morante, Maria and Castells, Eva},
	editor = {O'Hara, Robert B.},
	month = jul,
	year = {2019},
	pages = {949--959},
	file = {blasco‐moreno et al 2019 - What does a zero mean - understanding false random and structural zeros in ecology.pdf:/Users/bill/D/Zotero/storage/UC37889D/blasco‐moreno et al 2019 - What does a zero mean - understanding false random and structural zeros in ecology.pdf:application/pdf},
}

@article{douma2019mee,
	title = {Analysing continuous proportions in ecology and evolution: {A} practical introduction to beta and {Dirichlet} regression},
	volume = {10},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Analysing continuous proportions in ecology and evolution},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13234},
	doi = {10.1111/2041-210X.13234},
	abstract = {Abstract
            
              
                
                  Proportional data, in which response variables are expressed as percentages or fractions of a whole, are analysed in many subfields of ecology and evolution. The scale‐independence of proportions makes them appropriate to analyse many biological phenomena, but statistical analyses are not straightforward, since proportions can only take values from zero to one and their variance is usually not constant across the range of the predictor. Transformations to overcome these problems are often applied, but can lead to biased estimates and difficulties in interpretation.
                
                
                  In this paper, we provide an overview of the different types of proportional data and discuss the different analysis strategies available. In particular, we review and discuss the use of promising, but little used, techniques for analysing continuous (also called non‐count‐based or non‐binomial) proportions (e.g. percent cover, fraction time spent on an activity): beta and Dirichlet regression, and some of their most important extensions.
                
                
                  A major distinction can be made between proportions arising from counts and those arising from continuous measurements. For proportions consisting of two categories, count‐based data are best analysed using well‐developed techniques such as logistic regression, while continuous proportions can be analysed with beta regression models. In the case of {\textgreater}2 categories, multinomial logistic regression or Dirichlet regression can be applied. Both beta and Dirichlet regression techniques model proportions at their original scale, which makes statistical inference more straightforward and produce less biased estimates relative to transformation‐based solutions. Extensions to beta regression, such as models for variable dispersion, zero‐one augmented data and mixed effects designs have been developed and are reviewed and applied to case studies. Finally, we briefly discuss some issues regarding model fitting, inference, and reporting that are particularly relevant to beta and Dirichlet regression.
                
                
                  
                    Beta regression and Dirichlet regression overcome some problems inherent in applying classic statistical approaches to proportional data. To facilitate the adoption of these techniques by practitioners in ecology and evolution, we present detailed, annotated demonstration scripts covering all variations of beta and Dirichlet regression discussed in the article, implemented in the freely available language for statistical computing,
                    r
                    .
                  
                
              
            
          , 
            Foreign Language Abstract 抽象
            
              
                
                  在生态学和进化学的许多子领域中分析比例数据时,其中的响应变量被表示为整体的百分比或分数。比例相对于数据尺度的独立性使得其适用于分析许多生物学现象。但是由于比例只能从0到1取值,并且它们的方差在预测值的范围内通常不恒定,使得统计分析结果不具有直观性。为了克服上述问题,研究者通常采用数学变换等方法,但也可能导致有有偏估计和解释上的困难。
                
                
                  在本文中,我们概述了不同类型的比例数据,并讨论了现行的不同类型的分析方法,特别是一些用来分析连续(也称为非计数或非二项式)比例(例如,百分比覆盖,动物特定行为时间比例):β回归和Dirichlet回归,以及它们最重要的一些扩展。目前,虽然这些方法的使用范围较窄,但是我们认为它们有着广泛的应用前景。
                
                
                  可以对计数产生的比例和连续测量产生的比例进行区分。对于由2个类别组成的比例,若数据是计数的,可以使用例如逻辑回归等完善的方法进行分析,若数据是连续的,可以使用β回归模型。对于类别大于2的比例,可以使用多项逻辑回归或Dirichlet回归。β回归和Dirichlet回归均在数据的原始尺度上对比例建模,这不仅使得统计推断更加简单,并且相对数学变换的方法能产生更少的有偏估计。我们对β回归的扩展方法做了概述,例如变量扩散模型、0‐1增强数据和混合效应设计,并将这些方法应用于案例研究。最后,我们简要地讨论了与β回归和Dirichlet回归特别相关的模型拟合和统计推断。
                
                
                  β回归和Dirichlet回归克服了将经典统计方法应用于比例数据时固有的一些问题。为了帮助生态学和进化学研究者采用这些技术,我们提供了详细的、带注释的演示脚本,涵盖了文章中讨论的所有β回归和Dirichlet回归变体,可以用免费的统计学计算机语言R来实现。},
	language = {en},
	number = {9},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Douma, Jacob C. and Weedon, James T.},
	editor = {Warton, David},
	month = sep,
	year = {2019},
	pages = {1412--1430},
	file = {douma weedon 2019 - Analysing continuous proportions in ecology and evolution A practical introduction to beta and Dirichlet regression.pdf:/Users/bill/D/Zotero/storage/HIRKIXWR/douma weedon 2019 - Analysing continuous proportions in ecology and evolution A practical introduction to beta and Dirichlet regression.pdf:application/pdf},
}

@article{wang2012mee,
	title = {mvabund– an {R} package for model‐based analysis of multivariate abundance data},
	volume = {3},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00190.x},
	doi = {10.1111/j.2041-210X.2012.00190.x},
	abstract = {Summary
            
              1.
               The
              mvabund
              package for
              R
              provides tools for model‐based analysis of multivariate abundance data in ecology.
            
            
              2.
               This includes methods for visualising data, fitting predictive models, checking model assumptions, as well as testing hypotheses about the community–environment association.
            
            
              3.
               This paper briefly introduces the package and demonstrates its functionality by example.},
	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Wang, Yi and Naumann, Ulrike and Wright, Stephen T. and Warton, David I.},
	month = jun,
	year = {2012},
	pages = {471--474},
	file = {wang et al 2012 - mvabund an R package for model‐based analysis of multivariate abundance data.pdf:/Users/bill/D/Zotero/storage/B33NRV53/wang et al 2012 - mvabund an R package for model‐based analysis of multivariate abundance data.pdf:application/pdf},
}

@article{hudson2013mee,
	title = {Cheddar: analysis and visualisation of ecological communities in {R}},
	volume = {4},
	issn = {2041-210X, 2041-210X},
	shorttitle = {Cheddar},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12005},
	doi = {10.1111/2041-210X.12005},
	abstract = {Summary
            
              
                
                  There has been a lack of software available to ecologists for the management, visualisation and analysis of ecological community and food web data. Researchers have been forced to implement their own data formats and software, often from scratch, resulting in duplicated effort and bespoke solutions that are difficult to apply to future analyses and comparative studies.
                
                
                  We introduce Cheddar – an R package that provides standard, transparent implementations of a wide range of food web and community‐level analyses and plots, focussing on ecological network data that are augmented with estimates of body mass and/or numerical abundance.
                
                
                  The package allows analysis of individual communities, as well as collections of communities, allowing examination of changes in structure through time, across environmental gradients, or due to experimental manipulations. Several commonly analysed food web data sets are included and used in worked examples.
                
                
                  This is the first time these important features have been combined in a single package that helps improve research efficiency and serves as a unified framework for future development.},
	language = {en},
	number = {1},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Hudson, Lawrence N. and Emerson, Rob and Jenkins, Gareth B. and Layer, Katrin and Ledger, Mark E. and Pichler, Doris E. and Thompson, Murray S. A. and O'Gorman, Eoin J. and Woodward, Guy and Reuman, Daniel C.},
	editor = {Spencer, Mathew},
	month = jan,
	year = {2013},
	pages = {99--104},
	file = {hudson et al 2013 - Cheddar analysis and visualisation of ecological communities in R.pdf:/Users/bill/D/Zotero/storage/VH2UY9YG/hudson et al 2013 - Cheddar analysis and visualisation of ecological communities in R.pdf:application/pdf},
}

@article{adams2013mee,
	title = {geomorph: an {\textless}span style="font-variant:small-caps;"{\textgreater}r{\textless}/span{\textgreater} package for the collection and analysis of geometric morphometric shape data},
	volume = {4},
	issn = {2041-210X, 2041-210X},
	shorttitle = {geomorph},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12035},
	doi = {10.1111/2041-210X.12035},
	abstract = {Summary
            
              
                
                  
                    Many ecological and evolutionary studies seek to explain patterns of shape variation and its covariation with other variables. Geometric morphometrics is often used for this purpose, where a set of shape variables are obtained from landmark coordinates following a
                    P
                    rocrustes superimposition.
                  
                
                
                  
                    We introduce geomorph: a software package for performing geometric morphometric shape analysis in the
                    r
                    statistical computing environment.
                  
                
                
                  
                    Geomorph provides routines for all stages of landmark‐based geometric morphometric analyses in two and three‐dimensions. It is an open source package to read, manipulate, and digitize landmark data, generate shape variables via
                    P
                    rocrustes analysis for points, curves and surfaces, perform statistical analyses of shape variation and covariation, and to provide graphical depictions of shapes and patterns of shape variation. An important contribution of geomorph is the ability to perform
                    P
                    rocrustes superimposition on landmark points, as well as semilandmarks from curves and surfaces.
                  
                
                
                  A wide range of statistical methods germane to testing ecological and evolutionary hypotheses of shape variation are provided. These include standard multivariate methods such as principal components analysis, and approaches for multivariate regression and group comparison. Methods for more specialized analyses, such as for assessing shape allometry, comparing shape trajectories, examining morphological integration, and for assessing phylogenetic signal, are also included.
                
                
                  Several functions are provided to graphically visualize results, including routines for examining variation in shape space, visualizing allometric trajectories, comparing specific shapes to one another and for plotting phylogenetic changes in morphospace.
                
                
                  
                    Finally, geomorph participates to make available advanced geometric morphometric analyses through the
                    r
                    statistical computing platform.},
	language = {en},
	number = {4},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Adams, Dean C. and Otárola‐Castillo, Erik},
	editor = {Paradis, Emmanuel},
	month = apr,
	year = {2013},
	pages = {393--399},
	file = {adams otarola-castillo 2013 - geomorph an r package for the collection and analysis of geometric morphometric shape data.pdf:/Users/bill/D/Zotero/storage/N96EYLPG/adams otarola-castillo 2013 - geomorph an r package for the collection and analysis of geometric morphometric shape data.pdf:application/pdf},
}

@article{thiele2012mee,
	title = {{RNETLOGO}: an {R} package for running and exploring individual‐based models implemented in {NETLOGO}},
	volume = {3},
	issn = {2041-210X, 2041-210X},
	shorttitle = {{RNETLOGO}},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00180.x},
	doi = {10.1111/j.2041-210X.2011.00180.x},
	abstract = {Summary
            
              1.
               
              NetLogo
              is a free software platform for implementing individual‐based and agent‐based models. However,
              NetLogo
              ’s support of systematic design, performance and analysis of simulation experiments is limited. The statistics software R includes such support.
            
            
              2.
               
              RNetLogo
              is an R package that links R and
              NetLogo
              : any
              NetLogo
              program can be controlled and run from R and model results can be transferred back to R for statistical analyses.
              RNetLogo
              includes 16 functions, which are explained and demonstrated in the user manual and tutorial. The design of
              RNetLogo
              was inspired by a similar link between Mathematica and
              NetLogo
              .
            
            
              3.
               
              RNetLogo
              is a powerful tool for making individual‐based modelling more efficient and less
              ad hoc
              . It links two fast growing user communities and constitutes a new interface for integrating descriptive statistical analyses and individual‐based modelling.},
	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Thiele, Jan C. and Kurth, Winfried and Grimm, Volker},
	month = jun,
	year = {2012},
	pages = {480--483},
	file = {thiele kurth grimm 2012 - RNETLOGO an R package for running and exploring individual‐based models implemented in NetLogo.pdf:/Users/bill/D/Zotero/storage/NXV95I4Z/thiele kurth grimm 2012 - RNETLOGO an R package for running and exploring individual‐based models implemented in NetLogo.pdf:application/pdf},
}

@article{casanoves2011mee,
	title = {{FDiversity}: a software package for the integrated analysis of functional diversity},
	volume = {2},
	issn = {2041-210X, 2041-210X},
	shorttitle = {{FDiversity}},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00082.x},
	doi = {10.1111/j.2041-210X.2010.00082.x},
	abstract = {Summary
            
              1.
               The growing interest in functional diversity has been accompanied by a proliferation of indices proposed to calculate its different components; however, empirical studies have been hampered by a lack of integrated tools for their easy calculation based on field data sets.
            
            
              2.
               We present FDiversity, a free, user‐friendly, open source‐based software package for the calculation and integrated statistical analysis of most functional diversity indices and metrics published to date.
            
            
              3.
               This tool greatly facilitates the analysis of functional diversity patterns and also the links of different dimensions of functional diversity with environmental factors and ecosystem properties and services.},
	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Casanoves, Fernando and Pla, Laura and Di Rienzo, Julio A. and Díaz, Sandra},
	month = jun,
	year = {2011},
	pages = {233--237},
	file = {casanoves et al 2011 - FDiversity a software package for the integrated analysis of functional diversity - FUNCTIONAL DIVERSITY.pdf:/Users/bill/D/Zotero/storage/4VYN5EUE/casanoves et al 2011 - FDiversity a software package for the integrated analysis of functional diversity - FUNCTIONAL DIVERSITY.pdf:application/pdf},
}

@article{tingley2020mee,
	title = {Multi‐species occupancy models as robust estimators of community richness},
	volume = {11},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13378},
	doi = {10.1111/2041-210X.13378},
	abstract = {Abstract
            
              
                
                  
                    Understanding patterns of diversity is central to ecology and conservation, yet estimates of diversity are often biased by imperfect detection. In recent years, multi‐species occupancy models (MSOM) have been developed as a statistical tool to account for species‐specific heterogeneity in detection while estimating true measures of diversity. Although the power of these models has been tested in various ways, their ability to estimate gamma diversity—or true community size,
                    N
                    is a largely unrecognized feature that needs rigorous evaluation.
                  
                
                
                  
                    We use both simulations and an empirical dataset to evaluate the bias, precision, accuracy and coverage of estimates of
                    N
                    from MSOM compared to the widely applied iChao2 non‐parametric estimator. We simulated 5,600 datasets across seven scenarios of varying average occupancy and detectability covariates, as well as varying numbers of sites, replicates and true community size. Additionally, we use a real dataset of surveys over 9 years (where species accumulation reached an asymptote, indicating true
                    N
                    ), to estimate
                    N
                    from each annual survey.
                  
                
                
                  
                    Simulations showed that both MSOM and iChao2 estimators are generally accurate (i.e. unbiased and precise) except under unideal scenarios where mean species occupancy is low. In such scenarios, MSOM frequently overestimated
                    N
                    . Across all scenarios, MSOM estimates were less certain than iChao2, but this led to over‐confident iChao2 estimates that showed poor coverage. Results from the real dataset largely confirmed the simulation findings, with MSOM estimates showing greater accuracy and coverage than iChao2.
                  
                
                
                  
                    Community ecologists have a wide choice of analytical methods, and both iChao2 and MSOM estimates of
                    N
                    are substantially preferable to raw species counts. The simplicity of non‐parametric estimators has obvious advantages, but our results show that in many cases, MSOM may provide superior estimates that also account more accurately for uncertainty. Both methods can show strong bias when average occupancy is very low, and practitioners should show caution when using estimates derived from either method under such conditions.},
	language = {en},
	number = {5},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Tingley, Morgan W. and Nadeau, Christopher P. and Sandor, Manette E.},
	editor = {Baselga, Andrés},
	month = may,
	year = {2020},
	pages = {633--642},
	file = {tingley et al 2020 - Multi‐species occupancy models as robust estimators of community richness.pdf:/Users/bill/D/Zotero/storage/BFCJEMAL/tingley et al 2020 - Multi‐species occupancy models as robust estimators of community richness.pdf:application/pdf},
}

@article{barido-sottani2019mee,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{FossilSim}{\textless}/span{\textgreater} : {An} {\textless}span style="font-variant:small-caps;"{\textgreater}r{\textless}/span{\textgreater} package for simulating fossil occurrence data under mechanistic models of preservation and recovery},
	volume = {10},
	issn = {2041-210X, 2041-210X},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13170},
	doi = {10.1111/2041-210X.13170},
	abstract = {Abstract
            
              
                
                  Key features of the fossil record that present challenges for integrating palaeontological and phylogenetic datasets include (i) non‐uniform fossil recovery, (ii) stratigraphic age uncertainty and (iii) inconsistencies in the definition of species origination and taxonomy.
                
                
                  
                    We present an
                    r
                    package
                    FossilSim
                    that can be used to simulate and visualise fossil data for phylogenetic analysis under a range of flexible models. The package includes interval‐, environment‐ and lineage‐dependent models of fossil recovery that can be combined with models of stratigraphic age uncertainty and species evolution.
                  
                
                
                  
                    The package input and output can be used in combination with the wide range of existing phylogenetic and palaeontological
                    r
                    packages. We also provide functions for converting between
                    FossilSim
                    and
                    paleotree
                    objects.
                  
                
                
                  Simulated datasets provide enormous potential to assess the performance of phylogenetic methods and to explore the impact of using fossil occurrence databases on parameter estimation in macroevolution.},
	language = {en},
	number = {6},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Barido‐Sottani, Joëlle and Pett, Walker and O’Reilly, Joseph E. and Warnock, Rachel C. M.},
	editor = {Hsiang Liow, Lee},
	month = jun,
	year = {2019},
	pages = {835--840},
	file = {Barido‐Sottani et al 2019 - FossilSim An r package for simulating fossil occurrence data under mechanistic models of preservation and recovery - GUPPY - PALEO - JACK WILLIAMS.pdf:/Users/bill/D/Zotero/storage/P6X6F287/Barido‐Sottani et al 2019 - FossilSim An r package for simulating fossil occurrence data under mechanistic models of preservation and recovery - GUPPY - PALEO - JACK WILLIAMS.pdf:application/pdf},
}

@article{dorosariopetrucci2022mee,
	title = {paleobuddy: {An} {R} package for flexible simulations of diversification and fossil sampling},
	volume = {13},
	issn = {2041-210X, 2041-210X},
	shorttitle = {paleobuddy},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13996},
	doi = {10.1111/2041-210X.13996},
	abstract = {Abstract
            
              
                
                  Simulations are powerful tools for investigating evolutionary questions. Statistical methods for the analysis of diversification and fossil sampling rates require extensive testing through simulation. Simulations also aid in the investigation of evolutionary dynamics that lack analytical solutions, besides model adequacy tests. Flexible simulation of fossil records and phylogenies is a necessary tool to fully investigate diversity through time.
                
                
                  paleobuddy is an R package that aims to fill several gaps between current simulators by providing a more general framework, with flexible, robust and independent simulations of birth–death processes, fossil records and phylogenetic trees, while allowing for straightforward implementation of novel scenarios.
                
                
                  Here we describe the main algorithms involved in paleobuddy simulations, give a summary of the capabilities of the package in terms of evolutionary scenarios and visualization and illustrate a paleobuddy workflow.
                
                
                  paleobuddy expands the available parameter space to investigate complex inference models. Given that it allows for simulations of phylogenies (complete, reconstructed or ultrametric) and fossil records from the same underlying birth–death simulations, it can increase our understanding about the differences in palaeontological and neontological approaches, and consequently expand our capacity of analysing a combination of evidence from extinct and extant species. Finally, the focus on birth–death simulations with budding speciation events allows for more generality in the diversification processes considered, expanding the flexibility and capabilities of simulation in the field.},
	language = {en},
	number = {12},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Do Rosario Petrucci, Bruno and Januario, Matheus and Quental, Tiago},
	month = dec,
	year = {2022},
	pages = {2692--2698},
	file = {petrucci et al 2022 - paleobuddy An R package for flexible simulations of diversification and fossil sampling - GUPPY - PALEO - JACK WILLIAMS.pdf:/Users/bill/D/Zotero/storage/ZU2ALPSJ/petrucci et al 2022 - paleobuddy An R package for flexible simulations of diversification and fossil sampling - GUPPY - PALEO - JACK WILLIAMS.pdf:application/pdf},
}

@article{juodakis2022mee,
	title = {Wind‐robust sound event detection and denoising for bioacoustics},
	volume = {13},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13928},
	doi = {10.1111/2041-210X.13928},
	abstract = {Abstract
            
              
                
                  Sound recordings are used in various ecological studies, including wildlife monitoring by acoustic surveys. Such surveys often require automatic detection of target sound events in the large amount of data produced. However, current processing methods, especially those relying on sound intensity for detection, are severely impacted by wind, which causes transient intensity peaks. The rapid dynamics of this noise invalidate standard noise estimators, and no satisfactory method for dealing with wind exists in bioacoustics, where simple training and generalization between conditions are important.
                
                
                  We estimate the transient noise level by fitting short‐term spectrum models to a wavelet packet representation. This estimator is then combined with log‐spectral subtraction to stabilize the background level. The resulting adjusted wavelet series can be analysed by standard detectors. We use real data from long‐term acoustic monitoring to tune this workflow, demonstrate its denoising capabilities and test the improved detection in two population surveys of birds.
                
                
                  The proposed short‐term estimator was more effective than standard (constant) noise estimates in both denoising and detection tasks. In the surveys, the noise‐robust workflow greatly reduced the number of false alarms. As a result, the survey efficiency (precision of the estimated call density) improved for both species.
                
                
                  In contrast to existing methods, the proposed estimator can adjust for transient broadband noises without requiring additional hardware or extensive tuning to each species. It improved the detection workflow based on very little training data, making it particularly attractive for detection of rare species or general soundscape analysis.},
	language = {en},
	number = {9},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Juodakis, Julius and Marsland, Stephen},
	month = sep,
	year = {2022},
	pages = {2005--2017},
	file = {juodakis marsland 2022 - Wind‐robust sound event detection and denoising for bioacoustics.pdf:/Users/bill/D/Zotero/storage/TKHEGHNF/juodakis marsland 2022 - Wind‐robust sound event detection and denoising for bioacoustics.pdf:application/pdf},
}

@article{rhinehart2022mee,
	title = {A continuous‐score occupancy model that incorporates uncertain machine learning output from autonomous biodiversity surveys},
	volume = {13},
	issn = {2041-210X, 2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13905},
	doi = {10.1111/2041-210X.13905},
	abstract = {Abstract
            
              
                
                  Ecologists often study biodiversity by evaluating species occupancy and the relationship between occupancy and other covariates. Occupancy models are now widely used to account for false absences in field surveys and to reduce bias in estimates of covariate relationships. Existing occupancy models take as inputs binary detection/non‐detection observations of species at each visit to each site. However, autonomous sensing devices and machine learning models are increasingly used to survey biodiversity, generating a new type of observation record (i.e. continuous‐score data) that reflects the model's confidence a species is present in each autonomously sensed file, instead of binary detection/non‐detection data. These data are not directly compatible with traditional binary occupancy modelling methods.
                
                
                  Here, we develop a new occupancy model that models continuous scores on a visit level as a Gaussian mixture, combining a distribution of scores for files that do contain the species of interest and a distribution of scores for files that do not. The model takes as input continuous scores for each autonomously sensed and classified file, along with an optional small number of binary, manually verified detection and non‐detection annotations.
                
                
                  We present a simulation study that shows that over a range of empirically realistic parameters, our model outperforms traditional occupancy models that are based on binary annotation alone. We also apply this new model to an empirical case study using data generated from five machine learning classifiers applied to autonomous acoustic recordings gathered in the eastern United States.
                
                
                  Because our occupancy model generalizes allowable input data beyond binary observations, it is particularly well‐suited to the increasing volume of machine learning classified data in ecology and conservation.},
	language = {en},
	number = {8},
	urldate = {2023-12-12},
	journal = {Methods in Ecology and Evolution},
	author = {Rhinehart, Tessa A. and Turek, Daniel and Kitzes, Justin},
	month = aug,
	year = {2022},
	pages = {1778--1789},
	file = {rhinehart et al 2022 - A continuous‐score occupancy model that incorporates uncertain machine learning.pdf:/Users/bill/D/Zotero/storage/TBG8XYZY/rhinehart et al 2022 - A continuous‐score occupancy model that incorporates uncertain machine learning.pdf:application/pdf},
}

@article{jinnai2012,
	title = {A new optimization method of the geometric distance in an automatic recognition system for bird vocalisations},
	language = {en},
	author = {Jinnai, Michihiro and Boucher, Neil and Fukumi, Minoru and Taylor, Hollis},
	year = {2012},
	file = {jinnai ... HOLLIS TAYLOR 2012 - A new optimization method of the geometric distance in an automatic recognition system for bird vocalisations.pdf:/Users/bill/D/Zotero/storage/38J8HYIY/jinnai ... HOLLIS TAYLOR 2012 - A new optimization method of the geometric distance in an automatic recognition system for bird vocalisations.pdf:application/pdf},
}

@article{taylor2015cmr,
	title = {Bowing {Australia}'s {Outback} {Fences}: {A} {Sonic} {Cartography}},
	volume = {34},
	issn = {0749-4467, 1477-2256},
	shorttitle = {Bowing {Australia}'s {Outback} {Fences}},
	url = {http://www.tandfonline.com/doi/full/10.1080/07494467.2016.1140474},
	doi = {10.1080/07494467.2016.1140474},
	abstract = {Environmentalist and composer Murray Schafer understands soundscape compositions as ‘wide-angle tableaux; the composer observes the landscape at a distance. Nature performs and he provides the secretarial services. Only in the landscapes of the romantic era does the composer intrude to colour nature with his own personality or moods’ (Schafer, 1977, p. 105). Bowing fences forces us to close the distance gap, so Schafer’s understanding of distance is not applicable to our embodied, too-hot-or-toocold fence excursions that present regular challenges to personal comfort and safety. Our video footage may feature close-ups of hands and bows, but the sweeping outward zooms reveal a harsh and desolate land. We imagine ourselves as cartographers compiling an audio-visual map of the great fences of Australia.},
	language = {en},
	number = {4},
	urldate = {2023-12-12},
	journal = {Contemporary Music Review},
	author = {Taylor, Hollis},
	month = jul,
	year = {2015},
	pages = {350--363},
	file = {HOLLIS TAYLOR 2015 - Bowing Australia's Outback Fences- A Sonic Cartography.pdf:/Users/bill/D/Zotero/storage/RB3VM6JM/HOLLIS TAYLOR 2015 - Bowing Australia's Outback Fences- A Sonic Cartography.pdf:application/pdf},
}

@article{xing2021,
	title = {Examining song complexity of {Australian} pied butcherbirds},
	abstract = {Songbird vocal communication signals (birdsongs) often evolve in secondary sexual selection contexts and are shaped by female preferences. Such female preferences to male songs are typically linked to information-theoretic features of song complexity such as repertoire size or syntax of discrete song elements. Recent discoveries of similar structures between birdsong and human music suggest, however, that musical features of song complexity may also be relevant to female preferences. Australian pied butcherbirds (Cracticus nigrogularis) exhibit songs with recombinatory syntax and structural similarities to human music, but the depth of their song complexity remains ill-defined. To holistically examine pied butcherbird song complexity, we characterized relevant features of song complexity through a combination of symbolic sequence and musical rhythm analysis. This integrated approach reveals long-range dependencies in pied butcherbird songs, in which song elements are dependent on song history, and strong categorical song rhythms, in which the temporal spacings between song element onsets occur at discretely organized intervals. As categorical song rhythms can facilitate song history encoding, it may be critical in enabling long-range dependencies in pied butcherbird songs. A systematic conception of song complexity that takes into account such potential interactions between song features stands to benefit songbird communication research.},
	language = {en},
	author = {Xing, Jeffrey and Sainburg, Tim and Gentner, Timothy Q and Taylor, Hollis},
	year = {2021},
	file = {xing ... HOLLIS TAYLOR 2021 - examining song complexity of australian pied butcherbirds - SONG COMPLEXITY.pdf:/Users/bill/D/Zotero/storage/8ISQ46ZI/xing ... HOLLIS TAYLOR 2021 - examining song complexity of australian pied butcherbirds - SONG COMPLEXITY.pdf:application/pdf},
}

@article{delattre1963i-irallt,
	title = {{COMPARING} {THE} {PROSODIC} {FEATURES} {OF} {ENGLISH}, {GERMAN}, {SPANISH} {AND} {FRENCH}},
	volume = {1},
	issn = {0019-042X, 1613-4141},
	url = {https://www.degruyter.com/document/doi/10.1515/iral.1963.1.1.193/html},
	doi = {10.1515/iral.1963.1.1.193},
	language = {en},
	number = {1},
	urldate = {2023-12-12},
	journal = {IRAL - International Review of Applied Linguistics in Language Teaching},
	author = {Delattre, Pierre},
	year = {1963},
	file = {delattre 1963 - Comparing the Prosodic Features of English, German, Spanish and French - from scihub - PROSODY.pdf:/Users/bill/D/Zotero/storage/3NXNKWYC/delattre 1963 - Comparing the Prosodic Features of English, German, Spanish and French - from scihub - PROSODY.pdf:application/pdf},
}

@inproceedings{kember2017i2,
	title = {Similar {Prosodic} {Structure} {Perceived} {Differently} in {German} and {English}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/kember17_interspeech.html},
	doi = {10.21437/Interspeech.2017-544},
	abstract = {English and German have similar prosody, but their speakers realize some pitch falls (not rises) in subtly different ways. We here test for asymmetry in perception. An ABX discrimination task requiring F0 slope or duration judgements on isolated vowels revealed no cross-language difference in duration or F0 fall discrimination, but discrimination of rises (realized similarly in each language) was less accurate for English than for German listeners. This unexpected finding may reflect greater sensitivity to rising patterns by German listeners, or reduced sensitivity by English listeners as a result of extensive exposure to phrase-final rises (“uptalk”) in their language.},
	language = {en},
	urldate = {2023-12-12},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Kember, Heather and Grohe, Ann-Kathrin and Zahner, Katharina and Braun, Bettina and Weber, Andrea and Cutler, Anne},
	month = aug,
	year = {2017},
	pages = {1388--1392},
	file = {kember et al 2017 - Similar prosodic structure perceived differently in German and English.pdf:/Users/bill/D/Zotero/storage/8VLTTUF2/kember et al 2017 - Similar prosodic structure perceived differently in German and English.pdf:application/pdf},
}

@inproceedings{kember2017i2a,
	title = {Similar {Prosodic} {Structure} {Perceived} {Differently} in {German} and {English}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/kember17_interspeech.html},
	doi = {10.21437/Interspeech.2017-544},
	abstract = {English and German have similar prosody, but their speakers realize some pitch falls (not rises) in subtly different ways. We here test for asymmetry in perception. An ABX discrimination task requiring F0 slope or duration judgements on isolated vowels revealed no cross-language difference in duration or F0 fall discrimination, but discrimination of rises (realized similarly in each language) was less accurate for English than for German listeners. This unexpected finding may reflect greater sensitivity to rising patterns by German listeners, or reduced sensitivity by English listeners as a result of extensive exposure to phrase-final rises (“uptalk”) in their language.},
	language = {en},
	urldate = {2023-12-12},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Kember, Heather and Grohe, Ann-Kathrin and Zahner, Katharina and Braun, Bettina and Weber, Andrea and Cutler, Anne},
	month = aug,
	year = {2017},
	pages = {1388--1392},
	file = {kember et al 2017 - Similar prosodic structure perceived differently in German and English.pdf:/Users/bill/D/Zotero/storage/93RMVLV7/kember et al 2017 - Similar prosodic structure perceived differently in German and English.pdf:application/pdf},
}

@article{hemming2022cb,
	title = {An introduction to decision science for conservation},
	volume = {36},
	issn = {0888-8892, 1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.13868},
	doi = {10.1111/cobi.13868},
	abstract = {Biodiversity conservation decisions are difﬁcult, especially when they involve differing values, complex multidimensional objectives, scarce resources, urgency, and considerable uncertainty. Decision science embodies a theory about how to make difﬁcult decisions and an extensive array of frameworks and tools that make that theory practical. We sought to improve conceptual clarity and practical application of decision science to help decision makers apply decision science to conservation problems. We addressed barriers to the uptake of decision science, including a lack of training and awareness of decision science; confusion over common terminology and which tools and frameworks to apply; and the mistaken impression that applying decision science must be time consuming, expensive, and complex. To aid in navigating the extensive and disparate decision science literature, we clarify meaning of common terms: decision science, decision theory, decision analysis, structured decision-making, and decision-support tools. Applying decision science does not have to be complex or time consuming; rather, it begins with knowing how to think through the components of a decision utilizing decision analysis (i.e., deﬁne the problem, elicit objectives, develop alternatives, estimate consequences, and perform trade-offs). This is best achieved by applying a rapid-prototyping approach. At each step, decision-support tools can provide additional insight and clarity, whereas decision-support frameworks (e.g., priority threat management and systematic conservation planning) can aid navigation of multiple steps of a decision analysis for particular contexts. We summarize key decision-support frameworks and tools and describe to which step of a decision analysis, and to which contexts, each is most useful to apply. Our introduction to decision science will aid in contextualizing current approaches and new developments, and help decision makers begin to apply decision science to conservation problems.},
	language = {en},
	number = {1},
	urldate = {2023-12-12},
	journal = {Conservation Biology},
	author = {Hemming, Victoria and Camaclang, Abbey E. and Adams, Megan S. and Burgman, Mark and Carbeck, Katherine and Carwardine, Josie and Chadès, Iadine and Chalifour, Lia and Converse, Sarah J. and Davidson, Lindsay N. K. and Garrard, Georgia E. and Finn, Riley and Fleri, Jesse R. and Huard, Jacqueline and Mayfield, Helen J. and Madden, Eve McDonald and Naujokaitis‐Lewis, Ilona and Possingham, Hugh P. and Rumpff, Libby and Runge, Michael C. and Stewart, Daniel and Tulloch, Vivitskaia J. D. and Walshe, Terry and Martin, Tara G.},
	month = feb,
	year = {2022},
	pages = {e13868},
	file = {hemming 2022 - An introduction to decision science for conservation.pdf:/Users/bill/D/Zotero/storage/64BBNL8U/hemming 2022 - An introduction to decision science for conservation.pdf:application/pdf},
}

@article{antuori2021,
	title = {On {How} {Turing} and {Singleton} {Arc} {Consistency} {Broke} the {Enigma} {Code}},
	abstract = {In this paper, we highlight an intriguing connection between the cryptographic attacks on Enigma’s code and local consistency reasoning in constraint programming. The coding challenge proposed to the students during the 2020 ACP summer school, to be solved by constraint programming, was to decipher a message encoded using the well known Enigma machine, with as only clue a tiny portion of the original message. A number of students quickly crafted a model, thus nicely showcasing CP technology – as well as their own brightness. The detail that is slightly less favorable to CP technology is that solving this model on modern hardware is challenging, whereas the “Bombe”, an antique computing device, could solve it eighty years ago.},
	language = {en},
	author = {Antuori, Valentin and Portoleau, Tom and Rivière, Louis and Hebrard, Emmanuel},
	year = {2021},
	file = {antuori et al 2022 - On How Turing and Singleton Arc Consistency Broke the Enigma Code.pdf:/Users/bill/D/Zotero/storage/3L6WTHGQ/antuori et al 2022 - On How Turing and Singleton Arc Consistency Broke the Enigma Code.pdf:application/pdf},
}

@incollection{macia2009hais,
	address = {Berlin, Heidelberg},
	title = {Beyond {Homemade} {Artificial} {Data} {Sets}},
	volume = {5572},
	isbn = {978-3-642-02318-7 978-3-642-02319-4},
	url = {http://link.springer.com/10.1007/978-3-642-02319-4_73},
	abstract = {One of the most important challenges in supervised learning is how to evaluate the quality of the models evolved by diﬀerent machine learning techniques. Up to now, we have relied on measures obtained by running the methods on a wide test bed composed of realworld problems. Nevertheless, the unknown inherent characteristics of these problems and the bias of learners may lead to inconclusive results. This paper discusses the need to work under a controlled scenario and bets on artiﬁcial data set generation. A list of ingredients and some ideas about how to guide such generation are provided, and promising results of an evolutionary multi-objective approach which incorporates the use of data complexity estimates are presented.},
	language = {en},
	urldate = {2023-12-13},
	booktitle = {Hybrid {Artificial} {Intelligence} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Macià, Núria and Orriols-Puig, Albert and Bernadó-Mansilla, Ester},
	editor = {Corchado, Emilio and Wu, Xindong and Oja, Erkki and Herrero, Álvaro and Baruque, Bruno},
	year = {2009},
	doi = {10.1007/978-3-642-02319-4_73},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {605--612},
	file = {macia et al 2009 - Beyond Homemade Artificial Data Sets - BDPG - BENCHMARKING - ARTIFICIAL DATA - SYNTHETIC DATA - ANNO.pdf:/Users/bill/D/Zotero/storage/9XBDW4F9/macia et al 2009 - Beyond Homemade Artificial Data Sets - BDPG - BENCHMARKING - ARTIFICIAL DATA - SYNTHETIC DATA - ANNO.pdf:application/pdf},
}

@article{antolinez,
	title = {On the {Necessity} of {Dataset} {Characterization} for {Experimental} {Analysis}},
	language = {en},
	author = {Antolínez, Núria Macià},
	file = {macia - on the necessity of dataset characterization for experimental analysis - towards artificial datasets - SLIDES - BDPG.pdf:/Users/bill/D/Zotero/storage/DT3XW2BW/macia - on the necessity of dataset characterization for experimental analysis - towards artificial datasets - SLIDES - BDPG.pdf:application/pdf},
}

@article{antolineza,
	title = {Data complexity in supervised learning: {A} far-reaching implication},
	language = {en},
	author = {Antolínez, Núria Macià and Llull, Universitat Ramon},
	file = {macia 2011 - data complexity in supervised learning - A far-reaching implication - THESIS - DATA COMPLEXITY - BENCHMARKING - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/6PURMEZE/macia 2011 - data complexity in supervised learning - A far-reaching implication - THESIS - DATA COMPLEXITY - BENCHMARKING - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{antolinezb,
	title = {On the {Necessity} of {Dataset} {Characterization} for {Experimental} {Analysis}},
	language = {en},
	author = {Antolínez, Núria Macià},
	file = {macia - on the necessity of dataset characterization for experimental analysis - towards artificial datasets - SLIDES - BDPG.pdf:/Users/bill/D/Zotero/storage/793XQXFT/macia - on the necessity of dataset characterization for experimental analysis - towards artificial datasets - SLIDES - BDPG.pdf:application/pdf},
}

@article{antolinezc,
	title = {Data complexity in supervised learning: {A} far-reaching implication},
	language = {en},
	author = {Antolínez, Núria Macià and Llull, Universitat Ramon},
	file = {macia 2011 - data complexity in supervised learning - A far-reaching implication - THESIS - DATA COMPLEXITY - BENCHMARKING - PROBLEM DIFFICULTY - BDPG.pdf:/Users/bill/D/Zotero/storage/LIURF75S/macia 2011 - data complexity in supervised learning - A far-reaching implication - THESIS - DATA COMPLEXITY - BENCHMARKING - PROBLEM DIFFICULTY - BDPG.pdf:application/pdf},
}

@article{xu2022sj,
	title = {Generating {Hard} {Satisfiable} {Instances} by {Planting} into {Random} {K} -{Constraint} {Satisfaction} {Problem}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4137090},
	doi = {10.2139/ssrn.4137090},
	language = {en},
	urldate = {2024-02-02},
	journal = {SSRN Electronic Journal},
	author = {Xu, Wei and Zhang, Zhe and Zhou, Guangyan},
	year = {2022},
	file = {xu zhang zhou 2022 - Generating hard satisfiable instances by planting into random k-constraint satisfaction problem.pdf:/Users/bill/D/Zotero/storage/5I547CM2/xu zhang zhou 2022 - Generating hard satisfiable instances by planting into random k-constraint satisfaction problem.pdf:application/pdf},
}

@misc{wu2018,
	title = {Statistical {Problems} with {Planted} {Structures}: {Information}-{Theoretical} and {Computational} {Limits}},
	shorttitle = {Statistical {Problems} with {Planted} {Structures}},
	url = {http://arxiv.org/abs/1806.00118},
	abstract = {Over the past few years, insights from computer science, statistical physics, and information theory have revealed phase transitions in a wide array of high-dimensional statistical problems at two distinct thresholds: One is the information-theoretical (IT) threshold below which the observation is too noisy so that inference of the ground truth structure is impossible regardless of the computational cost; the other is the computational threshold above which inference can be performed eﬃciently, i.e., in time that is polynomial in the input size. In the intermediate regime, inference is information-theoretically possible, but conjectured to be computationally hard.},
	language = {en},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Wu, Yihong and Xu, Jiaming},
	month = aug,
	year = {2018},
	note = {arXiv:1806.00118 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
	annote = {Comment: Chapter in "Information-Theoretic Methods in Data Science". Edited by Yonina Eldar and Miguel Rodrigues, Cambridge University Press, forthcoming},
	annote = {This is chapter 13 in: 
1. Wu Y, Xu J. Statistical Problems with Planted Structures: Information-Theoretical and Computational Limits. In: Rodrigues MRD, Eldar YC, eds. Information-Theoretic Methods in Data Science. Cambridge University Press; 2021:383-424.
},
	file = {wu xu 2018 - Statistical Problems with Planted Structures - Information-Theoretical and Computational Limits.pdf:/Users/bill/D/Zotero/storage/CMPIWZW2/wu xu 2018 - Statistical Problems with Planted Structures - Information-Theoretical and Computational Limits.pdf:application/pdf},
}

@article{chena,
	title = {Statistical-{Computational} {Tradeoffs} in {Planted} {Problems} and {Submatrix} {Localization} with a {Growing} {Number} of {Clusters} and {Submatrices}},
	abstract = {We consider two closely related problems: planted clustering and submatrix localization. In the planted clustering problem, a random graph is generated based on an underlying cluster structure of the nodes; the task is to recover these clusters given the graph. The submatrix localization problem concerns locating hidden submatrices with elevated means inside a large real-valued random matrix. Of particular interest is the setting where the number of clusters/submatrices is allowed to grow unbounded with the problem size. These formulations cover several classical models such as planted clique, planted densest subgraph, planted partition, planted coloring, and the stochastic block model, which are widely used for studying community detection, graph clustering and bi-clustering.},
	language = {en},
	author = {Chen, Yudong and Xu, Jiaming},
	file = {chen xu 2016 - Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices.pdf:/Users/bill/D/Zotero/storage/A6Z3YBTT/chen xu 2016 - Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices.pdf:application/pdf},
}

@article{gorissen2015o,
	title = {A practical guide to robust optimization},
	volume = {53},
	issn = {03050483},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305048314001698},
	doi = {10.1016/j.omega.2014.12.006},
	language = {en},
	urldate = {2024-02-11},
	journal = {Omega},
	author = {Gorissen, Bram L. and Yanıkoğlu, İhsan and Den Hertog, Dick},
	month = jun,
	year = {2015},
	pages = {124--137},
	file = {gorissen yanikoglu hertog 2015 - A Practical Guide to Robust Optimization - BDPG - ROBUST OPTIMIZATION - UNCERTAINTY - ANNO.pdf:/Users/bill/D/Zotero/storage/DGW8ETP7/gorissen yanikoglu hertog 2015 - A Practical Guide to Robust Optimization - BDPG - ROBUST OPTIMIZATION - UNCERTAINTY - ANNO.pdf:application/pdf},
}

@article{karimireddy,
	title = {Learning from {History} for {Byzantine} {Robust} {Optimization}},
	abstract = {Byzantine robustness has received signiﬁcant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe ﬂaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the inﬂuence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the ﬁrst provably robust method for the standard stochastic optimization setting. Our code is open sourced at this link2.},
	language = {en},
	author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
	file = {karimireddy he jaggi 2021 - Learning from History for Byzantine Robust Optimization - BDPG - ROBUST OPTIMIZATION - AGGREGATION - ANNO.pdf:/Users/bill/D/Zotero/storage/C468N267/karimireddy he jaggi 2021 - Learning from History for Byzantine Robust Optimization - BDPG - ROBUST OPTIMIZATION - AGGREGATION - ANNO.pdf:application/pdf},
}

@misc{karimireddy2021,
	title = {Learning from {History} for {Byzantine} {Robust} {Optimization}},
	url = {http://arxiv.org/abs/2012.10333},
	abstract = {Byzantine robustness has received signiﬁcant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe ﬂaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the inﬂuence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the ﬁrst provably robust method for the standard stochastic optimization setting. Our code is open sourced at this link2.},
	language = {en},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
	month = jun,
	year = {2021},
	note = {arXiv:2012.10333 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.6, I.5.1},
	annote = {Comment: ICML 2021. v2 contains stronger theory; v3 fixes some errors in the proof},
	file = {karimireddy he jaggi 2021 - Learning from History for Byzantine Robust Optimization - More Complete Arxiv Version - BDPG - ROBUST OPTIMIZATION - AGGREGATION - ANNO.pdf:/Users/bill/D/Zotero/storage/NCFJ5RTQ/karimireddy he jaggi 2021 - Learning from History for Byzantine Robust Optimization - More Complete Arxiv Version - BDPG - ROBUST OPTIMIZATION - AGGREGATION - ANNO.pdf:application/pdf},
}

@article{velazco2020bc,
	title = {Overprediction of species distribution models in conservation planning: {A} still neglected issue with strong effects},
	volume = {252},
	issn = {00063207},
	shorttitle = {Overprediction of species distribution models in conservation planning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320720308806},
	doi = {10.1016/j.biocon.2020.108822},
	abstract = {Species distribution models (SDM) are increasingly used in conservation planning to identify priority areas for the establishment of protected areas. Nevertheless, the quality of SDM varies widely and may compromise the effectiveness of protected areas. Here we reviewed whether SDM overprediction is considered in spatial con­ servation prioritization exercises and evaluated how model overprediction influences the effectiveness and the spatial arrangement of priority areas. To do so, we carried out a systematic review to analyze how researchers have handled SDM overprediction when identifying priority areas for conservation. To show how spatial con­ servation prioritization outcomes are affected by SDM overprediction, we used SDM of native palm at three geographic scales (Neotropics, Amazon ecoregion, and Ecuadorian Amazon). We found that only 10\% of the evaluated manuscripts accounted for model overprediction. Our spatial conservation prioritization based on SDM with overprediction conferred high priority rank values in a region where species do not occur, underestimated the efficiency of selected priority areas, and over or underestimated the efficiency of current protected areas. Such effects were lower at smaller geographic extents. Our findings highlight the importance of improving future spatial conservation prioritization studies through the correction of SDM overprediction, resulting in the detection of more adequate areas for species conservation, especially at broader extents.},
	language = {en},
	urldate = {2024-02-22},
	journal = {Biological Conservation},
	author = {Velazco, Santiago José Elías and Ribeiro, Bruno R. and Laureto, Livia Maira Orlandi and De Marco Júnior, Paulo},
	month = dec,
	year = {2020},
	pages = {108822},
	file = {velazco et al 2020 - Overprediction of species distribution models in conservation planning - A still neglected issue with strong effects - SDMs - UNCERTAINTY - BDPG.pdf:/Users/bill/D/Zotero/storage/EQDB7SRC/velazco et al 2020 - Overprediction of species distribution models in conservation planning - A still neglected issue with strong effects - SDMs - UNCERTAINTY - BDPG.pdf:application/pdf},
}

@article{beiranvand2017oeb,
	title = {Best practices for comparing optimization algorithms},
	volume = {18},
	issn = {1389-4420, 1573-2924},
	url = {http://link.springer.com/10.1007/s11081-017-9366-1},
	doi = {10.1007/s11081-017-9366-1},
	abstract = {Comparing, or benchmarking, of optimization algorithms is a complicated task that involves many subtle considerations to yield a fair and unbiased evaluation. In this paper, we systematically review the benchmarking process of optimization algorithms, and discuss the challenges of fair comparison. We provide suggestions for each step of the comparison process and highlight the pitfalls to avoid when evaluating the performance of optimization algorithms. We also discuss various methods of reporting the benchmarking results. Finally, some suggestions for future research are presented to improve the current benchmarking process.},
	language = {en},
	number = {4},
	urldate = {2024-03-07},
	journal = {Optimization and Engineering},
	author = {Beiranvand, Vahid and Hare, Warren and Lucet, Yves},
	month = dec,
	year = {2017},
	pages = {815--848},
	file = {beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING - PUBLISHED VERSION - ANNO.pdf.pdf:/Users/bill/D/Zotero/storage/SD5XDSG4/beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING - PUBLISHED VERSION - ANNO.pdf.pdf:application/pdf},
}

@misc{rcoreteam,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {R Core Team},
	annote = {Get current version number using version\$version.string at R command line.
},
}

@misc{engstrom2020,
	title = {Identifying {Statistical} {Bias} in {Dataset} {Replication}},
	url = {http://arxiv.org/abs/2005.09619},
	abstract = {Dataset replication is a useful tool for assessing whether improvements in test accuracy on a speciﬁc benchmark correspond to improvements in models’ ability to generalize reliably. In this work, we present unintuitive yet signiﬁcant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a signiﬁcant (11-14\%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for the identiﬁed statistical bias, only an estimated 3.6\% ± 1.5\% of the original 11.7\% ± 1.0\% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available1.},
	language = {en},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Steinhardt, Jacob and Madry, Aleksander},
	month = sep,
	year = {2020},
	note = {arXiv:2005.09619 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {engstrom et al 2020 - Identifying Statistical Bias in Dataset Replication - v 2020 09 - BDPG.pdf:/Users/bill/D/Zotero/storage/ZXBNS7ZR/engstrom et al 2020 - Identifying Statistical Bias in Dataset Replication - v 2020 09 - BDPG.pdf:application/pdf},
}

@inproceedings{engstrom2020a,
	title = {Identifying statistical bias in dataset replication},
	publisher = {PMLR},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shaibani and Tsipras, Dimitris and Steinhardt, Jacob and Madry, Aleksander},
	year = {2020},
	pages = {2922--2932},
	file = {engstrom et al 2020 - Identifying Statistical Bias in Dataset Replication - ICML 2020 PAPER - BDPG.pdf:/Users/bill/D/Zotero/storage/SHCSREK2/engstrom et al 2020 - Identifying Statistical Bias in Dataset Replication - ICML 2020 PAPER - BDPG.pdf:application/pdf},
}

@article{fahrig2022br,
	title = {Resolving the {SLOSS} dilemma for biodiversity conservation: a research agenda},
	volume = {97},
	issn = {1464-7931, 1469-185X},
	shorttitle = {Resolving the {\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/brv.12792},
	doi = {10.1111/brv.12792},
	abstract = {ABSTRACT
            The legacy of the ‘SL {\textgreater} SS principle’, that a single or a few large habitat patches (SL) conserve more species than several small patches (SS), is evident in decisions to protect large patches while down‐weighting small ones. However, empirical support for this principle is lacking, and most studies find either no difference or the opposite pattern (SS {\textgreater} SL). To resolve this dilemma, we propose a research agenda by asking, ‘are there consistent, empirically demonstrated conditions leading to SL {\textgreater} SS?’ We first review and summarize ‘single large or several small’ (SLOSS) theory and predictions. We found that most predictions of SL {\textgreater} SS assume that between‐patch variation in extinction rate dominates the outcome of the extinction–colonization dynamic. This is predicted to occur when populations in separate patches are largely independent of each other due to low between‐patch movements, and when species differ in minimum patch size requirements, leading to strong nestedness in species composition along the patch size gradient. However, even when between‐patch variation in extinction rate dominates the outcome of the extinction–colonization dynamic, theory can predict SS {\textgreater} SL. This occurs if extinctions are caused by antagonistic species interactions or disturbances, leading to spreading‐of‐risk of landscape‐scale extinction across SS. SS {\textgreater} SL is also predicted when variation in colonization dominates the outcome of the extinction–colonization dynamic, due to higher immigration rates for SS than SL, and larger species pools in proximity to SS than SL. Theory that considers change in species composition among patches also predicts SS {\textgreater} SL because of higher beta diversity across SS than SL. This results mainly from greater environmental heterogeneity in SS due to greater variation in micro‐habitats within and across SS habitat patches (‘across‐habitat heterogeneity’), and/or more heterogeneous successional trajectories across SS than SL. Based on our review of the relevant theory, we develop the ‘SLOSS cube hypothesis’, where the combination of three variables – between‐patch movement, the role of spreading‐of‐risk in landscape‐scale population persistence, and across‐habitat heterogeneity – predict the SLOSS outcome. We use the SLOSS cube hypothesis and existing SLOSS empirical evidence, to predict SL {\textgreater} SS only when all of the following are true: low between‐patch movement, low importance of spreading‐of‐risk for landscape‐scale population persistence, and low across‐habitat heterogeneity. Testing this prediction will be challenging, as it will require many studies of species groups and regions where these conditions hold. Each such study would compare gamma diversity across multiple landscapes varying in number and sizes of patches. If the prediction is not generally supported across such tests, then the mechanisms leading to SL {\textgreater} SS are extremely rare in nature and the SL {\textgreater} SS principle should be abandoned.},
	language = {en},
	number = {1},
	urldate = {2024-03-20},
	journal = {Biological Reviews},
	author = {Fahrig, Lenore and Watling, James I. and Arnillas, Carlos Alberto and Arroyo‐Rodríguez, Víctor and Jörger‐Hickfang, Theresa and Müller, Jörg and Pereira, Henrique M. and Riva, Federico and Rösch, Verena and Seibold, Sebastian and Tscharntke, Teja and May, Felix},
	month = feb,
	year = {2022},
	pages = {99--114},
	file = {fahrig et al 2022 - Resolving the SLOSS dilemma for biodiversityconservation- a research agenda - SLOSS - SINGLE LARGE SEVERAL SMALL.pdf:/Users/bill/D/Zotero/storage/RT4MX3ZY/fahrig et al 2022 - Resolving the SLOSS dilemma for biodiversityconservation- a research agenda - SLOSS - SINGLE LARGE SEVERAL SMALL.pdf:application/pdf},
}

@article{szangolies2022baae,
	title = {Single large {AND} several small habitat patches: {A} community perspective on their importance for biodiversity},
	volume = {65},
	issn = {14391791},
	shorttitle = {Single large {AND} several small habitat patches},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1439179122000780},
	doi = {10.1016/j.baae.2022.09.004},
	abstract = {The debate whether single large or several small (SLOSS) patches beneﬁt biodiversity has existed for decades, but recent literature provides increasing evidence for the importance of small habitats. Possible beneﬁcial mechanisms include reduced presence of predators and competitors in small habitat areas or speciﬁc functions such as stepping stones for dispersal. Given the increasing amount of studies highlighting individual behavioral differences that may inﬂuence these functions, we hypothesize that the advantage of small versus large habitat patches not only depends on patch functionality but also on the presence of animal personalities (i.e., risk-tolerant vs. risk-averse). Using an individual-based, spatially-explicit community model, we analyzed the diversity of mammal communities in landscapes consisting of a few large habitat islands interspersed with different amounts and sizes of small habitat patches. Within these heterogeneous environments, individuals compete for resources and form home-ranges, with only risk-tolerant individuals using habitat edges. Results show that when risk-tolerant individuals exist, small patches increase species diversity. A strong peak occurs at approximately 20\% habitat cover in small patches when those small habitats are only used for foraging but not for breeding and home-range core position. Additional usage as stepping stones for juvenile dispersal further increases species persistence. Overall, our results reveal that a combination of a few large and several small habitat patches promotes biodiversity by enhancing landscape heterogeneity. Here, heterogeneity is created by pronounced differences in habitat functionality, increasing edge density, and variability in habitat use by different behavioral types. The ﬁnding that a combination of single large AND several small (SLASS) patches is needed for effective biodiversity preservation has implications for advancing landscape conservation. Particularly in structurally poor agricultural areas, modern technology enables precise management with the opportunity to create small foraging habitats by excluding less proﬁtable agricultural land from cultivation.},
	language = {en},
	urldate = {2024-03-20},
	journal = {Basic and Applied Ecology},
	author = {Szangolies, Leonna and Rohwäder, Marie-Sophie and Jeltsch, Florian},
	month = dec,
	year = {2022},
	pages = {16--27},
	file = {szangolies et al 2022 - Single large AND several small habitat patches- A community perspective on their importance for biodiversity.pdf:/Users/bill/D/Zotero/storage/UQZT57WN/szangolies et al 2022 - Single large AND several small habitat patches- A community perspective on their importance for biodiversity.pdf:application/pdf},
}

@techreport{holsinger2012,
	title = {Theory and design of nature reserves},
	url = {https://opencommons.uconn.edu/eeb_articles/41},
	language = {en},
	number = {41},
	institution = {University of Connecticut - Storrs},
	author = {Holsinger, Kent E},
	year = {2012},
	file = {holsinger 2012 - Theory and design of nature reserves.pdf:/Users/bill/D/Zotero/storage/3UYTZXK6/holsinger 2012 - Theory and design of nature reserves.pdf:application/pdf},
}

@article{rocchini2011pipgeae,
	title = {Accounting for uncertainty when mapping species distributions: {The} need for maps of ignorance},
	volume = {35},
	issn = {0309-1333, 1477-0296},
	shorttitle = {Accounting for uncertainty when mapping species distributions},
	url = {http://journals.sagepub.com/doi/10.1177/0309133311399491},
	doi = {10.1177/0309133311399491},
	abstract = {Accurate mapping of species distributions is a fundamental goal of modern biogeography, both for basic and applied purposes. This is commonly done by plotting known species occurrences, expert-drawn range maps or geographical estimations derived from species distribution models. However, all three kinds of maps are implicitly subject to uncertainty, due to the quality and bias of raw distributional data, the process of map building, and the dynamic nature of species distributions themselves. Here we review the main sources of uncertainty suggesting a code of good practices in order to minimize their effects.},
	language = {en},
	number = {2},
	urldate = {2024-03-21},
	journal = {Progress in Physical Geography: Earth and Environment},
	author = {Rocchini, Duccio and Hortal, Joaquín and Lengyel, Szabolcs and Lobo, Jorge M. and Jiménez-Valverde, Alberto and Ricotta, Carlo and Bacaro, Giovanni and Chiarucci, Alessandro},
	month = apr,
	year = {2011},
	pages = {211--226},
	file = {rocchini et al 2011 - Accounting for uncertainty when mapping species distributions - The need for maps of ignorance - GUPPY - ERROR MODELS - SPATIAL ERROR - BDPG.pdf:/Users/bill/D/Zotero/storage/59PZ2TK3/rocchini et al 2011 - Accounting for uncertainty when mapping species distributions - The need for maps of ignorance - GUPPY - ERROR MODELS - SPATIAL ERROR - BDPG.pdf:application/pdf},
}

@article{arbia1998ijogis,
	title = {Error propagation modelling in raster {GIS}: overlay operations},
	volume = {12},
	issn = {1365-8816, 1362-3087},
	shorttitle = {Error propagation modelling in raster {GIS}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/136588198241932},
	doi = {10.1080/136588198241932},
	language = {en},
	number = {2},
	urldate = {2024-03-21},
	journal = {International Journal of Geographical Information Science},
	author = {Arbia, Giuseppe and Griffith, Daniel and Haining, Robert},
	month = mar,
	year = {1998},
	pages = {145--167},
	file = {arbia et al 1998 - Error Propagation Modelling in Raster GIS- Overlay Operations.pdf:/Users/bill/D/Zotero/storage/UKSTUTQ4/arbia et al 1998 - Error Propagation Modelling in Raster GIS- Overlay Operations.pdf:application/pdf},
}

@article{wang2020bc,
	title = {Inventory incompleteness and collecting priority on the plant diversity in tropical {East} {Africa}},
	volume = {241},
	issn = {00063207},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006320719310390},
	doi = {10.1016/j.biocon.2019.108313},
	abstract = {Inventory incompleteness has seriously aﬀected the accuracy of the spatial distribution pattern of biodiversity, but the causes of incompleteness and the priority investigation with quantitative methods have received far less attention. In this study, we constructed a plant database of tropical East Africa, evaluated and explained the inventory incompleteness, and identiﬁed the priority collecting area. The results showed that the spatial distribution pattern of collection density and species richness is very uneven in tropical East Africa, with 16 \% of regions having zero-collection, and more than half of the regions having inventory incompleteness. Species collection and completeness are mainly aﬀected by species richness and road density, followed by national boundaries and insecurity in some areas. We quantitatively selected priority investigation areas in tropical East Africa to supplement biodiversity data in the area. We recommend prioritizing collections especially around western Kenya, southern Tanzania, and around the border of Tanzania and Kenya. Future work should focus on improving the digitization of specimens and the strengthening of cooperation among countries, for these are the best ways to raise awareness of the biodiversity patterns in tropical East Africa.},
	language = {en},
	urldate = {2024-03-21},
	journal = {Biological Conservation},
	author = {Wang, Shengwei and Zhou, Yadong and Musili, Paul Mutuku and Mwachala, Geoffrey and Hu, Guangwan and Wang, Qingfeng},
	month = jan,
	year = {2020},
	pages = {108313},
	file = {wang et al 2020 - Inventory incompleteness and collecting priority on the plant diversity intropical East Africa - BDPG - FN - SDM - ERROR MODELS - GUPPY - ANNO.pdf:/Users/bill/D/Zotero/storage/JCPI5K9R/wang et al 2020 - Inventory incompleteness and collecting priority on the plant diversity intropical East Africa - BDPG - FN - SDM - ERROR MODELS - GUPPY - ANNO.pdf:application/pdf},
}

@article{ide2016os,
	title = {Robustness for uncertain multi-objective optimization: a survey and analysis of different concepts},
	volume = {38},
	issn = {0171-6468, 1436-6304},
	shorttitle = {Robustness for uncertain multi-objective optimization},
	url = {http://link.springer.com/10.1007/s00291-015-0418-7},
	doi = {10.1007/s00291-015-0418-7},
	abstract = {In this paper, we discuss various concepts of robustness for uncertain multiobjective optimization problems. We extend the concepts of ﬂimsily, highly, and lightly robust efﬁciency and we collect different versions of minmax robust efﬁciency and concepts based on set order relations from the literature. Altogether, we compare and analyze ten different concepts and point out their relations to each other. Furthermore, we present reduction results for the class of objective-wise uncertain multi-objective optimization problems.},
	language = {en},
	number = {1},
	urldate = {2024-03-21},
	journal = {OR Spectrum},
	author = {Ide, Jonas and Schöbel, Anita},
	month = jan,
	year = {2016},
	pages = {235--271},
	file = {ide schobel 2016 - Robustness for uncertain multi-objective optimization- a survey and analysis of different concepts.pdf:/Users/bill/D/Zotero/storage/4U6JJUFC/ide schobel 2016 - Robustness for uncertain multi-objective optimization- a survey and analysis of different concepts.pdf:application/pdf},
}

@article{armsworth2014aotnyaos,
	title = {Inclusion of costs in conservation planning depends on limited datasets and hopeful assumptions},
	volume = {1322},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0077-8923, 1749-6632},
	url = {https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.12455},
	doi = {10.1111/nyas.12455},
	abstract = {Many conservation organizations use spatial prioritization to help identify locations in which to work. Increasingly, prioritizations seek to account for spatial heterogeneity in the costs of conservation, motivated in part by claims of large efficiency savings when these costs are included. I critically review the cost estimates on which such claims are based, focusing on acquisition and management costs associated with terrestrial protected areas. If researchers are to evaluate how including costs affects conservation planning outcomes, estimation methods need to preserve the covariation between and relative variation within costs and benefits of conservation activities. However, widely used methods for estimating costs and incorporating them into prioritizations may not meet these standards. For example, among relevant studies, there is surprisingly little attention given to the costs that conservation organizations actually face. Instead, there is a heavy reliance on untested proxies for conservation costs. Analytical shortcuts are also common. Now that debate is moving beyond whether to account for costs in conservation planning, it is time to evaluate just how we can include them to greatest effect.},
	language = {en},
	number = {1},
	urldate = {2024-04-02},
	journal = {Annals of the New York Academy of Sciences},
	author = {Armsworth, Paul R.},
	month = aug,
	year = {2014},
	pages = {61--76},
	file = {armsworth 2014 - Inclusion of costs in conservation planning depends on limited datasets and hopeful assumptions - COST - GUPPY - BDPG.pdf:/Users/bill/D/Zotero/storage/V5CJ3ZNR/armsworth 2014 - Inclusion of costs in conservation planning depends on limited datasets and hopeful assumptions - COST - GUPPY - BDPG.pdf:application/pdf},
}

@article{armsworth2006pnasu,
	title = {Land market feedbacks can undermine biodiversity conservation},
	volume = {103},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0505278103},
	doi = {10.1073/pnas.0505278103},
	abstract = {The full or partial purchase of land has become a cornerstone of efforts to conserve biodiversity in countries with strong private property rights. Methods used to target areas for acquisition typically ignore land market dynamics. We show how conservation purchases affect land prices and generate feedbacks that can undermine conservation goals, either by displacing development toward biologically valuable areas or by accelerating its pace. The impact of these market feedbacks on the effectiveness of conservation depends on the ecological value of land outside nature reserves. Traditional, noneconomic approaches to site prioritization should perform adequately in places where land outside reserves supports little biodiversity. However, these approaches will perform poorly in locations where the countryside surrounding reserves is important for species’ persistence. Conservation investments can sometimes even be counterproductive, condemning more species than they save. Conservation is most likely to be compromised in the absence of accurate information on species distributions, which provides a strong argument for improving inventories of biodiversity. Accounting for land market dynamics in conservation planning is crucial for making smart investment decisions.},
	language = {en},
	number = {14},
	urldate = {2024-04-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Armsworth, Paul R. and Daily, Gretchen C. and Kareiva, Peter and Sanchirico, James N.},
	month = apr,
	year = {2006},
	pages = {5403--5408},
	file = {Armsworth et al. - 2006 - Land market feedbacks can undermine biodiversity conservation - BDPG - COSTS - ANNO.pdf:/Users/bill/D/Zotero/storage/KJ9G496G/Armsworth et al. - 2006 - Land market feedbacks can undermine biodiversity conservation - BDPG - COSTS - ANNO.pdf:application/pdf},
}

@article{tsoukias2008ejoor,
	title = {From decision theory to decision aiding methodology},
	volume = {187},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221707002949},
	doi = {10.1016/j.ejor.2007.02.039},
	abstract = {The paper presents the author’s partial and personal historical reconstruction of how decision theory is evolving to a decision aiding methodology. The presentation shows mainly how ‘‘alternative’’ approaches to classic decision theory evolved. In the paper it is claimed that all such decision ‘‘theories’’ share a common methodological feature, which is the use of formal and abstract languages as well as of a model of rationality. Diﬀerent decision aiding approaches can thus be deﬁned, depending on the origin of the model of rationality used in the decision aiding process. The concept of decision aiding process is then introduced and analysed. The paper’s ultimate claim is that all such decision aiding approaches can be seen as part of a decision aiding methodology.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {European Journal of Operational Research},
	author = {Tsoukiàs, Alexis},
	month = may,
	year = {2008},
	pages = {138--161},
	file = {Tsoukias - 2008 - From decision theory to decision aiding methodology - European Journal of Operational Research.pdf:/Users/bill/D/Zotero/storage/3EJJZD7I/Tsoukias - 2008 - From decision theory to decision aiding methodology - European Journal of Operational Research.pdf:application/pdf},
}

@misc{hanson2023,
	title = {prioritizr: {Systematic} {Conservation} {Prioritization} in {R}. {R} package version 8.0.3.},
	url = {Available at https://CRAN.R-project.org/package=prioritizr.},
	collaborator = {Hanson, JO and Schuster, R and Morrell, N and Strimas-Mackey, M and Edwards, BPM and Watts, ME and Arcese, P and Bennett, J and Possingham, HP},
	year = {2023},
}

@article{wickham2010jocags,
	title = {A {Layered} {Grammar} of {Graphics}},
	volume = {19},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2009.07098},
	doi = {10.1198/jcgs.2009.07098},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Wickham, Hadley},
	month = jan,
	year = {2010},
	keywords = {ggplot},
	pages = {3--28},
	file = {wickham 2010 - A Layered Grammar of Graphics.pdf:/Users/bill/D/Zotero/storage/UC7IBZ9C/wickham 2010 - A Layered Grammar of Graphics.pdf:application/pdf},
}

@article{meinard2019ejoor,
	title = {On the rationality of decision aiding processes},
	volume = {273},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221718307550},
	doi = {10.1016/j.ejor.2018.09.009},
	abstract = {The notion of rationality plays a crucial role in Decision Aiding (DA), both as a scientiﬁc discipline and as a professional practice. Indeed, a pervasive and possibly constitutive feature of DA is that it constantly faces challenges as to whether it is valid, legitimate, useful, practical, etc. Rationality plays a pivotal role in participating to determine whether DA fulﬁlls such requirements. In this article, we take advantage of arguments developed in the philosophical literature, mainly by Habermas, to introduce a framework deﬁning a series of conceptions of rationality. We use this framework in order to introduce a typology of DA approaches, distinguishing objectivist, conformist, adjustive and reﬂexive approaches. Whereas the underlying conception of rationality plays a key role in determining the features of DA processes, we argue that tools are largely independent of conceptions of rationality. Our reasoning has direct operational implications, which makes it of distinctive interest, not only for philosophers and operational research theoreticians, but also for practitioners. We explain how practitioners should reason in practice to identify which DA approach they should implement in a given situation. We then explain how they can take advantage of our analysis to entrench the legitimacy and validity of their recommendations.},
	language = {en},
	number = {3},
	urldate = {2024-06-15},
	journal = {European Journal of Operational Research},
	author = {Meinard, Y. and Tsoukiàs, A.},
	month = mar,
	year = {2019},
	pages = {1074--1084},
	file = {meinard tsoukias 2019 - on the rationality of decision aiding processes.pdf:/Users/bill/D/Zotero/storage/64PVI3BP/meinard tsoukias 2019 - on the rationality of decision aiding processes.pdf:application/pdf},
}

@article{tsoukias2007aor,
	title = {On the concept of decision aiding process: an operational perspective},
	volume = {154},
	copyright = {http://www.springer.com/tdm},
	issn = {0254-5330, 1572-9338},
	shorttitle = {On the concept of decision aiding process},
	url = {http://link.springer.com/10.1007/s10479-007-0187-z},
	doi = {10.1007/s10479-007-0187-z},
	abstract = {The paper presents the concept of decision aiding process as an extension of the decision process. The aim of the paper is to analyse the type of activities occurring between a “client” and an “analyst” both engaged in a decision process. The decision aiding process is analysed both under a cognitive point of view and an operational point of view: i.e. considering the “products”, or cognitive artifacts the process will deliver at the end. Finally the decision aiding process is considered as a reasoning process for which the update and revision problems hold.},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Annals of Operations Research},
	author = {Tsoukiàs, Alexis},
	month = jul,
	year = {2007},
	pages = {3--27},
	file = {tsoukias 2007 - On the concept of decision aiding process- an operational perspective.pdf:/Users/bill/D/Zotero/storage/X7GMARR3/tsoukias 2007 - On the concept of decision aiding process- an operational perspective.pdf:application/pdf},
}

@article{dormann2008rn,
	title = {Introducing the bipartite package: {Analysing} ecological networks.},
	volume = {8},
	number = {2},
	journal = {R News},
	author = {Dormann, Carsten F. and Gruber, Bernd and Fruend, Jochen},
	year = {2008},
	pages = {8--11},
}

@article{dormann2011nb,
	title = {How to be a specialist? {Quantifying} specialisation in pollination networks.},
	volume = {1},
	number = {1},
	journal = {Network Biology},
	author = {Dormann, Carsten F.},
	year = {2011},
	pages = {1--20},
}

@article{cabeza2001tie,
	title = {Design of reserve networks and the persistence of biodiversity},
	volume = {16},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01695347},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534701021255},
	doi = {10.1016/S0169-5347(01)02125-5},
	language = {en},
	number = {5},
	urldate = {2025-06-22},
	journal = {Trends in Ecology \& Evolution},
	author = {Cabeza, Mar and Moilanen, Atte},
	month = may,
	year = {2001},
	pages = {242--248},
	file = {Cabeza Moilanen 2001 Design of reserve networks and the persistence of biodiversity.pdf:/Users/bill/D/Zotero/storage/3BZSIR5I/Cabeza Moilanen 2001 Design of reserve networks and the persistence of biodiversity.pdf:application/pdf},
}
