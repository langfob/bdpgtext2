
@article{1988orl,
  title = {Resolution vs. Cutting Plane Solution of Inference Problems: Some Computational Experience},
  year = {1988},
  journal = {Operations Research Letters},
  volume = {7},
  number = {1},
  pages = {1--7},
  keywords = {bdpg},
  file = {/Users/bill/D/Zotero/storage/2EPSYV5X/hooker 1988 - Resolution vs. cutting plane solution of inference problems - some computational experience - BDPG.pdf}
}

@incollection{abadiGeneratingSolvedInstances1990,
  title = {On {{Generating Solved Instances}} of {{Computational Problems}}},
  booktitle = {Advances in {{Cryptology}} \textemdash{} {{CRYPTO}}' 88},
  author = {Abadi, Mart{\'i}n and Allendert, Eric and Broder, Andrei and Feigenbaum, Joan and Hemachandra, Lane A.},
  editor = {Goldwasser, Shafi},
  year = {1990},
  volume = {403},
  pages = {297--310},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-34799-2_23},
  abstract = {We consider the efficient generation of solved instances of computational problems. In particular, we consider ;nvulnerable generators. Let S be a subset of ( 0 , l ) and be a Turing Machine that accepts S; a n accepting computation w of M on input x is called a ``witness'' that x E S. Informally, a program is an winvulnerable generator if, on b p u t I'',it produces instance-witness pairs (2,w),with 1x1 = n,according to a distribution under which any polynomial-time adversary who is given I fails to 6nd a witness that x E S , with probability at least a,for infinitely many lengths n.},
  isbn = {978-0-387-97196-4 978-0-387-34799-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8VDFQYIW/AABFH-Crypto88.pdf}
}

@incollection{abadiGeneratingSolvedInstances1990a,
  title = {On {{Generating Solved Instances}} of {{Computational Problems}}},
  booktitle = {Advances in {{Cryptology}} \textemdash{} {{CRYPTO}}' 88},
  author = {Abadi, Mart{\'i}n and Allendert, Eric and Broder, Andrei and Feigenbaum, Joan and Hemachandra, Lane A.},
  editor = {Goldwasser, Shafi},
  year = {1990},
  volume = {403},
  pages = {297--310},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-34799-2_23},
  abstract = {We consider the efficient generation of solved instances of computational problems. In particular, we consider ;nvulnerable generators. Let S be a subset of ( 0 , l ) and be a Turing Machine that accepts S; a n accepting computation w of M on input x is called a ``witness'' that x E S. Informally, a program is an winvulnerable generator if, on b p u t I'',it produces instance-witness pairs (2,w),with 1x1 = n,according to a distribution under which any polynomial-time adversary who is given I fails to 6nd a witness that x E S , with probability at least a,for infinitely many lengths n.},
  isbn = {978-0-387-97196-4 978-0-387-34799-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/R7UDBTAM/Abadi1990_Chapter_OnGeneratingSolvedInstancesOfC.pdf}
}

@article{abdulrahman2017,
  title = {An {{Overview}} of the {{Algorithm Selection Problem}}},
  author = {Abdulrahman, Salisu Mamman and Adamu, Alhassan and Ibrahim, Yazid Ado and Muhammad, Rilwan},
  year = {2017},
  volume = {26},
  number = {1},
  pages = {11},
  abstract = {Users of machine learning algorithms need methods that can help them to identify algorithm or their combinations (workflows) that achieve the potentially best performance. Selecting the best algorithm to solve a given problem has been the subject of many studies over the past four decades. This survey presents an overview of the contributions made in the area of algorithm selection problems. We present different methods for solving the algorithm selection problem identifying some of the future research challenges in this domain.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CXFETVRC/Abdulrahmana et al 2017 - An Overview of the Algorithm Selection Problem - BDPG - ALGORITHM SELECTION.pdf}
}

@incollection{achlioptas1997papocp,
  title = {Random Constraint Satisfaction: {{A}} More Accurate Picture},
  shorttitle = {Random Constraint Satisfaction},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming-CP97}}},
  author = {Achlioptas, Dimitris and Kirousis, Lefteris M. and Kranakis, Evangelos and Krizanc, Danny and Molloy, Michael S. O. and Stamatiou, Yannis C.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Smolka, Gert},
  year = {1997},
  volume = {1330},
  pages = {107--120},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0017433},
  abstract = {Recently there has been a great amount of interest in Random Constraint Satisfaction Problems, both from an experimental and a theoretical point of view. Rather intriguingly, experimental results with various models for generating random CSP instances suggest a "threshold-like" behavior and some theoretical work has been done in analyzing these models when the number of variables becomes large (asymptotic). In this paper we prove that the models commonly used for generating random CSP instances do not have an asymptotic threshold. In particular, we prove that as the number of variables becomes large, almost all instances they generate are trivially overconstrained. We then present a new model for random CSP and, in the spirit of random k-SAT, we derive lower and upper bounds for its parameters so that instances are "almost surely" underconstrained and overconstrained, respectively.},
  isbn = {978-3-540-63753-0 978-3-540-69642-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Q7JFEAX8/achlioptas et al 1997 - Random Constraint Satisfaction - a more accurate picture - BDPG - ANNO.pdf}
}

@inproceedings{achlioptas2000psncaia,
  title = {Generating {{Satisfiable Problem Instances}}},
  booktitle = {Proceedings of the {{Seventeenth National Conference}} on {{Artificial Intelligence}} ({{AAAI-00}}),},
  author = {Achlioptas, Dimitris and Gomes, Carla and Kautz, Henry and Selman, Bart},
  year = {2000},
  pages = {256--261},
  address = {{Austin, TX}},
  abstract = {A major difficulty in evaluating incomplete local search style algorithms for constraint satisfaction problems is the need for a source of hard problem instances that are guaranteed to be satisfiable. A standard approach to evaluate incomplete search methods has been to use a general problem generator and a complete search method to filter out the unsatisfiable instances. Unfortunately, this approach cannot be used to create problem instances that are beyond the reach of complete search methods. So far, it has proven to be surprisingly difficult to develop a direct generator for satisfiable instances only. In this paper, we propose a generator that only outputs satisfiable problem instances. We also show how one can finely control the hardness of the satisfiable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. Finally, we use our problem distribution to show the easy-hard-easy pattern in search complexity for local search procedures, analogous to the previously reported pattern for complete search methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BKKMWQ6U/achlioptas et al 2000 - Generating Satisfiable Problem Instances.pdf}
}

@article{achlioptas2005j,
  title = {Hiding {{Satisfying Assignments}}: {{Two}} Are {{Better}} than {{One}}},
  shorttitle = {Hiding {{Satisfying Assignments}}},
  author = {Achlioptas, D. and Jia, H. and Moore, C.},
  year = {2005},
  month = nov,
  journal = {Journal of Artificial Intelligence Research},
  volume = {24},
  pages = {623--639},
  issn = {1076-9757},
  doi = {10.1613/jair.1681},
  abstract = {The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random kSAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, for a number of different algorithms, A acts as a stronger and stronger attractor as the formula's density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this ``symmetrization'' the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.},
  langid = {english},
  keywords = {bdpg,planted solution},
  file = {/Users/bill/D/Zotero/storage/YY45JH2R/achlioptas et al 2005 - hiding satisfying assignments - two are better than one.pdf}
}

@article{achlioptas2021rsa,
  title = {The Number of Satisfying Assignments of Random 2-{{SAT}} Formulas},
  author = {Achlioptas, Dimitris and Coja-Oghlan, Amin and Hahn-Klimroth, Max and Lee, Joon and M{\"u}ller, No{\"e}la and Penschuck, Manuel and Zhou, Guangyan},
  year = {2021},
  month = jan,
  journal = {Random Structures \& Algorithms},
  pages = {rsa.20993},
  issn = {1042-9832, 1098-2418},
  doi = {10.1002/rsa.20993},
  abstract = {We show that throughout the satisfiable phase the normalized number of satisfying assignments of a random 2-SAT formula converges in probability to an expression predicted by the cavity method from statistical physics. The proof is based on showing that the Belief Propagation algorithm renders the correct marginal probability that a variable is set to ``true'' under a uniformly random satisfying assignment.},
  langid = {english},
  keywords = {2-SAT,bdpg,statistical mechanics},
  file = {/Users/bill/D/Zotero/storage/8TQEWSHT/achlioptas et al 2020 - The number of satisfying assignments of random2-SAT formulas - BDPG - STATISTICAL MECHANICS.pdf}
}

@article{achlioptasAlgorithmicBarriersPhase2008,
  title = {Algorithmic Barriers from Phase Transitions},
  author = {Achlioptas, Dimitris and {Coja-Oghlan}, Amin},
  year = {2008},
  month = oct,
  journal = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  eprint = {0803.2122},
  eprinttype = {arxiv},
  pages = {793--802},
  doi = {10.1109/FOCS.2008.11},
  abstract = {For many random Constraint Satisfaction Problems, by now, we have asymptotically tight estimates of the largest constraint density for which they have solutions. At the same time, all known polynomial-time algorithms for many of these problems already completely fail to find solutions at much smaller densities. For example, it is well-known that it is easy to color a random graph using twice as many colors as its chromatic number. Indeed, some of the simplest possible coloring algorithms already achieve this goal. Given the simplicity of those algorithms, one would expect there is a lot of room for improvement. Yet, to date, no algorithm is known that uses (2 - \k{o}){$\chi$} colors, in spite of efforts by numerous researchers over the years.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/bill/D/Zotero/storage/4QA68EIL/0803.2122.pdf}
}

@article{achlioptasRigorousLocationPhase2005,
  title = {Rigorous Location of Phase Transitions in Hard Optimization Problems},
  author = {Achlioptas, Dimitris and Naor, Assaf and Peres, Yuval},
  year = {2005},
  month = jun,
  journal = {Nature},
  volume = {435},
  number = {7043},
  pages = {759--764},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature03602},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/47VM9J3Q/achlioptas naor peres 2005 - Rigorous location of phase transitions in hard optimization problems.pdf}
}

@incollection{akgunInstanceGenerationGenerator2019,
  title = {Instance {{Generation}} via {{Generator Instances}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {Akg{\"u}n, {\"O}zg{\"u}r and Dang, Nguyen and Miguel, Ian and Salamon, Andr{\'a}s Z. and Stone, Christopher},
  editor = {Schiex, Thomas and {de Givry}, Simon},
  year = {2019},
  volume = {11802},
  pages = {3--19},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30048-7_1},
  abstract = {Access to good benchmark instances is always desirable when developing new algorithms, new constraint models, or when comparing existing ones. Hand-written instances are of limited utility and are timeconsuming to produce. A common method for generating instances is constructing special purpose programs for each class of problems. This can be better than manually producing instances, but developing such instance generators also has drawbacks. In this paper, we present a method for generating graded instances completely automatically starting from a class-level problem specification. A graded instance in our present setting is one which is neither too easy nor too difficult for a given solver. We start from an abstract problem specification written in the Essence language and provide a system to transform the problem specification, via automated type-specific rewriting rules, into a new abstract specification which we call a generator specification. The generator specification is itself parameterised by a number of integer parameters; these are used to characterise a certain region of the parameter space. The solutions of each such generator instance form valid problem instances. We use the parameter tuner irace to explore the space of possible generator parameters, aiming to find parameter values that yield graded instances. We perform an empirical evaluation of our system for five problem classes from CSPlib, demonstrating promising results.},
  isbn = {978-3-030-30047-0 978-3-030-30048-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/94DVN6ES/akgun et al 2019 - Instance Generation via Generator Instances - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf}
}

@incollection{akrourPreferencebasedPolicyLearning2011,
  title = {Preference-Based Policy Learning},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  year = {2011},
  pages = {12--27},
  publisher = {{Springer}},
  file = {/Users/bill/D/Zotero/storage/XHCVGFJI/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf}
}

@incollection{akrourPreferenceBasedPolicyLearning2011,
  title = {Preference-{{Based Policy Learning}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  volume = {6911},
  pages = {12--27},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23780-5_11},
  abstract = {Many machine learning approaches in robotics, based on reinforcement learning, inverse optimal control or direct policy learning, critically rely on robot simulators. This paper investigates a simulatorfree direct policy learning, called Preference-based Policy Learning (PPL). PPL iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies, and the process is iterated until the desired behavior is obtained. PPL requires a good representation of the policy search space be available, enabling one to learn accurate policy return estimates and limiting the human ranking effort needed to yield a good policy. Furthermore, this representation cannot use informed features (e.g., how far the robot is from any target) due to the simulator-free setting. As a second contribution, this paper proposes a representation based on the agnostic exploitation of the robotic log.},
  isbn = {978-3-642-23779-9 978-3-642-23780-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WE7Q6X72/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf}
}

@incollection{akrourPreferenceBasedPolicyLearning2011a,
  title = {Preference-{{Based Policy Learning}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  volume = {6911},
  pages = {12--27},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23780-5_11},
  abstract = {Many machine learning approaches in robotics, based on reinforcement learning, inverse optimal control or direct policy learning, critically rely on robot simulators. This paper investigates a simulatorfree direct policy learning, called Preference-based Policy Learning (PPL). PPL iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies, and the process is iterated until the desired behavior is obtained. PPL requires a good representation of the policy search space be available, enabling one to learn accurate policy return estimates and limiting the human ranking effort needed to yield a good policy. Furthermore, this representation cannot use informed features (e.g., how far the robot is from any target) due to the simulator-free setting. As a second contribution, this paper proposes a representation based on the agnostic exploitation of the robotic log.},
  isbn = {978-3-642-23779-9 978-3-642-23780-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/J4CSIJBN/akrour et al 2011 - preference-based policy learning - OPTISEVIL - RL.pdf}
}

@article{alagador2016mee,
  title = {Climate Change, Species Range Shifts and Dispersal Corridors: An Evaluation of Spatial Conservation Models},
  shorttitle = {Climate Change, Species Range Shifts and Dispersal Corridors},
  author = {Alagador, Diogo and Cerdeira, Jorge Orestes and Ara{\'u}jo, Miguel Bastos},
  editor = {Anderson, Barbara},
  year = {2016},
  month = jul,
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {7},
  pages = {853--866},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.12524},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3J5W8VPT/alagador et al 2016 - climate change species range shifts and dispersal corridors - an evaluation of spatial conservation models - BDPG - GUPPY.pdf}
}

@article{alagador2020mee,
  title = {Revisiting the Minimum Set Cover, the Maximal Coverage Problems and a Maximum Benefit Area Selection Problem to Make Climate-change-concerned Conservation Plans Effective},
  author = {Alagador, Diogo and Cerdeira, Jorge Orestes},
  editor = {Freckleton, Robert},
  year = {2020},
  month = oct,
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {10},
  pages = {1325--1337},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13455},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HI2PSJAE/flagador cerdeira 2020 - revisiting the minimum set cover the maximal coverage problems and a maximum benefit area selection problem to make climate-change-concerned conservation plans effective - BDPG.pdf}
}

@article{albers2016po,
  title = {Spatially-{{Correlated Risk}} in {{Nature Reserve Site Selection}}},
  author = {Albers, Heidi J. and Busby, Gwenlyn M. and Hamaide, Bertrand and Ando, Amy W. and Polasky, Stephen},
  editor = {Linkov, Igor},
  year = {2016},
  month = jan,
  journal = {PLOS ONE},
  volume = {11},
  number = {1},
  pages = {e0146023},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0146023},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5FY8EAR9/albers et al 2016 - Spatially-Correlated Risk in Nature Reserve Site Selection - GUPPY.pdf}
}

@article{albers2017i,
  title = {Economics in {{Systematic Conservation Planning}} for {{Lower-income Countries}}: {{A Literature Review}} and {{Assessment}}},
  shorttitle = {Economics in {{Systematic Conservation Planning}} for {{Lower-income Countries}}},
  author = {Albers, H. J. and Maloney, M. and Robinson, E. J. Z.},
  year = {2017},
  month = may,
  journal = {International Review of Environmental and Resource Economics},
  volume = {/10},
  number = {2},
  pages = {145--182},
  issn = {19321473},
  doi = {10.1561/101.00000085},
  abstract = {Lower-income countries contain much of the world's biodiversity but often lack the institutions and resources for effective biodiversity conservation. Systematic conservation planning (SCP) frameworks provide tools to identify and implement conservation areas effectively and efficiently but rarely address issues central to lower-income countries, which limits SCP's usefulness in these settings. This paper reviews SCP and discusses how to make SCP more relevant in lowerincome countries. Lower-income countries have small conservation budgets, imperfect measures of conservation costs and benefits, and unique institutions that all influence the siting, management, and implementation of protected area networks. In addition, these aspects of the lower-income country setting inform the reaction of people to a protected area, which determines the conservation effectiveness of the protected areas. Overall, the institutional and socioeconomic settings of lower-income countries create additional layers of complexity that should be incorporated into SCP frameworks at the stage of selecting reserve sites to improve the efficiency of conservation policies.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AYBRR44S/albers et al 2017 - Economics in systematic conservation planning for lower Â­income countries - a literature review and assessment - BDPG - COST - SCP - RESERVE SELECTION - UNCERTAINTY - ECONOMICS.pdf}
}

@article{albers2020ere,
  title = {Optimal {{Siting}}, {{Sizing}}, and {{Enforcement}} of {{Marine Protected Areas}}},
  author = {Albers, H. J. and Preonas, L. and Capit{\'a}n, T. and Robinson, E. J. Z. and {Madrigal-Ballestero}, R.},
  year = {2020},
  month = sep,
  journal = {Environmental and Resource Economics},
  volume = {77},
  number = {1},
  pages = {229--269},
  issn = {0924-6460, 1573-1502},
  doi = {10.1007/s10640-020-00472-7},
  abstract = {The design of protected areas, whether marine or terrestrial, rarely considers how people respond to the imposition of no-take sites with complete or incomplete enforcement. Consequently, these protected areas may fail to achieve their intended goal. We present and solve a spatial bio-economic model in which a manager chooses the optimal location, size, and enforcement level of a marine protected area (MPA). This manager acts as a Stackelberg leader, and her choices consider villagers' best response to the MPA in a spatial Nash equilibrium of fishing site and effort decisions. Relevant to lower income country settings but general to other settings, we incorporate limited enforcement budgets, distance costs of traveling to fishing sites, and labor allocation to onshore wage opportunities. The optimal MPA varies markedly across alternative manager goals and budget sizes, but always induce changes in villagers' decisions as a function of distance, dispersal, and wage. We consider MPA managers with ecological conservation goals and with economic goals, and identify the shortcomings of several common manager decision rules, including those focused on: (1) fishery outcomes rather than broader economic goals, (2) fish stocks at MPA sites rather than across the full marinescape, (3) absolute levels rather than additional values, and (4) costless enforcement. Our results demonstrate that such na\"ive or overly narrow decision rules can lead to inefficient MPA designs that miss economic and conservation opportunities.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/S3AXW53A/albers et al 2020 - optimal siting sizing and enforcement of marine protected areas - RESERVE SELECTION - ANNO.pdf}
}

@article{albuquerqueRarityWeightedRichnessSimple2015,
  title = {Rarity-{{Weighted Richness}}: {{A Simple}} and {{Reliable Alternative}} to {{Integer Programming}} and {{Heuristic Algorithms}} for {{Minimum Set}} and {{Maximum Coverage Problems}} in {{Conservation Planning}}},
  shorttitle = {Rarity-{{Weighted Richness}}},
  author = {Albuquerque, Fabio and Beier, Paul},
  editor = {Sueur, C{\'e}dric},
  year = {2015},
  month = mar,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0119905},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0119905},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TRMZ6RQN/albuquerque beier 2015 - rarity-weighted richness - a simple and reliable alternative to integer programming and heuristic algorithms for minimum set and maximum coverage problems in consrvation planning - BD.PDF}
}

@article{alekhnovichMoreAverageCase2003,
  title = {More on Average Case vs Approximation Complexity},
  author = {Alekhnovich, Michael},
  year = {2003},
  journal = {th Annual IEEE Symposium on Foundations of Computer Science},
  pages = {10},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4WBJ4A74/misha.pdf}
}

@article{aliMetalearningApproachAutomatic2006,
  title = {A Meta-Learning Approach to Automatic Kernel Selection for Support Vector Machines},
  author = {Ali, Shawkat and {Smith-Miles}, Kate A.},
  year = {2006},
  month = dec,
  journal = {Neurocomputing},
  volume = {70},
  number = {1-3},
  pages = {173--186},
  issn = {09252312},
  doi = {10.1016/j.neucom.2006.03.004},
  abstract = {Appropriate choice of a kernel is the most important ingredient of the kernel-based learning methods such as support vector machine (SVM). Automatic kernel selection is a key issue given the number of kernels available, and the current trial-and-error nature of selecting the best kernel for a given problem. This paper introduces a new method for automatic kernel selection, with empirical results based on classification. The empirical study has been conducted among five kernels with 112 different classification problems, using the popular kernel based statistical learning algorithm SVM. We evaluate the kernels' performance in terms of accuracy measures. We then focus on answering the question: which kernel is best suited to which type of classification problem? Our meta-learning methodology involves measuring the problem characteristics using classical, distance and distribution-based statistical information. We then combine these measures with the empirical results to present a rule-based method to select the most appropriate kernel for a classification problem. The rules are generated by the decision tree algorithm C5.0 and are evaluated with 10 fold cross validation. All generated rules offer high accuracy ratings.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2VNFKXS7/ali smith-miles 2006 - a meta-learning approach to automatic kernel selection for support vector machines.pdf}
}

@article{allisonMARINERESERVESARE1998,
  title = {{{MARINE RESERVES ARE NECESSARY BUT NOT SUFFICIENT FOR MARINE CONSERVATION}}},
  author = {Allison, Gary W. and Lubchenco, Jane and Carr, Mark H.},
  year = {1998},
  month = feb,
  journal = {Ecological Applications},
  volume = {8},
  number = {sp1},
  pages = {S79-S92},
  issn = {1051-0761},
  doi = {10.1890/1051-0761(1998)8[S79:MRANBN]2.0.CO;2},
  abstract = {The intensity of human pressure on marine systems has led to a push for stronger marine conservation efforts. Recently, marine reserves have become one highly advocated form of marine conservation, and the number of newly designated reserves has increased dramatically. Reserves will be essential for conservation efforts because they can provide unique protection for critical areas, they can provide a spatial escape for intensely exploited species, and they can potentially act as buffers against some management miscalculations and unforeseen or unusual conditions. Reserve design and effectiveness can be dramatically improved by better use of existing scientific understanding. Reserves are insufficient protection alone, however, because they are not isolated from all critical impacts. Communities residing within marine reserves are strongly influenced by the highly variable conditions of the water masses that continuously flow through them. To a much greater degree than in terrestrial systems, the scales of fundamental processes, such as population replenishment, are often much larger than reserves can encompass. Further, they offer no protection from some important threats, such as contamination by chemicals. Therefore, without adequate protection of species and ecosystems outside reserves, effectiveness of reserves will be severely compromised. We outline conditions under which reserves are likely to be effective, provide some guidelines for increasing their conservation potential, and suggest some research priorities to fill critical information gaps. We strongly support vastly increasing the number and size of marine reserves; at the same time, strong conservation efforts outside reserves must complement this effort. To date, most reserve design and site selection have involved little scientific justification. They must begin to do so to increase the likelihood of attaining conservation objectives.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VTGGBGDI/5 - allison lubchenko carr 1998 - marine reserves are necessary but not sufficient for marine conservation.pdf}
}

@article{alonOnlineSetCover,
  title = {The {{Online Set Cover Problem}}},
  author = {Alon, Noga and Awerbuch, Baruch and Azar, Yossi and Buchbinder, Niv},
  pages = {6},
  abstract = {Let X = \{1, 2, . . . , n\} be a ground set of n elements, and let S be a family of subsets of X, |S| = m, with a positive cost cS associated with each S {$\in$} S.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZN3TUJTY/aaabnproc2.pdf}
}

@article{amesHowFindHidden,
  title = {How to Find a Hidden Clique},
  author = {Ames, Brendan},
  pages = {35},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6XDB2QB9/2013-08-16_MOPTA.pdf}
}

@incollection{amiriPushingRandomWalk2007,
  title = {Pushing {{Random Walk Beyond Golden Ratio}}},
  booktitle = {Computer {{Science}} \textendash{} {{Theory}} and {{Applications}}},
  author = {Amiri, Ehsan and Skvortsov, Evgeny},
  editor = {Diekert, Volker and Volkov, Mikhail V. and Voronkov, Andrei},
  year = {2007},
  volume = {4649},
  pages = {44--55},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-74510-5_8},
  abstract = {We propose a simple modification of a well-known Random Walk algorithm for solving the Satisfiability problem and analyze its performance on random CNFs with a planted solution. We rigorously prove that the new algorithm solves the Full CNF with high probability, and for random CNFs with a planted solution of high density finds an assignment that differs from the planted in only {$\epsilon$}-fraction of variables. In the experiments the algorithm solves random CNFs with a planted solution of any density.},
  isbn = {978-3-540-74509-9 978-3-540-74510-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/63XJBV7G/chp%3A10.1007%2F978-3-540-74510-5_8.pdf}
}

@article{amitContributionBiodiversityEcosystem2011,
  title = {Contribution of Biodiversity to Ecosystem Functioning: A Non-Equilibrium Thermodynamic Perspective: {{Contribution}} of Biodiversity to Ecosystem Functioning: A Non-Equilibrium Thermodynamic Perspective},
  shorttitle = {Contribution of Biodiversity to Ecosystem Functioning},
  author = {Amit, Chakraborty and B Larry, Li},
  year = {2011},
  month = feb,
  journal = {Journal of Arid Land},
  volume = {3},
  number = {1},
  pages = {71--74},
  issn = {1674-6767},
  doi = {10.3724/SP.J.1227.2011.00071},
  abstract = {Ecosystem stays far from thermodynamic equilibrium. Through the interactions among biotic and abiotic components, and encompassing physical environments, ecosystem forms a dissipative structure that allows it to dissipate energy continuously and thereby remains functional over time. Biotic regulation of energy and material fluxes in and out of the ecosystem allows it to maintain a homeostatic state which corresponds to a self-organized state emerged in a non-equilibrium thermodynamic system. While the associated self-organizational processes approach to homeostatic state, entropy (a measure of irreversibility) degrades and dissipation of energy increases. We propose here that at a homeostatic state of ecosystem, biodiversity which includes both phenotypic and functional diversity, attains optimal values. As long as biodiversity remains within its optimal range, the corresponding homeostatic state is maintained. However, while embedded environmental conditions fluctuate along the gradient of accelerating changes, phenotypic diversity and functional diversity contribute inversely to the associated self-organizing processes. Furthermore, an increase or decrease in biodiversity outside of its optimal range makes the ecosystem vulnerable to transition into a different state.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NK5EAG5F/Contribution_of_biodiversity_to_ecosystem_function.pdf}
}

@article{andamMeasuringEffectivenessProtected2008,
  title = {Measuring the Effectiveness of Protected Area Networks in Reducing Deforestation},
  author = {Andam, K. S. and Ferraro, P. J. and Pfaff, A. and {Sanchez-Azofeifa}, G. A. and Robalino, J. A.},
  year = {2008},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {42},
  pages = {16089--16094},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0800437105},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BMSAU7I3/3 - andam et al 2008 - measuring the effectiveness of protected area networks in reducing deforestation .pdf}
}

@article{andoLessonsFinanceNew2011,
  title = {Lessons from {{Finance}} for {{New Land-Conservation Strategies Given Climate-Change Uncertainty}}: {{Lessons}} for {{Conservation}} from {{Finance}}},
  shorttitle = {Lessons from {{Finance}} for {{New Land-Conservation Strategies Given Climate-Change Uncertainty}}},
  author = {Ando, Amy W. and Hannah, Lee},
  year = {2011},
  month = apr,
  journal = {Conservation Biology},
  volume = {25},
  number = {2},
  pages = {412--414},
  issn = {08888892},
  doi = {10.1111/j.1523-1739.2011.01648.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IVN498JA/ando hannah 2011 - Lessons from Finance for New Land-Conservation Strategies Given Climate-Change Uncertainty - BDPG - UNCERTAINTY.pdf}
}

@article{andoOptimalPortfolioDesign2012,
  title = {Optimal Portfolio Design to Reduce Climate-Related Conservation Uncertainty in the {{Prairie Pothole Region}}},
  author = {Ando, A. W. and Mallory, M. L.},
  year = {2012},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {17},
  pages = {6484--6489},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1114653109},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WSZ24MJ6/ando mallory 2012 - Optimal portfolio design to reduce climate-related conservation uncertainty in the Prairie Pothole Region - COST - BDPG - ANNO.pdf}
}

@article{angeliniStatisticalPhysicsInference,
  title = {Statistical {{Physics}} of {{Inference}} Problems},
  author = {Angelini, Maria Chiara and Caltagirone, Francesco and Krzakala, Florent},
  pages = {36},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HAEZUZ5P/notes.pdf}
}

@article{antonarakisUncertaintyInitialForest2014,
  title = {Uncertainty in Initial Forest Structure and Composition When Predicting Carbon Dynamics in a Temperate Forest},
  author = {Antonarakis, A.S.},
  year = {2014},
  month = nov,
  journal = {Ecological Modelling},
  volume = {291},
  pages = {134--141},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.07.030},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FFUKDVHF/1-s2.0-S0304380014003780-main.pdf}
}

@inproceedings{applebaumPublickeyCryptographyDifferent2010,
  title = {Public-Key Cryptography from Different Assumptions},
  booktitle = {Proceedings of the 42nd {{ACM}} Symposium on {{Theory}} of Computing - {{STOC}} '10},
  author = {Applebaum, Benny and Barak, Boaz and Wigderson, Avi},
  year = {2010},
  pages = {171},
  publisher = {{ACM Press}},
  address = {{Cambridge, Massachusetts, USA}},
  doi = {10.1145/1806689.1806715},
  isbn = {978-1-4503-0050-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WYJWAT67/ncpkcFull1.pdf}
}

@article{ApproximationAlgorithmsWeighted,
  title = {Approximation {{Algorithms}} for {{Weighted Vertex Cover}}},
  pages = {16},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IAIBQN6Q/iowa state 2010 - approximation algorithms for weighted vertex cover - SLIDES.pdf}
}

@article{arponen2008jae,
  title = {A Successful Community-Level Strategy for Conservation Prioritization},
  author = {Arponen, Anni and Moilanen, Atte and Ferrier, Simon},
  year = {2008},
  month = oct,
  journal = {Journal of Applied Ecology},
  volume = {45},
  number = {5},
  pages = {1436--1445},
  issn = {00218901, 13652664},
  doi = {10.1111/j.1365-2664.2008.01513.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RI8PVF4Q/Arponen, Moilanen, Ferrier - 2008 - A successful community-level strategy for conservation prioritization - Journal of Applied Ecology.pdf}
}

@article{arpQuietingNoisyAntenna2020,
  title = {Quieting a Noisy Antenna Reproduces Photosynthetic Light-Harvesting Spectra},
  author = {Arp, Trevor B. and {Kistner-Morris}, Jed and Aji, Vivek and Cogdell, Richard J. and {van Grondelle}, Rienk and Gabor, Nathaniel M.},
  year = {2020},
  month = jun,
  journal = {Science},
  volume = {368},
  number = {6498},
  pages = {1490--1495},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aba6630},
  abstract = {Photosynthesis achieves near unity light-harvesting quantum efficiency yet it remains unknown whether there exists a fundamental organizing principle giving rise to robust light harvesting in the presence of dynamic light conditions and noisy physiological environments. Here, we present a noise-canceling network model that relates noisy physiological conditions, power conversion efficiency, and the resulting absorption spectra of photosynthetic organisms. Using light conditions in full solar exposure, light filtered by oxygenic phototrophs, and light filtered under seawater, we derived optimal absorption characteristics for efficient solar power conversion. We show how light-harvesting antennae can be tuned to maximize power conversion efficiency by minimizing excitation noise, thus providing a unified theoretical basis for the observed wavelength dependence of absorption in green plants, purple bacteria, and green sulfur bacteria.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/I63MWLUC/arp et al 2020 - Quieting a noisy antenna reproduces photosynthetic light-harvesting spectra - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf}
}

@article{austinSpeciesDistributionModels2007,
  title = {Species Distribution Models and Ecological Theory: {{A}} Critical Assessment and Some Possible New Approaches},
  shorttitle = {Species Distribution Models and Ecological Theory},
  author = {Austin, Mike},
  year = {2007},
  month = jan,
  journal = {Ecological Modelling},
  volume = {200},
  number = {1-2},
  pages = {1--19},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.07.005},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/52A8JVJZ/Austin_2007_Species distribution models and ecological theory.pdf}
}

@book{avrielNonlinearProgrammingAnalysis2003,
  title = {Nonlinear Programming: Analysis and Methods},
  shorttitle = {Nonlinear Programming},
  author = {Avriel, M.},
  year = {2003},
  publisher = {{Dover Publications}},
  address = {{Mineola, NY}},
  isbn = {0-486-43227-0},
  lccn = {T57.8 .A9 2003},
  keywords = {Nonlinear programming},
  file = {/Users/bill/D/Zotero/storage/9DMSGPP9/Avriel_2003_Nonlinear programming.pdf}
}

@article{ayadIndependentSetVertex,
  title = {Independent {{Set}} and {{Vertex Cover}}},
  author = {Ayad, Hanan},
  pages = {2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SG9EWH52/IndependentSet - independent set and vertex cover.pdf}
}

@article{baaschEvaluationThreeStatistical2010,
  title = {An Evaluation of Three Statistical Methods Used to Model Resource Selection},
  author = {Baasch, David M. and Tyre, Andrew J. and Millspaugh, Joshua J. and Hygnstrom, Scott E. and Vercauteren, Kurt C.},
  year = {2010},
  month = feb,
  journal = {Ecological Modelling},
  volume = {221},
  number = {4},
  pages = {565--574},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2009.10.033},
  abstract = {The performance of statistical methods for modeling resource selection by animals is difficult to evaluate with field data because true selection patterns are unknown. Simulated data based on a known probability distribution, though, can be used to evaluate statistical methods. Models should estimate true selection patterns if they are to be useful in analyzing and interpreting field data. We used simulation techniques to evaluate the effectiveness of three statistical methods used in modeling resource selection. We generated 25 use locations per animal and included 10, 20, 40, or 80 animals in samples of use locations. To simulate species of different mobility, we generated use locations at four levels according to a known probability distribution across DeSoto National Wildlife Refuge (DNWR) in eastern Nebraska and western Iowa, USA. We either generated 5 random locations per use location or 10,000 random locations (total) within 4 predetermined areas around use locations to determine how the definition of availability and the number of random locations affected results. We analyzed simulated data using discrete choice, logisticregression, and a maximum entropy method (Maxent). We used a simple linear regression of estimated and known probability distributions and area under receiver operating characteristic curves (AUC) to evaluate the performance of each method. Each statistical method was affected differently by number of animals and random locations used in analyses, level at which selection of resources occurred, and area considered available. Discrete-choice modeling resulted in precise and accurate estimates of the true probability distribution when the area in which use locations were generated was {$\geq$} the area defined to be available. Logistic-regression models were unbiased and precise when the area in which use locations were generated and the area defined to be available were the same size; the fit of these models improved with increased numbers of random locations. Maxent resulted in unbiased and precise estimates of the known probability distribution when the area in which use locations were generated was small (homerange level) and the area defined to be available was large (study area). Based on AUC analyses, all models estimated the selection distribution better than random chance. Results from AUC analyses, however, often contradicted results of the linear regression method used to evaluate model performance. Discretechoice modeling was best able to estimate the known selection distribution in our study area regardless of sample size or number of random locations used in the analyses, but we recommend further studies using simulated data over different landscapes and different resource metrics to confirm our results. Our study offers an approach and guidance for others interested in assessing the utility of techniques for modeling resource selection in their study area.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X9WTIA8I/Baasch et al. - 2010 - An evaluation of three statistical methods used to.pdf}
}

@article{baaschEvaluationThreeStatistical2010a,
  title = {An Evaluation of Three Statistical Methods Used to Model Resource Selection},
  author = {Baasch, David M. and Tyre, Andrew J. and Millspaugh, Joshua J. and Hygnstrom, Scott E. and Vercauteren, Kurt C.},
  year = {2010},
  month = feb,
  journal = {Ecological Modelling},
  volume = {221},
  number = {4},
  pages = {565--574},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2009.10.033},
  abstract = {The performance of statistical methods for modeling resource selection by animals is difficult to evaluate with field data because true selection patterns are unknown. Simulated data based on a known probability distribution, though, can be used to evaluate statistical methods. Models should estimate true selection patterns if they are to be useful in analyzing and interpreting field data. We used simulation techniques to evaluate the effectiveness of three statistical methods used in modeling resource selection. We generated 25 use locations per animal and included 10, 20, 40, or 80 animals in samples of use locations. To simulate species of different mobility, we generated use locations at four levels according to a known probability distribution across DeSoto National Wildlife Refuge (DNWR) in eastern Nebraska and western Iowa, USA. We either generated 5 random locations per use location or 10,000 random locations (total) within 4 predetermined areas around use locations to determine how the definition of availability and the number of random locations affected results. We analyzed simulated data using discrete choice, logisticregression, and a maximum entropy method (Maxent). We used a simple linear regression of estimated and known probability distributions and area under receiver operating characteristic curves (AUC) to evaluate the performance of each method. Each statistical method was affected differently by number of animals and random locations used in analyses, level at which selection of resources occurred, and area considered available. Discrete-choice modeling resulted in precise and accurate estimates of the true probability distribution when the area in which use locations were generated was {$\geq$} the area defined to be available. Logistic-regression models were unbiased and precise when the area in which use locations were generated and the area defined to be available were the same size; the fit of these models improved with increased numbers of random locations. Maxent resulted in unbiased and precise estimates of the known probability distribution when the area in which use locations were generated was small (homerange level) and the area defined to be available was large (study area). Based on AUC analyses, all models estimated the selection distribution better than random chance. Results from AUC analyses, however, often contradicted results of the linear regression method used to evaluate model performance. Discretechoice modeling was best able to estimate the known selection distribution in our study area regardless of sample size or number of random locations used in the analyses, but we recommend further studies using simulated data over different landscapes and different resource metrics to confirm our results. Our study offers an approach and guidance for others interested in assessing the utility of techniques for modeling resource selection in their study area.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2QPAHRIA/fulltext(4).pdf}
}

@book{balintProceedingsSATCHALLENGE2012,
  title = {Proceedings of {{SAT CHALLENGE}} 2012 {{Solver}} and {{Benchmark Descriptions}}},
  shorttitle = {{{SAT CHALLENGE}} 2012},
  author = {Balint, Adrian},
  year = {2012},
  isbn = {978-952-10-8106-4},
  file = {/Users/bill/D/Zotero/storage/TBLWSH3I/sc2012_proceedings.pdf}
}

@incollection{ball2009scpqmact,
  title = {Marxan and Relatives: Software for Spatial Conservation Prioritisation},
  booktitle = {Spatial {{Conservation Prioritisation}}: {{Quantitative Methods}} and {{Computational Tools}}},
  author = {Ball, Ian R and Possingham, Hugh P and Watts, Matthew E.},
  editor = {Moilanen, Atte and Wilson, Kerrie and Possingham, Hugh},
  year = {2009},
  pages = {185--195},
  publisher = {{Oxford University Press}},
  address = {{Oxford, UK}},
  abstract = {chap. 14. - no place to put this in zotero record also, not sure if this citation is completely correct.  copied from beyer ILP paper's bibliography and entered here by hand as best I could}
}

@incollection{balujaRemovingGeneticsStandard1995,
  title = {Removing the {{Genetics}} from the {{Standard Genetic Algorithm}}},
  booktitle = {Machine {{Learning Proceedings}} 1995},
  author = {Baluja, Shumeet and Caruana, Rich},
  year = {1995},
  pages = {38--46},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-377-6.50014-1},
  abstract = {We present an abstraction of the genetic algorithm (GA), termed population-based incremental learning (PBIL), that explicitly maintains the statistics contained in a GA's population, but which abstracts away the crossover operator and redefines the role of the population. This results in PBIL being simpler, both computationally and theoretically, than the GA. Empirical results reported elsewhere show that PBIL is faster and more effective than the GA on a large set of commonly used benchmark problems. Here we present results on a problem custom designed to benefit both from the GA's crossover operator and from its use of a population. The results show that PBIL performs as well as, or better than, GAs carefully tuned to do well on this problem. This suggests that even on problems custom designed for GAs, much of the power of the GA may derive from the statistics maintained implicitly in its population, and not from the population itself nor from the crossover operator.},
  isbn = {978-1-55860-377-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QEM3AD9M/baluja caruana 1995 - removing the genetics from the standard genetic algorithm.pdf}
}

@article{banApplyingEmpiricalEstimates2014,
  title = {Applying Empirical Estimates of Marine Protected Area Effectiveness to Assess Conservation Plans in {{British Columbia}}, {{Canada}}},
  author = {Ban, Natalie C. and McDougall, Chris and Beck, Martina and Salomon, Anne K. and Cripps, Ken},
  year = {2014},
  month = dec,
  journal = {Biological Conservation},
  volume = {180},
  pages = {134--148},
  issn = {00063207},
  doi = {10.1016/j.biocon.2014.09.037},
  abstract = {While efforts to meet international commitments to counter biodiversity declines by establishing networks of marine protected areas (MPAs) continue, assessments of MPAs rarely take into account measures of effectiveness of different categories of protection, or other design principles (size, spacing, governance considerations). We carried out a meta-analysis of ecological effectiveness of IUCN Categories I\textendash II (no-take), IV and VI (MPAs) compared to unprotected areas. We then applied our ecological effectiveness estimates \textendash{} the added benefit of marine protection over and above conventional fisheries management \textendash{} to a gap analysis of existing MPAs, and MPAs proposed by four indigenous groups on the Central Coast of British Columbia, Canada. Additionally, we assessed representation, size, spacing, and governance considerations against MPA design criteria outlined in the literature. We found significant differences in response ratios for IUCN Categories IV and VI MPAs compared to no-take reserves and areas open to fishing, although variability in responses was high. By rescaling the predicted ecological effectiveness ratios (including confidence estimates), we found that, compared to no-take reserves (biodiversity conservation effectiveness 100\%) and open fishing areas (0\% additional biodiversity contribution over and above conventional fisheries management), IUCN Category IV had a predicted effectiveness score of 60\%, ranging between 34\% and 89\% (95\% lower and upper confidence intervals, respectively), and IUCN Category VI had a predicted effectiveness score of 24\% (ranging between \`A12\% and 72\% for the 95\% lower and upper confidence intervals, respectively). We found that the existing MPAs did poorly when compared against most MPA design criteria, whereas the proposed MPA network achieved many of the best practices identified in the literature, and could achieve all if some additional sites were added. By using the Central Coast of British Columbia as a case study, we demonstrated a method for applying empirically-based ecological effectiveness estimates to an assessment of MPA design principles for an existing and proposed network of MPAs.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BP6Y4W7M/9 - ban et al 2014 - Applying empirical estimates of marine protected area effectiveness to assess conservation plans in British Columbia, Canada.pdf}
}

@article{barryErrorUncertaintyHabitat2006,
  title = {Error and Uncertainty in Habitat Models},
  author = {Barry, Simon and Elith, Jane},
  year = {2006},
  journal = {Journal of Applied Ecology},
  volume = {43},
  number = {3},
  pages = {413--423},
  file = {/Users/bill/D/Zotero/storage/ZXYZK86G/Barry_Elith_2006_Error and uncertainty in habitat models.pdf}
}

@article{barthel2002prl,
  title = {Hiding {{Solutions}} in {{Random Satisfiability Problems}}: {{A Statistical Mechanics Approach}}},
  shorttitle = {Hiding {{Solutions}} in {{Random Satisfiability Problems}}},
  author = {Barthel, W. and Hartmann, A. K. and Leone, M. and {Ricci-Tersenghi}, F. and Weigt, M. and Zecchina, R.},
  year = {2002},
  month = apr,
  journal = {Physical Review Letters},
  volume = {88},
  number = {18},
  pages = {188701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.188701},
  langid = {english},
  keywords = {bdpg,solution hiding,solution planting,statistical mechanics},
  file = {/Users/bill/D/Zotero/storage/39QSZASR/barthel et al 2002 - Hiding Solutions in Random Satisfiability Problems - A Statistical Mechanics Approach - BDPG - ANNO.pdf}
}

@article{bartz-beielstein2020acms,
  title = {Benchmarking in {{Optimization}}: {{Best Practice}} and {{Open Issues}}},
  shorttitle = {Benchmarking in {{Optimization}}},
  author = {{Bartz-Beielstein}, Thomas and Doerr, Carola and Bossek, Jakob and Chandrasekaran, Sowmya and Eftimov, Tome and Fischbach, Andreas and Kerschke, Pascal and {Lopez-Ibanez}, Manuel and Malan, Katherine M. and Moore, Jason H. and Naujoks, Boris and Orzechowski, Patryk and Volz, Vanessa and Wagner, Markus and Weise, Thomas},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.03488 [cs, math, stat]},
  eprint = {2007.03488},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, wellspecified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {benchmarking,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Mathematics - Optimization and Control,Statistics - Applications},
  file = {/Users/bill/D/Zotero/storage/WTD7J625/2007.03488.pdf}
}

@article{bealeIncorporatingUncertaintyPredictive2012,
  title = {Incorporating Uncertainty in Predictive Species Distribution Modelling},
  author = {Beale, Colin M. and Lennon, Jack J.},
  year = {2012},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  number = {1586},
  pages = {247--258},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2011.0178},
  abstract = {Motivated by the need to solve ecological problems (climate change, habitat fragmentation and biological invasions), there has been increasing interest in species distribution models (SDMs). Predictions from these models inform conservation policy, invasive species management and disease-control measures. However, predictions are subject to uncertainty, the degree and source of which is often unrecognized. Here, we review the SDM literature in the context of uncertainty, focusing on three main classes of SDM: niche-based models, demographic models and process-based models. We identify sources of uncertainty for each class and discuss how uncertainty can be minimized or included in the modelling process to give realistic measures of confidence around predictions. Because this has typically not been performed, we conclude that uncertainty in SDMs has often been underestimated and a false precision assigned to predictions of geographical distribution. We identify areas where development of new statistical tools will improve predictions from distribution models, notably the development of hierarchical models that link different types of distribution model and their attendant uncertainties across spatial scales. Finally, we discuss the need to develop more defensible methods for assessing predictive performance, quantifying model goodness-of-fit and for assessing the significance of model covariates.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SW9YQV2G/beale lennon 2012 - incorporating uncertainty in predictive species distribution modelling - BDPG - GUPPY - SDM.pdf}
}

@article{beechStochasticApproachMarine2008,
  title = {A Stochastic Approach to Marine Reserve Design: {{Incorporating}} Data Uncertainty},
  shorttitle = {A Stochastic Approach to Marine Reserve Design},
  author = {Beech, Talia and Dowd, Michael and Field, Chris and Hatcher, Bruce and Andr{\'e}fou{\"e}t, Serge},
  year = {2008},
  month = oct,
  journal = {Ecological Informatics},
  volume = {3},
  number = {4-5},
  pages = {321--333},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2008.09.001},
  abstract = {Marine reserves, or protected areas, are used to meet an array of biodiversity and conservation objectives. The design of regional networks of marine reserves is concerned with the problem of where to place the marine protected areas and how to spatially configure them. Quantitative methods for doing this provide important decision support tools for marine managers. The central problem is to balance the costs and benefits of the reserve network, whilst satisfying conservation objectives (hence solving a constrained optimization problem). Current optimization algorithms for reserve design are widely used, but none allow for the systematic incorporation of data uncertainty and its effect on the reserve design solutions. The central purpose of this study is to provide a framework for incorporating uncertain ecological input data into algorithms for designing networks of marine reserves.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FNBLVRIM/beech et al 2008 - a stochastic approach to marine reserve design - incorporating data uncertainty - ecoinf.pdf}
}

@article{beiranvand2017oe,
  title = {Best Practices for Comparing Optimization Algorithms},
  author = {Beiranvand, Vahid and Hare, Warren and Lucet, Yves},
  year = {2017},
  month = dec,
  journal = {Optimization and Engineering},
  volume = {18},
  number = {4},
  eprint = {1709.08242},
  eprinttype = {arxiv},
  pages = {815--848},
  issn = {1389-4420, 1573-2924},
  doi = {10.1007/s11081-017-9366-1},
  abstract = {The final publication is available at Springer via http://dx.doi.org/10.1007/s11081017-9366-1 Comparing, or benchmarking, of optimization algorithms is a complicated task that involves many subtle considerations to yield a fair and unbiased evaluation. In this paper, we systematically review the benchmarking process of optimization algorithms, and discuss the challenges of fair comparison. We provide suggestions for each step of the comparison process and highlight the pitfalls to avoid when evaluating the performance of optimization algorithms. We also discuss various methods of reporting the benchmarking results. Finally, some suggestions for future research are presented to improve the current benchmarking process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65K05,F.2.1; G.1.6,Mathematics - Optimization and Control},
  file = {/Users/bill/D/Zotero/storage/EN2GCNIH/beiranvand et al 2017 - Best Practices for Comparing Optimization Algorithms - BDPG - BENCHMARKING.pdf}
}

@article{ben-shimonMessagePassingColoring2007,
  title = {Message Passing for the Coloring Problem: {{Gallager}} Meets {{Alon}} and {{Kahale}}},
  shorttitle = {Message Passing for the Coloring Problem},
  author = {{Ben-Shimon}, Sonny and Vilenchik, Dan},
  year = {2007},
  month = oct,
  journal = {arXiv:0710.3928 [cs, math]},
  eprint = {0710.3928},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Message passing algorithms are popular in many combinatorial optimization problems. For example, experimental results show that survey propagation (a certain message passing algorithm) is effective in finding proper k-colorings of random graphs in the near-threshold regime. In 1962 Gallager introduced the concept of Low Density Parity Check (LDPC) codes, and suggested a simple decoding algorithm based on message passing. In 1994 Alon and Kahale exhibited a coloring algorithm and proved its usefulness for finding a k-coloring of graphs drawn from a certain planted-solution distribution over kcolorable graphs. In this work we show an interpretation of Alon and Kahale's coloring algorithm in light of Gallager's decoding algorithm, thus showing a connection between the two problems - coloring and decoding. This also provides a rigorous evidence for the usefulness of the message passing paradigm for the graph coloring problem. Our techniques can be applied to several other combinatorial optimization problems and networking-related issues.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/bill/D/Zotero/storage/WLXNHC7Q/0710.3928.pdf}
}

@article{BenchmarksHiddenOptimum,
  title = {Benchmarks with {{Hidden Optimum Solutions}} for {{Set Covering}}, {{Set Packing}} and {{Winner Determination}}},
  pages = {3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/S6BPQ692/xu web page- Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination.pdf}
}

@article{berkePropagationConnectivityRandom,
  title = {Propagation {{Connectivity}} of {{Random Hypergraphs}}},
  author = {Berke, Robert and Onsjo, Mikael},
  pages = {10},
  abstract = {We consider the average-case MLS-3LIN problem, the problem of finding a most likely solution for a given system of perturbed 3LIN-equations generated with equation probability p and perturbation probability q. Our purpose is to investigate the situation for certain message passing algorithms to work for this problem whp We consider the case q = 0 (no perturbation occurs) and analyze the execution of (a simple version of) our algorithm. For characterizing problem instances for which the execution succeeds, we consider their natural 3-uniform hypergraph representation and introduce the notion of ``propagation connectivity'', a generalized connectivity property on 3-uniform hypergraphs. The execution succeeds on a given system of 3LIN-equations if and only if the representing hypergraph is propagation connected. We show upper and lower bounds for equation probability p such that propagation connectivity holds whp on random hypergraphs representing MLS-3LIN instances.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/T2UJQYEY/chp%3A10.1007%2F978-3-642-04944-6_10.pdf}
}

@article{berthet2015acms,
  title = {Optimal {{Testing}} for {{Planted Satisfiability Problems}}},
  author = {Berthet, Quentin},
  year = {2015},
  month = feb,
  journal = {arXiv:1401.2205 [cs, math, stat]},
  eprint = {1401.2205},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We study the problem of detecting planted solutions in a random satisfiability formula. Adopting the formalism of hypothesis testing in statistical analysis, we describe the minimax optimal rates. Our analysis relies on the study of the number of satisfying assignments, for which we prove new results. We also address algorithmic issues, and describe the performance of a new computationally efficient test. This result is compared to a related hypothesis on the hardness of detecting satisfiability of random formulas.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,Computer Science - Computational Complexity,Mathematics - Probability,Mathematics - Statistics Theory,planted solution,satisfiability},
  file = {/Users/bill/D/Zotero/storage/AZCEDL73/berthet 2014 - Optimal Testing for Planted Satisfiability Problems.pdf}
}

@article{berthet2015ejs,
  title = {Optimal Testing for Planted Satisfiability Problems},
  author = {Berthet, Quentin},
  year = {2015},
  journal = {Electronic Journal of Statistics},
  volume = {9},
  number = {1},
  pages = {298--317},
  issn = {1935-7524},
  doi = {10.1214/15-EJS1001},
  abstract = {We study the problem of detecting planted solutions in a random satisfiability formula. Adopting the formalism of hypothesis testing in statistical analysis, we describe the minimax optimal rates of detection. Our analysis relies on the study of the number of satisfying assignments, for which we prove new results. We also address algorithmic issues, and give a computationally efficient test with optimal statistical performance. This result is compared to an average-case hypothesis on the hardness of refuting satisfiability of random formulas.},
  langid = {english},
  keywords = {bdpg,planted solution,satisfiability},
  file = {/Users/bill/D/Zotero/storage/Z6FGBAKF/Berthet_2015_Optimal testing for planted satisfiability problems.pdf}
}

@article{beyer2016em,
  title = {Solving Conservation Planning Problems with Integer Linear Programming},
  author = {Beyer, Hawthorne L. and Dujardin, Yann and Watts, Matthew E. and Possingham, Hugh P.},
  year = {2016},
  month = may,
  journal = {Ecological Modelling},
  volume = {328},
  pages = {14--22},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2016.02.005},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K4KDTT7L/beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - supplementary materials - mmc1.pdf;/Users/bill/D/Zotero/storage/SD4BI2GB/beyer dujardin watts possingham 2016 - Solving conservation planning problems with integer linear programming - BDPG - ANNO.pdf}
}

@article{beyerem,
  title = {Solving Conservation Planning Problems with Integer Linear Programming (Appendices)},
  author = {Beyer, Hawthorne L and Dujardin, Yann and Watts, Matt},
  journal = {Ecological Modelling},
  volume = {328},
  langid = {english}
}

@article{bhatComputingNashEquilibria2004,
  title = {Computing {{Nash Equilibria}} of {{Action-Graph Games}}},
  author = {Bhat, Navin A R and {Leyton-Brown}, Kevin},
  year = {2004},
  pages = {8},
  abstract = {Action-graph games (AGGs) are a fully expressive game representation which can compactly express both strict and context-specific independence between players' utility functions. Actions are represented as nodes in a graph G, and the payoff to an agent who chose the action s depends only on the numbers of other agents who chose actions connected to s. We present algorithms for computing both symmetric and arbitrary equilibria of AGGs using a continuation method. We analyze the worst-case cost of computing the Jacobian of the payoff function, the exponential-time bottleneck step, and in all cases achieve exponential speedup. When the indegree of G is bounded by a constant and the game is symmetric, the Jacobian can be computed in polynomial time.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/9ZWM29GN/1207.4128.pdf}
}

@article{billionnet2015ema,
  title = {Designing {{Robust Nature Reserves Under Uncertain Survival Probabilities}}},
  author = {Billionnet, Alain},
  year = {2015},
  month = aug,
  journal = {Environmental Modeling \& Assessment},
  volume = {20},
  number = {4},
  pages = {383--397},
  issn = {1420-2026, 1573-2967},
  doi = {10.1007/s10666-014-9437-z},
  abstract = {We address the problem of optimal selection of sites to constitute a nature reserve which ensures that a given set of species has fixed survival probabilities. This classic problem has already been considered in the literature of conservation biology. The originality of this article is to consider that the values of the survival probabilities of each species in each potential site may be subject to a certain error while assuming that the number of sites where these probabilities are wrong is limited. We thus define a set of possible survival probability values in each site. We then show how to determine, by solving a relatively simple mixed-integer linear program, an optimal robust reserve, i.e., a reserve which ensures that each species has a certain survival probability whatever the values taken by the survival probabilities in each site, in the set of possible values. The fact of being able to formulate the search for an optimal robust reserve by a mixed-integer linear program provides an easy way to take into account additional constraints on the selection of sites such as, for example, spatial constraints. We report some computational experiments carried out on many hypothetical landscapes to illustrate the concept of robust reserve and show the effectiveness of the approach.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EK4IVBKR/billionnet 2015 - designing robust nature reserves under uncertain survival probabilities - GUPPY - BDPG - ERROR MODELS - ANNO.pdf}
}

@article{billionnet2016ema,
  title = {Designing {{Connected}} and {{Compact Nature Reserves}}},
  author = {Billionnet, Alain},
  year = {2016},
  month = apr,
  journal = {Environmental Modeling \& Assessment},
  volume = {21},
  number = {2},
  pages = {211--219},
  issn = {1420-2026, 1573-2967},
  doi = {10.1007/s10666-015-9465-3},
  abstract = {It is generally accepted that for many species, the ability to get around a reserve promotes their longterm persistence. Here, we measure the ease with which species can move by two spatial criteria: (i) the connectivity of the reserve, that is to say, the possibility to go through the whole reserve without leaving it, and (ii) the compactness of the reserve, that is to say, the remoteness of the sites in relation to each other, the distance between two sites being measured by the shortest distance to travel to get from one site to another without leaving the reserve. To protect the reserve of external disturbances, we also impose a connectivity constraint for the area outside the reserve. This article presents a method based on integer linear programming to define connected and compact reserves. Computational experiments carried out on artificial instances with 400 sites and 100 species are presented to illustrate the effectiveness of the approach.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XTQWPT9U/billionnet 2016 - designing connected and compact nature reserves.pdf}
}

@article{billionnet2017ema,
  title = {How to {{Take}} into {{Account Uncertainty}} in {{Species Extinction Probabilities}} for {{Phylogenetic Conservation Prioritization}}},
  author = {Billionnet, Alain},
  year = {2017},
  month = dec,
  journal = {Environmental Modeling \& Assessment},
  volume = {22},
  number = {6},
  pages = {535--548},
  issn = {1420-2026, 1573-2967},
  doi = {10.1007/s10666-017-9561-7},
  abstract = {In this article, we are concerned with the general problem of choosing from a set of taxa T a subset S to protect in order to try to contribute to halting biodiversity loss as efficiently as possible given limited resources. The protection of a taxon decreases its extinction probability, and the impact of protecting the taxa of S is measured by the resulting expected phylogenetic diversity (ePD) of the set T. The primary challenge posed by this approach lies in determining the extinction probability of a taxon (protected or unprotected). To deal with this difficulty, the uncertainty about the extinction probabilities can be described through a set of possible scenarios, each corresponding to different extinction probability values for each taxon. We show how to determine an Boptimal robust set\^ of taxa to protect, defined as the set of taxa that minimizes the maximum Bregret,\^ i.e., the maximum relative gap, over all the scenarios, between (1) the ePD of T obtained by protecting the taxa of this set and (2) the ePD of T which would be produced by protecting the subset of taxa optimal for the considered scenario. In our experimental conditions covering 100 cases, this gap is almost always less than 1\%. Consequently, the ePD of T obtained by protecting the taxa of the optimal robust set is not far from the maximum ePD of T that could have been obtained if we had known the true scenario. In other words, a way of escaping (in large part, at least) from the uncertainty related to the extinction probabilities of the taxa consists of choosing to protect those belonging to the optimal robust set. We also compare the optimal robust set to other relevant subsets of T.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AY3B6P9C/billionnet 2017 - how to take into account uncertinty in species extinction probabilities for phylogenetic conservation prioritization.pdf}
}

@article{billionnet2018bc,
  title = {Quantifying Extinction Probabilities of Endangered Species for Phylogenetic Conservation Prioritization May Not Be as Sensitive as Might Be Feared},
  author = {Billionnet, Alain},
  year = {2018},
  month = apr,
  journal = {Biodiversity and Conservation},
  volume = {27},
  number = {5},
  pages = {1189--1200},
  issn = {0960-3115, 1572-9710},
  doi = {10.1007/s10531-017-1487-5},
  abstract = {In this study we are concerned with the general problem of choosing from a set of endangered species T a subset S of k species to protect as a priority. Here, the interest to protect the species of S is assessed by the resulting expected phylogenetic diversity (ePD) of the set T, a widely used criterion for measuring the expected amount of evolutionary history associated with T. We consider that the survival of the protected species is assured and, on the contrary, that there is a risk of extinction for the unprotected species. The problem is easy to solve by a greedy type method if the extinction probabilities of the unprotected species are known but these probabilities are generally not easy to quantify. We show in this note that the choice of the precise values attributed to the extinction probabilities\textemdash provided it respects the rank of imperilment of each species\textemdash is not as decisive as might be feared for the considered problem. The values of these probabilities have a clear impact on the selection of the species to be protected but a little impact on the resulting ePD. More precisely, if T1 and T2 are the two optimal subsets of species corresponding to two scenarios (two different sets of probabilities) the ePDs of T1 and T2, calculated with the probabilities of the first scenario\textemdash or with the probabilities of the second scenario\textemdash are not very different.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DHZYKREW/billionnet 2018 - Quantifying extinction probabilities of endangered species for phylogenetic conservation prioritization may not be as sensitive as might be feared - BDPG.pdf}
}

@article{billionnetMathematicalOptimizationIdeas2013,
  title = {Mathematical Optimization Ideas for Biodiversity Conservation},
  author = {Billionnet, Alain},
  year = {2013},
  month = dec,
  journal = {European Journal of Operational Research},
  volume = {231},
  number = {3},
  pages = {514--534},
  issn = {03772217},
  doi = {10.1016/j.ejor.2013.03.025},
  abstract = {Several major environmental issues like biodiversity loss and climate change currently concern the international community. These topics that are related to the development of human societies have become increasingly important since the United Nations Conference on Environment and Development (UNCED) or Earth Summit in Rio de Janeiro in 1992. In this article, we are interested in the first issue. We present here many examples of the help that using mathematical programming can provide to decision-makers in the protection of biodiversity. The examples we have chosen concern the selection of nature reserves, the control of adverse effects caused by landscape fragmentation, including the creation or restoration of biological corridors, the ecological exploitation of forests, the control of invasive species, and the maintenance of genetic diversity. Most of the presented models are \textendash{} or can be approximated with \textendash{} linear-, quadratic- or fractional-integer formulations and emphasize spatial aspects of conservation planning. Many of them represent decisions taken in a static context but temporal dimension is also considered. The problems presented are generally difficult combinatorial optimization problems, some are well solved and others less well. Research is still needed to progress in solving them in order to deal with real instances satisfactorily. Moreover, relations between researchers and practitioners have to be strengthened. Furthermore, many recent achievements in the field of robust optimization could probably be successfully used for biodiversity protection, a domain in which many data are uncertain.},
  langid = {english},
  keywords = {bdpg,ILP,reserve selection},
  file = {/Users/bill/D/Zotero/storage/PRZKSG8U/billionnet 2013 - Mathematical optimization ideas for biodiversity conservation - BDPG.pdf}
}

@article{billionnetPhylogeneticConservationPrioritization2018,
  title = {Phylogenetic Conservation Prioritization with Uncertainty},
  author = {Billionnet, Alain},
  year = {2018},
  month = oct,
  journal = {Biodiversity and Conservation},
  volume = {27},
  number = {12},
  pages = {3137--3153},
  issn = {0960-3115, 1572-9710},
  doi = {10.1007/s10531-018-1593-z},
  abstract = {We consider a set of species S and are interested in the assessment of the subsets of S from a phylogenetic diversity viewpoint. Several measures can be used for this assessment. Here we have retained phylogenetic diversity (PD) in the sense of Faith, a measure widely used to reflect the evolutionary history accumulated by a group of species. The PD of a group of species X included in S is easy to calculate when the phylogenetic tree associated with S is perfectly known but this situation is rarely verified. We are interested here in cases where uncertainty regarding the length of branches and the topology of the tree is reflected in the fact that several phylogenetic trees are considered to be plausible for the set S. We propose several measures of the phylogenetic diversity to take account of the uncertainty arising from this situation. A natural problem in the field of biological conservation is to select the best subset of species to protect from a group of threatened species. Here, the best subset is the one that optimizes the proposed measures. We show how to solve these optimal selection problems by integer linear programming. The approach is illustrated by several examples.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SVCZLNA2/Billionnet 2018_Article_PhylogeneticConservationPriori.pdf}
}

@article{billionnetSolvingProbabilisticReserve2011,
  title = {Solving the Probabilistic Reserve Selection Problem},
  author = {Billionnet, Alain},
  year = {2011},
  month = feb,
  journal = {Ecological Modelling},
  volume = {222},
  number = {3},
  pages = {546--554},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2010.10.009},
  abstract = {The Reserve Selection Problem consists in selecting certain sites among a set of potential sites for biodiversity protection. In many models of the literature, the species present and able to survive in each site are supposed to be known. Here, for every potential site and for every species considered, only the probability that the species survives in the site is supposed to be known. The problem to select, under a budgetary constraint, a set of sites which maximizes the expected number of species is known in the literature under the name of probabilistic reserve selection problem. In this article, this problem is studied with species weighting to deal differently with common species and rare species. A spatial constraint is also considered preventing to obtain too fragmented reserve networks. As in Polasky et al. (2000), the problem is formulated by a nonlinear mathematical program in Boolean variables. Camm et al. (2002) developed a mixed-integer linear programming approximation that may be solved with standard integer programming software. The method gives tight approximate solutions but does not allow to tell how far these solutions are from the optimum. In this paper, a slightly different approach is proposed to approximate the problem. The interesting aspect of the approach, which also uses only standard mixed-integer programming software, is that it leads, not only to an approximate solution, but also to an upper limit on the true optimal value. In other words, the method gives an approximate solution with a guarantee on its accuracy. The linear reformulation is based on an upper approximation of the logarithmic function by a piecewise-linear function. The approach is very effective on artificial instances that include up to 400 sites and 300 species. Within an average CPU time of about 12 min, near-optimal solutions are obtained with an average relative error, in comparison to the optimum, of less than 0.2\%.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MK3PIBVN/Billionnet 2010 - Solving the probabilistic reserve selection problem - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf}
}

@article{biroliPhaseTransitionsComplexity2002,
  title = {Phase Transitions and Complexity in Computer Science: An Overview of the Statistical Physics Approach to the Random Satis\"yability Problem},
  author = {Biroli, Giulio and Cocco, Simona and Monasson, Re{\~A}mi},
  year = {2002},
  journal = {Physica A},
  pages = {14},
  abstract = {Phase transitions, ubiquitous in condensed matter physics, are encountered in computer science too. The existence of critical phenomena has deep consequences on computational complexity, that is the resolution times of various optimization or decision problems. Concepts and methods borrowed from the statistical physics of disordered and out-of-equilibrium systems shed new light on the dynamical operation of solving algorithms. c 2002 Published by Elsevier Science B.V.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/I5UAUIZG/biroli et al 2002 - Phase transitions and complexity in computer science - an overview of the statistical physics approach to the random satisfiability problem.pdf}
}

@article{bischlASlibBenchmarkLibrary2016,
  title = {{{ASlib}}: {{A}} Benchmark Library for Algorithm Selection},
  shorttitle = {{{ASlib}}},
  author = {Bischl, Bernd and Kerschke, Pascal and Kotthoff, Lars and Lindauer, Marius and Malitsky, Yuri and Fr{\'e}chette, Alexandre and Hoos, Holger and Hutter, Frank and {Leyton-Brown}, Kevin and Tierney, Kevin and Vanschoren, Joaquin},
  year = {2016},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {237},
  pages = {41--58},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.04.003},
  abstract = {The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it difficult to share and compare different approaches effectively, as is done in other, more established fields. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of different scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve significant performance improvements across a broad range of problems and algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GN2Z4WWN/2016-AIJ-ASlib.pdf}
}

@article{blandSTATISTICALMETHODSASSESSING,
  title = {{{STATISTICAL METHODS FOR ASSESSING AGREEMENT BETWEEN TWO METHODS OF CLINICAL MEASUREMENT}}},
  author = {Bland, J Martin and Altman, Douglas G},
  pages = {9},
  abstract = {In clinical measurement comparison of a new measurement technique with an established one is often needed to see whether they agree sufficiently for the new to replace the old. Such investigations are often analysed inappropriately, notably by using correlation coefficients. The use of correlation is misleading. An alternative approach, based on graphical techniques and simple calculations, is described, together with the relation between this analysis and the assessment of repeatability.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WFNPVMS8/Statistical methods for assessing agreement.pdf}
}

@article{bluthgenWHATINTERACTIONNETWORK2008,
  title = {{{WHAT DO INTERACTION NETWORK METRICS TELL US ABOUT SPECIALIZATION AND BIOLOGICAL TRAITS}}},
  author = {Bl{\"u}thgen, Nico and Fr{\"u}nd, Jochen and V{\'a}zquez, Diego P. and Menzel, Florian},
  year = {2008},
  month = dec,
  journal = {Ecology},
  volume = {89},
  number = {12},
  pages = {3387--3399},
  issn = {0012-9658},
  doi = {10.1890/07-2121.1},
  abstract = {The structure of ecological interaction networks is often interpreted as a product of meaningful ecological and evolutionary mechanisms that shape the degree of specialization in community associations. However, here we show that both unweighted network metrics (connectance, nestedness, and degree distribution) and weighted network metrics (interaction evenness, interaction strength asymmetry) are strongly constrained and biased by the number of observations. Rarely observed species are inevitably regarded as ``specialists,'' irrespective of their actual associations, leading to biased estimates of specialization. Consequently, a skewed distribution of species observation records (such as the lognormal), combined with a relatively low sampling density typical for ecological data, already generates a ``nested'' and poorly ``connected'' network with ``asymmetric interaction strengths'' when interactions are neutral. This is confirmed by null model simulations of bipartite networks, assuming that partners associate randomly in the absence of any specialization and any variation in the correspondence of biological traits between associated species (trait matching). Variation in the skewness of the frequency distribution fundamentally changes the outcome of network metrics. Therefore, interpretation of network metrics in terms of fundamental specialization and trait matching requires an appropriate control for such severe constraints imposed by information deficits. When using an alternative approach that controls for these effects, most natural networks of mutualistic or antagonistic systems show a significantly higher degree of reciprocal specialization (exclusiveness) than expected under neutral conditions. A higher exclusiveness is coherent with a tighter coevolution and suggests a lower ecological redundancy than implied by nested networks.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CNNXBMER/bluethgen_et_al_2008.pdf}
}

@article{bolMatthewEffectScience2018,
  title = {The {{Matthew}} Effect in Science Funding},
  author = {Bol, Thijs and {de Vaan}, Mathijs and {van de Rijt}, Arnout},
  year = {2018},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {19},
  pages = {4887--4890},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719557115},
  abstract = {A classic thesis is that scientific achievement exhibits a ``Matthew effect'': Scientists who have previously been successful are more likely to succeed again, producing increasing distinction. We investigate to what extent the Matthew effect drives the allocation of research funds. To this end, we assembled a dataset containing all review scores and funding decisions of grant proposals submitted by recent PhDs in a \texteuro 2 billion granting program. Analyses of review scores reveal that early funding success introduces a growing rift, with winners just above the funding threshold accumulating more than twice as much research funding (\texteuro 180,000) during the following eight years as nonwinners just below it. We find no evidence that winners' improved funding chances in subsequent competitions are due to achievements enabled by the preceding grant, which suggests that early funding itself is an asset for acquiring later funding. Surprisingly, however, the emergent funding gap is partly created by applicants, who, after failing to win one grant, apply for another grant less often.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K8IN2XKJ/1719557115.full.pdf}
}

@article{bondRealEffectsFinancial2012,
  title = {The {{Real Effects}} of {{Financial Markets}}},
  author = {Bond, Philip and Edmans, Alex and Goldstein, Itay},
  year = {2012},
  month = oct,
  journal = {Annual Review of Financial Economics},
  volume = {4},
  number = {1},
  pages = {339--360},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110311-101826},
  abstract = {A large amount of activity in the financial sector occurs in secondary financial markets, where securities are traded among investors without capital flowing to firms. The stock market is the archetypal example, which in most developed economies captures a lot of attention and resources. Is the stock market just a sideshow or does it affect real economic activity? In this review, we discuss the potential real effects of financial markets that stem from the informational role of market prices. We review the theoretical literature and show that accounting for the feedback effect from market prices to the real economy significantly changes our understanding of the price formation process, the informativeness of the price, and speculators' trading behavior. We make two main points. First, we argue that a new definition of price efficiency is needed to account for the extent to which prices reflect information that is useful for the efficiency of real decisions (rather than the extent to which they forecast future cash flows). Second, incorporating the feedback effect into models of financial markets can explain various market phenomena that otherwise seem puzzling. Finally, we review empirical evidence on the real effects of secondary financial markets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MU7LLEIX/bond et al 2012 - The Real Effects of Financial Markets - FINANCE - OPTISEVIL.pdf}
}

@article{bordewichBudgetedNatureReserve2012,
  title = {Budgeted {{Nature Reserve Selection}} with Diversity Feature Loss and Arbitrary Split Systems},
  author = {Bordewich, Magnus and Semple, Charles},
  year = {2012},
  month = jan,
  journal = {Journal of Mathematical Biology},
  volume = {64},
  number = {1-2},
  pages = {69--85},
  issn = {0303-6812, 1432-1416},
  doi = {10.1007/s00285-011-0405-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XBTLKHZ7/BS11.pdf}
}

@article{bordewichNatureReserveSelection2008,
  title = {Nature {{Reserve Selection Problem}}: {{A Tight Approximation Algorithm}}},
  shorttitle = {Nature {{Reserve Selection Problem}}},
  author = {Bordewich, M. and Semple, C.},
  year = {2008},
  month = apr,
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {5},
  number = {2},
  pages = {275--280},
  issn = {1545-5963},
  doi = {10.1109/TCBB.2007.70252},
  abstract = {The Nature Reserve Selection Problem is a problem that arises in the context of studying biodiversity conservation. Subject to budgetary constraints, the problem is to select a set of regions to be conserved so that the phylogenetic diversity of the set of species contained within those regions is maximized. Recently, it has been shown in a paper by Moulton et al. that this problem is NP-hard. In this paper, we establish a tight polynomial-time approximation algorithm for the Nature Reserve Section Problem. Furthermore, we resolve a question on the computational complexity of a related problem left open by Moulton et al.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XGYZ33EG/6307.pdf}
}

@article{bordewichNatureReserveSelection2008a,
  title = {Nature {{Reserve Selection Problem}}: {{A Tight Approximation Algorithm}}},
  shorttitle = {Nature {{Reserve Selection Problem}}},
  author = {Bordewich, M. and Semple, C.},
  year = {2008},
  month = apr,
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {5},
  number = {2},
  pages = {275--280},
  issn = {1545-5963},
  doi = {10.1109/TCBB.2007.70252},
  abstract = {The Nature Reserve Selection Problem is a problem that arises in the context of studying biodiversity conservation. Subject to budgetary constraints, the problem is to select a set of regions to conserve so that the phylogenetic diversity of the set of species contained within those regions is maximized. Recently, it was shown in a paper by Moulton et al. that this problem is NP-hard. In this paper, we establish a tight polynomial-time approximation algorithm for the Nature Reserve Section Problem. Furthermore, we resolve a question on the computational complexity of a related problem left open in Moulton et al.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HAVRPXG2/BS07.pdf}
}

@article{borrett2001c,
  title = {A {{Context}} for {{Constraint Satisfaction Problem Formulation Selection}}},
  author = {Borrett, James E and Tsang, Edward P K},
  year = {2001},
  journal = {Constraints},
  volume = {6},
  number = {4},
  pages = {299--327},
  issn = {1572-9354},
  doi = {10.1023/A:1011432307724},
  abstract = {Much research effort has been applied to finding effective ways for solving constraint satisfaction problems. However, the most fundamental aspect of constraint satisfaction problem solving, problem formulation, has received much less attention. This is important because the selection of an appropriate formulation can have dramatic effects on the efficiency of any constraint satisfaction problem solving algorithm.},
  langid = {english},
  keywords = {bdpg,constraint satisfaction problems,csp,EFs},
  file = {/Users/bill/D/Zotero/storage/YJDG4ESI/borrett tsang 2001 - a context for constraint satisfaction problem formulation selection - BDPG - ANNO.pdf}
}

@article{botsfordPRINCIPLESDESIGNMARINE2003,
  title = {{{PRINCIPLES FOR THE DESIGN OF MARINE RESERVES}}},
  author = {Botsford, Louis W. and Micheli, Fiorenza and Hastings, Alan},
  year = {2003},
  month = feb,
  journal = {Ecological Applications},
  volume = {13},
  number = {sp1},
  pages = {25--31},
  issn = {1051-0761},
  doi = {10.1890/1051-0761(2003)013[0025:PFTDOM]2.0.CO;2},
  abstract = {The theory underlying the design of marine reserves, whether the goal is to preserve biodiversity or to manage fisheries, is still in its infancy. For both of these goals, there is a need for general principles on which to base marine reserve design, and because of the paucity of empirical experience, these principles must be based on models. However, most of the theoretical studies to date have been specific to a single situation, with few attempts to deduce general principles. Here we attempt to distill existing results into general principles useful to designers of marine reserves. To answer the question of how fishery management using reserves compares to conventional management, we provide two principles: (1) the effect of reserves on yield per recruit is similar to increasing the age of first capture, and (2) the effect of reserves on yield is similar to reducing effort. Another two principles answer the question of how to design reserve configurations so that species with movement in various stages will be sustainable: (3) higher juvenile and adult movement lowers sustainability of reserves for biodiversity, but an intermediate level of adult movement is required for reserves for fishery management, and (4) longer larval dispersal distance requires larger reserves for sustainability. These principles provide general guidelines for design, and attention to them will allow more rapid progress in future modeling studies. Whether populations or communities will persist under any specific reserve design is uncertain, and we suggest ways of dealing with that uncertainty.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y9FA7FT7/6 - botsford et al 2003 - principles for the design of marine reserves.pdf}
}

@article{boussemart,
  title = {Description and {{Representation}} of the {{Problems}} Selected for the {{First International Constraint Satisfaction Solver Competition}}},
  author = {Boussemart, Frederic and Hemery, Fred and Lecoutre, Christophe},
  pages = {20},
  abstract = {In this paper, we present the problems that have been selected for the first international competition of CSP solvers. First, we introduce a succinct description of each problem and then, we present the two formats that have been used to represent the CSP instances.},
  langid = {english},
  keywords = {bdpg,benchmarking,csp},
  file = {/Users/bill/D/Zotero/storage/AVYIAATI/boussemart et al 2006 - Description and Representation of the Problemsselected for the First International ConstraintSatisfaction Solver Competition - BDPG.pdf}
}

@article{bowlyGenerationTechniquesLinear2019,
  title = {Generation Techniques for Linear Programming Instances with Controllable Properties},
  author = {Bowly, Simon and {Smith-Miles}, Kate and Baatar, Davaatseren and Mittelmann, Hans},
  year = {2019},
  month = aug,
  journal = {Mathematical Programming Computation},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-019-00170-6},
  abstract = {This paper addresses the problem of generating synthetic test cases for experimentation in linear programming. We propose a method which maps instance generation and instance space search to an alternative encoded space. This allows us to develop a generator for feasible bounded linear programming instances with controllable properties. We show that this method is capable of generating any feasible bounded linear program, and that parameterised generators and search algorithms using this approach generate only feasible bounded instances. Our results demonstrate that controlled generation and instance space search using this method achieves feature diversity more effectively than using a direct representation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EFQD2YAQ/bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf}
}

@article{bowlyGenerationTechniquesLinear2020,
  title = {Generation Techniques for Linear Programming Instances with Controllable Properties},
  author = {Bowly, Simon and {Smith-Miles}, Kate and Baatar, Davaatseren and Mittelmann, Hans},
  year = {2020},
  month = sep,
  journal = {Mathematical Programming Computation},
  volume = {12},
  number = {3},
  pages = {389--415},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-019-00170-6},
  abstract = {This paper addresses the problem of generating synthetic test cases for experimentation in linear programming. We propose a method which maps instance generation and instance space search to an alternative encoded space. This allows us to develop a generator for feasible bounded linear programming instances with controllable properties. We show that this method is capable of generating any feasible bounded linear program, and that parameterised generators and search algorithms using this approach generate only feasible bounded instances. Our results demonstrate that controlled generation and instance space search using this method achieves feature diversity more effectively than using a direct representation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HSWS7UXS/bowly smith-miles baatar mittelmann 2019 - generation techniques for linear programming instances with controllable properties - BDPG - PROBLEM DIFFICULTY - PROBLEM GENERATION.pdf}
}

@article{brockington1994,
  title = {Camouflaging {{Independent Sets}} in {{Quasi-Random Graphs}}},
  author = {Brockington, Mark and Culberson, Joseph C},
  year = {1994},
  pages = {15},
  abstract = {In this paper, we look at the problem of how one might try to hide a large independent set in a graph in which all other independent sets are signi cantly smaller.},
  langid = {english},
  keywords = {bdpg,solution hiding,test generation},
  file = {/Users/bill/D/Zotero/storage/CVM5RYFN/brockington culberson 1994 - camouflaging independent sets in squasi-random graphs - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{brotonsSpeciesDistributionModels2014,
  title = {Species {{Distribution Models}} and {{Impact Factor Growth}} in {{Environmental Journals}}: {{Methodological Fashion}} or the {{Attraction}} of {{Global Change Science}}},
  shorttitle = {Species {{Distribution Models}} and {{Impact Factor Growth}} in {{Environmental Journals}}},
  author = {Brotons, Llu{\'i}s},
  editor = {Fontaneto, Diego},
  year = {2014},
  month = nov,
  journal = {PLoS ONE},
  volume = {9},
  number = {11},
  pages = {e111996},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0111996},
  abstract = {In this work, I evaluate the impact of species distribution models (SDMs) on the current status of environmental and ecological journals by asking the question to which degree development of SDMs in the literature is related to recent changes in the impact factors of ecological journals. The hypothesis evaluated states that research fronts are likely to attract research attention and potentially drive citation patterns, with journals concentrating papers related to the research front receiving more attention and benefiting from faster increases in their impact on the ecological literature. My results indicate a positive relationship between the number of SDM related articles published in a journal and its impact factor (IF) growth during the period 2000\textendash 09. However, the percentage of SDM related papers in a journal was strongly and positively associated with the percentage of papers on climate change and statistical issues. The results support the hypothesis that global change science has been critical in the development of SDMs and that interest in climate change research in particular, rather than the usage of SDM per se, appears as an important factor behind journal IF increases in ecology and environmental sciences. Finally, our results on SDM application in global change science support the view that scientific interest rather than methodological fashion appears to be the major driver of research attraction in the scientific literature.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TJGX9QML/brotons 2014 - Species Distribution Models and Impact Factor Growth in Environmental Journals - Methodological Fashion or the Attraction of Global Change Science - BDPG - GUPPY - SDM.pdf}
}

@article{brown2015pnasu,
  title = {Effective Conservation Requires Clear Objectives and Prioritizing Actions, Not Places or Species},
  author = {Brown, Christopher J. and Bode, Michael and Venter, Oscar and Barnes, Megan D. and McGowan, Jennifer and Runge, Claire A. and Watson, James E. M. and Possingham, Hugh P.},
  year = {2015},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {32},
  pages = {E4342-E4342},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1509189112},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QFM7G3G8/brown et al 2015 - effective conservation requires clear objectives and prioritizing actions not places or species - BDPG.pdf}
}

@article{burgFindingSmoothestPath2010,
  title = {Finding the {{Smoothest Path}} to {{Success}}: {{Model Complexity}} and the {{Consideration}} of {{Nonlinear Patterns}} in {{Nest-Survival Data}}},
  shorttitle = {Finding the {{Smoothest Path}} to {{Success}}},
  author = {van der Burg, Max Post and Powell, Larkin A. and Tyre, Andrew J.},
  year = {2010},
  month = aug,
  journal = {The Condor},
  volume = {112},
  number = {3},
  pages = {421--431},
  issn = {0010-5422, 1938-5129},
  doi = {10.1525/cond.2010.090053},
  abstract = {Quantifying patterns of nest survival is a first step toward understanding why birds decide when and where to breed. Most studies of nest survival have relied on generalized linear models (GLM) to explore these patterns. However, GLMs require assumptions about the models' structure that might preclude finding nonlinear patterns in survival data. Generalized additive models (GAM) provide a flexible alternative to GLMs for estimating linear and nonlinear patterns in data. Here we present a comparison of GLMs and GAMs for explaining variation in nest-survival data. We used two different model-selection criteria, the Bayes (BIC) and Akaike (AIC) information criteria, to select among simple and complex models. Our study was focused on the analysis of Redwinged Blackbird (Agelaius phoeniceus) nests in the Rainwater Basin wetlands of south-central Nebraska. Under BIC, our quadratic model of nest age had the most support, and the model predicted a concave pattern of daily nest survival. We found more model-selection uncertainty under AIC and found support for additive models with ordinal effects of both day and age. These models predicted much more temporal variation than did the linear models. Following our analysis, we discuss some of the advantages and disadvantages of GAMs. Despite the possible limitations of GAMs, our results suggest that they provide an efficient and flexible way to demonstrate nonlinear patterns in nest-survival data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6JN4MDV2/Burg et al. - 2010 - Finding the Smoothest Path to Success Model Compl.pdf}
}

@article{burgFindingSmoothestPath2010a,
  title = {Finding the {{Smoothest Path}} to {{Success}}: {{Model Complexity}} and the {{Consideration}} of {{Nonlinear Patterns}} in {{Nest-Survival Data}}},
  shorttitle = {Finding the {{Smoothest Path}} to {{Success}}},
  author = {van der Burg, Max Post and Powell, Larkin A. and Tyre, Andrew J.},
  year = {2010},
  month = aug,
  journal = {The Condor},
  volume = {112},
  number = {3},
  pages = {421--431},
  issn = {0010-5422, 1938-5129},
  doi = {10.1525/cond.2010.090053},
  abstract = {Quantifying patterns of nest survival is a first step toward understanding why birds decide when and where to breed. Most studies of nest survival have relied on generalized linear models (GLM) to explore these patterns. However, GLMs require assumptions about the models' structure that might preclude finding nonlinear patterns in survival data. Generalized additive models (GAM) provide a flexible alternative to GLMs for estimating linear and nonlinear patterns in data. Here we present a comparison of GLMs and GAMs for explaining variation in nest-survival data. We used two different model-selection criteria, the Bayes (BIC) and Akaike (AIC) information criteria, to select among simple and complex models. Our study was focused on the analysis of Redwinged Blackbird (Agelaius phoeniceus) nests in the Rainwater Basin wetlands of south-central Nebraska. Under BIC, our quadratic model of nest age had the most support, and the model predicted a concave pattern of daily nest survival. We found more model-selection uncertainty under AIC and found support for additive models with ordinal effects of both day and age. These models predicted much more temporal variation than did the linear models. Following our analysis, we discuss some of the advantages and disadvantages of GAMs. Despite the possible limitations of GAMs, our results suggest that they provide an efficient and flexible way to demonstrate nonlinear patterns in nest-survival data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/D2KIJ64B/cond.2010.090053.pdf}
}

@article{cabeza2003cb,
  title = {Site-{{Selection Algorithms}} and {{Habitat Loss}}},
  author = {Cabeza, Mar and Moilanen, Atte},
  year = {2003},
  month = oct,
  journal = {Conservation Biology},
  volume = {17},
  number = {5},
  pages = {1402--1413},
  issn = {0888-8892, 1523-1739},
  doi = {10.1046/j.1523-1739.2003.01421.x},
  abstract = {Site-selection algorithms are used in reserve design to select networks of sites that maximize biodiversity, given some constraints. These algorithms are based on a snapshot of species occurrence, and they typically aim to minimize the area or cost needed to represent all the species once or a few times. Most of these algorithms ignore the question of how well species are likely to persist in the set of selected sites in the long term. Furthermore, the role of the unselected habitat in biodiversity persistence has received no attention in this context. We used a theoretical approach to evaluate the long-term performance of reserve networks in preserving biodiversity by using a model of spatiotemporal population dynamics (a metapopulation model). We compared extinction rates of species in reserve networks in two situations: when all sites remain suitable habitat for the species and, conversely, when the habitat in the unselected sites is lost. We made this comparison to explore the significance of unselected sites for spatial population dynamics and for the continued presence of species in the reserve network. Basic site-selection algorithms are liable to perform badly in terms of biodiversity maintenance because the persistence of species may be strongly dependent on sites not included in the reserve network. Our results support recent calls for the integration of spatial population modeling into reserve network design. Advances in metapopulation theory provide tools that can be used for this purpose.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/24LDZHFR/Cabeza, Moilanen - 2003 - Site-Selection Algorithms and Habitat Loss - October.pdf}
}

@article{cadotte2020esae,
  title = {Making the Applied Research That Practitioners Need and Want Accessible},
  author = {Cadotte, Marc W. and Jones, Holly P. and Newton, Erika L.},
  year = {2020},
  month = jul,
  journal = {Ecological Solutions and Evidence},
  volume = {1},
  number = {1},
  issn = {2688-8319, 2688-8319},
  doi = {10.1002/2688-8319.12000},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WFRYUAPG/cadotte et al 2020 - Making the applied research that practitioners need and want accessible - GUPPY - BDPG - KOALA.pdf}
}

@article{cai2013j,
  title = {{{NuMVC}}: {{An Efficient Local Search Algorithm}} for {{Minimum Vertex Cover}}},
  shorttitle = {{{NuMVC}}},
  author = {Cai, S. and Su, K. and Luo, C. and Sattar, A.},
  year = {2013},
  month = apr,
  journal = {Journal of Artificial Intelligence Research},
  volume = {46},
  pages = {687--716},
  issn = {1076-9757},
  doi = {10.1613/jair.3907},
  abstract = {The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial optimization problem of great importance in both theory and application. Local search has proved successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local search algorithms. First, they select a pair of vertices to exchange simultaneously, which is timeconsuming. Secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. The strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. These two strategies are used in designing a new MVC local search algorithm, which is referred to as NuMVC.},
  langid = {english},
  keywords = {bdpg,benchmarking,minimum vertex cover},
  file = {/Users/bill/D/Zotero/storage/3VFFX8Y3/cai et al 2013 - NuMVC - An Efficient Local Search Algorithm for Minimum Vertex Cover - BDPG - XU - BENCHMARKS.pdf}
}

@article{cameronBiasAlgorithmPortfolio,
  title = {Bias in {{Algorithm Portfolio Performance Evaluation}}},
  author = {Cameron, Chris and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {8},
  abstract = {A Virtual Best Solver (VBS) is a hypothetical algorithm that selects the best solver from a given portfolio of alternatives on a per-instance basis. The VBS idealizes performance when all solvers in a portfolio are run in parallel, and also gives a valuable bound on the performance of portfolio-based algorithm selectors. Typically, VBS performance is measured by running every solver in a portfolio once on a given instance and reporting the best performance over all solvers. Here, we argue that doing so results in a flawed measure that is biased to reporting better performance when a randomized solver is present in an algorithm portfolio. Specifically, this flawed notion of VBS tends to show performance better than that achievable by a perfect selector that for each given instance runs the solver with the best expected running time. We report results from an empirical study using solvers and instances submitted to several SAT competitions, in which we observe significant bias on many random instances and some combinatorial instances. We also show that the bias increases with the number of randomized solvers and decreases as we average solver performance over many independent runs per instance. We propose an alternative VBS performance measure by (1) empirically obtaining the solver with best expected performance for each instance and (2) taking bootstrap samples for this solver on every instance, to obtain a confidence interval on VBS performance. Our findings shed new light on widely studied algorithm selection benchmarks and help explain performance gaps observed between VBS and state-of-the-art algorithm selection approaches.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EDV9FHPF/2016-IJCAI-VBS.pdf}
}

@article{campeloSampleSizeEstimation2019,
  title = {Sample Size Estimation for Power and Accuracy in the Experimental Comparison of Algorithms},
  author = {Campelo, Felipe and Takahashi, Fernanda},
  year = {2019},
  month = apr,
  journal = {Journal of Heuristics},
  volume = {25},
  number = {2},
  eprint = {1808.02997},
  eprinttype = {arxiv},
  pages = {305--338},
  issn = {1381-1231, 1572-9397},
  doi = {10.1007/s10732-018-9396-7},
  abstract = {Experimental comparisons of performance represent an important aspect of research on optimization algorithms. In this work we present a methodology for defining the required sample sizes for designing experiments with desired statistical properties for the comparison of two methods on a given problem class. The proposed approach allows the experimenter to define desired levels of accuracy for estimates of mean performance differences on individual problem instances, as well as the desired statistical power for comparing mean performances over a problem class of interest. The method calculates the required number of problem instances, and runs the algorithms on each test instance so that the accuracy of the estimated differences in performance is controlled at the predefined level. Two examples illustrate the application of the proposed method, and its ability to achieve the desired statistical properties with a methodologically sound definition of the relevant sample sizes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/bill/D/Zotero/storage/5F3NTZBA/1808.02997.pdf}
}

@incollection{carbonnel2016papocp,
  title = {Propagation via {{Kernelization}}: {{The Vertex Cover Constraint}}},
  shorttitle = {Propagation via {{Kernelization}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {Carbonnel, Cl{\'e}ment and Hebrard, Emmanuel},
  editor = {Rueher, Michel},
  year = {2016},
  volume = {9892},
  pages = {147--156},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-44953-1_10},
  abstract = {The technique of kernelization consists in extracting, from an instance of a problem, an essentially equivalent instance whose size is bounded in a parameter k. Besides being the basis for efficient parameterized algorithms, this method also provides a wealth of information to reason about in the context of constraint programming. We study the use of kernelization for designing propagators through the example of the Vertex Cover constraint. Since the classic kernelization rules often correspond to dominance rather than consistency, we introduce the notion of ``loss-less'' kernel. While our preliminary experimental results show the potential of the approach, they also show some of its limits. In particular, this method is more effective for vertex covers of large and sparse graphs, as they tend to have, relatively, smaller kernels.},
  isbn = {978-3-319-44952-4 978-3-319-44953-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MFRTTKWX/carbonnel hebrel 2017 - propagation via kernelization - the vertex cover constraint - BDPG.pdf}
}

@article{carroll2010gcb,
  title = {Optimizing Resiliency of Reserve Networks to Climate Change: Multispecies Conservation Planning in the {{Pacific Northwest}}, {{USA}}},
  shorttitle = {Optimizing Resiliency of Reserve Networks to Climate Change},
  author = {Carroll, Carlos and Dunk, Jeffrey R. and Moilanen, Atte},
  year = {2010},
  month = mar,
  journal = {Global Change Biology},
  volume = {16},
  number = {3},
  pages = {891--904},
  issn = {13541013, 13652486},
  doi = {10.1111/j.1365-2486.2009.01965.x},
  abstract = {The effectiveness of a system of reserves may be compromised under climate change as species' habitat shifts to nonreserved areas, a problem that may be compounded when well-studied vertebrate species are used as conservation umbrellas for other taxa. The Northwest Forest Plan was among the first efforts to integrate conservation of wideranging focal species and localized endemics into regional conservation planning. We evaluated how effectively the plan's focal species, the Northern Spotted Owl, acts as an umbrella for localized species under current and projected future climates and how the regional system of reserves can be made more resilient to climate change. We used the program MAXENT to develop distribution models integrating climate data with vegetation variables for the owl and 130 localized species. We used the program ZONATION to identify a system of areas that efficiently captures habitat for both the owl and localized species and prioritizes refugial areas of climatic and topographic heterogeneity where current and future habitat for dispersal-limited species is in proximity. We projected future species' distributions based on an ensemble of contrasting climate models, and incorporating uncertainty between alternate climate projections into the prioritization process. Reserve solutions based on the owl overlap areas of high localized-species richness but poorly capture core areas of localized species' distribution. Congruence between priority areas across taxa increases when refugial areas are prioritized. Although corearea selection strategies can potentially increase the conservation value and resilience of regional reserve systems, they accentuate contrasts in priority areas between species and over time and should be combined with a broadened taxonomic scope and increased attention to potential effects of climate change. Our results suggest that systems of fixed reserves designed for resilience can increase the likelihood of retaining the biological diversity of forest ecosystems under climate change.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/I9RVCY5H/Carroll, Dunk, Moilanen - 2010 - Optimizing resiliency of reserve networks to climate change multispecies conservation planning in the Pacific Northwest, USA - Global Change Biology.pdf}
}

@article{caruanaAlgorithmsApplicationsMultitask,
  title = {Algorithms and {{Applications}} for {{Multitask Learning}}},
  author = {Caruana, Rich},
  pages = {9},
  abstract = {Multitask Learning is an inductive transfer method that improves generalization by using domain information implicit in the training signals of related tasks as an inductive bias. It does this by learning multiple tasks in parallel using a shared representation. Multitask transfer in connectionist nets has already been proven. But questions remain about how often training data for useful extra tasks will be available, and if multitask transfer will work in other learning methods. This paper argues that many real world problems present opportunities for multitask learning if they are not rst overly sanitized. We present eight prototypical applications of multitask transfer where the training signals for related tasks are available and can be leveraged. We also outline algorithms for multitask transfer in decision trees and k-nearest neighbor. We conclude that multitask transfer has broad utility.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/S9CJ4TW2/10.1.1.52.975.pdf}
}

@article{caruanaBenefittingVariablesThat,
  title = {Benefitting from the {{Variables}} That {{Variable Selection Discards}}},
  author = {Caruana, Rich},
  pages = {20},
  abstract = {In supervised learning variable selection is used to find a subset of the available inputs that accurately predict the output. This paper shows that some of the variables that variable selection discards can beneficially be used as extra outputs for inductive transfer. Using discarded input variables as extra outputs forces the model to learn mappings from the variables that were selected as inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available to the model that is being trained on the main output, often resulting in improved performance on that main output.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/86PXUIE7/CaruanaS03.pdf}
}

@article{caruanaCaseBasedExplanationNonCaseBased,
  title = {Case-{{Based Explanation}} of {{Non-Case-Based Learning Methods}}},
  author = {Caruana, Rich and Kangarloo, Hooshang and David, John and Dionisio, N and Sinha, Usha and Johnson, David},
  pages = {4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WR8UEHVK/procamiasymp00004-0249.pdf}
}

@inproceedings{caruanaEmpiricalEvaluationSupervised2008,
  title = {An Empirical Evaluation of Supervised Learning in High Dimensions},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
  year = {2008},
  pages = {96--103},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390169},
  abstract = {In this paper we perform an empirical evaluation of supervised learning on highdimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
  isbn = {978-1-60558-205-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/I57AGA5M/10.1.1.442.6328.pdf}
}

@inproceedings{caruanaGettingMostOut2006,
  title = {Getting the {{Most Out}} of {{Ensemble Selection}}},
  booktitle = {Sixth {{International Conference}} on {{Data Mining}} ({{ICDM}}'06)},
  author = {Caruana, Rich and Munson, Art and {Niculescu-Mizil}, Alexandru},
  year = {2006},
  month = dec,
  pages = {828--833},
  publisher = {{IEEE}},
  address = {{Hong Kong, China}},
  issn = {1550-4786},
  doi = {10.1109/ICDM.2006.76},
  abstract = {We investigate four previously unexplored aspects of ensemble selection, a procedure for building ensembles of classifiers. First we test whether adjusting model predictions to put them on a canonical scale makes the ensembles more effective. Second, we explore the performance of ensemble selection when different amounts of data are available for ensemble hillclimbing. Third, we quantify the benefit of ensemble selection's ability to optimize to arbitrary metrics. Fourth, we study the performance impact of pruning the number of models available for ensemble selection. Based on our results we present improved ensemble selection methods that double the benefit of the original method.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZM9AUJAF/caruana et al 2006 or 2007 - Getting the Most Out of Ensemble Selection- GUPPY - ENSEMBLES.pdf}
}

@article{caruanaHighPrecisionInformation,
  title = {High {{Precision Information Extraction}}},
  author = {Caruana, Rich and Hodor, Paul G and Rosenberg, John},
  pages = {7},
  abstract = {Most fully automatic information extraction systems achieve less than 100\% extraction precision and recall. On real applications these parameters typically vary between 50\% to 95\%, depending on the extraction method and source data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8DMNJE3J/10.1.1.37.6062.pdf}
}

@article{caruanaHowUsefulRelevance,
  title = {How {{Useful}} Is {{Relevance}}?},
  author = {Caruana, Rich and Freitag, Dayne},
  pages = {5},
  abstract = {Eliminating irrelevant attributes prior to induction boosts the performanceof manylearning algorithms. Relevance,however,is no guarantee of usefulness to a particular learner. Wetest two methods of finding relevant attributes, FOCUS and RELIEF,to see howthe attributes they select performwith ID3/C4.5on two learning problems from a calendar scheduling domain. A more direct attribute selection procedure,hillclimbing in attribute space, finds superior attribute sets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/T3STMZBX/FS94-02-007.pdf}
}

@article{caruanaLearningImbalancedData,
  title = {Learning from {{Imbalanced Data}}: {{Rank Metrics}} and {{Extra Tasks}}},
  author = {Caruana, Rich},
  pages = {7},
  abstract = {Imbalanceddata creates two problemsfor machinelearning. First, evenif the training set is large, the samplesize of smallerclasses maybe small. Learningaccurate modelsfromsmall samples is hard.Multitasklearningis onewayto learn moreaccurate modelsfromsmall samplesthat is particularlywellsuited to imbalanceddata. Asecondproblemwhenlearning fromimbalanceddata is that the usualerror metrics(e.g., accuracyor squarederror) causelearningto pay moreattention to large classes than to smallclasses. This problemcan be mitigatedby careful selection of the error metric. Wefind rankbasederror metrics often performbetter whenan importantclass is under-represented.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5JT5HRCZ/caruana 2000 - learning from imbalanced data - rank metrics and extra tasks - GUPPY.pdf}
}

@article{caruanaLearningManyRelated,
  title = {Learning {{Many Related Tasks}} at the {{Same Time With Backpropagation}}},
  author = {Caruana, Rich},
  pages = {8},
  abstract = {Hinton 6] proposed that generalization in arti cial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work 1] shows that the outputs of a backprop net can be used as inputs through which domainspeci c information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify ve mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QYLMK9ME/10.1.1.54.6346.pdf}
}

@article{caruanaMiningCitizenScience,
  title = {Mining {{Citizen Science Data}} to {{Predict Prevalence}} of {{Wild Bird Species}}},
  author = {Caruana, Rich and Elhawary, Mohamed and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Fink, Daniel and Hochachka, Wesley M and Kelling, Steve},
  pages = {7},
  abstract = {The Cornell Laboratory of Ornithology's mission is to interpret and conserve the earth's biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and potentially are very noisy. The ecologists are interested in identifying which features have the strongest effect on the distribution and abundance of bird species as well as describing the forms of these relationships. We show how data mining can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/254LDBPR/BirdMining.pdf}
}

@article{caruanaObtainingCalibratedProbabilities,
  title = {Obtaining {{Calibrated Probabilities}} from {{Boosting}}},
  author = {Caruana, Alexandru Niculescu-Mizil Rich},
  pages = {6},
  abstract = {Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss instead of the usual exponential loss. Experiments show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, significantly improve the probabilities predicted by both boosted stumps and boosted trees. After calibration, boosted full decision trees predict better probabilities than other learning methods such as SVMs, neural nets, bagged decision trees, and KNNs, even after these methods are calibrated.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UCFVCUN6/WS07-05-006.pdf}
}

@article{caruanaOverfittingNeuralNets,
  title = {Overfitting in {{Neural Nets}}: {{Backpropagation}}, {{Conjugate Gradient}}, and {{Early Stopping}}},
  author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
  pages = {7},
  abstract = {The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/M2E8UMYE/10.1.1.21.1952.pdf}
}

@article{caruanaPromotingPoorFeatures,
  title = {Promoting {{Poor Features}} to {{Supervisors}}: {{Some Inputs Work Better}} as {{Outputs}}},
  author = {Caruana, Rich and {de Sa}, Virginia R},
  pages = {7},
  abstract = {In supervised learning there is usually a clear distinction between inputs and outputs | inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classi cation problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FIFIEYT6/caruana et al 1997 - promoting poor features to supervisors - some inputs work better as outpus - MULTITASK.pdf}
}

@article{caruanaUtshinagtFWeaotrukrBeEStetleerctaisonExtotrFaiOndutIpnuptusts,
  title = {Utshinagt {{FWeaotrukrBe eStetleerctaisonExtotrFaiOndutIpnuptusts}}},
  author = {Caruana, Rich and {de Sa}, Virginia R},
  pages = {6},
  abstract = {In supervised learning there is usually a clear distinction between inputs and outputs | inputs are what you measure, outputs are what you predict from those measurements. The distinction between inputs and outputs is not this simple. Previously, we demonstrated that on synthetic problems some input features are more useful when used as extra outputs than when used as inputs 6]. This paper shows the same e ect on a real problem, and presents a means of determining what features can be used as extra outputs. We show that the feature selection method devised by Koller and Sahami 11] can be used to select features to use as extra outputs, and that using some features as as extra outputs instead of as inputs yields better performance on the DNA splice-junction domain.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WIL4SDY2/10.1.1.57.4668.pdf}
}

@article{carvalhoConservationPlanningClimate2011,
  title = {Conservation Planning under Climate Change: {{Toward}} Accounting for Uncertainty in Predicted Species Distributions to Increase Confidence in Conservation Investments in Space and Time},
  shorttitle = {Conservation Planning under Climate Change},
  author = {Carvalho, S{\'i}lvia B. and Brito, Jos{\'e} C. and Crespo, Eduardo G. and Watts, Matthew E. and Possingham, Hugh P.},
  year = {2011},
  month = jul,
  journal = {Biological Conservation},
  volume = {144},
  number = {7},
  pages = {2020--2030},
  issn = {00063207},
  doi = {10.1016/j.biocon.2011.04.024},
  abstract = {Climate warming challenges our approach to building systems of protected areas because it is likely to drive accelerating shifts in species distributions, and the projections of those future species distributions are uncertain. There are several important sources of uncertainty intrinsic to using species occurrence projections for reserve system design including uncertainty in the number of occurrences captured by any reserve selection solution, and uncertainty arising from the different approaches used to fit predictive models. Here we used the present and future predicted distributions of Iberian herptiles to analyze how dynamics and uncertainty in species distributions may affect decisions about resource allocation for conservation in space and time. We identified priority areas maximizing coverage of current and future (2020 and 2080) predicted distributions of 65 species, under ``Mild'' and ``Severe'' uncertainty. Next, we applied a return-on-investment analysis to quantify and make explicit trade-offs between investing in areas selected when optimizing for different times and with different uncertainty levels. Areas identified as important for conservation in every time frame and uncertainty level were the ones considered to be robust climate adaptation investments, and included chiefly already protected areas. Areas identified only under ``Mild'' uncertainty were considered good candidates for investment if extra resources are available and were mainly located in northern Iberia. However, areas selected only in the ``Severe'' uncertainty case should not be completely disregarded as they may become climatic refugia for some species. Our study provides an objective methodology to deliver ``no regrets'' conservation investments.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SA46L8ET/carvalho et al 2011 - Conservation planning under climate change -  Toward accounting for uncertainty in predicted species distributions ... - SDM - RESERVE SELECTION - UNCERTAINTY - BDPG - ANNO.pdf}
}

@article{carwardineConservationPlanningIrreplaceability2007,
  title = {Conservation Planning with Irreplaceability: Does the Method Matter?},
  shorttitle = {Conservation Planning with Irreplaceability},
  author = {Carwardine, J. and Rochester, W. A. and Richardson, K. S. and Williams, K. J. and Pressey, R. L. and Possingham, H. P.},
  year = {2007},
  month = jan,
  journal = {Biodiversity and Conservation},
  volume = {16},
  number = {1},
  pages = {245--258},
  issn = {0960-3115, 1572-9710},
  doi = {10.1007/s10531-006-9055-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2SMX5UAW/carwardine et al 2007 - conservation planning with irreplaceability - does the method matter - BDPG - RESERVE SELECTION - ANNO.pdf}
}

@article{carwardineConservationPlanningWhen2010,
  title = {Conservation {{Planning}} When {{Costs Are Uncertain}}: {{Conservation Planning}}},
  shorttitle = {Conservation {{Planning}} When {{Costs Are Uncertain}}},
  author = {Carwardine, Josie and Wilson, Kerrie A. and Hajkowicz, Stefan A. and Smith, Robert J. and Klein, Carissa J. and Watts, Matt and Possingham, Hugh P.},
  year = {2010},
  month = dec,
  journal = {Conservation Biology},
  volume = {24},
  number = {6},
  pages = {1529--1537},
  issn = {08888892},
  doi = {10.1111/j.1523-1739.2010.01535.x},
  abstract = {Spatially explicit information on the financial costs of conservation actions can improve the ability of conservation planning to achieve ecological and economic objectives, but the magnitude of this improvement may depend on the accuracy of the cost estimates. Data on costs of conservation actions are inherently uncertain. For example, the cost of purchasing a property for addition to a protected-area network depends on the individual landholder's preferences, values, and aspirations, all of which vary in space and time, and the effect of this uncertainty on the conservation priority of a site is relatively untested. We investigated the sensitivity of the conservation priority of sites to uncertainty in cost estimates. We explored scenarios for expanding (four-fold) the protected-area network in Queensland, Australia to represent a range of vegetation types, species, and abiotic environments, while minimizing the cost of purchasing new properties. We estimated property costs for 17, 790 10 \texttimes{} 10 km sites with data on unimproved land values. We systematically changed property costs and noted how these changes affected conservation priority of a site. The sensitivity of the priority of a site to changes in cost data was largely dependent on a site's importance for meeting conservation targets. Sites that were essential or unimportant for meeting targets maintained high or low priorities, respectively, regardless of cost estimates. Sites of intermediate conservation priority were sensitive to property costs and represented the best option for efficiency gains, especially if they could be purchased at a lower price than anticipated. Thus, uncertainty in cost estimates did not impede the use of cost data in conservation planning, and information on the sensitivity of the conservation priority of a site to estimates of the price of land can be used to inform strategic conservation planning before the actual price of the land is known.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GYSNYMC5/carwardine et al. - 2010 - Conservation Planning when Costs Are Uncertain - BDPG - COST UNCERTAINTY - RESERVE SELECTION - ANNO.pdf}
}

@misc{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{chagnonThesePresenteeAu,
  title = {{th\`ese pr\'esent\'ee au D\'epartement de biologie en vue de l'obtention du grade de docteur \`es sciences (Ph.D.)}},
  author = {Chagnon, Pierre-Luc},
  pages = {256},
  langid = {french},
  file = {/Users/bill/D/Zotero/storage/2F3IDDCT/Chagnon_Pierre_Luc_PhD_2015.pdf}
}

@inproceedings{chanEvolvingStellarModels2019,
  title = {Evolving Stellar Models to Find the Origins of Our Galaxy},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '19},
  author = {Chan, Conrad and Aleti, Aldeida and Heger, Alexander and {Smith-Miles}, Kate},
  year = {2019},
  pages = {1129--1137},
  publisher = {{ACM Press}},
  address = {{Prague, Czech Republic}},
  doi = {10.1145/3321707.3321714},
  abstract = {A er the Big Bang, it took about 200 million years before the very rst stars would form \textendash{} now more than 13 billion years ago. Unfortunately, we will not be able to observe these stars directly. Instead, we can observe the 'fossil' records that these stars have le behind, preserved in the oldest stars of our own galaxy. When the rst stars exploded as supernovae, their ashes were dispersed and the next generation of stars formed, incorporating some of the debris. We can now measure the chemical abundances in those old stars, which is similar to a genetic ngerprint that allows us to identify the parents.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4Y3V2JCF/p1129-chan.pdf}
}

@inproceedings{chanEvolvingStellarModels2019a,
  title = {Evolving Stellar Models to Find the Origins of Our Galaxy},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Chan, Conrad and Aleti, Aldeida and Heger, Alexander and {Smith-Miles}, Kate},
  year = {2019},
  month = jul,
  pages = {1129--1137},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321714},
  abstract = {A er the Big Bang, it took about 200 million years before the very rst stars would form \textendash{} now more than 13 billion years ago. Unfortunately, we will not be able to observe these stars directly. Instead, we can observe the 'fossil' records that these stars have le behind, preserved in the oldest stars of our own galaxy. When the rst stars exploded as supernovae, their ashes were dispersed and the next generation of stars formed, incorporating some of the debris. We can now measure the chemical abundances in those old stars, which is similar to a genetic ngerprint that allows us to identify the parents.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KQNDXL2P/p1129-chan.pdf}
}

@misc{chang2021,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Sievert, Carson and Schloerke, Barret and Xie, Yihui and Allen, Jeff and McPherson, Jonathan and Dipert, Alan and Borges, Barbara},
  year = {2021}
}

@inproceedings{cheeseman1991ip1ijcai,
  title = {Where the Really Hard Problems Are},
  booktitle = {{{IJCAI}}'91: {{Proceedings}} of the 12th International Joint Conference on {{Artificial}} Intelligence},
  author = {Cheeseman, Peter},
  year = {1991},
  volume = {1},
  pages = {331--337},
  publisher = {{Morgan Kaufmann}},
  address = {{San Mateo, CA, USA}},
  file = {/Users/bill/D/Zotero/storage/XV4GR3PC/cheeseman et al 1991 - where the really hard problems are - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{chen,
  title = {Statistical-{{Computational Tradeoffs}} in {{Planted Problems}} and {{Submatrix Localization}} with a {{Growing Number}} of {{Clusters}} and {{Submatrices}}},
  author = {Chen, Yudong and Xu, Jiaming},
  pages = {57},
  abstract = {We consider two closely related problems: planted clustering and submatrix localization. In the planted clustering problem, a random graph is generated based on an underlying cluster structure of the nodes; the task is to recover these clusters given the graph. The submatrix localization problem concerns locating hidden submatrices with elevated means inside a large real-valued random matrix. Of particular interest is the setting where the number of clusters/submatrices is allowed to grow unbounded with the problem size. These formulations cover several classical models such as planted clique, planted densest subgraph, planted partition, planted coloring, and the stochastic block model, which are widely used for studying community detection, graph clustering and bi-clustering.},
  langid = {english},
  keywords = {bdpg,planted problem,planted solution},
  file = {/Users/bill/D/Zotero/storage/E44M4RVF/14-330.pdf}
}

@article{chen2006,
  title = {{{COMPARISON OF METHODS FOR UNCERTAINTY ANALYSIS OF HYDROLOGIC MODELS}}},
  author = {Chen, Changjun and Shrestha, Durga Lal and Perez, Gerald Corzo and Solomatine, Dimitri},
  year = {2006},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YSPJFGXM/Chen,Shrestha,Corzo,Solomatine,ComparisonMethodUncertainty,ProcHI,2006.pdf}
}

@article{chenSpatialAutocorrelationApproaches2016,
  title = {Spatial {{Autocorrelation Approaches}} to {{Testing Residuals}} from {{Least Squares Regression}}},
  author = {Chen, Yanguang},
  editor = {Schumann, Guy J-P.},
  year = {2016},
  month = jan,
  journal = {PLOS ONE},
  volume = {11},
  number = {1},
  pages = {e0146865},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0146865},
  abstract = {In statistics, the Durbin-Watson test is always employed to detect the presence of serial correlation of residuals from a least squares regression analysis. However, the Durbin-Watson statistic is only suitable for ordered time or spatial series. If the variables comprise cross-sectional data coming from spatial random sampling, the Durbin-Watson will be ineffectual because the value of Durbin-Watson's statistic depends on the sequences of data point arrangement. Based on the ideas from spatial autocorrelation, this paper presents two new statistics for testing serial correlation of residuals from least squares regression based on spatial samples. By analogy with the new form of Moran's index, an autocorrelation coefficient is defined with a standardized residual vector and a normalized spatial weight matrix. Then on the analogy of the Durbin-Watson statistic, a serial correlation index is constructed. As a case, the two statistics are applied to the spatial sample of 29 China's regions. These results show that the new spatial autocorrelation model can be used to test the serial correlation of residuals from regression analysis. In practice, the new statistics can make up for the deficiency of the Durbin-Watson test.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X96U3K4K/1503.04407.pdf}
}

@article{cheokSympathyDevilDetailing2016,
  title = {Sympathy for the {{Devil}}: {{Detailing}} the {{Effects}} of {{Planning-Unit Size}}, {{Thematic Resolution}} of {{Reef Classes}}, and {{Socioeconomic Costs}} on {{Spatial Priorities}} for {{Marine Conservation}}},
  shorttitle = {Sympathy for the {{Devil}}},
  author = {Cheok, Jessica and Pressey, Robert L. and Weeks, Rebecca and Andr{\'e}fou{\"e}t, Serge and Moloney, James},
  editor = {Baldwin, Robert F.},
  year = {2016},
  month = nov,
  journal = {PLOS ONE},
  volume = {11},
  number = {11},
  pages = {e0164869},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0164869},
  abstract = {Spatial data characteristics have the potential to influence various aspects of prioritising biodiversity areas for systematic conservation planning. There has been some exploration of the combined effects of size of planning units and level of classification of physical environments on the pattern and extent of priority areas. However, these data characteristics have yet to be explicitly investigated in terms of their interaction with different socioeconomic cost data during the spatial prioritisation process. We quantify the individual and interacting effects of three factors\textemdash planning-unit size, thematic resolution of reef classes, and spatial variability of socioeconomic costs\textemdash on spatial priorities for marine conservation, in typical marine planning exercises that use reef classification maps as a proxy for biodiversity. We assess these factors by creating 20 unique prioritisation scenarios involving combinations of different levels of each factor. Because output data from these scenarios are analogous to ecological data, we applied ecological statistics to determine spatial similarities between reserve designs. All three factors influenced prioritisations to different extents, with cost variability having the largest influence, followed by planning-unit size and thematic resolution of reef classes. The effect of thematic resolution on spatial design depended on the variability of cost data used. In terms of incidental representation of conservation objectives derived from finer-resolution data, scenarios prioritised with uniform cost outperformed those prioritised with variable cost. Following our analyses, we make recommendations to help maximise the spatial and cost efficiency and potential effectiveness of future marine conservation plans in similar planning scenarios. We recommend that planners: employ the smallest planning-unit size practical; invest in data at the highest possible resolution; and, when planning across regional extents with the intention of incidentally representing fine-resolution features, prioritise the whole region with uniform costs rather than using coarse-resolution data on variable costs.},
  langid = {english},
  keywords = {bdpg,cost,marxan,planning unit size,reserve selection,scale,spatial similarity,uncertainty},
  file = {/Users/bill/D/Zotero/storage/ZT3GKMGJ/cheok pressey et al 2016 - Sympathy for the Devil - Detailing the Effects of Planning-Unit Size, Thematic Resolution of Reef Classes, and Socioeconomic Costs on Spatial Priorities for Marine Conservation - RE.PDF}
}

@article{church1996bc,
  title = {Reserve Selection as a Maximal Covering Location Problem},
  author = {Church, Richard L. and Stoms, David M. and Davis, Frank W.},
  year = {1996},
  journal = {Biological Conservation},
  volume = {76},
  number = {2},
  pages = {105--112},
  issn = {00063207},
  doi = {10.1016/0006-3207(95)00102-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IBRKRYKU/1-s2.0-0006320795001026-main.pdf}
}

@article{cohenHowEvaluationGuides,
  title = {How {{Evaluation Guides AI Research}}: {{The Message Still Counts More}} than the {{Medium}}},
  author = {Cohen, Paul and Howe, Adele},
  pages = {10},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BT9CTGS8/cohen howe 1988 - How Evaluation Guides AI Research - The Message Still Counts More than the Medium.pdf}
}

@article{coja-oghlanSpectralApproachAnalyzing2009,
  title = {A {{Spectral Approach}} to {{Analyzing Belief Propagation}} for 3-{{Coloring}}},
  author = {{Coja-Oghlan}, Amin and Mossel, Elchanan and Vilenchik, Dan},
  year = {2009},
  month = nov,
  journal = {Combinatorics, Probability and Computing},
  volume = {18},
  number = {6},
  eprint = {0712.0171},
  eprinttype = {arxiv},
  pages = {881--912},
  issn = {0963-5483, 1469-2163},
  doi = {10.1017/S096354830900981X},
  abstract = {Belief Propagation (BP) is a message-passing algorithm that computes the exact marginal distributions at every vertex of a graphical model without cycles. While BP is designed to work correctly on trees, it is routinely applied to general graphical models that may contain cycles, in which case neither convergence, nor correctness in the case of convergence is guaranteed. Nonetheless, BP gained popularity as it seems to remain effective in many cases of interest, even when the underlying graph is ``far'' from being a tree. However, the theoretical understanding of BP (and its new relative Survey Propagation) when applied to CSPs is poor.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Discrete Mathematics},
  file = {/Users/bill/D/Zotero/storage/5YEUZMWG/0712.0171v1.pdf}
}

@article{coja-oghlanWhyAlmostAll,
  title = {Why Almost All Satisfiable K-{{CNF}} Formulas Are Easy},
  author = {{Coja-Oghlan}, Amin and Krivelevich, Michael and Vilenchik, Dan},
  pages = {14},
  abstract = {Finding a satisfying assignment for a k-CNF formula (k {$\geq$} 3), assuming such exists, is a notoriously hard problem. In this work we consider the uniform distribution over satisfiable k-CNF formulas with a linear number of clauses (clause-variable ratio greater than some constant). We rigorously analyze the structure of the space of satisfying assignments of a random formula in that distribution, showing that basically all satisfying assignments are clustered in one cluster, and agree on all but a small, though linear, number of variables. This observation enables us to describe a polynomial time algorithm that finds whp a satisfying assignment for such formulas, thus asserting that most satisfiable k-CNF formulas are easy (whenever the clause-variable ratio is greater than some constant). This should be contrasted with the setting of very sparse k-CNF formulas (which are satisfiable whp), where experimental results show some regime of clause density to be difficult for many SAT heuristics. One explanation for this phenomena, backed up by partially non-rigorous analytical tools from statistical physics, is the complicated clustering of the solution space at that regime, unlike the more ``regular'' structure that denser formulas possess. Thus in some sense, our result rigorously supports this explanation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4DP8HIP2/911-3036-2-PB.pdf}
}

@article{colditzResilienceFarmAnimals2016,
  title = {Resilience in Farm Animals: Biology, Management, Breeding and Implications for Animal Welfare},
  shorttitle = {Resilience in Farm Animals},
  author = {Colditz, Ian G. and Hine, Brad C.},
  year = {2016},
  journal = {Animal Production Science},
  volume = {56},
  number = {12},
  pages = {1961},
  issn = {1836-0939},
  doi = {10.1071/AN15297},
  abstract = {A capacity for the animal to recover quickly from the impact of physical and social stressors and disease challenges is likely to improve evolutionary fitness of wild species and welfare and performance of farm animals. Salience and valence of stimuli sensed through neurosensors, chemosensors and immunosensors are perceived and integrated centrally to generate emotions and engage physiological, behavioural, immune, cognitive and morphological responses that defend against noxious challenges. These responses can be refined through experience to provide anticipatory and learned reactions at lower cost than innate less-specific reactions. Influences of behaviour type, coping style, and affective state and the relationships between immune responsiveness, disease resistance and resilience are reviewed. We define resilience as the capacity of animals to cope with short-term perturbations in their environment and return rapidly to their pre-challenge status. It is manifested in response to episodic, sporadic or situation-specific attributes of the environment and can be optimised via facultative learning by the individual. It is a comparative measure of differences between individuals in the outcomes that follow exposure to potentially adverse situations. In contrast, robustness is the capacity to maintain productivity in a wide range of environments without compromising reproduction, health and wellbeing. Robustness is manifested in response to persistent or cyclical attributes of the environment and is effected via activity of innate regulatory pathways. We suggest that for farm animals, husbandry practices that incorporate physical and social stressors and interactions with humans such as weaning, change of housing, and introduction to the milking parlour can be used to characterise resilience phenotypes. In these settings, resilience is likely to be more readily identified through the rate of return of variables to pre-challenge or normal status rather than through measuring the activity of diverse stress response and adaptation mechanisms. Our strategy for phenotyping resilience of sheep and cattle during weaning is described. Opportunities are examined to increase resilience through genetic selection and through improved management practices that provide emotional and cognitive enrichment and stress inoculation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/44VTJ2KS/colditz hine 2015 - Resilience in farm animals - biology, management, breeding and implications for animal welfare - VOICELESS - OPTISEVIL.pdf}
}

@article{collenConservationPrioritizationContext2015,
  title = {Conservation Prioritization in the Context of Uncertainty: {{Conservation}} Prioritization in the Context of Uncertainty},
  shorttitle = {Conservation Prioritization in the Context of Uncertainty},
  author = {Collen, B.},
  year = {2015},
  month = aug,
  journal = {Animal Conservation},
  volume = {18},
  number = {4},
  pages = {315--317},
  issn = {13679430},
  doi = {10.1111/acv.12222},
  langid = {english},
  keywords = {bdpg,reserve selection,uncertainty},
  file = {/Users/bill/D/Zotero/storage/GZMWMXTA/collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf}
}

@article{collenConservationPrioritizationContext2015a,
  title = {Conservation Prioritization in the Context of Uncertainty: {{Conservation}} Prioritization in the Context of Uncertainty},
  shorttitle = {Conservation Prioritization in the Context of Uncertainty},
  author = {Collen, B.},
  year = {2015},
  month = aug,
  journal = {Animal Conservation},
  volume = {18},
  number = {4},
  pages = {315--317},
  issn = {13679430},
  doi = {10.1111/acv.12222},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SEHG6CKQ/collen 2015 - Conservation prioritization in the context of uncertainty - UNCERTAINTY - BDPG.pdf}
}

@article{colletteMultiobjectiveOptimization,
  title = {Multiobjective {{Optimization}}},
  author = {Collette, Yann and Siarry, Patrick},
  pages = {7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UMNN4648/Multiobjective_Optimization_Principles_and_Case_St.pdf}
}

@article{collinsEnsemblesProbabilitiesNew2007,
  title = {Ensembles and Probabilities: A New Era in the Prediction of Climate Change},
  shorttitle = {Ensembles and Probabilities},
  author = {Collins, Mat},
  year = {2007},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {365},
  number = {1857},
  pages = {1957--1970},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2007.2068},
  langid = {english},
  keywords = {climate change,ensembles},
  file = {/Users/bill/D/Zotero/storage/U44ILK2B/COLLIN~1.PDF}
}

@article{cookConservationDarkInformation2010,
  title = {Conservation in the Dark? {{The}} Information Used to Support Management Decisions},
  shorttitle = {Conservation in the Dark?},
  author = {Cook, Carly N and Hockings, Marc and Carter, RW (Bill)},
  year = {2010},
  month = may,
  journal = {Frontiers in Ecology and the Environment},
  volume = {8},
  number = {4},
  pages = {181--186},
  issn = {1540-9295},
  doi = {10.1890/090020},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9CMKJBLA/14 - cook et al 2010 - Conservation in the dark - The information used to support management decisions.pdf}
}

@article{cookeSocialContextRole2012,
  title = {Social Context and the Role of Collaborative Policy Making for Private Land Conservation},
  author = {Cooke, Benjamin and Langford, William T. and Gordon, Ascelin and Bekessy, Sarah},
  year = {2012},
  month = may,
  journal = {Journal of Environmental Planning and Management},
  volume = {55},
  number = {4},
  pages = {469--485},
  issn = {0964-0568, 1360-0559},
  doi = {10.1080/09640568.2011.608549},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZPN836E7/cooke langford et al 2012 - Social context and the role of collaborative policy making for private land conservation.pdf}
}

@article{corbett-daviesAlgorithmicDecisionMaking2017,
  title = {Algorithmic Decision Making and the Cost of Fairness},
  author = {{Corbett-Davies}, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  year = {2017},
  month = jun,
  journal = {arXiv:1701.08230 [cs, stat]},
  eprint = {1701.08230},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1145/3097983.309809},
  abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classi ed as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past de nitions of fairness, the optimal algorithms that result require detaining defendants above race-speci c risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. e unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally di er, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-o can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Statistics - Applications},
  file = {/Users/bill/D/Zotero/storage/YAAUXKFQ/corbett-davies pierson etl al 2017 - Algorithmic decision making and the cost of fairness - FAIRNESS - EFs - OPTISEVIL.pdf}
}

@book{cormen2009,
  title = {Introduction to {{Algorithms}} (3rd Ed.)},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  year = {2009},
  publisher = {{MIT Press and McGraw-Hill}},
  isbn = {0-262-03384-4}
}

@incollection{corneOptimisationGeneralisationFootprints2010,
  title = {Optimisation and {{Generalisation}}: {{Footprints}} in {{Instance Space}}},
  shorttitle = {Optimisation and {{Generalisation}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}, {{PPSN XI}}},
  author = {Corne, David W. and Reynolds, Alan P.},
  editor = {Schaefer, Robert and Cotta, Carlos and Ko{\l}odziej, Joanna and Rudolph, G{\"u}nter},
  year = {2010},
  pages = {22--31},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15844-5_3},
  abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for specific problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its `seen' instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm `footprints', indicating how performance generalises in instance space. We find much evidence that typical ways of choosing the `best' algorithm, via tests over a distribution of instances, are seriously flawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
  isbn = {978-3-642-15843-8 978-3-642-15844-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RD2J5PZR/corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf}
}

@incollection{corneOptimisationGeneralisationFootprints2010a,
  title = {Optimisation and {{Generalisation}}: {{Footprints}} in {{Instance Space}}},
  shorttitle = {Optimisation and {{Generalisation}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}, {{PPSN XI}}},
  author = {Corne, David W. and Reynolds, Alan P.},
  editor = {Schaefer, Robert and Cotta, Carlos and Ko{\l}odziej, Joanna and Rudolph, G{\"u}nter},
  year = {2010},
  pages = {22--31},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15844-5_3},
  abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for specific problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its `seen' instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm `footprints', indicating how performance generalises in instance space. We find much evidence that typical ways of choosing the `best' algorithm, via tests over a distribution of instances, are seriously flawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
  isbn = {978-3-642-15843-8 978-3-642-15844-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6LNTBBCW/footppsn10.pdf}
}

@incollection{corneOptimisationGeneralisationFootprints2010b,
  title = {Optimisation and {{Generalisation}}: {{Footprints}} in {{Instance Space}}},
  shorttitle = {Optimisation and {{Generalisation}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}, {{PPSN XI}}},
  author = {Corne, David W. and Reynolds, Alan P.},
  editor = {Schaefer, Robert and Cotta, Carlos and Ko{\l}odziej, Joanna and Rudolph, G{\"u}nter},
  year = {2010},
  pages = {22--31},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15844-5_3},
  abstract = {The chief purpose of research in optimisation is to understand how to design (or choose) the most suitable algorithm for a given distribution of problem instances. Ideally, when an algorithm is developed for specific problems, the boundaries of its performance should be clear, and we expect estimates of reasonably good performance within and (at least modestly) outside its `seen' instance distribution. However, we show that these ideals are highly over-optimistic, and suggest that standard algorithm-choice scenarios will rarely lead to the best algorithm for individual instances in the space of interest. We do this by examining algorithm `footprints', indicating how performance generalises in instance space. We find much evidence that typical ways of choosing the `best' algorithm, via tests over a distribution of instances, are seriously flawed. Also, understanding how footprints in instance spaces vary between algorithms and across instance space dimensions, may lead to a future platform for wiser algorithm-choice decisions.},
  isbn = {978-3-642-15843-8 978-3-642-15844-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ISCVIZUK/corne reynolds 2010 - Optimisation and Generalisation - Footprints in Instance Space - BDPG - PROBLEM DIFFICULTY.pdf}
}

@book{corryModernAlgebraRise2004,
  title = {Modern Algebra and the Rise of Mathematical Structures},
  author = {Corry, Leo},
  year = {2004},
  edition = {2nd rev. ed},
  publisher = {{Birkh\"auser Verlag}},
  address = {{Basel ; Boston}},
  isbn = {978-3-7643-7002-2},
  langid = {english},
  lccn = {QA151 .C67 2004},
  keywords = {Algebra,Categories (Mathematics),History},
  file = {/Users/bill/D/Zotero/storage/A54TSTYT/corry - Modern Algebra and the Rise of Mathematical Structures - 2nd ed - CREATIVITY - ABSTRACT STRATEGIES.pdf}
}

@article{corstjensCombinedApproachAnalysing2019,
  title = {A Combined Approach for Analysing Heuristic Algorithms},
  author = {Corstjens, Jeroen and Dang, Nguyen and Depaire, Beno{\^i}t and Caris, An and De Causmaecker, Patrick},
  year = {2019},
  month = oct,
  journal = {Journal of Heuristics},
  volume = {25},
  number = {4-5},
  pages = {591--628},
  issn = {1381-1231, 1572-9397},
  doi = {10.1007/s10732-018-9388-7},
  abstract = {When developing optimisation algorithms, the focus often lies on obtaining an algorithm that is able to outperform other existing algorithms for some performance measure. It is not common practice to question the reasons for possible performance differences observed. These types of questions relate to evaluating the impact of the various heuristic parameters and often remain unanswered. In this paper, the focus is on gaining insight in the behaviour of a heuristic algorithm by investigating how the various elements operating within the algorithm correlate with performance, obtaining indications of which combinations work well and which do not, and how all these effects are influenced by the specific problem instance the algorithm is solving. We consider two approaches for analysing algorithm parameters and components \textemdash{} functional analysis of variance and multilevel regression analysis \textemdash{} and study the benefits of using both approaches jointly. We present the results of a combined methodology that is able to provide more insights than when the two approaches are used separately. The illustrative case studies in this paper analyse a Large Neighbourhood Search algorithm applied to the Vehicle Routing Problem with Time Windows and an Iterated Local Search algorithm for the Unrelated Parallel Machine Scheduling Problem with Sequence-dependent Setup Times.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QUA4I3CN/A_combined_approach_for_analysing_heuristic_algorithms.pdf}
}

@article{costaManagersModelersMeasuring2018,
  title = {Managers, Modelers, and Measuring the Impact of Species Distribution Model Uncertainty on Marine Zoning Decisions},
  author = {Costa, Bryan and Kendall, Matthew and McKagan, Steven},
  editor = {{Januchowski-Hartley}, Fraser Andrew},
  year = {2018},
  month = oct,
  journal = {PLOS ONE},
  volume = {13},
  number = {10},
  pages = {e0204569},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0204569},
  abstract = {Marine managers routinely use spatial data to make decisions about their marine environment. Uncertainty associated with this spatial data can have profound impacts on these management decisions and their projected outcomes. Recent advances in modeling techniques, including species distribution models (SDMs), make it easier to generate continuous maps showing the uncertainty associated with spatial predictions and maps. However, SDM predictions and maps can be complex and nuanced. This complexity makes their use challenging for non-technical managers, preventing them from having the best available information to make decisions. To help bridge these communication and information gaps, we developed maps to illustrate how SDMs and associated uncertainty can be translated into readily usable products for managers. We also explicitly described the potential impacts of uncertainty on marine zoning decisions. This approach was applied to a case study in Saipan Lagoon, Commonwealth of the Northern Mariana Islands (CNMI). Managers in Saipan are interested in minimizing the potential impacts of personal watercraft (e.g., jet skis) on staghorn Acropora (i.e., Acropora aspera, A. formosa, and A. pulchra), which is an important coral assemblage in the lagoon. We used a recently completed SDM for staghorn Acropora to develop maps showing the sensitivity of zoning options to three different prediction and three different uncertainty thresholds (nine combinations total). Our analysis showed that the amount of area and geographic location of predicted staghorn Acropora presence changed based on these nine combinations. These dramatically different spatial patterns would have significant zoning implications when considering where to exclude and/or allow jet skis operations inside the lagoon. They also show that different uncertainty thresholds may lead managers to markedly different conclusions and courses of action. Defining acceptable levels of uncertainty upfront is critical for ensuring that managers can make more informed decisions, meet their marine resource goals and generate favorable outcomes for their stakeholders.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8GXHRMV7/costa et al 2018 - Managers modelers and measuring the impact of species distribution model uncertainty on marine zoning decisions - RESERVE SELECTION - SDM - SCP - BDPG - GUPPY - UNCERTAINTY.pdf}
}

@article{cowlingExpertAlgorithmComparison2003,
  title = {The Expert or the Algorithm?\textemdash Comparison of Priority Conservation Areas in the {{Cape Floristic Region}} Identified by Park Managers and Reserve Selection Software},
  shorttitle = {The Expert or the Algorithm?},
  author = {Cowling, R.M and Pressey, R.L and {Sims-Castley}, R and {le Roux}, A and Baard, E and Burgers, C.J and Palmer, G},
  year = {2003},
  month = jul,
  journal = {Biological Conservation},
  volume = {112},
  number = {1-2},
  pages = {147--167},
  issn = {00063207},
  doi = {10.1016/S0006-3207(02)00397-X},
  abstract = {Expert-based and systematic, algorithm-based approaches to identifying priority areas for conservation are sometimes posited as alternatives. While both approaches have pros and cons, the systematic approach does have the advantage of providing a regionwide assessment of the options for achieving explicit conservation targets. A distinct advantage of the expert-driven approach is its incorporation of expert knowledge on biodiversity persistence and pragmatic management and implementation issues not normally included in biodiversity feature-site data matrices. Given the widespread application of both approaches, surprisingly little research has been undertaken to evaluate their conservation planning outcomes. Here we compare priority conservation areas in South Africa's Cape Floristic Region identified by park managers and reserve-selection software. Managers identified 29 areas (a wishlist) that together, comprised 31\% of the planning domain and had 40\% of its area under some form of conservation management. This wishlist was assessed for the extent to which it achieved targets for biodiversity pattern and process over and above the existing conservation system, and its incorporation of priority areas identified in terms of conservation value and vulnerability to processes that threaten biodiversity. Overall, the wishlist reflected a desire by managers to improve management efficiency and facilitate rapid implementation by expanding existing, largely montane reserves into low-priority areas where land tenure is sympathetic to conservation. Consequently, it was not very effective and efficient in achieving pattern and process targets, and it excluded large areas of vulnerable and inadequately conserved lowland habitat\textemdash the areas currently in most need of conservation action. Further, it provided no basis for scheduling implementation or for exploring alternative areas to achieve the same goals, unlike systematic approaches. Nonetheless, the manager's wishlist did include many highly innovative and feasible projects that make important contributions to the conservation of the region's biodiversity. Rather than emphasize the dichotomy between expert and systematic approaches, conservation planners should devise ways of integrating them. In particular, priority areas identified by experts should be carefully considered against the backdrop of the outcomes of systematic conservation planning.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8AF7JKGA/15 - cowling et al 2003 - The expert or the algorithm - comparison of priority conservation areas in the Cape Floristic Region identified by park managers and reserve selection software.pdf}
}

@article{daolioProblemFeaturesAlgorithm2017,
  title = {Problem {{Features}} versus {{Algorithm Performance}} on {{Rugged Multiobjective Combinatorial Fitness Landscapes}}},
  author = {Daolio, Fabio and Liefooghe, Arnaud and Verel, S{\'e}bastien and Aguirre, Hern{\'a}n and Tanaka, Kiyoshi},
  year = {2017},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {25},
  number = {4},
  pages = {555--585},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco_a_00193},
  abstract = {In this article, we attempt to understand and to contrast the impact of problem features on the performance of randomized search heuristics for black-box multiobjective combinatorial optimization problems. At first, we measure the performance of two conventional dominance-based approaches with unbounded archive on a benchmark of enumerable binary optimization problems with tunable ruggedness, objective space dimension, and objective correlation ({$\rho$}MNK-landscapes). Precisely, we investigate the expected runtime required by a global evolutionary optimization algorithm with an ergodic variation operator (GSEMO) and by a neighborhood-based local search heuristic (PLS), to identify a (1 + {$\epsilon$})-approximation of the Pareto set. Then, we define a number of problem features characterizing the fitness landscape, and we study their intercorrelation and their association with algorithm runtime on the benchmark instances. At last, with a mixed-effects multilinear regression we assess the individual and joint effect of problem features on the performance of both algorithms, within and across the instance classes defined by benchmark parameters. Our analysis reveals further insights into the importance of ruggedness and multimodality to characterize instance hardness for this family of multiobjective optimization problems and algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EBXA6UVM/daolio et al 2017 - Problem Features versus Algorithm Performance on Rugged Multiobjective Combinatorial Fitness Landscapes.pdf}
}

@article{dasRecentAdvancesDifferential2016,
  title = {Recent Advances in Differential Evolution \textendash{} {{An}} Updated Survey},
  author = {Das, Swagatam and Mullick, Sankha Subhra and Suganthan, P.N.},
  year = {2016},
  month = apr,
  journal = {Swarm and Evolutionary Computation},
  volume = {27},
  pages = {1--30},
  issn = {22106502},
  doi = {10.1016/j.swevo.2016.01.004},
  abstract = {Differential Evolution (DE) is arguably one of the most powerful and versatile evolutionary optimizers for the continuous parameter spaces in recent times. Almost 5 years have passed since the first comprehensive survey article was published on DE by Das and Suganthan in 2011. Several developments have been reported on various aspects of the algorithm in these 5 years and the research on and with DE have now reached an impressive state. Considering the huge progress of research with DE and its applications in diverse domains of science and technology, we find that it is a high time to provide a critical review of the latest literatures published and also to point out some important future avenues of research. The purpose of this paper is to summarize and organize the information on these current developments on DE. Beginning with a comprehensive foundation of the basic DE family of algorithms, we proceed through the recent proposals on parameter adaptation of DE, DE-based single-objective global optimizers, DE adopted for various optimization scenarios including constrained, large-scale, multi-objective, multi-modal and dynamic optimization, hybridization of DE with other optimizers, and also the multifaceted literature on applications of DE. The paper also presents a dozen of interesting open problems and future research issues on DE.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8Z65ZJWK/1-s2.0-S2210650216000146-main.pdf}
}

@article{davidsonPaper782Identifying,
  title = {Paper \#782:{{Identifying}} and {{Generating Easy Sets}} of {{Constraints For Clustering}}},
  author = {Davidson, Ian and Ravi, S S},
  pages = {7},
  abstract = {Clustering under constraints is a recent innovation in the artificial intelligence community that has yielded significant practical benefit. However, recent work has shown that for some negative forms of constraints the associated subproblem of just finding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufficient conditions from graph theory to identify apriori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KIAZYPSZ/davidson ravi 2006 - Identifying and Generating Easy Sets of Constraints For Clustering - aaai06.pdf}
}

@inproceedings{DBLP:conf/dimacs/Sanchis92,
  title = {Test Case Construction for the Vertex Cover Problem},
  booktitle = {Computational Support for Discrete Mathematics, Proceedings of a {{DIMACS}} Workshop, Piscataway, New Jersey, {{USA}}, March 12-14, 1992},
  author = {Sanchis, Laura A.},
  editor = {Dean, Nathaniel and Shannon, Gregory E.},
  year = {1992},
  series = {{{DIMACS}} Series in Discrete Mathematics and Theoretical Computer Science},
  volume = {15},
  pages = {315--326},
  publisher = {{DIMACS/AMS}},
  doi = {10.1090/dimacs/015/21},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/dimacs/Sanchis92.bib},
  timestamp = {Tue, 16 Jul 2019 17:45:06 +0200}
}

@article{deliaSurrogatebasedEnsembleGrouping2017,
  title = {Surrogate-Based {{Ensemble Grouping Strategies}} for {{Embedded Sampling-based Uncertainty Quantification}}},
  author = {D'Elia, Marta and Phipps, Eric and Rushdi, Ahmad and Ebeida, Mohamed},
  year = {2017},
  month = may,
  journal = {arXiv:1705.02003 [stat]},
  eprint = {1705.02003},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The embedded ensemble propagation approach introduced in [49] has been demonstrated to be a powerful means of reducing the computational cost of sampling-based uncertainty quantification methods, particularly on emerging computational architectures. A substantial challenge with this method however is ensemble-divergence, whereby different samples within an ensemble choose different code paths. This can reduce the effectiveness of the method and increase computational cost. Therefore grouping samples together to minimize this divergence is paramount in making the method effective for challenging computational simulations. In this work, a new grouping approach based on a surrogate for computational cost built up during the uncertainty propagation is developed and applied to model diffusion problems where computational cost is driven by the number of (preconditioned) linear solver iterations. The approach is developed within the context of locally adaptive stochastic collocation methods, where a surrogate for the number of linear solver iterations, generated from previous levels of the adaptive grid generation, is used to predict iterations for subsequent samples, and group them based on similar numbers of iterations. The effectiveness of the method is demonstrated by applying it to highly anisotropic diffusion problems with a wide variation in solver iterations from sample to sample. It extends the parameter-based grouping approach developed in [17] to more general problems without requiring detailed knowledge of how the uncertain parameters affect the simulation's cost, and is also less intrusive to the simulation code.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation},
  file = {/Users/bill/D/Zotero/storage/B7AMVXJJ/d elia et al 2017 - Surrogate-based Ensemble Grouping Strategies for Embedded Sampling-based Uncertainty Quantification - PROBLEM DIFFICULTY - UNCERTAINTY -BDPG.pdf}
}

@misc{diaz2019,
  title = {{{IPBES}} (2019): {{Summary}} for Policymakers of the Global Assessment Report on Biodiversity and Ecosystem Services of the {{Intergovernmental Science-Policy Platform}} on {{Biodiversity}} and {{Ecosystem Services}}},
  editor = {D{\'i}az, S. and Settele, J. and Brond{\'i}zio, E.S. and Ngo, H.T. and Gu{\`e}ze, M. and Agard, J. and Arneth, A. and Balvanera, P. and Brauman, K.A. and Butchart, S.H.M. and Chan, K.M.A. and Garibaldi, L.A. and Ichii, K. and Liu, J. and Subramanian, S.M. and Midgley, G.F. and Miloslavich, P. and Moln{\'a}r, Z. and Obura, D. and Pfaff, A. and Polasky, S. and Purvis, A. and Razzaque, J. and Reyers, B. and Chowdhury, R. Roy and Shin, Y.J. and {Visseren-Hamakers}, I.J. and Willis, K.J. and Zayas, C.N.},
  year = {2019},
  publisher = {{IPBES secretariat, Bonn, Germany}},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CY9BX4SF/IPBES 2019 - Summary for policymakers of the global assessment report on biodiversity and ecosystem services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services - BDPG.pdf}
}

@article{diaz2019s,
  title = {Pervasive Human-Driven Decline of Life on {{Earth}} Points to the Need for Transformative Change},
  author = {D{\'i}az, Sandra and Settele, Josef and Brond{\'i}zio, Eduardo S. and Ngo, Hien T. and Agard, John and Arneth, Almut and Balvanera, Patricia and Brauman, Kate A. and Butchart, Stuart H. M. and Chan, Kai M. A. and Garibaldi, Lucas A. and Ichii, Kazuhito and Liu, Jianguo and Subramanian, Suneetha M. and Midgley, Guy F. and Miloslavich, Patricia and Moln{\'a}r, Zsolt and Obura, David and Pfaff, Alexander and Polasky, Stephen and Purvis, Andy and Razzaque, Jona and Reyers, Belinda and Chowdhury, Rinku Roy and Shin, Yunne-Jai and {Visseren-Hamakers}, Ingrid and Willis, Katherine J. and Zayas, Cynthia N.},
  year = {2019},
  month = dec,
  journal = {Science},
  volume = {366},
  number = {6471},
  pages = {eaax3100},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax3100},
  abstract = {The human impact on life on Earth has increased sharply since the 1970s, driven by the demands of a growing population with rising average per capita income. Nature is currently supplying more materials than ever before, but this has come at the high cost of unprecedented global declines in the extent and integrity of ecosystems, distinctness of local ecological communities, abundance and number of wild species, and the number of local domesticated varieties. Such changes reduce vital benefits that people receive from nature and threaten the quality of life of future generations. Both the benefits of an expanding economy and the costs of reducing nature's benefits are unequally distributed. The fabric of life on which we all depend\textemdash nature and its contributions to people\textemdash is unravelling rapidly. Despite the severity of the threats and lack of enough progress in tackling them to date, opportunities exist to change future trajectories through transformative action. Such action must begin immediately, however, and address the root economic, social, and technological causes of nature's deterioration.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YJPUN6SU/diaz et al 2019 - pervasive human-driven decline of life on earth points to theneed for transformative change - BDPG.pdf}
}

@article{dicksonSystematicIdentificationPotential2014,
  title = {Systematic Identification of Potential Conservation Priority Areas on Roadless {{Bureau}} of {{Land Management}} Lands in the Western {{United States}}},
  author = {Dickson, Brett G. and Zachmann, Luke J. and Albano, Christine M.},
  year = {2014},
  month = oct,
  journal = {Biological Conservation},
  volume = {178},
  pages = {117--127},
  issn = {00063207},
  doi = {10.1016/j.biocon.2014.08.001},
  abstract = {With ongoing global change, there is an urgent need to expand existing networks of important conservation areas around the world. In the western United States, vast areas of public land, including those administered by the Bureau of Land Management (BLM), present substantial conservation opportunities. For 11 contiguous western states, we used a novel multiple-criteria analysis to model and map contiguous areas of roadless BLM land that possessed important ecological indicators of high biodiversity, resilience to climate change, and landscape connectivity. Specifically, we leveraged available spatial datasets to implement a systematic and statistically robust analysis of seven key indicators at three different spatial scales, and to identify the locations of potential conservation priority areas (CPAs) across 294,274 km2 of roadless BLM land. Within this extent, and based on conservative thresholds in our results, we identified 43,417 km2 of land with relatively high conservation value and 117 unique CPAs totaling 6291 km2. Most CPA lands were located in Utah, Colorado, Arizona, Oregon, and Nevada. Overall, CPAs had higher species richness, vegetation community diversity, topographic complexity, and surface water availability than existing BLM protected areas. CPAs often corresponded with locations known to have important wilderness characteristics or were adjacent to established areas of ecological, social, or cultural importance. These CPAs represent a diverse set of places that can be used by multiple stakeholders in ongoing or future landscape conservation and special designation efforts in BLM and adjacent ownerships. Our methodological framework and novel weighting approach can accommodate a wide range of input variables and is readily applicable to other jurisdictions and regions within the U.S. and beyond. \'O 2014 Elsevier Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RS2XXYXJ/1-s2.0-S0006320714002882-main.pdf}
}

@article{dietterich2017a,
  title = {Steps {{Toward Robust Artificial Intelligence}}},
  author = {Dietterich, Thomas G.},
  year = {2017},
  month = oct,
  journal = {AI Magazine},
  volume = {38},
  number = {3},
  pages = {3--24},
  issn = {2371-9621, 0738-4602},
  doi = {10.1609/aimag.v38i3.2756},
  abstract = {Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system's models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5W6C2W4S/dietterich 2017 - Steps Toward Robust Artificial Intelligence - UNCERTAINTY - AI - ML - ROBUSTNESS - OVERFITTING - REGULARIZATION.pdf}
}

@article{dilkina2014amai,
  title = {Tradeoffs in the Complexity of Backdoors to Satisfiability: Dynamic Sub-Solvers and Learning during Search},
  shorttitle = {Tradeoffs in the Complexity of Backdoors to Satisfiability},
  author = {Dilkina, Bistra and Gomes, Carla P. and Sabharwal, Ashish},
  year = {2014},
  month = apr,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {70},
  number = {4},
  pages = {399--431},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-014-9407-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P7GYPKCW/dilkina gomes sabharwal 2014 - Tradeoffs in the complexity of backdoors to satisfiability - dynamic sub-solvers and learning during search - PROBLEM DIFFICULTY - BDPG - CSPs.pdf}
}

@article{dimitriou,
  title = {On {{SAT Distributions}} with {{Planted Assignments}} ({{Extended Abstract}})},
  author = {Dimitriou, Tassos},
  pages = {6},
  abstract = {While it is known how to generate satisfiable instances by reducing certain computational problems to SAT, it is not known how a similar generator can be developed directly for k-SAT. In this work we almost answer this question affirmatively by improving upon previous results in many ways. First, we give a generator for instances of MAX k-SAT, the version of k-SAT where one wants to maximize the number of satisfied clauses. Second, we provide a useful characterization of the optimal solution. In our model not only we know how the optimal solution looks like but we also prove it is unique. Finally, we show that our generator has certain useful computational properties among which is the ability to control the hardness of the generated instances, the appearance of an easy-hard-easy pattern in the search complexity for good assignments and a new type of phase transition which is related to the uniqueness of the optimal solution.},
  langid = {english},
  keywords = {bdpg,planted solution},
  file = {/Users/bill/D/Zotero/storage/ZURL45N2/dimitriou - On SAT Distributions with Planted Assignments - Extended Abstract.pdf}
}

@article{dimitriouSATDistributionsPhase2003,
  title = {{{SAT Distributions}} with {{Phase Transitions}} between {{Decision}} and {{Optimization Problems}}},
  author = {Dimitriou, Tassos},
  year = {2003},
  month = oct,
  journal = {Electronic Notes in Discrete Mathematics},
  volume = {16},
  pages = {1--14},
  issn = {15710653},
  doi = {10.1016/S1571-0653(04)00459-7},
  abstract = {We present a generator for weighted instances of MAX k-SAT in which every clause has a weight associated with it and the goal is to maximize the total weight of satis\textasciimacron ed clauses. Our generator produces formulas whose hardness can be \textasciimacron nely tuned by two parameters p and {$\pm$} that control the weights of the clauses. Under the right choice of these parameters an easy-hard-easy pattern in the search complexity emerges which is similar to the patterns observed for traditional SAT distributions. What is remarkable, however, is that the generated distributions seem to lie in the middle ground between decision and optimization problems. Increasing the value of p from 0 to 1 has the e\textregistered ect of changing the shape of the computational cost from an easy-hard-easy pattern which is typical of decision problems to an easy-hard pattern which is typical of optimization problems. Thus our distributions seem to bridge the gap between decision and optimization versions of SAT.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IBD3VTU7/dimitriou 2003 - SAT Distributions with Phase Transitions between Decision and Optimization Problems.pdf}
}

@inproceedings{dinurComputationalBenefitCorrelated2015,
  title = {The {{Computational Benefit}} of {{Correlated Instances}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Innovations}} in {{Theoretical Computer Science}} - {{ITCS}} '15},
  author = {Dinur, Irit and Goldwasser, Shafi and Lin, Huijia},
  year = {2015},
  pages = {219--228},
  publisher = {{ACM Press}},
  address = {{Rehovot, Israel}},
  doi = {10.1145/2688073.2688082},
  abstract = {The starting point of this paper is that instances of computational problems often do not exist in isolation. Rather, multiple and correlated instances of the same problem arise naturally in the real world. The challenge is how to gain computationally from instance correlations when they exist. We will be interested in settings where significant computational gain can be made in solving a single primary instance by having access to additional auxiliary instances which are correlated to the primary instance via the solution space.},
  isbn = {978-1-4503-3333-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MPPCMSNJ/TR14-083.pdf}
}

@article{dochertyCaseStructuringDiscussion1999,
  title = {The Case for Structuring the Discussion of Scientific Papers},
  author = {Docherty, M. and Smith, R.},
  year = {1999},
  month = may,
  journal = {BMJ},
  volume = {318},
  number = {7193},
  pages = {1224--1225},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.318.7193.1224},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DXT6S9G8/The_case_for_structuring_discussion_of_scientific_.pdf}
}

@article{dormannCollinearityReviewMethods2013,
  title = {Collinearity: A Review of Methods to Deal with It and a Simulation Study Evaluating Their Performance},
  shorttitle = {Collinearity},
  author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carr{\'e}, Gabriel and Marqu{\'e}z, Jaime R. Garc{\'i}a and Gruber, Bernd and Lafourcade, Bruno and Leit{\~a}o, Pedro J. and others},
  year = {2013},
  journal = {Ecography},
  volume = {36},
  number = {1},
  pages = {27--46},
  file = {/Users/bill/D/Zotero/storage/MJAZDF2H/Dormann et al_2013_Collinearity.pdf}
}

@article{dormannIndicesGraphsNull2009,
  title = {Indices, {{Graphs}} and {{Null Models}}: {{Analyzing Bipartite Ecological Networks}}},
  shorttitle = {Indices, {{Graphs}} and {{Null Models}}},
  author = {Dormann, Carsten F. and Frund, Jochen and Bluthgen, Nico and Gruber, Bernd},
  year = {2009},
  month = feb,
  journal = {The Open Ecology Journal},
  volume = {2},
  number = {1},
  pages = {7--24},
  issn = {18742130},
  doi = {10.2174/1874213000902010007},
  abstract = {Many analyses of ecological networks in recent years have introduced new indices to describe network properties. As a consequence, tens of indices are available to address similar questions, differing in specific detail, sensitivity in detecting the property in question, and robustness with respect to network size and sampling intensity. Furthermore, some indices merely reflect the number of species participating in a network, but not their interrelationship, requiring a null model approach. Here we introduce a new, free software calculating a large spectrum of network indices, visualizing bipartite networks and generating null models. We use this tool to explore the sensitivity of 26 network indices to network dimensions, sampling intensity and singleton observations. Based on observed data, we investigate the interrelationship of these indices, and show that they are highly correlated, and heavily influenced by network dimensions and connectance. Finally, we re-evaluate five common hypotheses about network properties, comparing 19 pollination networks with three differently complex null models: 1. The number of links per species (``degree'') follow (truncated) power law distributions. 2. Generalist pollinators interact with specialist plants, and vice versa (dependence asymmetry). 3. Ecological networks are nested. 4. Pollinators display complementarity, owing to specialization within the network. 5. Plant-pollinator networks are more robust to extinction than random networks. Our results indicate that while some hypotheses hold up against our null models, others are to a large extent understandable on the basis of network size, rather than ecological interrelationships. In particular, null model pattern of dependence asymmetry and robustness to extinction are opposite to what current network paradigms suggest. Our analysis, and the tools we provide, enables ecologists to readily contrast their findings with null model expectations for many different questions, thus separating statistical inevitability from ecological process.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BHF2UX5H/OpenEcolJ2009,2_7-24_open.pdf}
}

@article{dormannMethodDetectingModules2014,
  title = {A Method for Detecting Modules in Quantitative Bipartite Networks},
  author = {Dormann, Carsten F. and Strauss, Rouven},
  editor = {{Peres-Neto}, Pedro},
  year = {2014},
  month = jan,
  journal = {Methods in Ecology and Evolution},
  volume = {5},
  number = {1},
  pages = {90--98},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12139},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ER9JL476/Dormann_et_al-2014-Methods_in_Ecology_and_Evolution.pdf}
}

@article{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  journal = {arXiv:1702.08608 [cs, stat]},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/4SG2BE2D/selbst powles 2017 - Meaningful information and the right to explanation - EFs - OPTISEVIL.pdf}
}

@article{doubleday2017tie&e,
  title = {Publishing with {{Objective Charisma}}: {{Breaking Science}}'s {{Paradox}}},
  shorttitle = {Publishing with {{Objective Charisma}}},
  author = {Doubleday, Zo{\"e} A. and Connell, Sean D.},
  year = {2017},
  month = nov,
  journal = {Trends in Ecology \& Evolution},
  volume = {32},
  number = {11},
  pages = {803--805},
  issn = {01695347},
  doi = {10.1016/j.tree.2017.06.011},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/879QSPP8/doubleday connell 2017 - Publishing with Objective  Charisma - Breaking  Sciences Paradox - SCIENTIFIC WRITING - BDPG.pdf}
}

@techreport{drira2019,
  type = {Preprint},
  title = {Are Trade-Offs between Flexibility and Efficiency in Systematic Conservation Planning Avoidable ?},
  author = {Drira, Sabrine and Lasram, Frida Ben Rais and Hattab, Tarek and Shin, Yunne Jai and Jenhani, Amel Ben Rejeb and Guilhaumon, Fran{\c c}ois},
  year = {2019},
  month = sep,
  institution = {{Ecology}},
  doi = {10.1101/775072},
  abstract = {Abstract           Species distribution models (SDMs) have been proposed as a way to provide robust inference about species-specific sites suitabilities, and have been increasingly used in systematic conservation planning (SCP) applications. However, despite the fact that the use of SDMs in SCP may raise some potential issues, conservation studies have overlooked to assess the implications of SDMs uncertainties. The integration of these uncertainties in conservation solutions requires the development of a reserve-selection approach based on a suitable optimization algorithm. A large body of research has shown that exact optimization algorithms give very precise control over the gap to optimality of conservation solutions. However, their major shortcoming is that they generate a single binary and indivisible solution. Therefore, they provide no flexibility in the implementation of conservation solutions by stakeholders. On the other hand, heuristic decision-support systems provide large amounts of sub-optimal solutions, and therefore more flexibility. This flexibility arises from the availability of many alternative and sub-optimal conservation solutions. The two principles of efficiency and flexibility are implicitly linked in conservation applications, with the most mathematically efficient solutions being inflexible and the flexible solutions provided by heuristics suffering sub-optimality. In order to avoid the trade-offs between flexibility and efficiency in systematic conservation planning, we propose in this paper a new reserve-selection framework based on mathematical programming optimization combined with a post-selection of SDM outputs. This approach leads to a reserve-selection framework that might provide flexibility while simultaneously addressing efficiency and representativeness of conservation solutions and the adequacy of conservation targets. To exemplify the approach we a nalyzed an experimental design crossing pre- and post-selection of SDM outputs versus heuristics and exact mathematical optimizations. We used the Mediterranean Sea as a biogeographical template for our analyses, integrating the outputs of 8 SDM techniques for 438 fishes species.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LMT8HP42/drira et al 2019 - Are trade-offs between flexibility and efficiency in systematic conservationplanning avoidable - BDPG - ENSEMBLES.pdf}
}

@article{duffySimplicityRobustLight2020,
  title = {The Simplicity of Robust Light Harvesting},
  author = {Duffy, Christopher D. P.},
  year = {2020},
  month = jun,
  journal = {Science},
  volume = {368},
  number = {6498},
  pages = {1427--1428},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abc8063},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V5W6Y6D4/duffy 2020 - The simplicity of robustlight harvesting - WHY ARE PLANTS GREEN - ROBUSTNESS - UNCERTAINTY - OPTISEVIL.pdf}
}

@article{dujardinSolvingMultiobjectiveOptimization2018,
  title = {Solving Multi-Objective Optimization Problems in Conservation with the Reference Point Method},
  author = {Dujardin, Yann and Chad{\`e}s, Iadine},
  editor = {{Soleimani-damaneh}, Majid},
  year = {2018},
  month = jan,
  journal = {PLOS ONE},
  volume = {13},
  number = {1},
  pages = {e0190748},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0190748},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K6BYAED3/dujardin chades 2018 - Solving multi-objective optimization problems in conservation with the reference point method - MULTI-OBJECTIVE.pdf}
}

@article{dvovrakMaximizingSubmodularFunction,
  title = {Maximizing a {{Submodular Function}} with {{Viability Constraints}}},
  author = {Dvo{\textasciicaron}rak, Wolfgang and Henzinger, Monika and Williamson, David P},
  pages = {15},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/83DBBMKI/dvorak et al - maximizing a submodular function with viability constraints.pdf}
}

@article{dvovrakMaximizingSubmodularFunctiona,
  title = {Maximizing a {{Submodular Function}} with {{Viability Constraints}}},
  author = {Dvo{\textasciicaron}rak, Wolfgang and Henzinger, Monika and Williamson, David P},
  pages = {15},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HQATPSXW/dvorak et al - maximizing a submodular function with viability constraints.pdf}
}

@article{eatonSpatialConservationPlanning2019,
  title = {Spatial Conservation Planning under Uncertainty: Adapting to Climate Change Risks Using Modern Portfolio Theory},
  shorttitle = {Spatial Conservation Planning under Uncertainty},
  author = {Eaton, Mitchell J. and Yurek, Simeon and Haider, Zulqarnain and Martin, Julien and Johnson, Fred A. and Udell, Bradley J. and Charkhgard, Hadi and Kwon, Changhyun},
  year = {2019},
  month = oct,
  journal = {Ecological Applications},
  volume = {29},
  number = {7},
  issn = {1051-0761, 1939-5582},
  doi = {10.1002/eap.1962},
  abstract = {Climate change and urban growth impact habitats, species, and ecosystem services. To buffer against global change, an established adaptation strategy is designing protected areas to increase representation and complementarity of biodiversity features. Uncertainty regarding the scale and magnitude of landscape change complicates reserve planning and exposes decision makers to the risk of failing to meet conservation goals. Conservation planning tends to treat risk as an absolute measure, ignoring the context of the management problem and risk preferences of stakeholders. Application of risk management theory to conservation emphasizes the diversification of a portfolio of assets, with the goal of reducing the impact of system volatility on investment return. We use principles of Modern Portfolio Theory (MPT), which quantifies risk as the variance and correlation among assets, to formalize diversification as an explicit strategy for managing risk in climate-driven reserve design. We extend MPT to specify a framework that evaluates multiple conservation objectives, allows decision makers to balance management benefits and risk when preferences are contested or unknown, and includes additional decision options such as parcel divestment when evaluating candidate reserve designs. We apply an efficient search algorithm that optimizes portfolio design for large conservation problems and a game theoretic approach to evaluate portfolio trade-offs that satisfy decision makers with divergent benefit and risk tolerances, or when a single decision maker cannot resolve their own preferences. Evaluating several risk profiles for a case study in South Carolina, our results suggest that a reserve design may be somewhat robust to differences in risk attitude but that budgets will likely be important determinants of conservation planning strategies, particularly when divestment is considered a viable alternative. We identify a possible fiscal threshold where adequate resources allow protecting a sufficiently diverse portfolio of habitats such that the risk of failing to achieve conservation objectives is considerably lower. For a range of sea-level rise projections, conversion of habitat to open water (14\textendash 180\%) and wetland loss (1\textendash 7\%) are unable to be compensated under the current protected network. In contrast, optimal reserve design outcomes are predicted to ameliorate expected losses relative to current and future habitat protected under the existing conservation estate.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CBJM2RG5/eap1962-sup-0002-datas1.zip;/Users/bill/D/Zotero/storage/E8LS9LPE/eap1962-sup-0001-appendixs1.pdf;/Users/bill/D/Zotero/storage/T37IVXAS/eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG.pdf;/Users/bill/D/Zotero/storage/WYNGGNF7/eap1962-sup-0003-metadatas1.pdf}
}

@article{eatonSpatialConservationPlanning2019a,
  title = {Spatial Conservation Planning under Uncertainty: Adapting to Climate Change Risks Using Modern Portfolio Theory},
  shorttitle = {Spatial Conservation Planning under Uncertainty},
  author = {Eaton, Mitchell J. and Yurek, Simeon and Haider, Zulqarnain and Martin, Julien and Johnson, Fred A. and Udell, Bradley J. and Charkhgard, Hadi and Kwon, Changhyun},
  year = {2019},
  month = oct,
  journal = {Ecological Applications},
  volume = {29},
  number = {7},
  issn = {1051-0761, 1939-5582},
  doi = {10.1002/eap.1962},
  abstract = {Climate change and urban growth impact habitats, species, and ecosystem services. To buffer against global change, an established adaptation strategy is designing protected areas to increase representation and complementarity of biodiversity features. Uncertainty regarding the scale and magnitude of landscape change complicates reserve planning and exposes decision makers to the risk of failing to meet conservation goals. Conservation planning tends to treat risk as an absolute measure, ignoring the context of the management problem and risk preferences of stakeholders. Application of risk management theory to conservation emphasizes the diversification of a portfolio of assets, with the goal of reducing the impact of system volatility on investment return. We use principles of Modern Portfolio Theory (MPT), which quantifies risk as the variance and correlation among assets, to formalize diversification as an explicit strategy for managing risk in climate-driven reserve design. We extend MPT to specify a framework that evaluates multiple conservation objectives, allows decision makers to balance management benefits and risk when preferences are contested or unknown, and includes additional decision options such as parcel divestment when evaluating candidate reserve designs. We apply an efficient search algorithm that optimizes portfolio design for large conservation problems and a game theoretic approach to evaluate portfolio trade-offs that satisfy decision makers with divergent benefit and risk tolerances, or when a single decision maker cannot resolve their own preferences. Evaluating several risk profiles for a case study in South Carolina, our results suggest that a reserve design may be somewhat robust to differences in risk attitude but that budgets will likely be important determinants of conservation planning strategies, particularly when divestment is considered a viable alternative. We identify a possible fiscal threshold where adequate resources allow protecting a sufficiently diverse portfolio of habitats such that the risk of failing to achieve conservation objectives is considerably lower. For a range of sea-level rise projections, conversion of habitat to open water (14\textendash 180\%) and wetland loss (1\textendash 7\%) are unable to be compensated under the current protected network. In contrast, optimal reserve design outcomes are predicted to ameliorate expected losses relative to current and future habitat protected under the existing conservation estate.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HIHG8RUH/eaton et al 2019 - Spatial conservation planning under uncertainty - adapting to climate change risks using modern portfolio theory - BDPG - ANNO.pdf}
}

@article{edmonds,
  title = {Search {{Time}} When {{Solving Random Jigsaw Puzzles}} with a {{Planted Solution}}},
  author = {Edmonds, Jeff and Edmonds, Alex},
  pages = {14},
  abstract = {John Tsotsos asked his AI students to work on having an AI solve a jigsaw puzzle, though it is NP-complete. This paper considers the search time of the recursive backtracking algorithm for solving it when the puzzle pieces are generated randomly with a planted solution. Feeling that a jigsaw puzzle should have a particular image when completed, we set the probability that two pieces fit together locally to be just small enough that with high probability the planted solution is unique. We were surprised to see that in this case the recursive backtracking algorithm which expands a rectangle from a corner has expected 1 + \k{o} branching width. In contrast, this width is exponential if it attempts to put together just the edge pieces into a frame, build a triangle from a corner, or build a block in the middle. If the probability of pieces fitting together is increased just to the point that there is an exponential number of complete solutions, then no matter which order the pieces are put together it takes an exponential amount of time to find one of these exponentially many solutions. It is also interesting that for the different orders of completing the puzzle, there are very different ``reasons'' for the exponential blowup. We coded the algorithm and it ran just as described.},
  langid = {english},
  keywords = {planted solution},
  file = {/Users/bill/D/Zotero/storage/TZY6XBVV/puzzle.pdf}
}

@book{efron2016,
  title = {Computer {{Age Statistical Inference}}},
  author = {Efron, Bradley and Hastie, Trevor},
  year = {2016},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8CN688ID/efron hastie - computer age statistical inference.pdf}
}

@article{eggenspergerEfficientBenchmarkingAlgorithm2018,
  title = {Efficient Benchmarking of Algorithm Configurators via Model-Based Surrogates},
  author = {Eggensperger, Katharina and Lindauer, Marius and Hoos, Holger H. and Hutter, Frank and {Leyton-Brown}, Kevin},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {15--41},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5683-z},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/ACUTAN5A/Eggensperger2018_Article_EfficientBenchmarkingOfAlgorit.pdf}
}

@article{eggenspergerPitfallsBestPractices2019,
  title = {Pitfalls and {{Best Practices}} in {{Algorithm Configuration}}},
  author = {Eggensperger, Katharina and Lindauer, Marius and Hutter, Frank},
  year = {2019},
  month = apr,
  journal = {Journal of Artificial Intelligence Research},
  volume = {64},
  pages = {861--893},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11420},
  abstract = {Good parameter settings are crucial to achieve high performance in many areas of artificial intelligence (AI), such as propositional satisfiability solving, AI planning, scheduling, and machine learning (in particular deep learning). Automated algorithm configuration methods have recently received much attention in the AI community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. However, practical applications of algorithm configuration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineffective. We identify several common issues and propose best practices for avoiding them. As one possibility for automatically handling as many of these as possible, we also propose a tool called GenericWrapper4AC.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9N2IH6KL/11420-Article (PDF)-21322-1-10-20190416.pdf}
}

@article{elhorstAppliedSpatialEconometrics2010,
  title = {Applied {{Spatial Econometrics}}: {{Raising}} the {{Bar}}},
  shorttitle = {Applied {{Spatial Econometrics}}},
  author = {Elhorst, J. Paul},
  year = {2010},
  month = mar,
  journal = {Spatial Economic Analysis},
  volume = {5},
  number = {1},
  pages = {9--28},
  issn = {1742-1772, 1742-1780},
  doi = {10.1080/17421770903541772},
  abstract = {This paper places the key issues and implications of the new `introductory' book on spatial econometrics by James LeSage \& Kelley Pace (2009) in a broader perspective: the argument in favour of the spatial Durbin model, the use of indirect effects as a more valid basis for testing whether spatial spillovers are significant, the use of Bayesian posterior model probabilities to determine which spatial weights matrix best describes the data, and the book's contribution to the literature on spatiotemporal models. The main conclusion is that the state of the art of applied spatial econometrics has taken a step change with the publication of this book.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FVFZPUK7/Elhorst_SEA2010.pdf}
}

@article{elithArtModellingRangeshifting2010,
  title = {The Art of Modelling Range-Shifting Species},
  author = {Elith, Jane and Kearney, Michael and Phillips, Steven},
  year = {2010},
  journal = {Methods in ecology and evolution},
  volume = {1},
  number = {4},
  pages = {330--342}
}

@article{elithSpeciesDistributionModels2009,
  title = {Species Distribution Models: Ecological Explanation and Prediction across Space and Time},
  shorttitle = {Species Distribution Models},
  author = {Elith, Jane and Leathwick, John R.},
  year = {2009},
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  volume = {40},
  number = {1},
  pages = {677}
}

@article{elithStatisticalExplanationMaxEnt2011,
  title = {A Statistical Explanation of {{MaxEnt}} for Ecologists},
  author = {Elith, Jane and Phillips, Steven J. and Hastie, Trevor and Dud{\'i}k, Miroslav and Chee, Yung En and Yates, Colin J.},
  year = {2011},
  journal = {Diversity and Distributions},
  volume = {17},
  number = {1},
  pages = {43--57},
  file = {/Users/bill/D/Zotero/storage/CQG3Z3C5/Elith et al_2011_A statistical explanation of MaxEnt for ecologists.pdf}
}

@article{elithTheyHowThey2009,
  title = {Do They? {{How}} Do They? {{WHY}} Do They Differ? {{On}} Finding Reasons for Differing Performances of Species Distribution Models},
  shorttitle = {Do They?},
  author = {Elith, Jane and Graham, Catherine H.},
  year = {2009},
  journal = {Ecography},
  volume = {32},
  number = {1},
  pages = {66--77},
  file = {/Users/bill/D/Zotero/storage/X9AEXHBG/Elith_Graham_2009_Do they.pdf}
}

@article{elithWorkingGuideBoosted2008,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  month = jul,
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {0021-8790, 1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CHWAKAAZ/Elith_et_al-2008-Journal_of_Animal_Ecology.pdf}
}

@article{embrechtsStatisticsQuantitativeRisk2014,
  title = {Statistics and {{Quantitative Risk Management}} for {{Banking}} and {{Insurance}}},
  author = {Embrechts, Paul and Hofert, Marius},
  year = {2014},
  month = jan,
  journal = {Annual Review of Statistics and Its Application},
  volume = {1},
  number = {1},
  pages = {493--514},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-022513-115631},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/32GXZ2MF/Embrechts_Hofert_2014_Statistics and Quantitative Risk Management for Banking and Insurance.pdf}
}

@article{erosNetworkThinkingRiverscape2011,
  title = {Network Thinking in Riverscape Conservation \textendash{} {{A}} Graph-Based Approach},
  author = {Er{\H o}s, Tibor and Schmera, D{\'e}nes and Schick, Robert S.},
  year = {2011},
  month = jan,
  journal = {Biological Conservation},
  volume = {144},
  number = {1},
  pages = {184--192},
  issn = {00063207},
  doi = {10.1016/j.biocon.2010.08.013},
  abstract = {Graph theoretic approaches have received increased interest recently in landscape planning and conservation in the terrestrial realm, because these approaches facilitate the effective modelling of connectivity among habitats. We examined whether basic principles of graph theory can be extended to other ecosystems. Specifically, we demonstrate how a network-based context can be used for enhancing the more effective conservation of riverine systems. We first show how to use graph theoretic techniques to model riverscapes at the segment level. Then we use a real stream network (Zagyva river basin, Hungary) to examine the topological importance of segments in maintaining riverscape connectivity, using betweenness centrality, a commonly used network measure. Using the undirected graph model of this riverscape, we then prioritize segments for conservation purpose. We examine the value of each of the 93 segments present in the Zagyva river basin by considering the conservation value of local fish assemblages, connectivity and the size of the habitat patches. For this purpose we use the `integral index of connectivity', a recently advocated habitat availability index. Based on the results the selection of the most valuable habitat segments can be optimized depending on conservation resources. Because of their inherent advantage in the consideration of connectivity relationships, we suggest that network analyses offer a simple, yet effective tool for searching for key segments (or junctions) in riverscapes for conservation and environmental management. Further, although the joint consideration of aquatic and terrestrial networks is challenging, the extension of network analyses to freshwater systems may facilitate the more effective selection of priority areas for conservation in continental areas.},
  langid = {english},
  keywords = {graph theory,networks,rivers,stef},
  file = {/Users/bill/D/Zotero/storage/D6HKFZ44/eros et al 2010 - network thinking in riverscape conservation - a graph-based approach - biocons.pdf}
}

@article{etesamiPseudorandomnessDepth2Circuits,
  title = {Pseudorandomness against {{Depth-2 Circuits}} and {{Analysis}} of {{Goldreich}}'s {{Candidate One-Way Function}}},
  author = {Etesami, Seyed Omid},
  pages = {84},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JBJGXTCJ/EECS-2010-180.pdf}
}

@article{eugster2014cs&da,
  title = {({{Psycho-}})Analysis of Benchmark Experiments: {{A}} Formal Framework for Investigating the Relationship between Data Sets and Learning Algorithms},
  shorttitle = {({{Psycho-}})Analysis of Benchmark Experiments},
  author = {Eugster, Manuel J.A. and Leisch, Friedrich and Strobl, Carolin},
  year = {2014},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {986--1000},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.08.007},
  abstract = {It is common knowledge that the performance of different learning algorithms depends on certain characteristics of the data\textemdash such as dimensionality, linear separability or sample size. However, formally investigating this relationship in an objective and reproducible way is not trivial. A new formal framework for describing the relationship between data set characteristics and the performance of different learning algorithms is proposed. The framework combines the advantages of benchmark experiments with the formal description of data set characteristics by means of statistical and information-theoretic measures and with the recursive partitioning of Bradley\textendash Terry models for comparing the algorithms' performances. The formal aspects of each component are introduced and illustrated by means of an artificial example. Its real-world usage is demonstrated with an application example consisting of thirteen widely-used data sets and six common learning algorithms. The Appendix provides information on the implementation and the usage of the framework within the R language.},
  langid = {english},
  keywords = {bdpg,benchmarking},
  file = {/Users/bill/D/Zotero/storage/WRJ75F24/eugster leisch strobl 2014 - (Psycho-)analysis of benchmark experiments - A formal framework for investigating the relationship between data sets and learning algorithms - GUPPY PROBLEM DIFFICULTY.pdf}
}

@article{fan2011ai,
  title = {On the Phase Transitions of Random K-Constraint Satisfaction Problems},
  author = {Fan, Yun and Shen, Jing},
  year = {2011},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {175},
  number = {3-4},
  pages = {914--927},
  issn = {00043702},
  doi = {10.1016/j.artint.2010.11.004},
  abstract = {Constraint satisfaction has received increasing attention over the years. Intense research has focused on solving all kinds of constraint satisfaction problems (CSPs). In this paper, first we propose a random CSP model, named k-CSP, that guarantees the existence of phase transitions under certain circumstances. The exact location of the phase transition is quantified and experimental results are provided to illustrate the performance of the proposed model. Second, we revise the model k-CSP to a random linear CSP by incorporating certain linear structure to constraint relations. We also prove the existence of the phase transition and exhibit its exact location for this random linear CSP model.},
  langid = {english},
  keywords = {bdpg,constraint satisfaction problems,CSP,phase transition,problem difficulty,problem generator},
  file = {/Users/bill/D/Zotero/storage/59WBZLE3/fan shen 2011 - on the phase transitions of random k-constraint satisfaction problems - BDPG - CSP.pdf}
}

@article{fan2012ai,
  title = {A General Model and Thresholds for Random Constraint Satisfaction Problems},
  author = {Fan, Yun and Shen, Jing and Xu, Ke},
  year = {2012},
  month = dec,
  journal = {Artificial Intelligence},
  volume = {193},
  pages = {1--17},
  issn = {00043702},
  doi = {10.1016/j.artint.2012.08.003},
  abstract = {In this paper, we study the relation among the parameters in their most general setting that define a large class of random CSP models d-k-CSP where d is the domain size and k is the length of the constraint scopes. The model d-k-CSP unifies several related models such as the model RB and the model k-CSP. We prove that the model d-k-CSP exhibits exact phase transitions if k ln d increases no slower than the logarithm of the number of variables. A series of experimental studies with interesting observations are carried out to illustrate the solubility phase transition and the hardness of instances around phase transitions.},
  langid = {english},
  keywords = {bdpg},
  file = {/Users/bill/D/Zotero/storage/7XNJZG2W/fan shen xu 2012 - a general model and thresholds for random constraint satisfaction problems - BDPG.pdf}
}

@incollection{fang2014fia,
  title = {Combining {{Edge Weight}} and {{Vertex Weight}} for {{Minimum Vertex Cover Problem}}},
  booktitle = {Frontiers in {{Algorithmics}}},
  author = {Fang, Zhiwen and Chu, Yang and Qiao, Kan and Feng, Xu and Xu, Ke},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Chen, Jianer and Hopcroft, John E. and Wang, Jianxin},
  year = {2014},
  volume = {8497},
  pages = {71--81},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08016-1_7},
  abstract = {The Minimum Vertex Cover (MVC) problem is an important NP-hard combinatorial optimization problem. Constraint weighting is an effective technique in stochastic local search algorithms for the MVC problem. The edge weight and the vertex weight have been used separately by different algorithms. We present a new local search algorithm, namely VEWLS, which integrates the edge weighting scheme with the vertex weighting scheme. To the best of our knowledge, it is the first time to combine two weighting schemes for the MVC problem. Experiments over both the DIMACS benchmark and the BHOSLIB benchmark show that VEWLS outperforms NuMVC, the state-of-the-art local search algorithm for MVC, on 73\% and 68\% of the instances, respectively.},
  isbn = {978-3-319-08015-4 978-3-319-08016-1},
  langid = {english},
  keywords = {bdpg,vertex cover},
  file = {/Users/bill/D/Zotero/storage/JMP4W4K6/fang ... xu 2014 - Combining Edge Weight and Vertex Weight for Minimum Vertex Cover Problem - BDPG - VERTEX COVER - XU STUDENT.pdf}
}

@article{fariaCellularFrustrationAlgorithms2019,
  title = {Cellular Frustration Algorithms for Anomaly Detection Applications},
  author = {Faria, Bruno and {Vistulo de Abreu}, Fernao},
  editor = {Zhang, Le},
  year = {2019},
  month = jul,
  journal = {PLOS ONE},
  volume = {14},
  number = {7},
  pages = {e0218930},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0218930},
  abstract = {Cellular frustrated models have been developed to describe how the adaptive immune system works. They are composed by independent agents that continuously pair and unpair depending on the information that one sub-set of these agents display. The emergent dynamics is sensitive to changes in the displayed information and can be used to detect anomalies, which can be important to accomplish the immune system main function of protecting the host. Therefore, it has been hypothesized that these models could be adequate to model the immune system activation. Likewise it has been hypothesized that these models could provide inspiration to develop new artificial intelligence algorithms for data mining applications. However, computational algorithms do not need to follow strictly the immunological reality. Here, we investigate efficient implementation strategies of these immune inspired ideas for anomaly detection applications and use real data to compare the performance of cellular frustration algorithms with standard implementations of one-class support vector machines and deep autoencoders. Our results demonstrate that more efficient implementations of cellular frustration algorithms are possible and also that cellular frustration algorithms can be advantageous for semi-supervised anomaly detection applications given their robustness and accuracy.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FXI9MEFV/pone.0218930.pdf}
}

@article{feigeRigorousAnalysisHeuristics,
  title = {Rigorous {{Analysis}} of {{Heuristics}} for {{NP-hard Problems}}},
  author = {Feige, Uriel},
  pages = {10},
  abstract = {The known NP-hardness results imply that for many combinatorial optimization problems there are no efficient algorithms that find an optimal solution, or even a near optimal solution, on every instance. A heuristic for an NP-hard problem is a polynomial time algorithm that produces optimal or near optimal solutions on some input instances, but may fail on others. The study of heuristics involves both an algorithmic issue (the design of the heuristic algorithm) and a conceptual challenge, namely, how does one evaluate the quality of a heuristic. Current methods for evaluating heuristics include experimental evidence, hand waving arguments, and rigorous analysis of the performance of the heuristic on some wide (in a sense that depends on the context) classes of inputs. This talk is concerned with the latter method. On the conceptual side, several frameworks that have been used in order to model the classes of inputs of interest (including random models, semi-random models, smoothed analysis) will be discussed. On the algorithmic side, several algorithmic techniques and principles of analysis that are often useful in these frameworks will be presented.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UXPJZC9D/japan.pdf}
}

@article{feldman2015acm,
  title = {Subsampled {{Power Iteration}}: A {{Unified Algorithm}} for {{Block Models}} and {{Planted CSP}}'s},
  shorttitle = {Subsampled {{Power Iteration}}},
  author = {Feldman, Vitaly and Perkins, Will and Vempala, Santosh},
  year = {2015},
  month = apr,
  journal = {arXiv:1407.2774 [cs, math]},
  eprint = {1407.2774},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We present a new algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems, via a common generalization in terms of random bipartite graphs. Our algorithm achieves the best-known bounds for the number of edges needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches. The main new features of the algorithm are two-fold: (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) it can be implemented statistically, i.e., with very limited access to the input distribution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,Computer Science - Data Structures and Algorithms,CSP,Mathematics - Combinatorics,Mathematics - Probability,planted solution},
  file = {/Users/bill/D/Zotero/storage/I65DT4YA/1407.2774v1.pdf}
}

@article{feldman2018acm,
  title = {On the {{Complexity}} of {{Random Satisfiability Problems}} with {{Planted Solutions}}},
  author = {Feldman, Vitaly and Perkins, Will and Vempala, Santosh},
  year = {2018},
  month = mar,
  journal = {arXiv:1311.4821 [cs, math]},
  eprint = {1311.4821},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {For a planted satisfiability problem on n variables with k variables per constraint, the planted assignment becomes the unique solution after O(n log n) random clauses. However, the bestknown algorithms need at least nr/2 to efficiently identify any assignment weakly correlated with the planted one for clause distributions that are (r - 1)-wise independent (r can be as high as k). Our main result is an unconditional lower bound, tight up to logarithmic factors, of {$\Omega$}\texttildelow{} (nr/2) clauses for statistical algorithms, a broad class of algorithms introduced in [50, 34]. We complement this with a nearly matching upper bound using a statistical algorithm. As known approaches for problems over distributions in general, and planted satisfiability problems in particular, all have statistical analogues (spectral, MCMC, gradient-based, convex optimization etc.), this provides a rigorous explanation of the large gap between the identifiability and algorithmic identifiability thresholds for random satisfiability problems with planted solutions. Our results imply that a strong form of Feige's refutation hypothesis for average-case SAT instances [31] holds for statistical algorithms. We also consider the closely related problem of finding the planted assignment of a random planted k-CSP which is the basis of Goldreich's proposed one-way function [41]. Our bounds extend to this problem and give concrete evidence for the security of the one-way function and the associated pseudorandom generator when used with a sufficiently hard predicate.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics,Mathematics - Probability,planted solution},
  file = {/Users/bill/D/Zotero/storage/SPPXESCP/1311.4821v3.pdf}
}

@article{fellows2002enitcs,
  title = {Parameterized {{Complexity}}},
  author = {Fellows, Michael R.},
  year = {2002},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {61},
  pages = {1--19},
  issn = {15710661},
  doi = {10.1016/S1571-0661(04)00301-9},
  langid = {english},
  keywords = {bdpg,learning to predict performance,parameterized complexity},
  file = {/Users/bill/D/Zotero/storage/J94QBJH7/fellows 2002 -parameterized Complexity - The Main Ideas and Connections to Practical Computing - BDPG.pdf}
}

@article{ferraroCounterfactualThinkingImpact2009,
  title = {Counterfactual Thinking and Impact Evaluation in Environmental Policy},
  author = {Ferraro, Paul J.},
  year = {2009},
  month = mar,
  journal = {New Directions for Evaluation},
  volume = {2009},
  number = {122},
  pages = {75--84},
  issn = {10976736, 1534875X},
  doi = {10.1002/ev.297},
  abstract = {Impact evaluations assess the degree to which changes in outcomes can be attributed to an intervention rather than to other factors. Such attribution requires knowing what outcomes would have looked like in the absence of the intervention. This counterfactual world can be inferred only indirectly through evaluation designs that control for confounding factors. Some have argued that environmental policy is different from other social policy fields, and thus attempting to establish causality through identification of counterfactual outcomes is quixotic. This chapter argues that elucidating causal relationships through counterfactual thinking and experimental or quasi-experimental designs is absolutely critical in environmental policy, and that many opportunities for doing so exist. Without more widespread application of such approaches, little progress will be made on building the evidence base in environmental policy. \textcopyright{} Wiley Periodicals, Inc.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6BIG2AJC/10 - ferraro 2009 - Counterfactual Thinking and Impact Evaluation in Environmental Policy.pdf}
}

@article{ferraroMoneyNothingCall2006,
  title = {Money for {{Nothing}}? {{A Call}} for {{Empirical Evaluation}} of {{Biodiversity Conservation Investments}}},
  shorttitle = {Money for {{Nothing}}?},
  author = {Ferraro, Paul J and Pattanayak, Subhrendu K},
  editor = {Mace, Georgina},
  year = {2006},
  month = apr,
  journal = {PLoS Biology},
  volume = {4},
  number = {4},
  pages = {e105},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0040105},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UCTXA5RB/2 - ferraro et al 2006 - money for nothing - a call for empirical evaluation of biodiversity conservation investments.PDF}
}

@article{ferrierUsingGeneralizedDissimilarity2007,
  title = {Using Generalized Dissimilarity Modelling to Analyse and Predict Patterns of Beta Diversity in Regional Biodiversity Assessment},
  author = {Ferrier, Simon and Manion, Glenn and Elith, Jane and Richardson, Karen},
  year = {2007},
  journal = {Diversity and distributions},
  volume = {13},
  number = {3},
  pages = {252--264},
  file = {/Users/bill/D/Zotero/storage/VF2INNX9/Ferrier et al_2007_Using generalized dissimilarity modelling to analyse and predict patterns of.pdf}
}

@article{fieldMinimizingCostEnvironmental2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonzen, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  month = aug,
  journal = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {669--675},
  issn = {1461-023X, 1461-0248},
  doi = {10.1111/j.1461-0248.2004.00625.x},
  abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate (a) of 0.05. We derive optimal a-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on a {$\frac{1}{4}$} 0.05 carries an expected cost over \$5 million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the species\~O value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional a-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical \^Oburden of proof\~O in environmental decision-making when the cost of Type II errors is relatively high.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GMPE7CK5/Field et al. - 2004 - Minimizing the cost of environmental management de.pdf}
}

@article{fieldMinimizingCostEnvironmental2004a,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonzen, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  month = aug,
  journal = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {669--675},
  issn = {1461-023X, 1461-0248},
  doi = {10.1111/j.1461-0248.2004.00625.x},
  abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate (a) of 0.05. We derive optimal a-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on a {$\frac{1}{4}$} 0.05 carries an expected cost over \$5 million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the species\~O value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional a-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical \^Oburden of proof\~O in environmental decision-making when the cost of Type II errors is relatively high.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XDKYNIAX/Minimizing_the_c.pdf}
}

@article{fischerClusteringCompactnessReserve,
  title = {Clustering and {{Compactness}} in {{Reserve Site Selection}}: {{An Extension}} of the {{Biodiversity Management Area Selection Model}}},
  author = {Fischer, Douglas T and Church, Richard L},
  pages = {12},
  abstract = {Over the last 15 yr, a number of formal mathematical models and heuristics have been developed for the purpose of selecting sites for biodiversity conservation. One of these models, the Biodiversity Management Area Selection (BMAS) model (Church et al. 1996a), places a major emphasis on protecting at least a certain area for each biodiversity element. Viewed spatially, solutions from this model tend to be a combination of isolated planning units and, sometimes, small clusters. One method to identify solutions with potentially less fragmentation is to add an objective to minimize the outside perimeter of selected areas. Outside perimeter only counts those edges of a planning unit that are not shared in common with another selected planning unit in a cluster, and, therefore, compact clustering is encouraged. This article presents a new math programming model that incorporates this perimeter objective into the BMAS model. We present an application using data from the USDA Forest Service-funded Sierra Nevada Ecosystem Project (Davis et al. 1996) and show that the model can be solved optimally by off-the-shelf software. Our tests indicate that the model can produce dramatic reductions in perimeter of the reserve system (increasing clustering and compactness) at the expense of relatively small decreases in performance against area and suitability measures. FOR. SCI. 49(4): 555\textendash 565.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZIMGPBIG/fischer church 2003 - Clustering and Compactness in ReserveSite Selection - An Extension of theBiodiversity Management AreaSelection Model - BDPG - RESERVE SELECTION.pdf}
}

@article{fischerEvaluationMethodologyVirtual,
  title = {An {{Evaluation Methodology}} for {{Virtual Network Embedding}}},
  author = {Fischer, Andreas and Passau, Universit{\"a}t},
  pages = {199},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FVC26LTP/Fischer_Andreas-An_Evaluation_Methodology_for_Virtual_Network_Embedding.pdf}
}

@article{fischerPerfectlyNestedSignificantly2005,
  title = {\'A {{Perfectly}} Nested or Significantly Nested / an Important Difference for Conservation Management},
  author = {Fischer, Joern and Lindenmayer, David B},
  year = {2005},
  pages = {10},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NFJSKSNX/fischer and lindenmayer 2005 - perfectly nested or significantly nested - an important difference for conservation management - oikos.pdf}
}

@article{fleishmanEMPIRICALVALIDATIONMETHOD2001,
  title = {{{EMPIRICAL VALIDATION OF A METHOD FOR UMBRELLA SPECIES SELECTION}}},
  author = {Fleishman, Erica and Blair, Robert B and Murphy, Dennis D},
  year = {2001},
  journal = {Ecological Applications},
  volume = {11},
  number = {5},
  pages = {13},
  abstract = {Empirical validation that putative umbrella species protect many co-occurring species is rare. Using 10 sets of data, representing two taxonomic groups and three ecoregions, we tested the effectiveness of a recently developed index for selection of umbrella species. We also tested whether species identified with the index were more effective umbrellas than species selected at random, evaluated whether sample size and intensity affect selection of umbrella species, and examined whether the index could identify cross-taxonomic umbrellas in a single ecoregion. Conserving all locations with at least one umbrella species would protect the vast majority of each assemblage. A more realistic scenario, conservation of subsets of locations with relatively high numbers of umbrella species, generally would protect Õ0.75 of each assemblage. Randomly selected sets of species often required that more locations be designated for protection than did sets selected using the umbrella index. The umbrella index tended to identify fewer locations that offered an equivalent level of species protection. Sampling intensity affected which species were identified as umbrellas, but not the proportion of species that would be protected. Umbrella species were no more effective than randomly selected species for cross-taxonomic applications; nonetheless, neither group was significantly less effective than same-taxon umbrellas. Particularly when selection of protected areas is not highly constrained, it may indeed be feasible to identify effective umbrella species. Unqualified utility of the umbrella index or umbrella species concept, however, is not supported by this study.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V4SATZ8X/7 - fleishman et al 2001 - EMPIRICAL VALIDATION OF A METHOD FOR UMBRELLA SPECIES SELECTION.pdf}
}

@article{flouvat2010jiis,
  title = {A New Classification of Datasets for Frequent Itemsets},
  author = {Flouvat, Fr{\'e}d{\'e}ric and De Marchi, Fabien and Petit, Jean-Marc},
  year = {2010},
  month = feb,
  journal = {Journal of Intelligent Information Systems},
  volume = {34},
  number = {1},
  pages = {1--19},
  issn = {0925-9902, 1573-7675},
  doi = {10.1007/s10844-008-0077-0},
  abstract = {The discovery of frequent patterns is a famous problem in data mining. While plenty of algorithms have been proposed during the last decade, only a few contributions have tried to understand the influence of datasets on the algorithms behavior. Being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question. In this setting, we describe a thorough experimental study of datasets with respect to frequent itemsets. We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations: frequent closed, frequent free and frequent essential itemsets. For each of them, we also study the distribution of their positive and negative borders whenever possible. The main outcome of these experiments is a new classification of datasets invariant w.r.t. minsup variations and robust to explain efficiency of several implementations.},
  langid = {english},
  keywords = {bdpg,classification of datasets},
  file = {/Users/bill/D/Zotero/storage/NJFX855L/flouvat et al 2010 - A new classification of datasets for frequent itemsets - BDPG.pdf}
}

@article{fodorLinkingBiodiversityMutualistic2013,
  title = {Linking Biodiversity to Mutualistic Networks \textendash{} Woody Species and Ectomycorrhizal Fungi},
  author = {Fodor, E},
  year = {2013},
  pages = {26},
  abstract = {Mutualistic interactions are currently mapped by bipartite networks with particular architecture and properties. The mycorrhizae connect the trees and permit them to share resources, therefore relaxing the competition. Ectomycorrhizal macrofungi associated with woody species (Quercus robur, Q. cerris, Q. petraea, Tilia tomentosa, Carpinus betulus, Corylus avellana, and Q. pubescens) growing in a temperate, broadleaved mixed forest, from a hilly area near the city of Cluj\textendash Napoca, central Romania were included in a bipartite mutualistic network. Community structure was investigated using several network metrics, modularity and nestedness algorithms in conjunction with C-score index cluster analysis and nonmetric multidimensional scaling (the Kulczynski similarity was index used as most appropriate metric selected by minimal stress criterion). The results indicate that the network presents high asymmetry (hosts are outnumbered by mycobionts at a great extent), high connectance, low modularity, and high nestedness, competition playing a secondary role in community assemblage (non significant difference between simulated and observed Cscore). The nestedness pattern is non-random and is comparable to previously published results for other similar interactions containing plants. In the proposed network, woody species function exclusively as generalists. Modularity analysis is a finer tool were identifying species roles than centrality measures, however, the two types of algorithms permit the separation of species according to their roles as for example connectors (generalist species) and ultraperipheral species (specialists). Supergeneralist woody species function as hubs for the diverse ectomycorrhizal community while supergeneralist ectomycorrhizal fungi glue the hubs into a coherent aggregate.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N6ENFQN6/44-140-1-SM.pdf}
}

@article{folkeResilienceRepublished2016,
  title = {Resilience ({{Republished}})},
  author = {Folke, Carl},
  year = {2016},
  journal = {Ecology and Society},
  volume = {21},
  number = {4},
  pages = {art44},
  issn = {1708-3087},
  doi = {10.5751/ES-09088-210444},
  abstract = {Resilience thinking in relation to the environment has emerged as a lens of inquiry that serves a platform for interdisciplinary dialogue and collaboration. Resilience is about cultivating the capacity to sustain development in the face of expected and surprising change and diverse pathways of development and potential thresholds between them. The evolution of resilience thinking is coupled to socialecological systems and a truly intertwined human-environment planet. Resilience as persistence, adaptability, and transformability of complex adaptive social-ecological systems is the focus, clarifying the dynamic and forward-looking nature of the concept. Resilience thinking emphasizes that social-ecological systems, from the individual, to community, to society as a whole, are embedded in the biosphere. The biosphere connection is an essential observation if sustainability is to be taken seriously. In the continuous advancement of resilience thinking there are efforts aimed at capturing resilience of social-ecological systems and finding ways for people and institutions to govern social-ecological dynamics for improved human well-being, at the local, across levels and scales, to the global. Consequently, in resilience thinking, development issues for human well-being, for people and planet, are framed in a context of understanding and governing complex social-ecological dynamics for sustainability as part of a dynamic biosphere.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZWHQAZ4Z/folke 2016 - resilience - OPTISEVIL.pdf}
}

@article{fortnowStatusNPProblem2009,
  title = {The Status of the {{P}} versus {{NP}} Problem},
  author = {Fortnow, Lance},
  year = {2009},
  month = sep,
  journal = {Communications of the ACM},
  volume = {52},
  number = {9},
  pages = {78--86},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1562164.1562186},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/G5I9RPDU/p78-fortnow.pdf}
}

@article{fourcadePaintingsPredictDistribution2018,
  title = {Paintings Predict the Distribution of Species, or the Challenge of Selecting Environmental Predictors and Evaluation Statistics},
  author = {Fourcade, Yoan and Besnard, Aur{\'e}lien G. and Secondi, Jean},
  year = {2018},
  month = feb,
  journal = {Global Ecology and Biogeography},
  volume = {27},
  number = {2},
  pages = {245--256},
  issn = {1466822X},
  doi = {10.1111/geb.12684},
  abstract = {Aim: Species distribution modelling, a family of statistical methods that predicts species distributions from a set of occurrences and environmental predictors, is now routinely applied in many macroecological studies. However, the reliability of evaluation metrics usually employed to validate these models remains questioned. Moreover, the emergence of online databases of environmental variables with global coverage, especially climatic, has favoured the use of the same set of standard predictors. Unfortunately, the selection of variables is too rarely based on a careful examination of the species' ecology. In this context, our aim was to highlight the importance of selecting ad hoc variables in species distribution models, and to assess the ability of classical evaluation statistics to identify models with no biological realism.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Z3KYAX9K/fourcade et al 2017 - Paintings predict the distribution of species, or the challenge of selecting environmental predictors and evaluation statistics - MATT WHITE - SDM.pdf}
}

@article{fourcadePaintingsPredictDistribution2018a,
  title = {Paintings Predict the Distribution of Species, or the Challenge of Selecting Environmental Predictors and Evaluation Statistics},
  author = {Fourcade, Yoan and Besnard, Aur{\'e}lien G. and Secondi, Jean},
  year = {2018},
  month = feb,
  journal = {Global Ecology and Biogeography},
  volume = {27},
  number = {2},
  pages = {245--256},
  issn = {1466822X},
  doi = {10.1111/geb.12684},
  abstract = {Aim: Species distribution modelling, a family of statistical methods that predicts species distributions from a set of occurrences and environmental predictors, is now routinely applied in many macroecological studies. However, the reliability of evaluation metrics usually employed to validate these models remains questioned. Moreover, the emergence of online databases of environmental variables with global coverage, especially climatic, has favoured the use of the same set of standard predictors. Unfortunately, the selection of variables is too rarely based on a careful examination of the species' ecology. In this context, our aim was to highlight the importance of selecting ad hoc variables in species distribution models, and to assess the ability of classical evaluation statistics to identify models with no biological realism.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GITCCB25/fourcade et al 2017 - Paintings predict the distribution of species, or the challenge ofselecting environmental predictors and evaluation statistics - SDM - GUPPY - BDPG - UNCERTAINTY.pdf}
}

@article{franca2019b,
  title = {Writing {{Papers}} to {{Be Memorable}}, {{Even When They Are Not Really Read}}},
  author = {Fran{\c c}a, Thiago F. A. and Monserrat, Jos{\'e} M.},
  year = {2019},
  month = may,
  journal = {BioEssays},
  volume = {41},
  number = {5},
  pages = {1900035},
  issn = {0265-9247, 1521-1878},
  doi = {10.1002/bies.201900035},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7DRUEPNJ/franca montserrat 2019 - Writing Papers to Be Memorable, Even When They Are Not Really Read - SCIENTIFIC WRITING - BDPG.pdf}
}

@article{franco1983dam,
  title = {Probabilistic Analysis of the {{Davis Putnam}} Procedure for Solving the Satisfiability Problem},
  author = {Franco, John and Paull, Marvin},
  year = {1983},
  month = jan,
  journal = {Discrete Applied Mathematics},
  volume = {5},
  number = {1},
  pages = {77--87},
  issn = {0166218X},
  doi = {10.1016/0166-218X(83)90017-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KIM2V9PG/franco paull 1983 - probabilistic analysis of the davis putnam procedure ofr solving the satisfiability problem - BDPG.pdf}
}

@techreport{franco1985,
  type = {Technical Report},
  title = {On the Probabilistic Performance of Algorithms for the Satisfiability Problem},
  author = {Franco, John},
  year = {1985},
  number = {167},
  address = {{Bloomington, IN}},
  institution = {{Department of Computer Science, Indiana University}},
  keywords = {bdpg},
  file = {/Users/bill/D/Zotero/storage/CNP56NM9/franco 1985 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf}
}

@article{franco1985sn,
  title = {Sensitivity of Probabilistic Results on Algorithms for {{NP-complete}} Problems to Input Distributions},
  author = {Franco, J.},
  year = {1985},
  month = jun,
  journal = {ACM SIGACT News},
  volume = {17},
  number = {1},
  pages = {40--59},
  issn = {0163-5700},
  doi = {10.1145/382250.382807},
  langid = {english},
  keywords = {bdpg},
  file = {/Users/bill/D/Zotero/storage/GDHDY2D8/franco 1985 - sensitivity of probabilistic results on algorithms for NP-complete problems to input distributions - BDPG - ANNO.pdf}
}

@article{franco1986ipl,
  title = {On the Probabilistic Performance of Algorithms for the Satisfiability Problem},
  author = {Franco, John},
  year = {1986},
  month = aug,
  journal = {Information Processing Letters},
  volume = {23},
  number = {2},
  pages = {103--106},
  issn = {00200190},
  doi = {10.1016/0020-0190(86)90051-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LSISED47/franco 1986 - on the probabilistic performance of algorithms for the satisfiability problem - BDPG.pdf}
}

@article{franklinEffectSpeciesRarity2009,
  title = {Effect of Species Rarity on the Accuracy of Species Distribution Models for Reptiles and Amphibians in Southern {{California}}},
  author = {Franklin, Janet and Wejnert, Katherine E. and Hathaway, Stacie A. and Rochester, Carlton J. and Fisher, Robert N.},
  year = {2009},
  month = jan,
  journal = {Diversity and Distributions},
  volume = {15},
  number = {1},
  pages = {167--177},
  issn = {13669516, 14724642},
  doi = {10.1111/j.1472-4642.2008.00536.x},
  abstract = {Aim Several studies have found that more accurate predictive models of species' occurrences can be developed for rarer species; however, one recent study found the relationship between range size and model performance to be an artefact of sample prevalence, that is, the proportion of presence versus absence observations in the data used to train the model. We examined the effect of model type, species rarity class, species' survey frequency, detectability and manipulated sample prevalence on the accuracy of distribution models developed for 30 reptile and amphibian species.},
  langid = {english},
  keywords = {problem attributes},
  file = {/Users/bill/D/Zotero/storage/DKQ4IVI7/franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf}
}

@article{franklinEffectSpeciesRarity2009a,
  title = {Effect of Species Rarity on the Accuracy of Species Distribution Models for Reptiles and Amphibians in Southern {{California}}},
  author = {Franklin, Janet and Wejnert, Katherine E. and Hathaway, Stacie A. and Rochester, Carlton J. and Fisher, Robert N.},
  year = {2009},
  month = jan,
  journal = {Diversity and Distributions},
  volume = {15},
  number = {1},
  pages = {167--177},
  issn = {13669516, 14724642},
  doi = {10.1111/j.1472-4642.2008.00536.x},
  abstract = {Aim Several studies have found that more accurate predictive models of species' occurrences can be developed for rarer species; however, one recent study found the relationship between range size and model performance to be an artefact of sample prevalence, that is, the proportion of presence versus absence observations in the data used to train the model. We examined the effect of model type, species rarity class, species' survey frequency, detectability and manipulated sample prevalence on the accuracy of distribution models developed for 30 reptile and amphibian species.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/U8B8GE5G/franklin et al 2009 - Effect of species rarity on the accuracy of species distribution models for reptiles and amphibians in southern California - divanddist.pdf}
}

@article{franzeseTestingSpatialAutoregressiveLag,
  title = {Testing for {{Spatial-Autoregressive Lag}} versus ({{Unobserved}}) {{Spatially Correlated Error-Components}}},
  author = {Franzese, Robert J and Hays, Jude C},
  pages = {26},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W5ZQTDRZ/Franzese_Hays_IOWA14.pdf}
}

@article{frechetteUsingShapleyValue,
  title = {Using the {{Shapley Value}} to {{Analyze Algorithm Portfolios}}},
  author = {Frechette, Alexandre and Kotthoff, Lars and Michalak, Tomasz and Rahwan, Talal and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {9},
  abstract = {Algorithms for NP-complete problems often have different strengths and weaknesses, and thus algorithm portfolios often outperform individual algorithms. It is surprisingly difficult to quantify a component algorithm's contribution to such a portfolio. Reporting a component's standalone performance wrongly rewards near-clones while penalizing algorithms that have small but distinct areas of strength. Measuring a component's marginal contribution to an existing portfolio is better, but penalizes sets of strongly correlated algorithms, thereby obscuring situations in which it is essential to have at least one algorithm from such a set. This paper argues for analyzing component algorithm contributions via a measure drawn from coalitional game theory\textemdash the Shapley value\textemdash and yields insight into a research community's progress over time. We conclude with an application of the analysis we advocate to SAT competitions, yielding novel insights into the behaviour of algorithm portfolios, their components, and the state of SAT solving technology.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/C3IRPU28/2016-AAAI-portfolio-shapley-extended.pdf}
}

@article{frechetteUsingShapleyValuea,
  title = {Using the {{Shapley Value}} to {{Analyze Algorithm Portfolios}}},
  author = {Frechette, Alexandre and Kotthoff, Lars and Michalak, Tomasz and Rahwan, Talal and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {7},
  abstract = {Algorithms for NP-complete problems often have different strengths and weaknesses, and thus algorithm portfolios often outperform individual algorithms. It is surprisingly difficult to quantify a component algorithm's contribution to such a portfolio. Reporting a component's standalone performance wrongly rewards near-clones while penalizing algorithms that have small but distinct areas of strength. Measuring a component's marginal contribution to an existing portfolio is better, but penalizes sets of strongly correlated algorithms, thereby obscuring situations in which it is essential to have at least one algorithm from such a set. This paper argues for analyzing component algorithm contributions via a measure drawn from coalitional game theory\textemdash the Shapley value\textemdash and yields insight into a research community's progress over time. We conclude with an application of the analysis we advocate to SAT competitions, yielding novel insights into the behaviour of algorithm portfolios, their components, and the state of SAT solving technology.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N4DMNUBP/2016-AAAI-portfolio-shapley.pdf}
}

@article{freeling2019pnasu,
  title = {Opinion: {{How}} Can We Boost the Impact of Publications? {{Try}} Better Writing},
  shorttitle = {Opinion},
  author = {Freeling, Benjamin and Doubleday, Zo{\"e} A. and Connell, Sean D.},
  year = {2019},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {2},
  pages = {341--343},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1819937116},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SD5VTHWN/freeling doubleday connell 2019 - How can we boost the impact of publications - Try better writing - SCIENTIFIC WRITING - BDPG.pdf}
}

@article{frundSamplingBiasChallenge2016,
  title = {Sampling Bias Is a Challenge for Quantifying Specialization and Network Structure: Lessons from a Quantitative Niche Model},
  shorttitle = {Sampling Bias Is a Challenge for Quantifying Specialization and Network Structure},
  author = {Fr{\"u}nd, Jochen and McCann, Kevin S. and Williams, Neal M.},
  year = {2016},
  month = apr,
  journal = {Oikos},
  volume = {125},
  number = {4},
  pages = {502--513},
  issn = {00301299},
  doi = {10.1111/oik.02256},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8EYNL3G5/OIK_Frund et al proofs.pdf}
}

@article{fujitoApproximationAlgorithmsSubmodular2000,
  title = {Approximation {{Algorithms}} for {{Submodular Set Cover}} with {{Applications}}},
  author = {Fujito, Toshihiro},
  year = {2000},
  pages = {8},
  abstract = {The main problem considered is submodular set cover, the problem of minimizing a linear function under a nondecreasing submodular constraint, which generalizes both wellknown set cover and minimum matroid base problems. The problem is NP-hard, and two natural greedy heuristics are introduced along with analysis of their performance. As applications of these heuristics we consider various special cases of submodular set cover, including partial cover variants of set cover and vertex cover, and node-deletion problems for hereditary and matroidal properties. An approximation bound derived for each of them is either matching or generalizing the best existing bounds.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/A6VRFEIG/fujito.pdf}
}

@techreport{game2008,
  title = {Marxan {{User Manual}}: {{For Marxan}} Version 1.8.10},
  author = {Game, E.T. and Grantham, H.S.},
  year = {2008},
  institution = {{University of Queensland, St. Lucia, Queensland, Australia, and Pacific Marine Analysis and Research Association, Vancouver, British Columbia, Canada}}
}

@article{gamePLANNINGPERSISTENCEMARINE2008,
  title = {{{PLANNING FOR PERSISTENCE IN MARINE RESERVES}}: {{A QUESTION OF CATASTROPHIC IMPORTANCE}}},
  shorttitle = {{{PLANNING FOR PERSISTENCE IN MARINE RESERVES}}},
  author = {Game, Edward T. and Watts, Matthew E. and Wooldridge, Scott and Possingham, Hugh P.},
  year = {2008},
  month = apr,
  journal = {Ecological Applications},
  volume = {18},
  number = {3},
  pages = {670--680},
  issn = {1051-0761},
  doi = {10.1890/07-1027.1},
  abstract = {Large-scale catastrophic events, although rare, lie generally beyond the control of local management and can prevent marine reserves from achieving biodiversity outcomes. We formulate a new conservation planning problem that aims to minimize the probability of missing conservation targets as a result of catastrophic events. To illustrate this approach we formulate and solve the problem of minimizing the impact of large-scale coral bleaching events on a reserve system for the Great Barrier Reef, Australia. We show that by considering the threat of catastrophic events as part of the reserve design problem it is possible to substantially improve the likely persistence of conservation features within reserve networks for a negligible increase in cost. In the case of the Great Barrier Reef, a 2\% increase in overall reserve cost was enough to improve the long-run performance of our reserve network by .60\%. Our results also demonstrate that simply aiming to protect the reefs at lowest risk of catastrophic bleaching does not necessarily lead to the best conservation outcomes, and enormous gains in overall persistence can be made by removing the requirement to represent all bioregions in the reserve network. We provide an explicit and well-defined method that allows the probability of catastrophic disturbances to be included in the site selection problem without creating additional conservation targets or imposing arbitrary presence/absence thresholds on existing data. This research has implications for reserve design in a changing climate.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/599JNVJ8/game et al 2008 - planning for persistence in marine reserves - a question of catastrophic importance - RESERVE SELECTION.pdf}
}

@article{gameSixCommonMistakes2013,
  title = {Six {{Common Mistakes}} in {{Conservation Priority Setting}}},
  author = {Game, Edward T. and Kareiva, Peter and Possingham, Hugh P.},
  year = {2013},
  month = jun,
  journal = {Conservation Biology},
  volume = {27},
  number = {3},
  pages = {480--485},
  issn = {0888-8892, 1523-1739},
  doi = {10.1111/cobi.12051},
  abstract = {A vast number of prioritization schemes have been developed to help conservation navigate tough decisions about the allocation of finite resources. However, the application of quantitative approaches to setting priorities in conservation frequently includes mistakes that can undermine their authors' intention to be more rigorous and scientific in the way priorities are established and resources allocated. Drawing on well-established principles of decision science, we highlight 6 mistakes commonly associated with setting priorities for conservation: not acknowledging conservation plans are prioritizations; trying to solve an illdefined problem; not prioritizing actions; arbitrariness; hidden value judgments; and not acknowledging risk of failure. We explain these mistakes and offer a path to help conservation planners avoid making the same mistakes in future prioritizations.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5Z55E27B/game kareiva possingham 2013 - Six Common Mistakes in Conservation Priority Setting.pdf}
}

@inproceedings{ganjisaffarBaggingGradientboostedTrees2011,
  title = {Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models},
  booktitle = {Proceedings of the 34th International {{ACM SIGIR}} Conference on {{Research}} and Development in {{Information}} - {{SIGIR}} '11},
  author = {Ganjisaffar, Yasser and Caruana, Rich and Lopes, Cristina Videira},
  year = {2011},
  pages = {85},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/2009916.2009932},
  abstract = {Recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks. In Learning-to-rank, boosted models such as RankBoost and LambdaMART have been shown to be among the best performing learning methods based on evaluations on public data sets. In this paper, we show how the combination of bagging as a variance reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking models. We perform thousands of parameter tuning experiments for LambdaMART to achieve a high precision boosting model. Then we show that a bagged ensemble of such LambdaMART boosted models results in higher accuracy ranking models while also reducing variance as much as 50\%. We report our results on three public learning-to-rank data sets using four metrics. Bagged LamdbaMART outperforms all previously reported results on ten of the twelve comparisons, and bagged LambdaMART outperforms non-bagged LambdaMART on all twelve comparisons. For example, wrapping bagging around LambdaMART increases NDCG@1 from 0.4137 to 0.4200 on the MQ2007 data set; the best prior results in the literature for this data set is 0.4134 by RankBoost.},
  isbn = {978-1-4503-0757-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5AJIZJB6/bagging_lmbamart_jforests.pdf}
}

@inproceedings{ganjisaffarDistributedTuningMachine2011,
  title = {Distributed Tuning of Machine Learning Algorithms Using {{MapReduce Clusters}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Large Scale Data Mining Theory}} and {{Applications}} - {{LDMTA}} '11},
  author = {Ganjisaffar, Yasser and Debeauvais, Thomas and Javanmardi, Sara and Caruana, Rich and Lopes, Cristina Videira},
  year = {2011},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{San Diego, California}},
  doi = {10.1145/2002945.2002947},
  abstract = {Obtaining the best accuracy in machine learning usually requires carefully tuning learning algorithm parameters for each problem. Parameter optimization is computationally challenging for learning methods with many hyperparameters. In this paper we show that MapReduce Clusters are particularly well suited for parallel parameter optimization. We use MapReduce to optimize regularization parameters for boosted trees and random forests on several text problems: three retrieval ranking problems and a Wikipedia vandalism problem. We show how model accuracy improves as a function of the percent of parameter space explored, that accuracy can be hurt by exploring parameter space too aggressively, and that there can be significant interaction between parameters that appear to be independent. Our results suggest that MapReduce is a two-edged sword: it makes parameter optimization feasible on a massive scale that would have been unimaginable just a few years ago, but also creates a new opportunity for overfitting that can reduce accuracy and lead to inferior learning parameters.},
  isbn = {978-1-4503-0844-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TDGX2QF3/2011-overtuning.pdf}
}

@article{gao2015scis,
  title = {Experimental Analyses on Phase Transitions in Compiling Satisfiability Problems},
  author = {Gao, Jian and Wang, JiaNan and Yin, MingHao},
  year = {2015},
  month = mar,
  journal = {Science China Information Sciences},
  volume = {58},
  number = {3},
  pages = {1--11},
  issn = {1674-733X, 1869-1919},
  doi = {10.1007/s11432-014-5154-0},
  abstract = {In the past decade, a kind of well-known phenomena in many complex combinatorial problems such as satisfiability problem, called phase transition, have been widely studied. In this paper, the phase transition phenomena are investigated during compiling k-satisfiability problems into tractable languages with empirical methods. Ordered binary decision diagram and deterministic-decomposable negation normal form are selected as the tractable target languages for the compilation. Via intensive experiments, it can be concluded that an easy\textendash hard\textendash easy pattern exists during the compilations, which is only related to the ratio of the number of clauses to that of variables if we set k to a fixed value, rather than to the target languages. Moreover, it can be concluded that the space exhausted during the compilations grows exponentially with the number of variables growing, whereas there is also a phase transition separating the polynomial-increment region from the exponential-increment region. Additionally, it can be observed that there is a phase transition of prime implicants around peak points of the easy\textendash hard\textendash easy pattern and the ratios of random instances whose average lengths of prime implicants are larger than the threshold 0.5 change sharply. From these analyses, it can be concluded that prime implicant length and solution interchangeability are crucial impacts on sizes of compilation results.},
  langid = {english},
  keywords = {bdpg,csp,phase transition,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/49KY6Y7J/Gao et al. - 2015 - Experimental analyses on phase transitions in comp.pdf}
}

@article{gaoIncentivizingEvaluationLimited2016,
  title = {Incentivizing {{Evaluation}} via {{Limited Access}} to {{Ground Truth}}: {{Peer-Prediction Makes Things Worse}}},
  shorttitle = {Incentivizing {{Evaluation}} via {{Limited Access}} to {{Ground Truth}}},
  author = {Gao, Alice and Wright, James R. and {Leyton-Brown}, Kevin},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.07042 [cs]},
  eprint = {1606.07042},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In many settings, an effective way of evaluating objects of interest is to collect evaluations from dispersed individuals and to aggregate these evaluations together. Some examples are categorizing online content and evaluating student assignments via peer grading. For this data science problem, one challenge is to motivate participants to conduct such evaluations carefully and to report them honestly, particularly when doing so is costly. Existing approaches, notably peer-prediction mechanisms, can incentivize truth telling in equilibrium. However, they also give rise to equilibria in which agents do not pay the costs required to evaluate accurately, and hence fail to elicit useful information. We show that this problem is unavoidable whenever agents are able to coordinate using low-cost signals about the items being evaluated (e.g., text labels or pictures). We then consider ways of circumventing this problem by comparing agents' reports to ground truth, which is available in practice when there exist trusted evaluators\textemdash such as teaching assistants in the peer grading scenario\textemdash who can perform a limited number of unbiased (but noisy) evaluations. Of course, when such ground truth is available, a simpler approach is also possible: rewarding each agent based on agreement with ground truth with some probability, and unconditionally rewarding the agent otherwise. Surprisingly, we show that the simpler mechanism achieves stronger incentive guarantees given less access to ground truth than a large set of peer-prediction mechanisms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/Users/bill/D/Zotero/storage/GWLRBNYH/1606.07042.pdf}
}

@article{gaoIncentivizingEvaluationLimited2016a,
  title = {Incentivizing {{Evaluation}} via {{Limited Access}} to {{Ground Truth}}: {{Peer-Prediction Makes Things Worse}}},
  shorttitle = {Incentivizing {{Evaluation}} via {{Limited Access}} to {{Ground Truth}}},
  author = {Gao, Alice and Wright, James R. and {Leyton-Brown}, Kevin},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.07042 [cs]},
  eprint = {1606.07042},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In many settings, an effective way of evaluating objects of interest is to collect evaluations from dispersed individuals and to aggregate these evaluations together. Some examples are categorizing online content and evaluating student assignments via peer grading. For this data science problem, one challenge is to motivate participants to conduct such evaluations carefully and to report them honestly, particularly when doing so is costly. Existing approaches, notably peer-prediction mechanisms, can incentivize truth telling in equilibrium. However, they also give rise to equilibria in which agents do not pay the costs required to evaluate accurately, and hence fail to elicit useful information. We show that this problem is unavoidable whenever agents are able to coordinate using low-cost signals about the items being evaluated (e.g., text labels or pictures). We then consider ways of circumventing this problem by comparing agents' reports to ground truth, which is available in practice when there exist trusted evaluators\textemdash such as teaching assistants in the peer grading scenario\textemdash who can perform a limited number of unbiased (but noisy) evaluations. Of course, when such ground truth is available, a simpler approach is also possible: rewarding each agent based on agreement with ground truth with some probability, and unconditionally rewarding the agent otherwise. Surprisingly, we show that the simpler mechanism achieves stronger incentive guarantees given less access to ground truth than a large set of peer-prediction mechanisms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/Users/bill/D/Zotero/storage/MYA36WLL/2016-ECDS-PeerPrediction.pdf}
}

@article{garciaBirdsEcologicalNetworks2016,
  title = {Birds in {{Ecological Networks}}: {{Insights}} from {{Bird-Plant Mutualistic Interactions}}},
  shorttitle = {Birds in {{Ecological Networks}}},
  author = {Garc{\'i}a, Daniel},
  year = {2016},
  month = jun,
  journal = {Ardeola},
  volume = {63},
  number = {1},
  pages = {151--180},
  issn = {0570-7358, 2341-0825},
  doi = {10.13157/arla.63.1.2016.rp7},
  abstract = {SUMMAry.\textemdash research in ecological networks has developed impressively in recent years. A significant part of this growth has been achieved using networks to represent the complexity of mutualistic interactions between species of birds and plants, such as pollination and seed dispersal. Bird-plant networks are built from matrices whose cells account for the field-sampled magnitudes of interaction (e.g. the number of plant fruits consumed by birds) in bird-plant species pairs. the comparative study of mutualistic networks evidences three general patterns in network structure: they are highly heterogeneous (many species having just a few interactions, but a few species being highly connected), nested (with specialists interacting with subsets of species with which generalists interact) and composed of weak and asymmetric relationships between birds and plants. this type of structure emerges from a set of ecological and evolutionary mechanisms accounting for the probabilistic role of species abundances and the deterministic role of species traits, often constrained by species phylogenies. Although bearing structural generalities, bird-plant networks are variable in space and time at very different scales: from habitat to latitudinal and biogeographical gradients, and from seasonal to inter-annual contrasts. they are also highly sensitive to human impact, being especially affected by habitat loss and fragmentation, defaunation and biological invasions. further research on bird-plant mutualistic networks should: 1) apply wide conceptual frameworks which integrate the mechanisms of interaction and the responses of species to environmental gradients, 2) enlarge the ecological scale of networks across interaction types and animal groups, and 3) account for the ultimate functional (i.e. demographic) effects of trophic interactions.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RQNM4V48/Garcia2016_Ardeola.pdf}
}

@book{garey1979,
  title = {Computers and {{Intractability}}: {{A Guide}} to the {{Theory}} of {{NP-Completeness}}},
  author = {Garey, Michael R. and Johnson, David S.},
  year = {1979},
  series = {Series of {{Books}} in the {{Mathematical Sciences}}},
  publisher = {{W.H. Freeman}},
  file = {/Users/bill/D/Zotero/storage/T67PFSQW/Garey_Johnson_1979_Computers and Intractability.pdf}
}

@article{gaveauEvaluatingWhetherProtected2009,
  title = {Evaluating Whether Protected Areas Reduce Tropical Deforestation in {{Sumatra}}},
  author = {Gaveau, David L. A. and Epting, Justin and Lyne, Owen and Linkie, Matthew and Kumara, Indra and Kanninen, Markku and {Leader-Williams}, Nigel},
  year = {2009},
  month = nov,
  journal = {Journal of Biogeography},
  volume = {36},
  number = {11},
  pages = {2165--2175},
  issn = {03050270, 13652699},
  doi = {10.1111/j.1365-2699.2009.02147.x},
  abstract = {Aim This study determines whether the establishment of tropical protected areas (PAs) has led to a reduction in deforestation within their boundaries or whether deforestation has been displaced to adjacent unprotected areas: a process termed neighbourhood leakage.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y3W9ZXXC/4 - gaveau et al 2009 - Evaluating whether protected areas reduce tropical deforestation in sumatra.pdf}
}

@article{gengFaceImageModeling,
  title = {Face Image Modeling by Multilinear Subspace Analysis with Missing Values},
  author = {Geng, Xin and {Smith-Miles}, Kate and Zhou, Zhi-Hua and Wang, Liang},
  pages = {4},
  abstract = {The main difficulty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-n product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difficult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M2SA, which can work on the training tensor with massive missing values. Thus M2SA can be used to model face images even when there are only a small number of face images with limited variations (which will cause missing values in the training tensor). Experiments on face recognition show that M2SA can work reasonably well with up to 70\% missing values in the training tensor.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8IDFN4PE/p629-geng.pdf}
}

@article{gengFaceImageModelinga,
  title = {Face Image Modeling by Multilinear Subspace Analysis with Missing Values},
  author = {Geng, Xin and {Smith-Miles}, Kate and Zhou, Zhi-Hua and Wang, Liang},
  pages = {4},
  abstract = {The main difficulty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-n product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difficult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M2SA, which can work on the training tensor with massive missing values. Thus M2SA can be used to model face images even when there are only a small number of face images with limited variations (which will cause missing values in the training tensor). Experiments on face recognition show that M2SA can work reasonably well with up to 70\% missing values in the training tensor.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IR275TVY/p629-geng.pdf}
}

@inproceedings{gengFacialAgeEstimation2008,
  title = {Facial Age Estimation by Nonlinear Aging Pattern Subspace},
  booktitle = {Proceeding of the 16th {{ACM}} International Conference on {{Multimedia}} - {{MM}} '08},
  author = {Geng, Xin and {Smith-Miles}, Kate and Zhou, Zhi-Hua},
  year = {2008},
  pages = {721},
  publisher = {{ACM Press}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.1145/1459359.1459469},
  abstract = {Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are defined as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identified.},
  isbn = {978-1-60558-303-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/852PU8S6/p721-geng.pdf}
}

@inproceedings{gengFacialAgeEstimation2008a,
  title = {Facial Age Estimation by Nonlinear Aging Pattern Subspace},
  booktitle = {Proceeding of the 16th {{ACM}} International Conference on {{Multimedia}} - {{MM}} '08},
  author = {Geng, Xin and {Smith-Miles}, Kate and Zhou, Zhi-Hua},
  year = {2008},
  pages = {721},
  publisher = {{ACM Press}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.1145/1459359.1459469},
  abstract = {Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are defined as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identified.},
  isbn = {978-1-60558-303-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SSWLC9PI/p721-geng.pdf}
}

@inproceedings{gent1994,
  title = {How Not to Do It},
  author = {Gent, Ian P. and Walsh, Toby},
  year = {1994},
  pages = {5},
  abstract = {We give some dos and don'ts for those analysing algorithms experimentally. We illustrate these with many examples from our own research. Where we have not followed these maxims, we have su ered as a result.},
  langid = {english},
  keywords = {benchmarking},
  file = {/Users/bill/D/Zotero/storage/JI4GITIX/gent walsh 1994 - how not to do it - BENCHMARKING - TESTING.pdf}
}

@techreport{gent1997,
  title = {How Not to Do It},
  author = {Gent, Ian P. and Grant, Stuart A. and Macintyre, Ewan and Prosser, Patrick and Shaw, Paul and Smith, Barbara M. and Walsh, Toby},
  year = {1997},
  number = {97.27},
  institution = {{University of Leeds}},
  abstract = {We give some dos and don'ts for those analysing algorithms experimentally.  We illustrate these with many examples from our own research on the study of algorithms for NP-complete problems such as satisfiability and constraint satisfaction.  Where we have not followed these maxims, we have suffered as a result.},
  keywords = {benchmarking},
  file = {/Users/bill/D/Zotero/storage/N3GVCE34/gent et al 1997 - how not to do it - REPORT - BENCHMARKING - TESTING.pdf}
}

@article{gent2001c,
  title = {Random {{Constraint Satisfaction}}: {{Flaws}} and {{Structure}}},
  author = {Gent, Ian P and Macintyre, Ewan and Prosser, Patrick and Smith, Barbara M. and Walsh, Toby},
  year = {2001},
  journal = {Constraints},
  volume = {6},
  number = {4},
  pages = {345--372},
  issn = {13837133},
  doi = {10.1023/A:1011454308633},
  abstract = {A recent theoretical result by Achlioptas et al. shows that many models of random binary constraint satisfaction problems become trivially insoluble as problem size increases. This insolubility is partly due to the presence of `flawed variables,' variables whose values are all `flawed' (or unsupported). In this paper, we analyse how seriously existing work has been affected. We survey the literature to identify experimental studies that use models and parameters that may have been affected by flaws. We then estimate theoretically and measure experimentally the size at which flawed variables can be expected to occur. To eliminate flawed values and variables in the models currently used, we introduce a `flawless' generator which puts a limited amount of structure into the conflict matrix. We prove that such flawless problems are not trivially insoluble for constraint tightnesses up to 1/2. We also prove that the standard models B and C do not suffer from flaws when the constraint tightness is less than the reciprocal of domain size. We consider introducing types of structure into the constraint graph which are rare in random graphs and present experimental results with such structured graphs.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F2IG2MVG/gent et al 2001 - random constraint satisfaction - flaws and structure - BDPG - ANNO.pdf}
}

@article{gerberBiodiversityMeasuresBased2011,
  title = {Biodiversity Measures Based on Species-Level Dissimilarities: {{A}} Methodology for Assessment},
  shorttitle = {Biodiversity Measures Based on Species-Level Dissimilarities},
  author = {Gerber, Nicolas},
  year = {2011},
  month = oct,
  journal = {Ecological Economics},
  volume = {70},
  number = {12},
  pages = {2275--2281},
  issn = {09218009},
  doi = {10.1016/j.ecolecon.2011.08.013},
  abstract = {Biodiversity is widely recognized as a valuable natural asset to conserve. Yet biodiversity is often reported to be declining worldwide. Biodiversity measures can help evaluating it and conserving it, but need to be clearly defined and assessed. In this paper, I review several biodiversity measures and develop a new one, all based on a matrix of species-level dissimilarity data. The data can be used in its raw form, regardless of its origin (e.g. studies of morphological traits, DNA hybridization experiments\ldots ) or of any graphical representation. Then, I propose a two-step assessment of the measures. First, I assess them in terms of their deviation from a strict additive law determining the contribution of each species to the diversity of the set in an ideal setting. This setting refers to a case where the data exactly determines the hierarchical ordering of the species. Second, I assess the measures based on their compliance with a list of axioms. These axioms reflect basic mathematical properties regarded as desirable for diversity measures, such as their monotonicity in species and dissimilarities. Finally, I show the importance of applying the new quantitative assessment and the axiomatic approach together when selecting a dissimilarity-based diversity measure.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IPB3H8WM/gerber 2011 - Biodiversity measures based on species-level dissimilarities - A methodology for assessment.pdf}
}

@article{gerrardSelectingConservationReserves1997,
  title = {Selecting Conservation Reserves Using Species-Covering Models: {{Adapting}} the {{ARC}}/{{INFO GIS}}},
  shorttitle = {Selecting Conservation Reserves Using Species-Covering Models},
  author = {Gerrard, Ross A and Church, Richard L and Stoms, David M and Davis, Frank W},
  year = {1997},
  month = oct,
  journal = {Transactions in GIS},
  volume = {2},
  number = {1},
  pages = {45--60},
  issn = {1361-1682, 1467-9671},
  doi = {10.1111/j.1467-9671.1997.tb00004.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F3FBLEC4/j.1467-9671.1997.tb00004.x.pdf}
}

@article{gershensonWhenSlowerFaster2015,
  title = {When Slower Is Faster},
  author = {Gershenson, Carlos and Helbing, Dirk},
  year = {2015},
  month = nov,
  journal = {Complexity},
  volume = {21},
  number = {2},
  pages = {9--15},
  issn = {10762787},
  doi = {10.1002/cplx.21736},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YQ749QJ4/gershenson helbing 2015 - when slower is faster - OPTISEVIL.pdf}
}

@article{ghasemisaghand2021e,
  title = {{{SiteOpt}}: An Open-source {{R}}-package for Site Selection and Portfolio Optimization},
  shorttitle = {{{SiteOpt}}},
  author = {Ghasemi Saghand, Payman and Haider, Zulqarnain and Charkhgard, Hadi and Eaton, Mitchell and Martin, Julien and Yurek, Simeon and Udell, Bradley J.},
  year = {2021},
  month = nov,
  journal = {Ecography},
  volume = {44},
  number = {11},
  pages = {1678--1685},
  issn = {0906-7590, 1600-0587},
  doi = {10.1111/ecog.05717},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CVX9AM8Z/Saghand Haider et al 2021 - SiteOpt - an open-source R-package for site selection and  portfolio optimization - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf}
}

@article{gianettoDynamicStructureCompetition2018,
  title = {Dynamic {{Structure}} of {{Competition Networks}} in {{Affordable Care Act Insurance Market}}},
  author = {Gianetto, David A. and Mosleh, Mohsen and Heydari, Babak},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {12700--12709},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2800659},
  abstract = {Stimulating competition is one of the main topics in most health care reform debates, and it has been a central issue in the Affordable Care Act in the United States since 2009. The goal of this paper is to use complex network methods to study dynamic and structure of competition under Affordable Care Act (ACA) and its evolution over time since its beginning until 2017. Using publicly available data, we construct a bipartite network of counties and insurance providers, create associated weighted single-mode networks, and analyze the evolution of network parameters that are related to competition and potential collusion in complex networks. These parameters have been previously tied to dynamics of collaboration and competition in earlier theoretical works. We argue that three parameters, namely network modularity, and eigenvector centrality mean and skewness are appropriate indicators of the overall competition in the insurance market. Based on these parameters, we show that the level of systemic competition among insurers as a function of time is an inverse U-shape trend, and that competition has returned back to what it was at the very beginning of ACA, indicating an undesirable resilience in the national health care system.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KSTBYN38/gianetto et al 2018 - Dynamic Structure of Competition Networks in Affordable Care Act Insurance Market - NETWORKS - ECONOMICS - COMPETITION - EFs - OPTISEVIL.pdf}
}

@article{gibsonHowEfficiencyShapes2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  year = {2019},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {389--407},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.02.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V86HSC4R/gibson et al 2019 - How Efficiency Shapes Human Language - LINGUISTICS - OPTISEVIL.pdf}
}

@article{gibsonSamplingMethodInfluences2011,
  title = {Sampling Method Influences the Structure of Plant-Pollinator Networks},
  author = {Gibson, Rachel H. and Knott, Ben and Eberlein, Tim and Memmott, Jane},
  year = {2011},
  month = jun,
  journal = {Oikos},
  volume = {120},
  number = {6},
  pages = {822--831},
  issn = {00301299},
  doi = {10.1111/j.1600-0706.2010.18927.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6QGGKE53/Gibson et al. Oikos 2011.pdf}
}

@incollection{gilmour2006acoasi,
  title = {Kernelization as {{Heuristic Structure}} for the {{Vertex Cover Problem}}},
  booktitle = {Ant {{Colony Optimization}} and {{Swarm Intelligence}}},
  author = {Gilmour, Stephen and Dras, Mark},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dorigo, Marco and Gambardella, Luca Maria and Birattari, Mauro and Martinoli, Alcherio and Poli, Riccardo and St{\"u}tzle, Thomas},
  year = {2006},
  volume = {4150},
  pages = {452--459},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11839088_45},
  abstract = {For solving combinatorial optimisation problems, exact methods accurately exploit the structure of the problem but are tractable only up to a certain size; approximation or heuristic methods are tractable for very large problems but may possibly be led into a bad solution. A question that arises is, From where can we obtain knowledge of the problem structure via exact methods that can be exploited on large-scale problems by heuristic methods? We present a framework that allows the exploitation of existing techniques and resources to integrate such structural knowledge into the Ant Colony System metaheuristic, where the structure is determined through the notion of kernelization from the field of parameterized complexity. We give experimental results using vertex cover as the problem instance, and show that knowledge of this type of structure improves performance beyond previously defined ACS algorithms.},
  isbn = {978-3-540-38482-3 978-3-540-38483-0},
  langid = {english},
  keywords = {ant colony optimization,bdpg,kernelization,learning to predict performance,parameterized complexity,vertex cover},
  file = {/Users/bill/D/Zotero/storage/I6JGZ3M9/gilmour dras 2006 - Kernelization as Heuristic Structure for the Vertex Cover Problem - BDPG.pdf}
}

@article{glassEconomicCaseSpatial,
  title = {The {{Economic Case}} for the {{Spatial Error Model}} with an {{Application}} to {{State Vehicle Usage}} in the {{U}}.{{S}}.},
  author = {Glass, Anthony J and Kenjegalieva, Karligash and Sickles, Robin},
  pages = {31},
  abstract = {LeSage and Pace (2009) make an econometric case for the spatial Durbin model over, among others, the spatial error model. We make an economic case for the spatial error model because it captures spatial dependence more fully (i.e. beyond that which can be attributed to the dependent variables in neighboring units). Also, when faced with the choice between aggregate or disaggregated data the spatial error model or a related model (e.g. the seemingly unrelated spatial error model) should be \ldots tted. This is to ensure that Wald tests of whole sets of coe\textcent{} cients against one another to establish if disaggregation is necessary are not invalidated. To illustrate the economic case which we make we extend the literature on the determinants of vehicle usage by modelling the spatial dependence of state travel for the U.S. over the period 1980 2008. On the basis of Wald tests, aggregate data on state vehicle usage is progressively disaggregated and spatial error models for travel on all twelve types of highway are \ldots tted. In all cases the spatial dependence in the spatial error models is greater than or approximately equal to that in the spatial lag models, con\ldots rming that the former does capture any additional sources of spatial dependence.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4Z29GQJI/the economic case for the spatial error model.pdf}
}

@article{glasser2011iac,
  title = {The Fault Tolerance of {{NP-hard}} Problems},
  author = {Gla{\ss}er, Christian and Pavan, A. and Travers, Stephen},
  year = {2011},
  month = mar,
  journal = {Information and Computation},
  volume = {209},
  number = {3},
  pages = {443--455},
  issn = {08905401},
  doi = {10.1016/j.ic.2010.11.012},
  abstract = {We study the effects of faulty data on NP-hard sets. We consider hard sets for several polynomial time reductions, add corrupt data and then analyze whether the resulting sets are still hard for NP. We explain that our results are related to a weakened deterministic variant of the notion of program self-correction by Blum, Luby, and Rubinfeld. Among other results, we use the Left-Set technique to prove that m-complete sets for NP are nonadaptively weakly deterministically self-correctable while btt-complete sets for NP are weakly deterministically self-correctable. Our results can also be applied to the study of Yesha's p-closeness. In particular, we strengthen a result by Ogiwara and Fu.},
  langid = {english},
  keywords = {fault tolerance,NP-hard,uncertainty},
  file = {/Users/bill/D/Zotero/storage/9DUUEIZ4/glasser pavan travers 2011 - the fault tolerance of np-hard problems.pdf}
}

@techreport{goldberg1979,
  type = {Courant {{Computer Science Report}}},
  title = {On the Complexity of the Satisfiability Problem},
  author = {Goldberg, Allen T},
  year = {1979},
  number = {\#16},
  address = {{New York University, NY}},
  institution = {{Courant Institute of Mathematical Sciences}},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YW6BD4JG/goldberg 1979 - on the complexity of the satisfiability problem - THESIS - BDPG.pdf}
}

@article{goldberg1982ipl,
  title = {Average Time Analyses of Simplified {{Davis-Putnam}} Procedures},
  author = {Goldberg, Allen and Purdom, Paul and Brown, Cynthia},
  year = {1982},
  month = sep,
  journal = {Information Processing Letters},
  volume = {15},
  number = {2},
  pages = {213},
  issn = {00200190},
  doi = {10.1016/0020-0190(83)90127-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V5TEVVJJ/goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - BDPG.pdf}
}

@article{goldberg1983ipl,
  title = {Average Time Analyses of Simplified {{Davis-Putnam}} Procedures: {{Corrigendum}}},
  author = {Goldberg, Allen and Purdom, Paul and Brown, Cynthia},
  year = {1983},
  month = may,
  journal = {Information Processing Letters},
  volume = {16},
  number = {4},
  pages = {213},
  issn = {00200190},
  doi = {10.1016/0020-0190(83)90127-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BE7BD2DL/goldberg purdom brown 1982 - average time analyses of simplified davis-putnam procedures - Corrigendum 1983 - BDPG.pdf.pdf}
}

@book{goldwasserDataStructuresNeighbor2002,
  title = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}: {{Fifth}} and {{Sixth DIMACS Implementation Challenges}}},
  shorttitle = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}},
  editor = {Goldwasser, Michael and Johnson, David and McGeoch, Catherine},
  year = {2002},
  month = dec,
  series = {{{DIMACS Series}} in {{Discrete Mathematics}} and                         {{Theoretical Computer Science}}},
  volume = {59},
  publisher = {{American Mathematical                     Society}},
  address = {{Providence, Rhode                     Island}},
  doi = {10.1090/dimacs/059},
  isbn = {978-0-8218-2892-2 978-1-4704-4017-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KF8T22NW/mcgeoch 1999 - A Bibliography of Algorithm Experimentation.pdf}
}

@inproceedings{gomes1997,
  title = {Problem {{Structure}} in the {{Presence}} of {{Perturbations}}},
  booktitle = {{{AAAI}}/{{IAAI}} 97},
  author = {Gomes, Carla P and Selman, Bart},
  year = {1997},
  pages = {221--226},
  abstract = {Recent progress on search and reasoning procedures has been driven by experimentation on computationally hard problem instances. Hard random problem distributions are an important source of such instances. Challenge problems from the area of nite algebra have also stimulated research on search and reasoning procedures. Nevertheless, the relation of such problems to practical applications is somewhat unclear. Realistic problem instances clearly have more structure than the random problem instances, but, on the other hand, they are not as regular as the structured mathematical problems. We propose a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain. We will show how to obtain interesting search problems in this manner, and how such problems can be used to study the robustness of search control mechanisms. Our experiments demonstrate that the performance of search strategies designed to mimic direct constructive methods degrade surprisingly quickly in the presence of even minor perturbations.},
  langid = {english},
  keywords = {bdpg,test generation},
  file = {/Users/bill/D/Zotero/storage/47C59SV5/gomes selman 1997 - problem structure in the presence of perturbations - BDPG.pdf}
}

@incollection{gomes2006foai,
  title = {Randomness and {{Structure}}},
  booktitle = {Foundations of {{Artificial Intelligence}}},
  author = {Gomes, Carla and Walsh, Toby},
  year = {2006},
  volume = {2},
  pages = {639--664},
  publisher = {{Elsevier}},
  doi = {10.1016/S1574-6526(06)80022-2},
  isbn = {978-0-444-52726-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RBS9KZRQ/gomes walsh 2006 - randomness and structure - BDPG - PROBLEM DIFFICULTY.pdf}
}

@incollection{gomesHeavytailedDistributionsCombinatorial1997,
  title = {Heavy-Tailed Distributions in Combinatorial Search},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming-CP97}}},
  author = {Gomes, Carla P. and Selman, Bart and Crato, Nuno},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Smolka, Gert},
  year = {1997},
  volume = {1330},
  pages = {121--135},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0017434},
  abstract = {Combinatorial search methods often exhibit a large variability in performance. We study the cost profiles of combinatorial search procedures. Our study reveals some intriguing properties of such cost profiles. The distributions are often characterized by very long tails or "heavy tails". We will show that these distributions are best characterized by a general class of distributions that have no moments (i.e., an infinite mean, variance, etc.). Such non-standard distributions have recently been observed in areas as diverse as economics, statistical physics, and geophysics. They are closely related to fractal phenomena, whose study was introduced by Mandelbrot. We believe this is the first finding of these distributions in a purely computational setting. We also show how random restarts can effectively eliminate heavy-tailed behavior, thereby dramatically improving the overall performance of a search procedure.},
  isbn = {978-3-540-63753-0 978-3-540-69642-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MTBWHEJL/chp%3A10.1007%2FBFb0017434.pdf}
}

@inproceedings{goodchildSpatialAccuracy2008,
  title = {Spatial {{Accuracy}} 2.0},
  booktitle = {Proceedings of the 8th {{International Symposium}} on {{Spatial Accuracy Assessment}} in {{Natural Resources}} and {{Environmental Sciences}}},
  author = {Goodchild, Michael F.},
  year = {2008},
  month = jun,
  pages = {1--7},
  publisher = {{World Academic Union (World Academic Press)}},
  address = {{Shanghai, P. R. China}},
  abstract = {Research on spatial accuracy assessment occurs within a broader context that provides its motivation. That broader context is dynamic, and has been changing at an accelerating rate. The concept of the Geospatial Web imagines a world of distributed, interoperable, georeferenced information in which it is possible to know where everything of importance is located in real time. It assumes an ability to conflate that is far beyond today's capabilities. Web 2.0 describes a substantial involvement of the user in creating the content of the Web, and has particular relevance to geospatial information. Metadata 2.0 shifts the onus for metadata production to the user, and addresses some of the growing issues surrounding existing standards. There is a growing need to address the accuracy assessment of the vast quantities of geospatial data being contributed by individual Web users.},
  isbn = {1-84626-170-8},
  keywords = {Geospatial Web,metadata,user-generated content,volunteered geographic information,Web 2.0},
  file = {/Users/bill/D/Zotero/storage/2XCKXFVY/goodchild 2008 - spatial accuracy 2.0.pdf}
}

@book{goodchildSpatialAutocorrelation1986,
  title = {Spatial Autocorrelation},
  author = {Goodchild, Michael F.},
  year = {1986},
  series = {{{CATMOG}}},
  number = {47},
  publisher = {{Geo Books}},
  address = {{Norwich}},
  isbn = {978-0-86094-223-8},
  langid = {english},
  lccn = {G70.3 .G66 1986},
  keywords = {Geography,Spatial analysis (Statistics),Statistical methods},
  file = {/Users/bill/D/Zotero/storage/XZINC9QR/goodchild 1988 - spatial-aurocorrelation.pdf}
}

@article{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  journal = {arXiv:1412.6572 [cs, stat]},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/HWCUQN3L/goodfellow et al 2015 - Explaining and Harnessing Adversarial Examples - ML BOOK - BDPG - GUPPY - UNCERTAINTY ENSEMBLE.pdf}
}

@article{gordonAssessingImpactsBiodiversity2011,
  title = {Assessing the Impacts of Biodiversity Offset Policies},
  author = {Gordon, Ascelin and Langford, William T. and Todd, James A. and White, Matt D. and Mullerworth, Daniel W. and Bekessy, Sarah A.},
  year = {2011},
  month = dec,
  journal = {Environmental Modelling \& Software},
  volume = {26},
  number = {12},
  pages = {1481--1488},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2011.07.021},
  abstract = {In response to the increasing loss of native vegetation and biodiversity, a growing number of countries have adopted ``offsetting'' policies that seek to balance local habitat destruction by restoring, enhancing and/or protecting similar but separate habitat. Although these policies often have a stated aim of producing a ``net gain'' or ``no net loss'' in environmental benefits, it is challenging to determine the potential impacts of a policy and if, or when, it will achieve its objectives. In this paper we address these questions with a general approach that uses predictive modelling under uncertainty to quantify the ecological impacts of different offset policies. This is demonstrated with a case study to the west of Melbourne, Australia where a proposed expansion of Melbourne's urban growth boundary would result in a loss of endangered native grassland, requiring offsets to be implemented as compensation. Three different offset policies were modelled: i) no restrictions on offset location, ii) offset locations spatially restricted to a strategically defined area and iii) offset locations spatially and temporally restricted, requiring all offsets to be implemented before commencing development. The ecological impact of the policies was determined with a system model that predicts future changes in the extent and condition of native grassland. The case study demonstrates how relative and absolute policy performance can be quantified in relation to best and worst-case scenarios. The study also shows that the ecological benefits of being temporally and spatially strategic in choosing offsets locations are substantially greater than being spatially strategic alone. We also show that even with considerable uncertainties in the system model predicting future grassland condition, the performance of the three offset policies can still be differentiated. Finally, we show the extent to which a policy achieves a ``net gain'' is dependent on the baseline against which policy performance is measured. The quantitative framework presented here can also be used to evaluate other offset policies or extended to deal with different types of environmental policies.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FNPZ2IZJ/gordon langford et al 2011 - Assessing the impacts of biodiversity offset policies.pdf}
}

@article{gordonModellingTradeOffs2011,
  title = {Modelling Trade Offs between Public and Private Conservation Policies},
  author = {Gordon, Ascelin and Langford, William T. and White, Matt D. and Todd, James A. and Bastin, Lucy},
  year = {2011},
  month = jan,
  journal = {Biological Conservation},
  volume = {144},
  number = {1},
  pages = {558--566},
  issn = {00063207},
  doi = {10.1016/j.biocon.2010.10.011},
  abstract = {To reduce global biodiversity loss, there is an urgent need to determine the most efficient allocation of conservation resources. Recently, there has been a growing trend for many governments to supplement public ownership and management of reserves with incentive programs for conservation on private land. At the same time, policies to promote conservation on private land are rarely evaluated in terms of their ecological consequences. This raises important questions, such as the extent to which private land conservation can improve conservation outcomes, and how it should be mixed with more traditional public land conservation. We address these questions, using a general framework for modelling environmental policies and a case study examining the conservation of endangered native grasslands to the west of Melbourne, Australia. Specifically, we examine three policies that involve: (i) spending all resources on creating public conservation areas; (ii) spending all resources on an ongoing incentive program where private landholders are paid to manage vegetation on their property with 5-year contracts; and (iii) splitting resources between these two approaches. The performance of each strategy is quantified with a vegetation condition change model that predicts future changes in grassland quality. Of the policies tested, no one policy was always best and policy performance depended on the objectives of those enacting the policy. This work demonstrates a general method for evaluating environmental policies and highlights the utility of a model which combines ecological and socioeconomic processes.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HUQA3BEA/gordon langford et al 2011 - Modelling trade offs between public and private conservation policies.pdf}
}

@article{gordonSimulatingValueCollaboration2013,
  title = {Simulating the Value of Collaboration in Multi-Actor Conservation Planning},
  author = {Gordon, Ascelin and Bastin, Lucy and Langford, William T. and Lechner, Alex M. and Bekessy, Sarah A.},
  year = {2013},
  month = jan,
  journal = {Ecological Modelling},
  volume = {249},
  pages = {19--25},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2012.07.009},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HIPMKV9T/gordon bastin langford et al 2013 - simulating the value of collaboration in multi-actor conservation planning.pdf}
}

@article{grandBiasedDataReduce2007,
  title = {Biased Data Reduce Efficiency and Effectiveness of Conservation Reserve Networks},
  author = {Grand, Joanna and Cummings, Michael P. and Rebelo, Tony G. and Ricketts, Taylor H. and Neel, Maile C.},
  year = {2007},
  month = may,
  journal = {Ecology Letters},
  volume = {10},
  number = {5},
  pages = {364--374},
  issn = {1461-023X, 1461-0248},
  doi = {10.1111/j.1461-0248.2007.01025.x},
  abstract = {Complementarity-based reserve selection algorithms efficiently prioritize sites for biodiversity conservation, but they are data-intensive and most regions lack accurate distribution maps for the majority of species. We explored implications of basing conservation planning decisions on incomplete and biased data using occurrence records of the plant family Proteaceae in South Africa. Treating this high-quality database as \^Ocomplete\~O, we introduced three realistic sampling biases characteristic of biodiversity databases: a detectability sampling bias and two forms of roads sampling bias. We then compared reserve networks constructed using complete, biased, and randomly sampled data. All forms of biased sampling performed worse than both the complete data set and equal-effort random sampling. Biased sampling failed to detect a median of 1\textendash 5\% of species, and resulted in reserve networks that were 9\textendash 17\% larger than those designed with complete data. Spatial congruence and the correlation of irreplaceability scores between reserve networks selected with biased and complete data were low. Thus, reserve networks based on biased data require more area to protect fewer species and identify different locations than those selected with randomly sampled or complete data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QGG36X88/grand et al 2007 - Biased data reduce efficiency and effectiveness of conservation reserve networks - BDPG - RESERVE SELECTION - UNCERTAINTY - SAMPLING ERROR MODEL - RARITY MEASURES - ANNO.pdf}
}

@article{grandoniSetCoveringOur,
  title = {Set {{Covering}} with {{Our Eyes Closed}}},
  author = {Grandoni, Fabrizio and Gupta, Anupam and Sankowski, Piotr and Leonardi, Stefano and Singh, Mohit and Miettinen, Pauli},
  pages = {10},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5RHV9ST2/GGLMSS08focs.pdf}
}

@article{grandoniSETCOVERINGOUR,
  title = {{{SET COVERING WITH OUR EYES CLOSED}}},
  author = {Grandoni, Fabrizio and Gupta, Anupam and Leonardi, Stefano and Sankowski, Piotr and Singh, Mohit},
  pages = {23},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8CK8GTZA/grandoni et al 2013 - SET COVERING WITH OUR EYES CLOSED.pdf}
}

@article{granthamDiminishingReturnInvestment2008,
  title = {Diminishing Return on Investment for Biodiversity Data in Conservation Planning: {{Conservation}} Value of Data},
  shorttitle = {Diminishing Return on Investment for Biodiversity Data in Conservation Planning},
  author = {Grantham, Hedley S. and Moilanen, Atte and Wilson, Kerrie A. and Pressey, Robert L. and Rebelo, Tony G. and Possingham, Hugh P.},
  year = {2008},
  month = sep,
  journal = {Conservation Letters},
  volume = {1},
  number = {4},
  pages = {190--198},
  issn = {1755263X},
  doi = {10.1111/j.1755-263X.2008.00029.x},
  abstract = {It is generally assumed that gathering more data is a good investment for conservation planning. However, the benefits of additional data have seldom been evaluated by analyzing the return on investment. If there are diminishing returns in terms of improved planning, then resources might be better directed toward other actions, depending on their relative costs and benefits. Our aim was to determine the return on investment from spending different amounts on survey data before undertaking a program of implementing new protected areas. We estimated how much protea data is obtained as a function of dollars invested in surveying. We then simulated incremental protection and loss of habitat to determine the benefit of investment in that data on the protection of proteas. We found that, after an investment of only US\$100,000 ({$\sim$}780,000 South Africa Rand [ZAR]), there was little increase in the effectiveness of conservation prioritizations, despite the full data set costing at least 25 times that amount.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FN9GUMJU/grantham et al 2008 - Diminishing return on investment for biodiversity data in conservation planning - RESERVE SELECTION - BDPG - COST - ANNO.pdf}
}

@article{grundel,
  title = {{{PROBABILISTIC ANALYSIS AND RESULTS OF COMBINATORIAL PROBLEMS WITH MILITARY APPLICATIONS}}},
  author = {Grundel, Don A},
  pages = {135},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/95EITJ55/grundel 2004 - probabilistic analysis and results of combinatorial problems with military applications - THESIS - BDPG - ANNO.pdf}
}

@article{grundelPROBABILISTICANALYSISRESULTS,
  title = {{{PROBABILISTIC ANALYSIS AND RESULTS OF COMBINATORIAL PROBLEMS WITH MILITARY APPLICATIONS}}},
  author = {Grundel, Don A},
  pages = {135},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SHRSZXF6/grundel 2004 - Probabilistic analysis and results of combinatorial problems with military applications - THESIS.pdf}
}

@article{guerriLearningTechniquesAutomatic,
  title = {Learning Techniques for {{Automatic Algorithm Portfolio Selection}}},
  author = {Guerri, Alessio and Milano, Michela},
  pages = {5},
  abstract = {The purpose of this paper is to show that a well known machine learning technique based on Decision Trees can be effectively used to select the best approach (in terms of efficiency) in an algorithm portfolio for a particular case study: the Bid Evaluation Problem (BEP) in Combinatorial Auctions. In particular, we are interested in deciding when to use a Constraint Programming (CP) approach and when an Integer Programming (IP) approach, on the basis of the structure of the instance considered. Different instances of the same problem present a different structure, and one aspect (e.g. feasibility or optimality) can prevail on the other. We have extracted from a set of BEP instances, a number of parameters representing the instance structure. Some of them (few indeed) precisely identify the best strategy and its corresponding tuning to be used to face that instance. We will show that this approach is very promising, since it identifies the most efficient algorithm in the 90\% of the cases.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9XV3P3RY/ECAI2004.pdf}
}

@article{guisanSensitivityPredictiveSpecies2007,
  title = {Sensitivity of Predictive Species Distribution Models to Change in Grain Size},
  author = {Guisan, Antoine and Graham, Catherine H. and Elith, Jane and Huettmann, Falk},
  year = {2007},
  journal = {Diversity and Distributions},
  volume = {13},
  number = {3},
  pages = {332--340}
}

@inproceedings{gunnersenSpecVCMVImprovingCluster2011,
  title = {{{SpecVCMV}}: {{Improving}} Cluster Visualisation},
  shorttitle = {{{SpecVCMV}}},
  booktitle = {{{IECON}} 2011 - 37th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {Gunnersen, Sverre and {Smith-Miles}, Kate and Lee, Vincent},
  year = {2011},
  month = nov,
  pages = {2255--2260},
  publisher = {{IEEE}},
  address = {{Melbourne, Vic, Australia}},
  doi = {10.1109/IECON.2011.6119660},
  abstract = {This paper proposes a new approach to validating and visualising cluster structure by combining fuzzy membership functions and spectral clustering. By modifying the Visual Cluster Validity algorithm (VCV) to use an external fuzzy membership function as the distance measure and using sum of cluster membership as the sorting function, computational experiments on both the Zelnik-Manor synthetic and UCI real datasets show the proposed method, SpecVCMV, more clearly identifies the underlying cluster structure in the data.},
  isbn = {978-1-61284-972-0 978-1-61284-969-0 978-1-61284-971-3},
  langid = {english},
  keywords = {clustering},
  file = {/Users/bill/D/Zotero/storage/CA2YDQYW/06119660.pdf}
}

@inproceedings{gunnersenSpecVCMVImprovingCluster2011a,
  title = {{{SpecVCMV}}: {{Improving}} Cluster Visualisation},
  shorttitle = {{{SpecVCMV}}},
  booktitle = {{{IECON}} 2011 - 37th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {Gunnersen, Sverre and {Smith-Miles}, Kate and Lee, Vincent},
  year = {2011},
  month = nov,
  pages = {2255--2260},
  publisher = {{IEEE}},
  address = {{Melbourne, Vic, Australia}},
  doi = {10.1109/IECON.2011.6119660},
  abstract = {This paper proposes a new approach to validating and visualising cluster structure by combining fuzzy membership functions and spectral clustering. By modifying the Visual Cluster Validity algorithm (VCV) to use an external fuzzy membership function as the distance measure and using sum of cluster membership as the sorting function, computational experiments on both the Zelnik-Manor synthetic and UCI real datasets show the proposed method, SpecVCMV, more clearly identifies the underlying cluster structure in the data.},
  isbn = {978-1-61284-972-0 978-1-61284-969-0 978-1-61284-971-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6XCC3923/06119660.pdf}
}

@article{guoUncertaintyEnsembleModelling2015,
  title = {Uncertainty in Ensemble Modelling of Large-Scale Species Distribution: {{Effects}} from Species Characteristics and Model Techniques},
  shorttitle = {Uncertainty in Ensemble Modelling of Large-Scale Species Distribution},
  author = {Guo, Chuanbo and Lek, Sovan and Ye, Shaowen and Li, Wei and Liu, Jiashou and Li, Zhongjie},
  year = {2015},
  month = jun,
  journal = {Ecological Modelling},
  volume = {306},
  pages = {67--75},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.08.002},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HVPD6VUM/guo et al 2014 - Uncertainty in ensemble modelling of large-scale species distribution - Effects from species characteristics and model techniques.pdf}
}

@techreport{gurobioptimizationllc2020,
  title = {Gurobi {{Optimizer Reference Manual}}},
  author = {{Gurobi Optimization, LLC}},
  year = {2020}
}

@article{guttorpStatisticsClimate2014,
  title = {Statistics and {{Climate}}},
  author = {Guttorp, Peter},
  year = {2014},
  month = jan,
  journal = {Annual Review of Statistics and Its Application},
  volume = {1},
  number = {1},
  pages = {87--101},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-022513-115648},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/58MP8XBN/Guttorp_2014_Statistics and Climate.pdf}
}

@article{guyonUnsupervisedTransferLearning,
  title = {Unsupervised and {{Transfer Learning}}},
  author = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel and Talbot, Nicola},
  pages = {324},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/24NT83PH/CiML-v7-book.pdf}
}

@article{haanpaa2006s,
  title = {Hard {{Satisfiable Clause Sets}} for {{Benchmarking Equivalence Reasoning Techniques}}},
  author = {Haanp{\"a}{\"a}, Harri and J{\"a}rvisalo, Matti and Kaski, Petteri and Niemel{\"a}, Ilkka},
  editor = {Le Berre, Daniel and Simon, Laurent},
  year = {2006},
  month = mar,
  journal = {Journal on Satisfiability, Boolean Modeling and Computation},
  volume = {2},
  number = {1-4},
  pages = {27--46},
  issn = {15740617},
  doi = {10.3233/SAT190015},
  abstract = {A family of satisfiable benchmark instances in conjunctive normal form is introduced. The instances are constructed by transforming a random regular graph into a system of linear equations followed by clausification. Schemes for introducing nonlinearity to the instances are developed, making the instances suitable for benchmarking solvers with equivalence reasoning techniques. An extensive experimental evaluation shows that state-ofthe-art solvers scale exponentially in the instance size. Compared with other well-known families of satisfiable benchmark instances, the present instances are among the hardest.},
  langid = {english},
  keywords = {bdpg,benchmarking,planted solution},
  file = {/Users/bill/D/Zotero/storage/XREPZ98N/haanpaa et al 2006 - Hard Satisable Clause Sets for Benchmarking Equivalence Reasoning Techniques - BDPG - ANNO.pdf}
}

@article{haider2018em,
  title = {A Robust Optimization Approach for Solving Problems in Conservation Planning},
  author = {Haider, Zulqarnain and Charkhgard, Hadi and Kwon, Changhyun},
  year = {2018},
  month = jan,
  journal = {Ecological Modelling},
  volume = {368},
  pages = {288--297},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2017.12.006},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MGYZ7I5Y/Haider et al 2018 - a robust optimization approach for solving problems in conservation planning - BDPG - RESERVE SELECTION - GUPPY - ANNO.pdf}
}

@article{hall2007or,
  title = {Performance {{Prediction}} and {{Preselection}} for {{Optimization}} and {{Heuristic Solution Procedures}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  year = {2007},
  month = aug,
  journal = {Operations Research},
  volume = {55},
  number = {4},
  pages = {703--716},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1070.0398},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SH7ZJR2N/hall posner 2007 - Performance Prediction and Preselection for Optimization and Heuristic Solution Procedures - PROBLEM DIFFICULTY - BDPG.pdf}
}

@incollection{hall2010emftaooa,
  title = {The {{Generation}} of {{Experimental Data}} for {{Computational Testing}} in {{Optimization}}},
  booktitle = {Experimental {{Methods}} for the {{Analysis}} of {{Optimization Algorithms}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  editor = {{Bartz-Beielstein}, Thomas and Chiarandini, Marco and Paquete, Lu{\'i}s and Preuss, Mike},
  year = {2010},
  pages = {73--101},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02538-9_4},
  abstract = {This chapter discusses approaches to generating synthetic data for use in scientific experiments. In many diverse scientific fields, the lack of availability, high cost or inconvenience of the collection of real-world data motivates the generation of synthetic data. In many experiments, the method chosen to generate synthetic data can significantly affect the results of an experiment. Unfortunately, the scientific literature does not contain general protocols for how synthetic data should be generated. The purpose of this chapter is to rectify that deficiency. The protocol we propose is based on several generation principles. These principles motivate and organize the data generation process. The principles are operationalized by generation properties. Then, together with information about the features of the application and of the experiment, the properties are used to construct a data generation scheme. Finally, we suggest procedures for validating the synthetic data generated. The usefulness of our protocol is illustrated by a discussion of numerous applications of data generation from the optimization literature. This discussion identifies examples of both good and bad data generation practice as it relates to our protocol.},
  isbn = {978-3-642-02537-2 978-3-642-02538-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X339P69G/hall posner 2010 - the generation of experimental data for computational testing in optimization - BDPG.pdf}
}

@article{hallGeneratingExperimentalData2001,
  title = {Generating {{Experimental Data}} for {{Computational Testing}} with {{Machine Scheduling Applications}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  year = {2001},
  month = dec,
  journal = {Operations Research},
  volume = {49},
  number = {6},
  pages = {854--865},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.49.6.854.10014},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/35ZZ3MGH/hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf}
}

@article{hallGeneratingExperimentalData2001a,
  title = {Generating {{Experimental Data}} for {{Computational Testing}} with {{Machine Scheduling Applications}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  year = {2001},
  month = dec,
  journal = {Operations Research},
  volume = {49},
  number = {6},
  pages = {854--865},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.49.6.854.10014},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/G9HNQX3R/hall posner 2001 - generating experimental data for computational testing with machine scheduling applications.pdf}
}

@article{hallPerformancePredictionPreselection2007,
  title = {Performance {{Prediction}} and {{Preselection}} for {{Optimization}} and {{Heuristic Solution Procedures}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  year = {2007},
  month = aug,
  journal = {Operations Research},
  volume = {55},
  number = {4},
  pages = {703--716},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1070.0398},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QRSHQTFI/hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf}
}

@article{hallPerformancePredictionPreselection2007a,
  title = {Performance {{Prediction}} and {{Preselection}} for {{Optimization}} and {{Heuristic Solution Procedures}}},
  author = {Hall, Nicholas G. and Posner, Marc E.},
  year = {2007},
  month = aug,
  journal = {Operations Research},
  volume = {55},
  number = {4},
  pages = {703--716},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1070.0398},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BT6J66G7/hall posner 2007 - performance prediction and preselection for optimization and heuristic solution procedures.pdf}
}

@article{hansenHindsightMarineProtected2011,
  title = {Hindsight in Marine Protected Area Selection: {{A}} Comparison of Ecological Representation Arising from Opportunistic and Systematic Approaches},
  shorttitle = {Hindsight in Marine Protected Area Selection},
  author = {Hansen, Gretchen J.A. and Ban, Natalie C. and Jones, Michael L. and Kaufman, Les and Panes, Hazel M. and Yasu{\'e}, Ma{\"i} and Vincent, Amanda C.J.},
  year = {2011},
  month = jun,
  journal = {Biological Conservation},
  volume = {144},
  number = {6},
  pages = {1866--1875},
  issn = {00063207},
  doi = {10.1016/j.biocon.2011.04.002},
  abstract = {Systematic approaches to site selection for marine protected areas (MPAs) are often favored over opportunistic approaches as a means to meet conservation objectives efficiently. In this study, we compared analytically the conservation value of these two approaches. We locate this study in Danajon Bank, central Philippines, where many MPAs were established opportunistically based on community preference, with few if any contributions from biophysical data. We began by identifying the biophysical data that would have been available when the first MPA was created in Danajon Bank (1995). We next used these data with the reserve selection software Marxan to identify MPAs that covered the same area as is protected under the current set of MPAs (0.32\% of the total study area) and that would protect the greatest number of conservation targets at the lowest cost. We finally compared the conservation value of the current MPAs to the value of those selected by Marxan. Because of the dearth of biophysical data available in 1995 and the small area currently under protection, Marxan identified multiple configurations of MPAs that would protect the same percentage of conservation targets, with little differentiation among sites. Further, we discovered that the costs of obtaining and analyzing these data to be used for conservation planning would have been large relative to resources typically available to conservation planners in developing countries. Finally, we found that the current set of MPAs protected more ecological features than would be expected by chance, although not as many as could be protected using a systematic approach. Our results suggest that an opportunistic approach can be a valuable component of conservation planning, especially when biophysical data are sparse and community acceptance is a critical factor affecting the success of an MPA.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CZIQ4937/11 - hanson et al 2011 - Hindsight in marine protected area selection - A comparison of ecological representation arising from opportunistic and systematic approaches.pdf}
}

@article{hansenNeuralNetworkEnsembles1990,
  title = {Neural Network Ensembles},
  author = {Hansen, L.K. and Salamon, P.},
  year = {Oct./1990},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {10},
  pages = {993--1001},
  issn = {01628828},
  doi = {10.1109/34.58871},
  abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual ``generalization'' error can be reduced by invoking ensembles of similar networks.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KHJZ2P5K/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf}
}

@article{hansenNeuralNetworkEnsembles1990a,
  title = {Neural Network Ensembles},
  author = {Hansen, L.K. and Salamon, P.},
  year = {Oct./1990},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {10},
  pages = {993--1001},
  issn = {01628828},
  doi = {10.1109/34.58871},
  abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual ``generalization'' error can be reduced by invoking ensembles of similar networks.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NGLLYADA/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf}
}

@article{hansenNeuralNetworkEnsembles1990b,
  title = {Neural Network Ensembles},
  author = {Hansen, L.K. and Salamon, P.},
  year = {Oct./1990},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {10},
  pages = {993--1001},
  issn = {01628828},
  doi = {10.1109/34.58871},
  abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual ``generalization'' error can be reduced by invoking ensembles of similar networks.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NB7FAGFV/hansen salaman 1990 - neural network ensembles - with lots about measuring problem difficulty.pdf}
}

@article{hanson2019mee,
  title = {Optimality in Prioritizing Conservation Projects},
  author = {Hanson, Jeffrey O. and Schuster, Richard and Strimas-Mackey, Matthew and Bennett, Joseph R.},
  editor = {Hodgson, Dave},
  year = {2019},
  month = oct,
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {10},
  pages = {1655--1663},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13264},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TV2MTFF7/hanson ... strimas-mackey ... 2019 - Optimality in prioritizing conservation projects - BDPG - ANNO.pdf}
}

@article{har-peledWEIGHTEDGEOMETRICSET2012,
  title = {{{WEIGHTED GEOMETRIC SET COVER PROBLEMS REVISITED}}},
  author = {{Har-Peled}, Sariel and Lee, Mira},
  year = {2012},
  pages = {21},
  abstract = {We study several set cover problems in low dimensional geometric settings. Specifically, we describe a PTAS for the problem of computing a minimum cover of given points by a set of weighted fat objects. Here, we allow the objects to expand by some prespecified {$\delta$}-fraction of their diameter.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DTY6J2QH/77-357-1-PB.pdf}
}

@inproceedings{harmanMetricsAreFitness2004,
  title = {Metrics Are Fitness Functions Too},
  booktitle = {10th {{International Symposium}} on {{Software Metrics}}, 2004. {{Proceedings}}.},
  author = {Harman, M. and Clark, J.},
  year = {2004},
  pages = {58--69},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}},
  doi = {10.1109/METRIC.2004.1357891},
  abstract = {Metrics, whether collected statically or dynamically, and whether constructed from source code, systems or processes, are largely regarded as a means of evaluating some property of interest. This viewpoint has been very successful in developing a body of knowledge, theory and experience in the application of metrics to estimation, predication, assessment, diagnosis, analysis and improvement. This paper shows that there is an alternative, complementary, view of a metric: as a fitness function, used to guide a search for optimal or near optimal individuals in a search space of possible solutions.},
  isbn = {978-0-7695-2129-9},
  langid = {english},
  keywords = {EFs,fitness functions,metrics,optimization},
  file = {/Users/bill/D/Zotero/storage/9U55U845/harman clark 2004 - metrics are fitness functions too.pdf}
}

@article{hartfordAbstractDeepCounterfactual,
  title = {Abstract: {{Deep Counterfactual Prediction}} Using {{Instrumental Variables}}},
  author = {Hartford, Jason and Lewis, Greg and {Leyton-Brown}, Kevin and Taddy, Matt},
  pages = {2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XAIDA9RM/2016-NIPSWS-Causenet.pdf}
}

@article{hartfordDeepIVFlexible,
  title = {Deep {{IV}}: {{A Flexible Approach}} for {{Counterfactual Prediction}}},
  author = {Hartford, Jason and Lewis, Greg and {Leyton-Brown}, Kevin and Taddy, Matt},
  pages = {10},
  abstract = {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs)\textemdash sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework1 allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2XDZNCB3/2017-ICML-DeepIV.pdf}
}

@article{hartfordDeepLearningPredicting,
  title = {Deep {{Learning}} for {{Predicting Human Strategic Behavior}}},
  author = {Hartford, Jason and Wright, James R and {Leyton-Brown}, Kevin},
  pages = {9},
  abstract = {Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TFCGJK3B/2016-NIPS-Gamenet.pdf}
}

@article{hartmann2005cpc,
  title = {Phase Transition and Finite-Size Scaling in the Vertex-Cover Problem},
  author = {Hartmann, Alexander K. and Barthel, Wolfgang and Weigt, Martin},
  year = {2005},
  month = jul,
  journal = {Computer Physics Communications},
  volume = {169},
  number = {1-3},
  pages = {234--237},
  issn = {00104655},
  doi = {10.1016/j.cpc.2005.03.054},
  abstract = {NP-complete problems play a fundamental role in theoretical computer science. Recently, phase transitions were discovered in such problems, when studying suitably parameterized random ensembles. By applying concepts and methods from statistical physics, it is possible to understand these models and the phase transitions which occur. Here, we consider the vertex-cover problem, one of the six ``basic'' NP-complete problems. We describe the phase transition and the running time of an exact algorithm around the phase transition. To investigate how this transition resembles a phase transition in physical systems, we apply finite-size scaling and we study a correlation-length like quantity.},
  langid = {english},
  keywords = {bdpg,vertex cover},
  file = {/Users/bill/D/Zotero/storage/BW2NN5N7/hartmann et al 2017 - Phase transition and finite-size scaling in the vertex-cover problem - BDPG.pdf}
}

@article{hasselbergTestCaseGenerators1993,
  title = {Test Case Generators and Computational Results for the Maximum Clique Problem},
  author = {Hasselberg, Jonas and Pardalos, Panos M. and Vairaktarakis, George},
  year = {1993},
  journal = {Journal of Global Optimization},
  volume = {3},
  number = {4},
  pages = {463--482},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/BF01096415},
  abstract = {In the last years many algorithms have been proposed for solving the maximum clique problem. Most of these algorithms have been tested on randomly generated graphs. In this paper we present different test problem generators that arise from a variety of practical applications, as well as graphs with known maximum cliques. In addition, we provide computational experience with two exact algorithms using the generated test problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/URFH7XQJ/Hasselberg et al 1993 - Test Case Generators And Computational results for the maximum clique problem - BDPG - SANCHIS VERTEX COVER.pdf}
}

@book{hastie2009,
  title = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GLIQAQPG/hastie et al 2009 - the elements of statistical learning - 2nd edition.pdf}
}

@book{hastie2015,
  title = {Statistical Learning with Sparsity - the Lasso and Generalizations},
  author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  year = {2015},
  series = {Monographs on {{Statistics}} and {{Applied Probability}}},
  number = {143},
  publisher = {{CRC Press}},
  abstract = {In this monograph, we have attempted to summarize the actively developing field of statistical learning with sparsity. A sparse statistical model is one having only a small number of nonzero parameters or weights. It represents a classic case of ``less is more'': a sparse model can be much easier to estimate and interpret than a dense model. In this age of big data, the number of features measured on a person or object can be large, and might be larger than the number of observations. The sparsity assumption allows us to tackle such problems and extract useful and reproducible patterns from big datasets.},
  isbn = {978-1-4987-1216-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MTUCSCUM/hastie tibshirani wainwright - statistical learning with sparsity - the lasso and generalizations.pdf}
}

@article{haugOptionTradersUse2011,
  title = {Option Traders Use (Very) Sophisticated Heuristics, Never the {{Black}}\textendash{{Scholes}}\textendash{{Merton}} Formula},
  author = {Haug, Espen Gaarder and Taleb, Nassim Nicholas},
  year = {2011},
  month = feb,
  journal = {Journal of Economic Behavior \& Organization},
  volume = {77},
  number = {2},
  pages = {97--106},
  issn = {01672681},
  doi = {10.1016/j.jebo.2010.09.013},
  abstract = {Option traders use a heuristically derived pricing formula which they adapt by fudging and changing the tails and skewness by varying one parameter, the standard deviation of a Gaussian. Such formula is popularly called ``Black\textendash Scholes\textendash Merton'' owing to an attributed eponymous discovery (though changing the standard deviation parameter is in contra- diction with it). However, we have historical evidence that: (1) the said Black, Scholes and Merton did not invent any formula, just found an argument to make a well known (and used) formula compatible with the economics establishment, by removing the ``risk'' parameter through ``dynamic hedging'', (2) option traders use (and evidently have used since 1902) sophisticated heuristics and tricks more compatible with the previous versions of the formula of Louis Bachelier and Edward O. Thorp (that allow a broad choice of probability distributions) and removed the risk parameter using put-call parity, (3) option traders did not use the Black\textendash Scholes\textendash Merton formula or similar formulas after 1973 but continued their bottom-up heuristics more robust to the high impact rare event. The paper draws on historical trading methods and 19th and early 20th century references ignored by the finance literature. It is time to stop using the wrong designation for option pricing.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SGB6ZDGS/haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf}
}

@article{haugOptionTradersUse2011a,
  title = {Option Traders Use (Very) Sophisticated Heuristics, Never the {{Black}}\textendash{{Scholes}}\textendash{{Merton}} Formula},
  author = {Haug, Espen Gaarder and Taleb, Nassim Nicholas},
  year = {2011},
  month = feb,
  journal = {Journal of Economic Behavior \& Organization},
  volume = {77},
  number = {2},
  pages = {97--106},
  issn = {01672681},
  doi = {10.1016/j.jebo.2010.09.013},
  abstract = {Option traders use a heuristically derived pricing formula which they adapt by fudging and changing the tails and skewness by varying one parameter, the standard deviation of a Gaussian. Such formula is popularly called ``Black\textendash Scholes\textendash Merton'' owing to an attributed eponymous discovery (though changing the standard deviation parameter is in contra- diction with it). However, we have historical evidence that: (1) the said Black, Scholes and Merton did not invent any formula, just found an argument to make a well known (and used) formula compatible with the economics establishment, by removing the ``risk'' parameter through ``dynamic hedging'', (2) option traders use (and evidently have used since 1902) sophisticated heuristics and tricks more compatible with the previous versions of the formula of Louis Bachelier and Edward O. Thorp (that allow a broad choice of probability distributions) and removed the risk parameter using put-call parity, (3) option traders did not use the Black\textendash Scholes\textendash Merton formula or similar formulas after 1973 but continued their bottom-up heuristics more robust to the high impact rare event. The paper draws on historical trading methods and 19th and early 20th century references ignored by the finance literature. It is time to stop using the wrong designation for option pricing.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7M6A6THP/haug taleb 2010 - option traders use very sophisticated heuristics never the black-scholes-merton formula - FINANCE - OPTIONS - EF - OPTISEVIL.pdf}
}

@article{hefley2014mee,
  title = {Correction of Location Errors for Presence-Only Species Distribution Models},
  author = {Hefley, Trevor J. and Baasch, David M. and Tyre, Andrew J. and Blankenship, Erin E.},
  editor = {Warton, David},
  year = {2014},
  month = mar,
  journal = {Methods in Ecology and Evolution},
  volume = {5},
  number = {3},
  pages = {207--214},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12144},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FWETV289/Hefley et al. - 2014 - Correction of location errors for presence-only sp.pdf}
}

@article{hefleyCorrectionLocationErrors2014a,
  title = {Correction of Location Errors for Presence-Only Species Distribution Models},
  author = {Hefley, Trevor J. and Baasch, David M. and Tyre, Andrew J. and Blankenship, Erin E.},
  editor = {Warton, David},
  year = {2014},
  month = mar,
  journal = {Methods in Ecology and Evolution},
  volume = {5},
  number = {3},
  pages = {207--214},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12144},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2ETWSJXQ/Hefley_et_al-2014-Methods_in_Ecology_and_Evolution.pdf}
}

@article{hefleyFavorableTeamScores2011,
  title = {Favorable {{Team Scores Under}} the {{Team-Based Learning Paradigm}}: {{A Statistical Artifact}}?},
  author = {Hefley, Trevor},
  year = {2011},
  pages = {11},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Z2RKRZT7/Hefley - 2011 - Favorable Team Scores Under the Team-Based Learnin.pdf}
}

@article{hefleyFavorableTeamScores2011a,
  title = {Favorable {{Team Scores Under}} the {{Team-Based Learning Paradigm}}: {{A Statistical Artifact}}?},
  author = {Hefley, Trevor},
  year = {2011},
  pages = {11},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YU2NMRS5/Favorable team scores.pdf}
}

@article{hefleyFittingPopulationGrowth2013,
  title = {Fitting Population Growth Models in the Presence of Measurement and Detection Error},
  author = {Hefley, Trevor J. and Tyre, Andrew J. and Blankenship, Erin E.},
  year = {2013},
  month = aug,
  journal = {Ecological Modelling},
  volume = {263},
  pages = {244--250},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2013.05.003},
  abstract = {Population time series data from field studies are complex and statistical analysis requires models that describe nonlinear population dynamics and observational errors. State-space formulations of stochastic population growth models have been used to account for measurement error caused by the data collection process. Parameter estimation, inference, and prediction are all sensitive to measurement error. The observational process may also result in detection errors and if unaccounted for will result in biased parameter estimates. We developed an N-mixture state-space modeling framework to estimate and correct for errors in detection while estimating population model parameters. We tested our methods using simulated data sets and compared the results to those obtained with state-space models when detection is perfect and when detection is ignored. Our N-mixture state-space model yielded parameter estimates of similar quality to a state-space model when detection is perfect. Our results show that ignoring detection errors can lead to biased parameter estimates including an overestimated growth rate, underestimated equilibrium population size and estimated population state that is misleading. We recommend that researchers consider the possibility of detection errors when collecting and analyzing population time series data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HJXFCIH5/Hefley et al. - 2013 - Fitting population growth models in the presence o.pdf}
}

@article{hefleyFittingPopulationGrowth2013a,
  title = {Fitting Population Growth Models in the Presence of Measurement and Detection Error},
  author = {Hefley, Trevor J. and Tyre, Andrew J. and Blankenship, Erin E.},
  year = {2013},
  month = aug,
  journal = {Ecological Modelling},
  volume = {263},
  pages = {244--250},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2013.05.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BP2FQYR9/Hefley_etal_2013_fitting_popn_growth_models_error.pdf}
}

@article{hefleyNondetectionSamplingBias2013,
  title = {Nondetection Sampling Bias in Marked Presence-Only Data},
  author = {Hefley, Trevor J. and Tyre, Andrew J. and Baasch, David M. and Blankenship, Erin E.},
  year = {2013},
  month = dec,
  journal = {Ecology and Evolution},
  volume = {3},
  number = {16},
  pages = {5225--5236},
  issn = {20457758},
  doi = {10.1002/ece3.887},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JRX5UDBA/Hefley et al. - 2013 - Nondetection sampling bias in marked presence-only.pdf}
}

@article{hefleyNondetectionSamplingBias2013a,
  title = {Nondetection Sampling Bias in Marked Presence-only Data},
  author = {Hefley, Trevor J. and Tyre, Andrew J. and Baasch, David M. and Blankenship, Erin E.},
  year = {2013},
  month = dec,
  journal = {Ecology and Evolution},
  volume = {3},
  number = {16},
  pages = {5225--5236},
  issn = {2045-7758, 2045-7758},
  doi = {10.1002/ece3.887},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LZ8EUGJD/Hefley_et_al-2013-Ecology_and_Evolution.pdf}
}

@article{henInformationSciencesInstitute,
  title = {Information {{Sciences Institute}}},
  author = {Hen, Itay},
  pages = {43},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QGY2VE72/lucas2.pdf}
}

@article{herbrichAlgorithmicLuckiness,
  title = {Algorithmic {{Luckiness}}},
  author = {Herbrich, Ralf and Williamson, Robert C.},
  abstract = {Classical statistical learning theory studies the generalisation performance of machine learning algorithms rather indirectly. One of the main detours is that algorithms are studied in terms of the hypothesis class that they draw their hypotheses from. In this paper, motivated by the luckiness framework of Shawe-Taylor et al. (1998), we study learning algorithms more directly and in a way that allows us to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits the margin, the sparsity of the resultant weight vector, and the degree of clustering of the training data in feature space.},
  keywords = {guppy,problem difficulty,problem difficulty guppy},
  file = {/Users/bill/D/Zotero/storage/4SBWR2DA/herbrich williamson 2002 - Errata for Algorithmic Luckiness.pdf;/Users/bill/D/Zotero/storage/H8KRBDMU/herbrich williamson 2002 - Algorithmic Luckiness.pdf}
}

@article{hermosoUncertaintyCoarseConservation2012,
  title = {Uncertainty in Coarse Conservation Assessments Hinders the Efficient Achievement of Conservation Goals},
  author = {Hermoso, Virgilio and Kennard, Mark J.},
  year = {2012},
  month = mar,
  journal = {Biological Conservation},
  volume = {147},
  number = {1},
  pages = {52--59},
  issn = {00063207},
  doi = {10.1016/j.biocon.2012.01.020},
  abstract = {Conservation planning is sensitive to a number of scale-related issues, such as the spatial extent of the planning area, or the size of units of planning. An extensive literature has reported a decline in efficiency of conservation outputs when planning at small spatial scales or when using large planning units. However, other key issues remain, such as the grain size used to represent the spatial distribution of conservation features. Here, we evaluate the effect of grain size of species distribution data vs. size of planning units on a set of performance measures describing efficiency (ratio of area where species are represented/ total area needed), rate of commission errors (species erroneously expected to occur), representativeness (proportion of species achieving the target) and a novel measure of overall conservation uncertainty (integrating commission errors and uncertainty in the actual locations where species occur). We compared priority areas for the conservation of freshwater fish in the Daly River basin (northern Australia). Our study demonstrates that the effect of grain size of species distribution data was more important than planning unit size on conservation planning performance, with an increase in commission errors up to 80\% and conservation uncertainty over 90\% when coarse data were used. This was more pronounced for rare than common species, where the mismatch between coarse representations of biodiversity patterns and the smaller areas of actual occupancy of species was more evident. Special attention should be paid to the high risk of misallocation of limited budgets when planning in heterogeneous or disturbed environments, where biodiversity is patchily distributed, or when planning for conservation of rare species.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HNDFX6I9/3.3.2_hermoso_kennard_apr_2012_bc_pre-print_version.pdf}
}

@article{hirstSettlersVagrantsMutual2011,
  title = {Settlers, Vagrants and Mutual Indifference: Unintended Consequences of Hot-desking},
  shorttitle = {Settlers, Vagrants and Mutual Indifference},
  author = {Hirst, Alison},
  editor = {H{\"o}pfl, Heather},
  year = {2011},
  month = oct,
  journal = {Journal of Organizational Change Management},
  volume = {24},
  number = {6},
  pages = {767--788},
  issn = {0953-4814},
  doi = {10.1108/09534811111175742},
  abstract = {Purpose \textendash{} The purpose of this paper is to provide a sociological analysis of emergent sociospatial structures in a hot-desking office environment, where space is used exchangeably. It considers hot-desking as part of broader societal shifts in the ownership of space.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BEQ3AFZU/hirst 2011 - Settlers, vagrants and mutual indifference - unintended consequences of hot-desking - OPTISEVIL.pdf}
}

@article{hochachkaDataMiningDiscoveryPattern2007,
  title = {Data-{{Mining Discovery}} of {{Pattern}} and {{Process}} in {{Ecological Systems}}},
  author = {Hochachka, Wesley M. and Caruana, Rich and Fink, Daniel and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Kelling, Steve},
  year = {2007},
  journal = {Journal of Wildlife Management},
  volume = {71},
  number = {7},
  pages = {2427},
  issn = {0022-541X, 1937-2817},
  doi = {10.2193/2006-503},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FFG6KRWT/0046352320e6e55cbe000000.pdf}
}

@article{hoeppke2021mee,
  title = {Maxnodf: {{An R}} Package for Fair and Fast Comparisons of Nestedness between Networks},
  shorttitle = {Maxnodf},
  author = {Hoeppke, Christoph and Simmons, Benno I.},
  editor = {Freckleton, Robert},
  year = {2021},
  month = jan,
  journal = {Methods in Ecology and Evolution},
  pages = {2041-210X.13545},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13545},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XP5QPTWV/hoeppke simmons 2021 - maxnodf - an R package for fair and fast comparisons of nestedness between networks - NETWORKS - GRAPH THEORY - NESTEDNESS - BDPG.pdf}
}

@article{hoffmanUseSimulatedData2010,
  title = {Use of Simulated Data from a Process-Based Habitat Model to Evaluate Methods for Predicting Species Occurrence},
  author = {Hoffman, Justin D. and {Aguilar-Amuchastegui}, Naikoa and Tyre, Andrew J.},
  year = {2010},
  month = apr,
  journal = {Ecography},
  volume = {33},
  number = {4},
  pages = {656--666},
  issn = {09067590},
  doi = {10.1111/j.1600-0587.2009.05495.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V4HHETRR/Hoffman et al. - 2010 - Use of simulated data from a process-based habitat.pdf}
}

@article{hoffmanUseSimulatedData2010a,
  title = {Use of Simulated Data from a Process-Based Habitat Model to Evaluate Methods for Predicting Species Occurrence},
  author = {Hoffman, Justin D. and {Aguilar-Amuchastegui}, Naikoa and Tyre, Andrew J.},
  year = {2010},
  month = apr,
  journal = {Ecography},
  volume = {33},
  number = {4},
  pages = {656--666},
  issn = {09067590},
  doi = {10.1111/j.1600-0587.2009.05495.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NSJAFTBP/hoffman aguilar-amuchastegui TYRE 2010 - use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence - SDM - SIMULATION - VIRTUAL SPECIES - GUPPY - ANNO.pdf}
}

@article{hogg1996ai,
  title = {Phase Transitions and the Search Problem},
  author = {Hogg, Tad and Huberman, Bernardo A. and Williams, Colin P.},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {1--15},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00044-5},
  abstract = {We describe how techniques that were originally developed in statistical mechanics can be applied to search problems that arise commonly in artificial intelligence. This approach is useful for understanding the typical behavior of classes of problems. In particular, these techniques predict that abrupt changes in computational cost, analogous to physical phase transitions, should occur universally, as heuristic effectiveness or search space topology is varied. We also present a number of open questions raised by these studies.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ELNSAH98/hogg huberman williams 1996 - Phase transitions and the search problem  - BDPG - SEARCH - PROBLEM DIFFICULTY.pdf}
}

@article{hoggHardestConstraintProblems1994,
  title = {The Hardest Constraint Problems: {{A}} Double Phase Transition},
  shorttitle = {The Hardest Constraint Problems},
  author = {Hogg, Tad and Williams, Colin P.},
  year = {1994},
  month = sep,
  journal = {Artificial Intelligence},
  volume = {69},
  number = {1-2},
  pages = {359--377},
  issn = {00043702},
  doi = {10.1016/0004-3702(94)90088-4},
  abstract = {The distribution of hard graph coloring problems as a function of graph connectivity is shown to have two distinct transition behaviors. The first, previously recognized, is a peak in the median search cost near the connectivity at which half the graphs have solutions. This region contains a high proportion of relatively hard problem instances. However, the hardest instances are in fact concentrated at a second, lower, transition point. Near this point, most problems are quite easy, but there are also a few very hard cases. This region of exceptionally hard problems corresponds to the transition between polynomial and exponential scaling of the average search cost, whose location we also estimate theoretically. These behaviors also appear to arise in other constraint problems. This work also shows the limitations of simple measures of the cost distribution, such as mean or median, for identifying outlying cases.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WRGC3HHR/1-s2.0-0004370294900884-main.pdf}
}

@article{hoggPhaseTransitionsSearch1996,
  title = {Phase Transitions and the Search Problem},
  author = {Hogg, Tad and Huberman, Bernardo A. and Williams, Colin P.},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {1--15},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00044-5},
  abstract = {We describe how techniques that were originally developed in statistical mechanics can be applied to search problems that arise commonly in artificial intelligence. This approach is useful for understanding the typical behavior of classes of problems. In particular, these techniques predict that abrupt changes in computational cost, analogous to physical phase transitions, should occur universally, as heuristic effectiveness or search space topology is varied. We also present a number of open questions raised by these studies.},
  langid = {english},
  keywords = {Hogg},
  file = {/Users/bill/D/Zotero/storage/4J2YTHBW/1-s2.0-0004370295000445-main.pdf}
}

@article{hoggRefiningPhaseTransition1996,
  title = {Refining the Phase Transition in Combinatorial Search},
  author = {Hogg, Tad},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {127--154},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00050-X},
  abstract = {Many recent studies have identified phase transitions from under- to overconstrained instances in classes of constraint satisfaction search problems, and their relation to search cost. For example, cases that are difficult to solve with a variety of search methods are concentrated near these transitions. These studies show that a simple parameter describing the problem structure predicts the difficulty of solving the problem, on average. However, this prediction is associated with a large variance and depends on the somewhat arbitrary choice of the problem class. Thus these results are of limited direct use for individual instances. To help address this limitation, additional parameters, describing problem structure as well as heuristic effectiveness, are introduced. These are shown to reduce the variation in some cases. This also provides further insight into the nature of intrinsically hard search problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WG2S8HMK/1-s2.0-000437029500050X-main.pdf}
}

@article{holteMaximizingMultiplePattern2006,
  title = {Maximizing over Multiple Pattern Databases Speeds up Heuristic Search},
  author = {Holte, Robert C. and Felner, Ariel and Newton, Jack and Meshulam, Ram and Furcy, David},
  year = {2006},
  month = nov,
  journal = {Artificial Intelligence},
  volume = {170},
  number = {16-17},
  pages = {1123--1136},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.09.002},
  abstract = {A pattern database (PDB) is a heuristic function stored as a lookup table. This paper considers how best to use a fixed amount (m units) of memory for storing pattern databases. In particular, we examine whether using n pattern databases of size m/n instead of one pattern database of size m improves search performance. In all the state spaces considered, the use of multiple smaller pattern databases reduces the number of nodes generated by IDA*. The paper provides an explanation for this phenomenon based on the distribution of heuristic values that occur during search.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Z6CSSD2S/1-s2.0-S0004370206000804-main.pdf}
}

@article{HonoursProjects,
  title = {Honours {{Projects}}},
  pages = {68},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6ESVRPHD/monash honours projects 2013.pdf}
}

@article{hooker1994or,
  title = {Needed: {{An Empirical Science}} of {{Algorithms}}},
  shorttitle = {Needed},
  author = {Hooker, J. N.},
  year = {1994},
  month = apr,
  journal = {Operations Research},
  volume = {42},
  number = {2},
  pages = {201--212},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.42.2.201},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6CT8ZX3D/hooker 1994 -  Needed - An Empirical Science of Algorithms - BDPG.pdf}
}

@article{hooker1995jh,
  title = {Testing Heuristics: {{We}} Have It All Wrong},
  author = {Hooker, John N.},
  year = {1995},
  month = sep,
  journal = {Journal of heuristics},
  volume = {1},
  number = {1},
  pages = {33--42},
  abstract = {The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. It is hard to make fair comparisons between algorithms and to assemble realistic test problems. Competitive testing tells us which algorithm is faster but not why. Because it requires polished code, it consumes time and energy that could be better spent doing more experiments. This article argues that a more scientific approach of controlled experimentation, similar to that used in other empirical sciences, avoids or alleviates these problems. We have confused research and development; competitive testing is suited only for the latter.},
  file = {/Users/bill/D/Zotero/storage/B7XPF86W/hooker 1995 - testing heuristics - we have it all wrong - BDPG - ANNO.pdf}
}

@misc{hopperCanWeBetter2014,
  title = {Can {{We}} Do {{Better}} than {{R-squared}}?},
  author = {Hopper, Tom},
  year = {2014},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7J77USEG/hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf}
}

@inproceedings{hoSimpleExplanationNo2001,
  title = {Simple {{Explanation}} of the {{No Free Lunch Theorem}} of {{Opimization}}},
  booktitle = {Proceedings of the 40th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Ho, Yu-Chi and Pepyne, David L.},
  year = {2001},
  month = dec,
  pages = {4409--4414},
  address = {{Orlando, Florida, USA}},
  abstract = {The No Free Lunch Theorem of Optimziation (NFLT) is an impossibility theorem telling us that a general-purpose universal optimization strategy is impossible, and the only way one strategy can outperform another is if it is specialized to the structure of the specific problem under consideration.  Since virtually all decision and control problems can be cast as optimziation problems, an appreciation of the NFLT and its conseqeunces is essential for controls engineers.  In this paper we present a framework for conceptualizing optimization problems that leads useful insights and a simple explanation of the NFLT.},
  isbn = {0-7803-7061-9},
  file = {/Users/bill/D/Zotero/storage/RS8VS9HB/nfl-optimization-explanation.pdf}
}

@article{howe2002j,
  title = {A {{Critical Assessment}} of {{Benchmark Comparison}} in {{Planning}}},
  author = {Howe, A. E. and Dahlman, E.},
  year = {2002},
  month = jul,
  journal = {Journal of Artificial Intelligence Research},
  volume = {17},
  pages = {1--33},
  issn = {1076-9757},
  doi = {10.1613/jair.935},
  abstract = {Recent trends in planning research have led to empirical comparison becoming commonplace. The eld has started to settle into a methodology for such comparisons, which for obvious practical reasons requires running a subset of planners on a subset of problems. In this paper, we characterize the methodology and examine eight implicit assumptions about the problems, planners and metrics used in many of these comparisons. The problem assumptions are: PR1) the performance of a general purpose planner should not be penalized/biased if executed on a sampling of problems and domains, PR2) minor syntactic di erences in representation do not a ect performance, and PR3) problems should be solvable by STRIPS capable planners unless they require ADL. The planner assumptions are: PL1) the latest version of a planner is the best one to use, PL2) default parameter settings approximate good performance, and PL3) time cut-o s do not unduly bias outcome. The metrics assumptions are: M1) performance degrades similarly for each planner when run on degraded runtime environments (e.g., machine platform) and M2) the number of plan steps distinguishes performance. We nd that most of these assumptions are not supported empirically; in particular, that planners are a ected di erently by these assumptions. We conclude with a call to the community to devote research resources to improving the state of the practice and especially to enhancing the available benchmark problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X9GJR757/howe dahlman 1993 - a critical assessment of benchmark comparison in planning.pdf}
}

@incollection{howeExploitingCompetitivePlanner2000,
  title = {Exploiting {{Competitive Planner Performance}}},
  booktitle = {Recent {{Advances}} in {{AI Planning}}},
  author = {Howe, Adele E. and Dahlman, Eric and Hansen, Christopher and Scheetz, Michael and {von Mayrhauser}, Anneliese},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Biundo, Susanne and Fox, Maria},
  year = {2000},
  volume = {1809},
  pages = {62--72},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/10720246_5},
  abstract = {To date, no one planner has demonstrated clearly superior performance. Although researchers have hypothesized that this should be the case, no one has performed a large study to test its limits. In this research, we tested performance of a set of planners to determine which is best on what types of problems. The study included six planners and over 200 problems. We found that performance, as measured by number of problems solved and computation time, varied with no one planner solving all the problems or being consistently fastest. Analysis of the data also showed that most planners either fail or succeed quickly and that performance depends at least in part on some easily observable problem/domain features. Based on these results, we implemented a meta-planner that interleaves execution of six planners on a problem until one of them solves it. The control strategy for ordering the planners and allocating time is derived from the performance study data. We found that our meta-planner is able to solve more problems than any single planner, but at the expense of computation time.},
  isbn = {978-3-540-67866-3 978-3-540-44657-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RRF7FEY7/howe et al 1999 - Exploiting Competitive Planner Performance.pdf}
}

@misc{HowReadUnderstand,
  title = {How\_to\_read\_understand\_and\_write\_{{Discussion}}\_sect.Pdf},
  file = {/Users/bill/D/Zotero/storage/SG7XCAJ4/How_to_read_understand_and_write_Discussion_sect.pdf}
}

@article{huberman1987ai,
  title = {Phase Transitions in Artificial Intelligence Systems},
  author = {Huberman, Bernardo A. and Hogg, Tad},
  year = {1987},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {33},
  number = {2},
  pages = {155--171},
  issn = {00043702},
  doi = {10.1016/0004-3702(87)90033-6},
  abstract = {We predict that large-scale artificial intelligence systems and cognitive models will undergo sudden phase transitions from disjointed parts into coherent structures as their topological connectivity increases beyond a critical value. These situations, ranging from production systems to semantic net computations, are characterized by event horizons in space-time that determine the range of causal connections between processes. At transition, these event horizons undergo explosive changes in size. This phenomenon, analogous to phase transitions in nature, provides a new paradigm with which to analyze the behavior of large-scale computation and determine its generic features.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5ZUFE543/huberman hogg 1987 - Phase Transitions in Artificial Intelligence Systems - BDPG - PROBLEM DIFFICULTY - ANNO.pdf}
}

@article{huiGameTheoryRisk2016,
  title = {Game Theory and Risk-based Leveed River System Planning with Noncooperation},
  author = {Hui, Rui and Lund, Jay R. and Madani, Kaveh},
  year = {2016},
  month = jan,
  journal = {Water Resources Research},
  volume = {52},
  number = {1},
  pages = {119--134},
  issn = {0043-1397, 1944-7973},
  doi = {10.1002/2015WR017707},
  abstract = {Optimal risk-based levee designs are usually developed for economic efficiency. However, in river systems with multiple levees, the planning and maintenance of different levees are controlled by different agencies or groups. For example, along many rivers, levees on opposite riverbanks constitute a simple leveed river system with each levee designed and controlled separately. Collaborative planning of the two levees can be economically optimal for the whole system. Independent and self-interested landholders on opposite riversides often are willing to separately determine their individual optimal levee plans, resulting in a less efficient leveed river system from an overall society-wide perspective (the tragedy of commons). We apply game theory to simple leveed river system planning where landholders on each riverside independently determine their optimal risk-based levee plans. Outcomes from noncooperative games are analyzed and compared with the overall economically optimal outcome, which minimizes net flood cost system-wide. The system-wide economically optimal solution generally transfers residual flood risk to the lower-valued side of the river, but is often impractical without compensating for flood risk transfer to improve outcomes for all individuals involved. Such compensation can be determined and implemented with landholders' agreements on collaboration to develop an economically optimal plan. By examining iterative multiple-shot noncooperative games with reversible and irreversible decisions, the costs of myopia for the future in making levee planning decisions show the significance of considering the externalities and evolution path of dynamic water resource problems to improve decision-making.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QRF6R87H/hui et al 2015 -  Game theory and risk-based leveed river system planning with noncooperation - OPTISEVIL - GAME THEORY.pdf}
}

@article{huntNCApproximationSchemesNP1998,
  title = {{{NC-Approximation Schemes}} for {{NP-}} and {{PSPACE-Hard Problems}} for {{Geometric Graphs}}},
  author = {Hunt, Harry B and Marathe, Madhav V and Radhakrishnan, Venkatesh and Ravi, S.S and Rosenkrantz, Daniel J and Stearns, Richard E},
  year = {1998},
  month = feb,
  journal = {Journal of Algorithms},
  volume = {26},
  number = {2},
  pages = {238--274},
  issn = {01966774},
  doi = {10.1006/jagm.1997.0903},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LYNF78LB/bunt-1998.pdf}
}

@article{hutterAlgorithmRuntimePrediction2013,
  title = {Algorithm {{Runtime Prediction}}: {{Methods}} \& {{Evaluation}}},
  shorttitle = {Algorithm {{Runtime Prediction}}},
  author = {Hutter, Frank and Xu, Lin and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2013},
  month = oct,
  journal = {arXiv:1211.0906 [cs, stat]},
  eprint = {1211.0906},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and\textemdash perhaps most importantly\textemdash a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Performance,Leyton-Brown,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/AN5KSPP2/1211.0906.pdf}
}

@article{hutterBayesianOptimizationCensored2013,
  title = {Bayesian {{Optimization With Censored Response Data}}},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  year = {2013},
  month = oct,
  journal = {arXiv:1310.1947 [cs, stat]},
  eprint = {1310.1947},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem: finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting rightcensored observations can substantially improve the state of the art in model-based algorithm configuration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Leyton-Brown,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/WK7AEIDL/1310.1947.pdf}
}

@article{hutterEfficientApproachAssessing,
  title = {An {{Efficient Approach}} for {{Assessing Hyperparameter Importance}}},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  pages = {9},
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that\textemdash even in very highdimensional cases\textemdash most performance variation is attributable to just a few hyperparameters.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/JLBQSAS3/hutter14.pdf}
}

@article{hutterParamILSAutomaticAlgorithm,
  title = {{{ParamILS}}: {{An Automatic Algorithm Configuration Framework}}},
  author = {Hutter, Frank and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {40},
  abstract = {The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/SMU7EHDP/JAIR-3606.pdf}
}

@incollection{hutterPerformancePredictionAutomated2006,
  title = {Performance {{Prediction}} and {{Automated Tuning}} of {{Randomized}} and {{Parametric Algorithms}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} - {{CP}} 2006},
  author = {Hutter, Frank and Hamadi, Youssef and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Benhamou, Fr{\'e}d{\'e}ric},
  year = {2006},
  volume = {4204},
  pages = {213--228},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11889205_17},
  abstract = {Machine learning can be utilized to build models that predict the runtime of search algorithms for hard combinatorial problems. Such empirical hardness models have previously been studied for complete, deterministic search algorithms. In this work, we demonstrate that such models can also make surprisingly accurate predictions of the run-time distributions of incomplete and randomized search methods, such as stochastic local search algorithms. We also show for the first time how information about an algorithm's parameter settings can be incorporated into a model, and how such models can be used to automatically adjust the algorithm's parameters on a per-instance basis in order to optimize its performance. Empirical results for Novelty+ and SAPS on structured and unstructured SAT instances show very good predictive performance and significant speedups of our automatically determined parameter settings when compared to the default and best fixed distribution-specific parameter settings.},
  isbn = {978-3-540-46267-5 978-3-540-46268-2},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/NR22CAAD/0912f508ebd0e0b6a0000000.pdf}
}

@article{hutterTradeoffsEmpiricalEvaluation2010,
  title = {Tradeoffs in the Empirical Evaluation of Competing Algorithm Designs},
  author = {Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2010},
  month = oct,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {60},
  number = {1-2},
  pages = {65--89},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-010-9191-0},
  abstract = {We propose an empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs. Our approach can provide insight into performance variation both across candidate algorithms and across instances. It can also identify the best tradeoff between evaluating a larger number of candidate algorithm designs, performing these evaluations on a larger number of problem instances, and allocating more time to each algorithm run. We applied our approach to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming, considering six different distributions of problem instances. We demonstrate that the resulting algorithm design scenarios differ in many ways, with important consequences for both automatic and manual algorithm design. We expect that both our methods and our findings will lead to tangible improvements in algorithm design methods.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/W2G8MM3K/hutter hoos leyton-brown 2010 - Tradeoffs in the empirical evaluation of competing algorithm designs.pdf}
}

@phdthesis{ide2014,
  title = {Concepts of Robustness for Uncertain Multi-Objective Optimization},
  author = {Ide, Jonas},
  year = {2014},
  address = {{Gottingen}},
  school = {Georg-August University},
  keywords = {bdpg,EFs,multi-objective,optimization,robust,uncertainty},
  file = {/Users/bill/D/Zotero/storage/T9EEQ7SQ/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf}
}

@article{ignizio1999eo,
  title = {Illusions of Optimality},
  author = {Ignizio, James P.},
  year = {1999},
  month = aug,
  journal = {Engineering Optimization},
  volume = {31},
  number = {6},
  pages = {749--761},
  issn = {0305-215X, 1029-0273},
  doi = {10.1080/03052159908941395},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ICEVK2AI/ignizio 1999 - illusions of optimality - STABILITY - OPTISEVIL - EFs - BDPG - ROBUST OPTIMIZATION - RESERVE SELECTION.pdf}
}

@article{ingberSimulatedAnnealingPractice1993,
  title = {Simulated Annealing: {{Practice}} versus Theory},
  shorttitle = {Simulated Annealing},
  author = {Ingber, L.},
  year = {1993},
  month = dec,
  journal = {Mathematical and Computer Modelling},
  volume = {18},
  number = {11},
  pages = {29--57},
  issn = {08957177},
  doi = {10.1016/0895-7177(93)90204-C},
  abstract = {Lester Ingber Lester Ingber Research, P.O.B. 857, McLean, VA 22101 ingber@alumni.caltech.edu Simulated annealing (SA) presents an optimization technique with several striking positive and negative features. Perhaps its most salient feature, statistically promising to deliver an optimal solution, in current practice is often spurned to use instead modified faster algorithms, ``simulated quenching'' (SQ). Using the author's Adaptive Simulated Annealing (ASA) code, some examples are given which demonstrate how SQ can be much faster than SA without sacrificing accuracy.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VN74QFZZ/ingber 1993 - simulated annealing - practice versus theory.pdf}
}

@inproceedings{insaniSelectingSuitableSolution2013,
  title = {Selecting Suitable Solution Strategies for {{Classes}} of Graph Coloring Instances Using Data Mining},
  booktitle = {2013 {{International Conference}} on {{Information Technology}} and {{Electrical Engineering}} ({{ICITEE}})},
  author = {Insani, Nur and {Smith-Miles}, Kate and Baatar, Davaatseren},
  year = {2013},
  month = oct,
  pages = {208--215},
  publisher = {{IEEE}},
  address = {{Yogyakarta, Indonesia}},
  doi = {10.1109/ICITEED.2013.6676240},
  abstract = {The Maximal Independent Set (MIS) formulation tackles the graph coloring problem (GCP) as the partitioning of vertices of a graph into a minimum number of maximal independent sets as each MIS can be assigned a unique color. Mehrotra and Trick [5] solved the MIS formulation with an exact IP approach, but they were restricted to solving smaller or easier instances. For harder instances, it might be impossible to get the optimal solution within a reasonable computation time. We develop a heuristic algorithm, hoping that we can solve these problems in more reasonable time. However, though heuristics can find a near-optimal solution extremely fast compared to the exact approaches, there is still significant variations in performance that can only be explained by the fact that certain structures or properties in graphs may be better suited to some heuristics more than others. Selecting the best algorithm on average across all instances does not help us pick the best one for a particular instance. The need to understand how the best heuristic for a particular class of instance depends on these graph properties is an important issue. In this research, we use data mining to select the best solution strategies for classes of graph coloring instances.},
  isbn = {978-1-4799-0425-9 978-1-4799-0423-5 978-1-4799-0424-2},
  langid = {english},
  keywords = {bdpg,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/WKL3Y57W/insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf}
}

@inproceedings{insaniSelectingSuitableSolution2013a,
  title = {Selecting Suitable Solution Strategies for {{Classes}} of Graph Coloring Instances Using Data Mining},
  booktitle = {2013 {{International Conference}} on {{Information Technology}} and {{Electrical Engineering}} ({{ICITEE}})},
  author = {Insani, Nur and {Smith-Miles}, Kate and Baatar, Davaatseren},
  year = {2013},
  month = oct,
  pages = {208--215},
  publisher = {{IEEE}},
  address = {{Yogyakarta, Indonesia}},
  doi = {10.1109/ICITEED.2013.6676240},
  abstract = {The Maximal Independent Set (MIS) formulation tackles the graph coloring problem (GCP) as the partitioning of vertices of a graph into a minimum number of maximal independent sets as each MIS can be assigned a unique color. Mehrotra and Trick [5] solved the MIS formulation with an exact IP approach, but they were restricted to solving smaller or easier instances. For harder instances, it might be impossible to get the optimal solution within a reasonable computation time. We develop a heuristic algorithm, hoping that we can solve these problems in more reasonable time. However, though heuristics can find a near-optimal solution extremely fast compared to the exact approaches, there is still significant variations in performance that can only be explained by the fact that certain structures or properties in graphs may be better suited to some heuristics more than others. Selecting the best algorithm on average across all instances does not help us pick the best one for a particular instance. The need to understand how the best heuristic for a particular class of instance depends on these graph properties is an important issue. In this research, we use data mining to select the best solution strategies for classes of graph coloring instances.},
  isbn = {978-1-4799-0425-9 978-1-4799-0423-5 978-1-4799-0424-2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4YSW5P4B/insani smith-miles baatar 2013 - selecting suitable solution strategies for classes of graph coloringinstances using data mining - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{ipekEfficientArchitecturalDesign2008,
  title = {Efficient Architectural Design Space Exploration via Predictive Modeling},
  author = {Ipek, Engin and McKee, Sally A. and Singh, Karan and Caruana, Rich and de Supinski, Bronis R. and Schulz, Martin},
  year = {2008},
  month = jan,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {4},
  number = {4},
  pages = {1--34},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/1328195.1328196},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZJMVE686/taco08.pdf}
}

@book{james2021,
  title = {An {{Introduction}} to {{Statistical Learning}} with {{Applications}} in {{R}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  edition = {Second Edition},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2SM2PB6M/james witten hastie tibshirani 2013-2017 8th printing - An Introduction to Statistical Learning with Applications in R - BOOK.pdf}
}

@article{jarvis2020esae,
  title = {Navigating Spaces between Conservation Research and Practice: {{Are}} We Making Progress?},
  shorttitle = {Navigating Spaces between Conservation Research and Practice},
  author = {Jarvis, Rebecca M. and Borrelle, Stephanie B. and Forsdick, Natalie J. and P{\'e}rez-H{\"a}mmerle, Katharina-Victoria and Dubois, Natalie S. and Griffin, Sean R. and Recalde-Salas, Angela and Buschke, Falko and Rose, David Christian and Archibald, Carla L. and Gallo, John A. and Mair, Louise and Kadykalo, Andrew N. and Shanahan, Danielle and Prohaska, Bianca K},
  year = {2020},
  month = dec,
  journal = {Ecological Solutions and Evidence},
  volume = {1},
  number = {2},
  issn = {2688-8319, 2688-8319},
  doi = {10.1002/2688-8319.12028},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BDKAB52H/jarvis et al 2020 - Navigating spaces between conservation research and practice - Are we making progress - GUPPY - BDPG.pdf}
}

@phdthesis{jia2007,
  title = {What Makes {{NP-complete}} Problems Hard?},
  author = {Jia, Haixia},
  year = {2007},
  langid = {english},
  school = {University of New Mexico},
  keywords = {bdpg,planted solution},
  file = {/Users/bill/D/Zotero/storage/BWY66SLI/jia 2007 - what makes np-complete problems hard - aka hard instances with hidden solutions - THESIS - BDPG - PLANTED SOLUTIONS.pdf}
}

@article{jia2007j,
  title = {Generating {{Hard Satisfiable Formulas}} by {{Hiding Solutions Deceptively}}},
  author = {Jia, H. and Moore, C. and Strain, D.},
  year = {2007},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {28},
  pages = {107--118},
  issn = {1076-9757},
  doi = {10.1613/jair.2039},
  abstract = {To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the ``hidden'' assignment A. Last year, (Achlioptas, Jia, \& Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time.},
  langid = {english},
  keywords = {bdpg,planted solution},
  file = {/Users/bill/D/Zotero/storage/EP2MSHHA/jia moore strain 2005 - generating hard satisfiable formulas by hiding solutions deceptively - AAAI05-061 - BDPG - PLANTED SOLUTIONS.pdf}
}

@article{jiangActionGraphGames2011,
  title = {Action-{{Graph Games}}},
  author = {Jiang, Albert Xin and {Leyton-Brown}, Kevin and Bhat, Navin A.R.},
  year = {2011},
  month = jan,
  journal = {Games and Economic Behavior},
  volume = {71},
  number = {1},
  pages = {141--173},
  issn = {08998256},
  doi = {10.1016/j.geb.2010.10.012},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/T5HKD2D3/AGG.pdf}
}

@inproceedings{jiangFocalTestBasedSpatialDecision2013,
  title = {Focal-{{Test-Based Spatial Decision Tree Learning}}: {{A Summary}} of {{Results}}},
  shorttitle = {Focal-{{Test-Based Spatial Decision Tree Learning}}},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Data Mining}}},
  author = {Jiang, Zhe and Shekhar, Shashi and Zhou, Xun and Knight, Joseph and Corcoran, Jennifer},
  year = {2013},
  month = dec,
  pages = {320--329},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/ICDM.2013.96},
  isbn = {978-0-7695-5108-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XLX7GD46/Publication_4.pdf}
}

@article{jimmyDeepNetsReally,
  title = {Do {{Deep Nets Really Need}} to Be {{Deep}}?},
  author = {Jimmy, Lei and Caruana, Rich},
  pages = {9},
  abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5TB4NK2A/ba caruana 2014 - do deep nets really need to be deep - GUPPY.pdf}
}

@incollection{johnson2002dsnnsamfasdic,
  title = {A Theoretician's Guide to the Experimental Analysis of Algorithms},
  booktitle = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}: {{Fifth}} and {{Sixth DIMACS Implementation Challenges}}},
  author = {Johnson, David S.},
  editor = {Goldwasser, Michael H. and Johnson, David S. and McGeogh, Catherine C.},
  year = {2002},
  series = {{{DIMACS Series}} in {{Discrete Mathematics}} and {{Theoretical Computer Science}}},
  volume = {59},
  pages = {215--250},
  publisher = {{American Mathematical Society}},
  address = {{Providence, RI, USA}},
  file = {/Users/bill/D/Zotero/storage/6YCZLK3Z/johnson 2001 - A Theoreticians Guide to the Exp erimental Analysis of Algorithms - BDPG.pdf}
}

@book{johnsonNonEquilibriumSocialScience2017,
  title = {Non-{{Equilibrium Social Science}} and {{Policy}}: {{Introduction}} and {{Essays}} on {{New}} and {{Changing Paradigms}} in {{Socio-Economic Thinking}}},
  shorttitle = {Non-{{Equilibrium Social Science}} and {{Policy}}},
  editor = {Johnson, Jeffrey and Nowak, Andrzej and Ormerod, Paul and Rosewell, Bridget and Zhang, Yi-Cheng},
  year = {2017},
  series = {Understanding {{Complex Systems}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-42424-8},
  isbn = {978-3-319-42422-4 978-3-319-42424-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KGTHF2F5/johnson et al 2017 - non-equilibrium social science and policy - BOOK - OPTISEVIL - ECONOMICS - EQUILIBRIUM - COMPLEXITY.pdf}
}

@article{jonesWhyWhatHow2011,
  title = {The {{Why}}, {{What}}, and {{How}} of {{Global Biodiversity Indicators Beyond}} the 2010 {{Target}}: {{Biodiversity Indicators Beyond}} 2010},
  shorttitle = {The {{Why}}, {{What}}, and {{How}} of {{Global Biodiversity Indicators Beyond}} the 2010 {{Target}}},
  author = {Jones, Julia P. G. and Collen, Ben and Atkinson, Giles and Baxter, Peter W. J. and Bubb, Philip and Illian, Janine B. and Katzner, Todd E. and Keane, Aidan and Loh, Jonathan and {Mcdonald-Madden}, Eve and Nicholson, Emily and Pereira, Henrique M. and Possingham, Hugh P. and Pullin, Andrew S. and Rodrigues, Ana S. L. and {Ruiz-Gutierrez}, Viviana and Sommerville, Matthew and {Milner-Gulland}, E. J.},
  year = {2011},
  month = jun,
  journal = {Conservation Biology},
  volume = {25},
  number = {3},
  pages = {450--457},
  issn = {08888892},
  doi = {10.1111/j.1523-1739.2010.01605.x},
  abstract = {The 2010 biodiversity target agreed by signatories to the Convention on Biological Diversity directed the attention of conservation professionals toward the development of indicators with which to measure changes in biological diversity at the global scale. We considered why global biodiversity indicators are needed, what characteristics successful global indicators have, and how existing indicators perform. Because monitoring could absorb a large proportion of funds available for conservation, we believe indicators should be linked explicitly to monitoring objectives and decisions about which monitoring schemes deserve funding should be informed by predictions of the value of such schemes to decision making. We suggest that raising awareness among the public and policy makers, auditing management actions, and informing policy choices are the most important global monitoring objectives. Using four well-developed indicators of biological diversity (extent of forests, coverage of protected areas, Living Planet Index, Red List Index) as examples, we analyzed the characteristics needed for indicators to meet these objectives. We recommend that conservation professionals improve on existing indicators by eliminating spatial biases in data availability, fill gaps in information about ecosystems other than forests, and improve understanding of the way indicators respond to policy changes. Monitoring is not an end in itself, and we believe it is vital that the ultimate objectives of global monitoring of biological diversity inform development of new indicators.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AY9EDPDC/jones et al 2011 - the why, what, and how of global biodiversity indicators beyond the 2010 target - GUPPY - EFs - OPTISEVIL.pdf}
}

@article{joppaNestednessEcologicalNetworks,
  title = {On Nestedness in Ecological Networks},
  author = {Joppa, Lucas N and Montoya, Jos{\'e} M and Sol{\'e}, Richard and Sanderson, Jim and Pimm, Stuart L},
  pages = {12},
  abstract = {Questions: Are interaction patterns in species interaction networks different from what one expects by chance alone? In particular, are these networks nested \textendash{} a pattern where resources taken by more specialized consumers form a proper subset of those taken by more generalized consumers? Organisms: Fifty-nine and 42 networks of mutualistic and host\textendash parasitoid interactions, respectively. Analytical methods: For each network, the observed degree of nestedness is compared with the distribution of nestedness values derived from a collection of 1000 random networks. Those networks with nestedness values lower than 95\% of all random values are considered `unusually nested'. The analysis considers two different metrics of nestedness and five different network randomization algorithms, each of which differs in the ecological assumptions imposed.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8WM4AQ9M/Montoya_evo_nes.pdf}
}

@article{jordanEvaluatingPerformanceReinforcement2020,
  title = {Evaluating the {{Performance}} of {{Reinforcement Learning Algorithms}}},
  author = {Jordan, Scott M. and Chandak, Yash and Cohen, Daniel and Zhang, Mengxue and Thomas, Philip S.},
  year = {2020},
  month = aug,
  journal = {arXiv:2006.16958 [cs, stat]},
  eprint = {2006.16958},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/2VX76Z3D/2006.16958.pdf}
}

@article{kaiser-bunburyIntegratingNetworkEcology2015,
  title = {Integrating Network Ecology with Applied Conservation: A Synthesis and Guide to Implementation},
  shorttitle = {Integrating Network Ecology with Applied Conservation},
  author = {{Kaiser-Bunbury}, Christopher N. and Bl{\"u}thgen, Nico},
  year = {2015},
  journal = {AoB Plants},
  volume = {7},
  pages = {plv076},
  issn = {2041-2851},
  doi = {10.1093/aobpla/plv076},
  abstract = {Ecological networks are a useful tool to study the complexity of biotic interactions at a community level. Advances in the understanding of network patterns encourage the application of a network approach in other disciplines than theoretical ecology, such as biodiversity conservation. So far, however, practical applications have been meagre. Here we present a framework for network analysis to be harnessed to advance conservation management by using plant \textendash{} pollinator networks and islands as model systems. Conservation practitioners require indicators to monitor and assess management effectiveness and validate overall conservation goals. By distinguishing between two network attributes, the `diversity' and `distribution' of interactions, on three hierarchical levels (species, guild/ group and network) we identify seven quantitative metrics to describe changes in network patterns that have implications for conservation. Diversity metrics are partner diversity, vulnerability/generality, interaction diversity and interaction evenness, and distribution metrics are the specialization indices d{${'}$} and H{${'}$}2, and modularity. Distribution metrics account for sampling bias and may therefore be suitable indicators to detect human-induced changes to plant \textendash pollinator communities, thus indirectly assessing the structural and functional robustness and integrity of ecosystems. We propose an implementation pathway that outlines the stages that are required to successfully embed a network approach in biodiversity conservation. Most importantly, only if conservation action and study design are aligned by practitioners and ecologists through joint experiments, are the findings of a conservation network approach equally beneficial for advancing adaptive management and ecological network theory. We list potential obstacles to the framework, highlight the shortfall in empirical, mostly experimental, network data and discuss possible solutions.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EHX9WEFW/AoB PLANTS-2015-Kaiser-Bunbury-aobpla_plv076.pdf}
}

@techreport{kaminerApplyingGAsSearching,
  title = {Applying {{GAs}} in {{Searching Motif Patterns}} in {{Gene Expression Data}}},
  author = {Kaminer, Tal and Laor, Nir},
  file = {/Users/bill/D/Zotero/storage/RQ3H4MAC/MotifDensity.pdf}
}

@book{kanevskiMachineLearningSpatial2009,
  title = {Machine Learning for Spatial Environmental Data: Theory, Applications and Software},
  shorttitle = {Machine Learning for Spatial Environmental Data},
  author = {Kanevski, Mikhail and Pozdnoukhov, Alexei and Timonin, Vadim},
  year = {2009},
  series = {Environmental Sciences, Environmental Engineering},
  publisher = {{Epfel}},
  address = {{Lausanne}},
  isbn = {978-0-8493-8237-6 978-2-940222-24-7},
  langid = {english},
  annotation = {OCLC: 635693164}
}

@inbook{kanevskiSupportVectorMachines2009,
  title = {Support {{Vector Machines}} and {{Kernel Methods}}},
  booktitle = {Machine {{Learning}} for {{Spatial Environmental Data}}: {{Theory}}, {{Applications}}, and {{Software}}},
  year = {2009},
  month = jun,
  edition = {First},
  pages = {247--346},
  publisher = {{EPFL Press}},
  address = {{Lausanne}},
  abstract = {This book discusses machine learning algorithms, such as artificial neural networks of different architectures, statistical learning theory, and Support Vector Machines used for the classification and mapping of spatially distributed data.~ It presents basic geostatistical algorithms as well.~The authors describe new trends in machine learning and their application to spatial data. The text also includes real case studies based on environmental and pollution data. It includes a CD-ROM with software that will allow both students and researchers to put the concepts to practice.},
  collaborator = {Kanevski, Mikhail and Timonin, Vadim and Pozdnukhov, Alexi},
  isbn = {978-0-8493-8237-6},
  langid = {english}
}

@article{kangGRATISGeneRAtingTIme,
  title = {{{GRATIS}}: {{GeneRAting TIme Series}} with Diverse and Controllable Characteristics},
  author = {Kang, Yanfei and Hyndman, Rob J and Li, Feng},
  pages = {33},
  abstract = {The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires a diverse collection of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We generate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7U9P4USC/1903.02787.pdf}
}

@inproceedings{kangHowExtractMeaningful2013,
  title = {How to Extract Meaningful Shapes from Noisy Time-Series Subsequences?},
  booktitle = {2013 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  author = {Kang, Yanfei and {Smith-Miles}, Kate and Belusic, Danijel},
  year = {2013},
  month = apr,
  pages = {65--72},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/CIDM.2013.6597219},
  abstract = {A method for extracting and classifying shapes from noisy time series is proposed. The method consists of two steps. The first step is to perform a noise test on each subsequence extracted from the series using a sliding window. All the subsequences recognised as noise are removed from further analysis, and the shapes are extracted from the remaining nonnoise subsequences. The second step is to cluster these extracted shapes. Although extracted from subsequences, these shapes form a non-overlapping set of time series subsequences and are hence amenable to meaningful clustering. The method is primarily designed for extracting and classifying shapes from very noisy real-world time series. Tests using artificial data with different levels of white noise and the red noise, and the real-world atmospheric turbulence data naturally characterised by strong red noise show that the method is able to correctly extract and cluster shapes from artificial data and that it has great potential for locating shapes in very noisy real-world time series.},
  isbn = {978-1-4673-5895-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8TW79ZQ7/kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf}
}

@inproceedings{kangHowExtractMeaningful2013a,
  title = {How to Extract Meaningful Shapes from Noisy Time-Series Subsequences?},
  booktitle = {2013 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  author = {Kang, Yanfei and {Smith-Miles}, Kate and Belusic, Danijel},
  year = {2013},
  month = apr,
  pages = {65--72},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/CIDM.2013.6597219},
  abstract = {A method for extracting and classifying shapes from noisy time series is proposed. The method consists of two steps. The first step is to perform a noise test on each subsequence extracted from the series using a sliding window. All the subsequences recognised as noise are removed from further analysis, and the shapes are extracted from the remaining nonnoise subsequences. The second step is to cluster these extracted shapes. Although extracted from subsequences, these shapes form a non-overlapping set of time series subsequences and are hence amenable to meaningful clustering. The method is primarily designed for extracting and classifying shapes from very noisy real-world time series. Tests using artificial data with different levels of white noise and the red noise, and the real-world atmospheric turbulence data naturally characterised by strong red noise show that the method is able to correctly extract and cluster shapes from artificial data and that it has great potential for locating shapes in very noisy real-world time series.},
  isbn = {978-1-4673-5895-8},
  langid = {english},
  keywords = {Curve Shapes Christy},
  file = {/Users/bill/D/Zotero/storage/6EQV8WTY/kang smith-miles belusic 2013 - How to Extract Meaningful Shapes from Noisy Time-Series Subsequences.pdf}
}

@article{kanIntroductionAnalysisApproximation1986,
  title = {An Introduction to the Analysis of Approximation Algorithms},
  author = {Kan, A.H.G.Rinnooy},
  year = {1986},
  month = jun,
  journal = {Discrete Applied Mathematics},
  volume = {14},
  number = {2},
  pages = {171--185},
  issn = {0166218X},
  doi = {10.1016/0166-218X(86)90059-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/985GJ8FD/1-s2.0-0166218X86900594-main.pdf}
}

@article{kaplanHumanCostsWorkplace,
  title = {The Human Costs of Workplace Monitoring},
  author = {Kaplan, Esther},
  pages = {11},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PFFHKT9S/kaplan 2015 - THE SPY WHO FIRED ME - The human costs of workplace monitoring - OPTISEVIL.pdf}
}

@inproceedings{karimzadehganExplorationexploitationTradeoffInteractive2010,
  title = {Exploration-Exploitation Tradeoff in Interactive Relevance Feedback},
  booktitle = {Proceedings of the 19th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Karimzadehgan, Maryam and Zhai, ChengXiang},
  year = {2010},
  pages = {1397--1400},
  publisher = {{ACM}},
  file = {/Users/bill/D/Zotero/storage/N66MWF6S/karimzadehgan zhai - exploration-exploitation tradeoff in interactive relevance feedback.pdf}
}

@article{kayastha,
  title = {{{EXPERIMENTS WITH SEVERAL METHODS OF PARAMETER UNCERTAINTY ESTIMATION IN HYDROLOGICAL MODELING}}},
  author = {Kayastha, Nagendra and Shrestha, Durga Lal and Solomatine, Dimitri},
  pages = {9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UW3Z3FUC/Kayastha,Shrestha,Solomatine,ExperimentsSevelMethods,HIC,2010.pdf}
}

@article{khalilDissertationPresentedAcademic,
  title = {A {{Dissertation Presented}} to {{The Academic Faculty}}},
  author = {Khalil, Elias B},
  journal = {DISCRETE OPTIMIZATION},
  pages = {172},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BLGBFHNJ/KHALIL-DISSERTATION-2019.pdf}
}

@article{kingsfordCMSC451Reductions,
  title = {{{CMSC}} 451: {{Reductions}} \& {{NP-completeness}}},
  author = {Kingsford, Carl},
  pages = {22},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/R6CYMCVR/npcomplete.pdf}
}

@inproceedings{kleinbergEfficiencyProcrastinationApproximately2017,
  title = {Efficiency {{Through Procrastination}}: {{Approximately Optimal Algorithm Configuration}} with {{Runtime Guarantees}}},
  shorttitle = {Efficiency {{Through Procrastination}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Kleinberg, Robert and {Leyton-Brown}, Kevin and Lucier, Brendan},
  year = {2017},
  month = aug,
  pages = {2023--2031},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/281},
  abstract = {Algorithm configuration methods have achieved much practical success, but to date have not been backed by meaningful performance guarantees. We address this gap with a new algorithm configuration framework, Structured Procrastination. With high probability and nearly as quickly as possible in the worst case, our framework finds an algorithm configuration that provably achieves near optimal performance. Further, its running time requirements asymptotically dominate those of existing methods.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LFE3DLN3/2017-StructuredProcrastination.pdf}
}

@article{kleinbergProcrastinatingConfidenceNearOptimal2019,
  title = {Procrastinating with {{Confidence}}: {{Near-Optimal}}, {{Anytime}}, {{Adaptive Algorithm Configuration}}},
  shorttitle = {Procrastinating with {{Confidence}}},
  author = {Kleinberg, Robert and {Leyton-Brown}, Kevin and Lucier, Brendan and Graham, Devon},
  year = {2019},
  month = nov,
  journal = {arXiv:1902.05454 [cs]},
  eprint = {1902.05454},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Algorithm configuration methods optimize the performance of a parameterized heuristic algorithm on a given distribution of problem instances. Recent work introduced an algorithm configuration procedure (``Structured Procrastination'') that provably achieves near optimal performance with high probability and with nearly minimal runtime in the worst case. It also offers an anytime property: it keeps tightening its optimality guarantees the longer it is run. Unfortunately, Structured Procrastination is not adaptive to characteristics of the parameterized algorithm: it treats every input like the worst case. Follow-up work (``LeapsAndBounds'') achieves adaptivity but trades away the anytime property. This paper introduces a new algorithm, ``Structured Procrastination with Confidence'', that preserves the near-optimality and anytime properties of Structured Procrastination while adding adaptivity. In particular, the new algorithm will perform dramatically faster in settings where many algorithm configurations perform poorly. We show empirically both that such settings arise frequently in practice and that the anytime property is useful for finding good configurations quickly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/NHLXV8DC/1902.05454.pdf}
}

@article{kollaPlayingRandomExpanding,
  title = {Playing {{Random}} and {{Expanding Unique Games}}},
  author = {Kolla, Alexandra and Tulsiani, Madhur},
  pages = {17},
  abstract = {In this work, we present a spectral algorithm that finds good assignments for instances of Unique Games when the underlying graph has some significant expansion and the constraints are arbitrary {$\Gamma$}-max-lin.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DULAJRFM/UGspec.pdf}
}

@book{koolenHedgingStructuredConcepts2010,
  title = {Hedging Structured Concepts},
  author = {Koolen, W.M and Warmuth, M.K and Kivinen, J and Kalai, A.T and Mohri, M},
  year = {2010},
  publisher = {{Omnipress}},
  abstract = {We develop an online algorithm called Component Hedge for learning structured concept classes when the loss of a structured concept sums over its components. Example classes include paths through a graph (composed of edges) and partial permutations (composed of assignments). The algorithm maintains a parameter vector with one non-negative weight per component, which always lies in the convex hull of the structured concept class. The algorithm predicts by decomposing the current parameter vector into a convex combination of concepts and choosing one of those concepts at random. The parameters are updated by first performing a multiplicative update and then projecting back into the convex hull. We show that Component Hedge has optimal regret bounds for a large variety of structured concept classes.},
  isbn = {978-0-9822529-2-5},
  langid = {english},
  annotation = {OCLC: 6899968046},
  file = {/Users/bill/D/Zotero/storage/TKY963RU/christiano14.pdf}
}

@article{kordikDiscoveringPredictiveEnsembles2018,
  title = {Discovering Predictive Ensembles for Transfer Learning and Meta-Learning},
  author = {Kord{\'i}k, Pavel and {\v C}ern{\'y}, Jan and Fr{\'y}da, Tom{\'a}{\v s}},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {177--207},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5682-0},
  abstract = {Recent meta-learning approaches are oriented towards algorithm selection, optimization or recommendation of existing algorithms. In this article we show how data-tailored algorithms can be constructed from building blocks on small data sub-samples. Building blocks, typically weak learners, are optimized and evolved into data-tailored hierarchical ensembles. Good-performing algorithms discovered by evolutionary algorithm can be reused on data sets of comparable complexity. Furthermore, these algorithms can be scaled up to model large data sets. We demonstrate how one particular template (simple ensemble of fast sigmoidal regression models) outperforms state-of-the-art approaches on the Airline data set. Evolved hierarchical ensembles can therefore be beneficial as algorithmic building blocks in meta-learning, including meta-learning at scale.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YPCM7YHJ/kordik et al 2018 - discovering predictive ensembles for transfer learning and meta-learning - KOALA - ENSEMBLE - TRANSFER LEARNING.pdf}
}

@incollection{kotthoffAutoWEKAAutomaticModel2019,
  title = {Auto-{{WEKA}}: {{Automatic Model Selection}} and {{Hyperparameter Optimization}} in {{WEKA}}},
  shorttitle = {Auto-{{WEKA}}},
  booktitle = {Automated {{Machine Learning}}},
  author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and {Leyton-Brown}, Kevin},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {81--95},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-05318-5_4},
  abstract = {WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K5PA6SBK/16-261(1).pdf}
}

@incollection{kotthoffAutoWEKAAutomaticModel2019a,
  title = {Auto-{{WEKA}}: {{Automatic Model Selection}} and {{Hyperparameter Optimization}} in {{WEKA}}},
  shorttitle = {Auto-{{WEKA}}},
  booktitle = {Automated {{Machine Learning}}},
  author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and {Leyton-Brown}, Kevin},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {81--95},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-05318-5_4},
  abstract = {WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W4LDV3LE/2016-MLOSS-AutoWeka2.pdf}
}

@inproceedings{krcahCombinationNoveltySearch2010,
  title = {Combination of Novelty Search and Fitness-Based Search Applied to Robot Body-Brain Co-Evolution},
  booktitle = {Czech-{{Japan Seminar}} on {{Data Analysis}} and {{Decision Making}} in {{Service Science}}},
  author = {Krcah, Peter and Toropila, Daniel},
  year = {2010},
  pages = {1--6},
  file = {/Users/bill/D/Zotero/storage/F47T6AVQ/krcah toropila - combination of novelty search and fitness-based search applied to robot body-brain co-evolution.pdf}
}

@article{krishnamurthy1987itc,
  title = {Constructing {{Test Cases}} for {{Partitioning Heuristics}}},
  author = {Krishnamurthy, Balakrishnan},
  year = {1987},
  month = sep,
  journal = {IEEE Transactions on Computers},
  volume = {C-36},
  number = {9},
  pages = {1112--1114},
  issn = {0018-9340},
  doi = {10.1109/TC.1987.5009543},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TX4L29U6/krishnamurthy 1987 - constructing test cases for partitioning heuristics - BDPG.pdf}
}

@article{krivelevichRandomSatisfiableProcess2009,
  title = {On the {{Random Satisfiable Process}}},
  author = {Krivelevich, Michael and Sudakov, Benny and Vilenchik, Dan},
  year = {2009},
  month = sep,
  journal = {Combinatorics, Probability and Computing},
  volume = {18},
  number = {5},
  pages = {775--801},
  issn = {0963-5483, 1469-2163},
  doi = {10.1017/S0963548309990356},
  abstract = {In this work we suggest a new model for generating random satisfiable               k               -CNF formulas. To generate such formulas. randomly permute all                                \$2\^k\textbackslash binom\{n\}\{k\}\$                              possible clauses over the variables               x               1               ,.~.~.,                                x                 n                              , and starting from the empty formula, go over the clauses one by one, including each new clause as you go along if, after its addition, the formula remains satisfiable. We study the evolution of this process, namely the distribution over formulas obtained after scanning through the first               m               clauses (in the random permutation's order).                                         Random processes with conditioning on a certain property being respected are widely studied in the context of graph properties. This study was pioneered by Ruci\'nski and Wormald in 1992 for graphs with a fixed degree sequence, and also by Erd\H{o}s, Suen and Winkler in 1995 for triangle-free and bipartite graphs. Since then many other graph properties have been studied, such as planarity and               H               -freeness. Thus our model is a natural extension of this approach to the satisfiability setting.                                         Our main contribution is as follows. For               m               {$\geq$}               cn               ,               c               =               c               (               k               ) a sufficiently large constant, we are able to characterize the structure of the solution space of a typical formula in this distribution. Specifically, we show that typically all satisfying assignments are essentially clustered in one cluster, and all but               e                                -{$\Omega$}(                 m                 /                 n                 )                              n               of the variables take the same value in all satisfying assignments. We also describe a polynomial-time algorithm that finds w.h.p. a satisfying assignment for such formulas.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6SFSYIR5/random-sat-model.pdf}
}

@inproceedings{krivelevichSolvingRandomSatisfiable2006,
  title = {Solving Random Satisfiable {{3CNF}} Formulas in Expected Polynomial Time},
  booktitle = {Proceedings of the Seventeenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithm  - {{SODA}} '06},
  author = {Krivelevich, Michael and Vilenchik, Dan},
  year = {2006},
  pages = {454--463},
  publisher = {{ACM Press}},
  address = {{Miami, Florida}},
  doi = {10.1145/1109557.1109608},
  abstract = {We present an algorithm for solving 3SAT instances. Several algorithms have been proved to work whp (with high probability) for various SAT distributions. However, an algorithm that works whp has a drawback. Indeed for typical instances it works well, however for some rare inputs it does not provide a solution at all. Alternatively, one could require that the algorithm always produce a correct answer but perform well on average. Expected polynomial time formalizes this notion. We prove that for some natural distribution on 3CNF formulas, called planted 3SAT, our algorithm has expected polynomial (in fact, almost linear) running time. The planted 3SAT distribution is the set of satisfiable 3CNF formulas generated in the following manner. First, a truth assignment is picked uniformly at random. Then, each clause satisfied by it is included in the formula with probability p. Extending previous work for the planted 3SAT distribution, we present, for the first time for a satisfiable SAT distribution, an expected polynomial time algorithm. Namely, it solves all 3SAT instances, and over the planted distribution (with p = d/n2, d {$>$} 0 a sufficiently large constant) it runs in expected polynomial time. Our results extend to k-SAT for any constant k.},
  isbn = {978-0-89871-605-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SFWTMDNT/ExpectedPoly3SAT.pdf}
}

@article{krzakala2009prl,
  title = {Hiding {{Quiet Solutions}} in {{Random Constraint Satisfaction Problems}}},
  author = {Krzakala, Florent},
  year = {2009},
  journal = {Physical Review Letters},
  volume = {102},
  number = {23},
  doi = {10.1103/PhysRevLett.102.238701},
  file = {/Users/bill/D/Zotero/storage/5JBVII8C/Krzakala_2009_Hiding Quiet Solutions in Random Constraint Satisfaction Problems.pdf}
}

@article{krzakala2009prla,
  title = {Hiding {{Quiet Solutions}} in {{Random Constraint Satisfaction Problems}}},
  author = {Krzakala, Florent and Zdeborov{\'a}, Lenka},
  year = {2009},
  month = jun,
  journal = {Physical Review Letters},
  volume = {102},
  number = {23},
  eprint = {0901.2130},
  eprinttype = {arxiv},
  pages = {238701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.102.238701},
  abstract = {We study constraint satisfaction problems on the so-called 'planted' random ensemble. We show that for a certain class of problems, e.g. graph coloring, many of the properties of the usual random ensemble are quantitatively identical in the planted random ensemble. We study the structural phase transitions, and the easy/hard/easy pattern in the average computational complexity. We also discuss the finite temperature phase diagram, finding a close connection with the liquid/glass/solid phenomenology.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,bdpg_P1,Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,planted problem,planted solution,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/59H2EIW3/krzakala et al 2009 - hiding quiet solutions in random constraint satisfaction problems.pdf}
}

@article{kwokOptimizationIntermittencySelfOrganizing2005,
  title = {Optimization via {{Intermittency}} with a {{Self-Organizing Neural Network}}},
  author = {Kwok, Terence and Smith, Kate A.},
  year = {2005},
  month = nov,
  journal = {Neural Computation},
  volume = {17},
  number = {11},
  pages = {2454--2481},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766054796860},
  abstract = {One of the major obstacles in using neural networks to solve combinatorial optimization problems is the convergence toward one of the many local minima instead of the global minima. In this letter, we propose a technique that enables a self-organizing neural network to escape from local minima by virtue of the intermittency phenomenon. It gives rise to novel search dynamics that allow the system to visit multiple global minima as meta-stable states. Numerical experiments performed suggest that the phenomenon is a combined effect of Kohonen-type competitive learning and the iterated softmax function operating near bifurcation. The resultant intermittent search exhibits fractal characteristics when the optimization performance is at its peak in the form of 1/f signals in the time evolution of the cost, as well as power law distributions in the meta-stable solution states. The N-Queens problem is used as an example to illustrate the meta-stable convergence process that sequentially generates, in a single run, 92 solutions to the 8-Queens problem and 4024 solutions to the 17-Queens problem.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZIIL4U5Y/0899766054796860.pdf}
}

@article{lagunaMathematicalModelLivestock2015,
  title = {Mathematical Model of Livestock and Wildlife: {{Predation}} and Competition under Environmental Disturbances},
  shorttitle = {Mathematical Model of Livestock and Wildlife},
  author = {Laguna, M.F. and Abramson, G. and Kuperman, M.N. and Lanata, J.L. and Monjeau, J.A.},
  year = {2015},
  month = aug,
  journal = {Ecological Modelling},
  volume = {309--310},
  pages = {110--117},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2015.04.020},
  langid = {english},
  keywords = {voiceless},
  file = {/Users/bill/D/Zotero/storage/UB8ISUUR/Laguna et al. - 2015 - Mathematical model of livestock and wildlife Pred.pdf}
}

@article{lahtinenPortfolioDecisionAnalysis2017,
  title = {Portfolio Decision Analysis Methods in Environmental Decision Making},
  author = {Lahtinen, Tuomas J. and H{\"a}m{\"a}l{\"a}inen, Raimo P. and Liesi{\"o}, Juuso},
  year = {2017},
  month = aug,
  journal = {Environmental Modelling \& Software},
  volume = {94},
  pages = {73--86},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2017.04.001},
  abstract = {Environmental modellers recurrently work with decisions where a portfolio of actions has to be formed to effectively address the overall situation at hand. When creating the portfolio, one needs to consider multiple objectives and constraints, identify promising action candidates and examine interactions among them. The area of portfolio decision analysis deals with such tasks. This paper reviews portfolio modelling approaches and software that are applicable in environmental management. A framework for environmental portfolio decision analysis is provided that consists of steps ranging from problem framing to modelling and optimization, as well as to the analysis of results. The use of this framework is demonstrated with an illustrative case describing planning of urban water services. The problem is analyzed with a recently introduced portfolio decision analysis method called Robust Portfolio Modelling, which enables the use of incomplete preference information and consequence data. This feature can be particularly useful in environmental applications.},
  langid = {english},
  keywords = {bdpg,portfolio analysis},
  file = {/Users/bill/D/Zotero/storage/XNUAM4MU/lahtinen et al 2017 - Portfolio decision analysis methods in environmental decision making - BDPG - PORTFOLIO ANALYSIS.pdf}
}

@article{laiCorrigendumTrophicOverlapbased2016,
  title = {Corrigendum to ``{{A}} Trophic Overlap-Based Measure for Species Uniqueness in Ecological Networks'' [{{Ecol}}. {{Model}}. 299 ({{March}}) (2015) 95\textendash 101]},
  author = {Lai, Shu-mei and Liu, Wei-chung and Jord{\'a}n, Ferenc},
  year = {2016},
  month = sep,
  journal = {Ecological Modelling},
  volume = {336},
  pages = {82},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2016.04.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WNF3IK2F/1-s2.0-S0304380016301004-main.pdf}
}

@article{laiTrophicOverlapbasedMeasure2015,
  title = {A Trophic Overlap-Based Measure for Species Uniqueness in Ecological Networks},
  author = {Lai, Shu-mei and Liu, Wei-chung and Jord{\'a}n, Ferenc},
  year = {2015},
  month = mar,
  journal = {Ecological Modelling},
  volume = {299},
  pages = {95--101},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.12.014},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WACEYM4R/1-s2.0-S0304380014006206-main.pdf}
}

@article{langfordBenchmarksHiddenOptimum,
  title = {Benchmarks with {{Hidden Optimum Solutions}} for {{Set Covering}}, {{Set Packing}} and {{Winner Determination}}},
  author = {Langford, Bill},
  pages = {3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TB9KFQWS/xu web page good - Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination- httpCOLON__www.nlsde.buaa.edu.cn_TILDAkexu_benchmarks_set-benchmarks.htm.pdf}
}

@article{langfordBipartisanWayImprove,
  title = {A {{Bipartisan Way}} to {{Improve Medical Care}} - {{The New Yorker}}},
  author = {Langford, Bill},
  pages = {5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QJKFBT45/new yorker 2017 - A Bipartisan Way to Improve Medical Care - ECONOMICS - HEALTHCARE - OPTISEVIL.pdf}
}

@article{langfordCanWeBetter,
  title = {Can {{We}} Do {{Better}} than {{R-squared}}? | {{Tom Hopper}}},
  author = {Langford, Bill},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4R5L87S9/hopper 2014 - Can We do Better than R-squared - blog entry - very useful - BIODIVPROBGEN - EFs.pdf}
}

@article{langfordGreatDivideEconomists,
  title = {The Great Divide: {{Economists}} versus the Markets | {{The Economist}}},
  author = {Langford, Bill},
  pages = {3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W29RR9PR/The great divide - Economists versus the markets - The Economist - OPTISEVIL.pdf}
}

@article{langfordMapMisclassificationCan2006,
  title = {Map {{Misclassification Can Cause Large Errors}} in {{Landscape Pattern Indices}}: {{Examples}} from {{Habitat Fragmentation}}},
  shorttitle = {Map {{Misclassification Can Cause Large Errors}} in {{Landscape Pattern Indices}}},
  author = {Langford, William T. and Gergel, Sarah E. and Dietterich, Thomas G. and Cohen, Warren},
  year = {2006},
  month = apr,
  journal = {Ecosystems},
  volume = {9},
  number = {3},
  pages = {474--488},
  issn = {1432-9840, 1435-0629},
  doi = {10.1007/s10021-005-0119-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NX4TEVM3/langford et al. - 2006 - Map Misclassification Can Cause Large Errors in Landscape Pattern Indices Examples from Habitat Fragmentation - Ecosystems.PDF}
}

@article{langfordNoFreeLunch,
  title = {No {{Free Lunch Theorems}}},
  author = {Langford, Bill},
  pages = {7},
  langid = {english},
  keywords = {NFL theorems},
  file = {/Users/bill/D/Zotero/storage/CFP5RPW3/No Free Lunch Theorems - good NFL web page - www.no-free-lunch.org.pdf}
}

@article{langfordRaisingBarSystematic2011,
  title = {Raising the Bar for Systematic Conservation Planning},
  author = {Langford, William T. and Gordon, Ascelin and Bastin, Lucy and Bekessy, Sarah A. and White, Matt D. and Newell, Graeme},
  year = {2011},
  month = dec,
  journal = {Trends in Ecology \& Evolution},
  volume = {26},
  number = {12},
  pages = {634--640},
  issn = {01695347},
  doi = {10.1016/j.tree.2011.08.001},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FS25UHGK/langford et al 2011 - raising the bar for systematic conservation planning - final published copy.pdf}
}

@article{langfordTroubleGDPEconomist,
  title = {The Trouble with {{GDP}} | {{The Economist}}},
  author = {Langford, Bill},
  pages = {9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DYHQC8SR/economist 2016 - the trouble with gdp - main article - version from Print in magazine - does not include comments - ECONOMICS - EF - OPTISEVIL.pdf}
}

@article{langfordWhenConservationPlanning2009,
  title = {When Do Conservation Planning Methods Deliver? {{Quantifying}} the Consequences of Uncertainty},
  shorttitle = {When Do Conservation Planning Methods Deliver?},
  author = {Langford, William T. and Gordon, Ascelin and Bastin, Lucy},
  year = {2009},
  month = aug,
  journal = {Ecological Informatics},
  volume = {4},
  number = {3},
  pages = {123--135},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2009.04.002},
  abstract = {The rapid global loss of biodiversity has led to a proliferation of systematic conservation planning methods. In spite of their utility and mathematical sophistication, these methods only provide approximate solutions to realworld problems where there is uncertainty and temporal change. The consequences of errors in these solutions are seldom characterized or addressed. We propose a conceptual structure for exploring the consequences of input uncertainty and oversimplified approximations to real-world processes for any conservation planning tool or strategy. We then present a computational framework based on this structure to quantitatively model species representation and persistence outcomes across a range of uncertainties. These include factors such as land costs, landscape structure, species composition and distribution, and temporal changes in habitat. We demonstrate the utility of the framework using several reserve selection methods including simple rules of thumb and more sophisticated tools such as Marxan and Zonation. We present new results showing how outcomes can be strongly affected by variation in problem characteristics that are seldom compared across multiple studies. These characteristics include number of species prioritized, distribution of species richness and rarity, and uncertainties in the amount and quality of habitat patches. We also demonstrate how the framework allows comparisons between conservation planning strategies and their response to error under a range of conditions. Using the approach presented here will improve conservation outcomes and resource allocation by making it easier to predict and quantify the consequences of many different uncertainties and assumptions simultaneously. Our results show that without more rigorously generalizable results, it is very difficult to predict the amount of error in any conservation plan. These results imply the need for standard practice to include evaluating the effects of multiple real-world complications on the behavior of any conservation planning method.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7MTYHKPF/langford, Gordon, Bastin - 2009 - When do conservation planning methods deliver Quantifying the consequences of uncertainty - Ecological Informatics.PDF}
}

@article{langfordWhenNotEnough,
  title = {When 2\% Is Not Enough | {{The Economist}}},
  author = {Langford, Bill},
  pages = {3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YSZNCYF3/2014 08 27 - When 2 percent is not enough - The Economist - EFs - OPTISEVIL.pdf}
}

@article{lariviereImpactFactorMatthew2009,
  title = {The Impact Factor's {{Matthew Effect}}: {{A}} Natural Experiment in Bibliometrics},
  shorttitle = {The Impact Factor's {{Matthew Effect}}},
  author = {Larivi{\`e}re, Vincent and Gingras, Yves},
  year = {2009},
  journal = {Journal of the American Society for Information Science and Technology},
  pages = {n/a-n/a},
  issn = {15322882, 15322890},
  doi = {10.1002/asi.21232},
  abstract = {Since the publication of Robert K. Merton's theory of cumulative advantage in science (Matthew Effect), several empirical studies have tried to measure its presence at the level of papers, individual researchers, institutions or countries. However, these studies seldom control for the intrinsic ``quality'' of papers or of researchers\textemdash ``better'' (however defined) papers or researchers could receive higher citation rates because they are indeed of better quality. Using an original method for controlling the intrinsic value of papers\textemdash identical duplicate papers published in different journals with different impact factors\textemdash this paper shows that the journal in which papers are published have a strong influence on their citation rates, as duplicate papers published in high impact journals obtain, on average, twice as much citations as their identical counterparts published in journals with lower impact factors. The intrinsic value of a paper is thus not the only reason a given paper gets cited or not; there is a specific Matthew effect attached to journals and this gives to paper published there an added value over and above their intrinsic quality.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/85RVWB2G/0908.3177.pdf}
}

@article{larocqueIntegratedModellingSoftware2015,
  title = {Integrated Modelling Software Platform Development for Effective Use of Ecosystem Models},
  author = {Larocque, Guy R. and Bhatti, Jagtar and Arsenault, Andr{\'e}},
  year = {2015},
  month = jun,
  journal = {Ecological Modelling},
  volume = {306},
  pages = {318--325},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.08.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TWRJ3IRW/1-s2.0-S0304380014003809-main.pdf}
}

@article{lawlerRareSpeciesUse2003,
  title = {Rare {{Species}} and the {{Use}} of {{Indicator Groups}} for {{Conservation Planning}}},
  author = {Lawler, Joshua J. and White, Denis and Sifneos, Jean C. and Master, Lawrence L.},
  year = {2003},
  month = jun,
  journal = {Conservation Biology},
  volume = {17},
  number = {3},
  pages = {875--882},
  issn = {0888-8892, 1523-1739},
  doi = {10.1046/j.1523-1739.2003.01638.x},
  abstract = {Indicators of biodiversity have been proposed as a potential tool for selecting areas for conservation when information about species distributions is scarce. Although tests of the concept have produced varied results, sites selected to address indicator groups can include a high proportion of other species. We tested the hypothesis that species at risk of extinction are not likely to be included in sites selected to protect indicator groups. Using a reserve-selection approach, we compared the ability of seven indicator groups\textemdash freshwater fish, birds, mammals, freshwater mussels, reptiles, amphibians, and at-risk species of those six taxa\textemdash to provide protection for other species in general and at-risk species in particular in the Middle Atlantic region of the United States. Although sites selected with single taxonomic indicator groups provided protection for between 61\% and 82\% of all other species, no taxonomic group provided protection for more than 58\% of all other at-risk species. The failure to cover at-risk species is likely linked to their rarity. By examining the relationship between a species' probability of coverage by each indicator group and the extent of its geographic range within the study area, we found that species with more restricted ranges were less likely to be protected than more widespread species. Furthermore, we found that although sites selected with indicator groups composed primarily of terrestrial species (birds and mammals) included relatively high percentages of those species (82\textendash 85\%) they included smaller percentages of strictly aquatic species (27\textendash 55\%). Finally, of both importance and possible utility, we found that at-risk species themselves performed well as an indicator group, covering an average of 84\% of all other species.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/H55DIP76/Indicators.pdf}
}

@article{lechnerAreLandscapeEcologists2012,
  title = {Are Landscape Ecologists Addressing Uncertainty in Their Remote Sensing Data?},
  author = {Lechner, Alex M. and Langford, William T. and Bekessy, Sarah A. and Jones, Simon D.},
  year = {2012},
  month = nov,
  journal = {Landscape Ecology},
  volume = {27},
  number = {9},
  pages = {1249--1261},
  issn = {0921-2973, 1572-9761},
  doi = {10.1007/s10980-012-9791-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/J289F36S/Lechner_2012_Are_landscape_ecologists_addressing_uncertainty_in_their_remote_sensing_data.pdf}
}

@article{lechnerInvestigatingSpeciesEnvironment2012,
  title = {Investigating Species\textendash Environment Relationships at Multiple Scales: {{Differentiating}} between Intrinsic Scale and the Modifiable Areal Unit Problem},
  shorttitle = {Investigating Species\textendash Environment Relationships at Multiple Scales},
  author = {Lechner, Alex M. and Langford, William T. and Jones, Simon D. and Bekessy, Sarah A. and Gordon, Ascelin},
  year = {2012},
  month = sep,
  journal = {Ecological Complexity},
  volume = {11},
  pages = {91--102},
  issn = {1476945X},
  doi = {10.1016/j.ecocom.2012.04.002},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9V4ACC63/lechner langford et al 2012 - Investigating speciesâenvironment relationships at multiple scales - Differentiating between intrinsic scale and the modifiable areal unit problem - in press corrected proof.pdf}
}

@article{lehmanAbandoningObjectivesEvolution2011,
  title = {Abandoning {{Objectives}}: {{Evolution Through}} the {{Search}} for {{Novelty Alone}}},
  shorttitle = {Abandoning {{Objectives}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2011},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {19},
  number = {2},
  pages = {189--223},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00025},
  abstract = {In evolutionary computation, the fitness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YRT6C89K/lehman stanley 2011 - abandoning objectives - evolution through the search for novelty alone - OPTISEVIL.pdf}
}

@inproceedings{lehmanEfficientlyEvolvingPrograms2010,
  title = {Efficiently Evolving Programs through the Search for Novelty},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '10},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2010},
  pages = {837},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/1830483.1830638},
  abstract = {A significant challenge in genetic programming is premature convergence to local optima, which often prevents evolution from solving problems. This paper introduces to genetic programming a method that originated in neuroevolution (i.e. the evolution of artificial neural networks) that circumvents the problem of deceptive local optima. The main idea is to search only for behavioral novelty instead of for higher fitness values. Although such novelty search abandons following the gradient of the fitness function, if such gradients are deceptive they may actually occlude paths through the search space towards the objective. Because there are only so many ways to behave, the search for behavioral novelty is often computationally feasible and differs significantly from random search. Counterintuitively, in both a deceptive maze navigation task and the artificial ant benchmark task, genetic programming with novelty search, which ignores the objective, outperforms traditional genetic programming that directly searches for optimal behavior. Additionally, novelty search evolves smaller program trees in every variation of the test domains. Novelty search thus appears less susceptible to bloat, another significant problem in genetic programming. The conclusion is that novelty search is a viable new tool for efficiently solving some deceptive problems in genetic programming.},
  isbn = {978-1-4503-0072-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZFPNYXUE/lehman stanley 2010 - efficiently evolving programs through the search for novelty - OPTISEVIL.pdf}
}

@article{lehmanEncouragingReactivityCreate2013,
  title = {Encouraging Reactivity to Create Robust Machines},
  author = {Lehman, Joel and Risi, Sebastian and D'Ambrosio, David and O Stanley, Kenneth},
  year = {2013},
  month = dec,
  journal = {Adaptive Behavior},
  volume = {21},
  number = {6},
  pages = {484--500},
  issn = {1059-7123, 1741-2633},
  doi = {10.1177/1059712313487390},
  abstract = {The robustness of animal behavior is unmatched by current machines, which often falter when exposed to unforeseen conditions. While animals are notably reactive to changes in their environment, machines often follow finely-tuned yet inflexible plans. Thus instead of the traditional approach of training such machines over many di{$\carriagereturn$}erent unpredictable scenarios in detailed simulations (which is the most intuitive approach to inducing robustness), this work proposes to train machines to be reactive to their environment. The idea is that robustness may result not from detailed internal models or finely-tuned control policies but from cautious exploratory behavior. Supporting this hypothesis, robots trained to navigate mazes with a reactive disposition prove more robust than those trained over many trials yet not rewarded for reactive behavior in both simulated tests and when embodied in real robots. The conclusion is that robustness may neither require an accurate model nor finely calibrated behavior.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XMTVLLS9/lehman et al 2013 - encouraging reactivity to create robust machines - OPTISEVIL.pdf}
}

@inproceedings{lehmannComputingMarkets2005,
  title = {Computing and {{Markets}}},
  booktitle = {Dagstuhl {{Seminar Proceedings}} 05011 {{Computing}} and {{Markets}}},
  author = {Lehmann, Daniel and Muller, Rudolf and Sandholm, Tuomas},
  year = {2005},
  abstract = {The seminar Computing and Markets facilitated a very fruit- ful interaction between economists and computer scientists, which intensified the understanding of the other disciplines' tool sets. The seminar helped to pave the way to a unified theory of markets that takes into account both the economic and the computational issues\textemdash and their deep interaction.},
  file = {/Users/bill/D/Zotero/storage/QQSIZAVJ/05011.Summary.proceedings_intro.224.pdf}
}

@inproceedings{lehmanOpenendednessQuantifyingImpressiveness2012,
  title = {Beyond {{Open-endedness}}: {{Quantifying Impressiveness}}},
  shorttitle = {Beyond {{Open-endedness}}},
  booktitle = {Artificial {{Life}} 13},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2012},
  month = jul,
  pages = {75--82},
  publisher = {{MIT Press}},
  doi = {10.7551/978-0-262-31050-5-ch011},
  abstract = {This paper seeks to illuminate and quantify a feature of natural evolution that correlates to our sense of its intuitive greatness: Natural evolution evolves impressive artifacts. Within artificial life, abstractions aiming to capture what makes natural evolution so powerful often focus on the idea of openendedness, which relates to boundless diversity, complexity, or adaptation. However, creative systems that have passed tests of open-endedness raise the possibility that openendedness does not always correlate to impressiveness in artificial life simulations. In other words, while natural evolution is both open-ended and demonstrates a drive towards evolving impressive artifacts, it may be a mistake to assume the two properties are always linked. Thus to begin to investigate impressiveness independently in artificial systems, a novel definition is proposed: Impressive artifacts readily exhibit significant design effort. That is, the difficulty of creating them is easy to recognize. Two heuristics, rarity and re-creation effort, are derived from this definition and applied to the products of an open-ended image evolution system. An important result is that that the heuristics intuitively separate different reward schemes and provide evidence for why each evolved picture is or is not impressive. The conclusion is that impressiveness may help to distinguish open-ended systems and their products, and potentially untangles an aspect of natural evolution's mystique that is masked by its co-occurrence with open-endedness.},
  isbn = {978-0-262-31050-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PTJR76ZY/lehman stanley 2012 - Beyond Open-endedness - Quantifying Impressiveness - OPTISEVIL.pdf}
}

@inproceedings{lehmanRewardingReactivityEvolve2012,
  title = {Rewarding {{Reactivity}} to {{Evolve Robust Controllers}} without {{Multiple Trials}} or {{Noise}}},
  booktitle = {Artificial {{Life}} 13},
  author = {Lehman, Joel and Risi, Sebastian and D'Ambrosio, David B. and Stanley, Kenneth O.},
  year = {2012},
  month = jul,
  pages = {379--386},
  publisher = {{MIT Press}},
  doi = {10.7551/978-0-262-31050-5-ch050},
  abstract = {Behaviors evolved in simulation are often not robust to variations of their original training environment. Thus often researchers must train explicitly to encourage such robustness. Traditional methods of training for robustness typically apply multiple non-deterministic evaluations with carefully modeled noisy distributions for sensors and effectors. In practice, such training is often computationally expensive and requires crafting accurate models. Taking inspiration from nature, where animals react appropriately to encountered stimuli, this paper introduces a measure called reactivity, i.e. the tendency to seek and react to changes in environmental input, that is applicable in single deterministic trials and can encourage robustness without exposure to noise. The measure is tested in four different maze navigation tasks, where training with reactivity proves more robust than training without noise, and equally or more robust than training with noise when testing with moderate noise levels. In this way, the results demonstrate the counterintuitive fact that sometimes training with no exposure to noise at all can evolve individuals significantly more robust to noise than by explicitly training with noise. The conclusion is that training for reactivity may often be a computationally more efficient means to encouraging robustness in evolved behaviors.},
  isbn = {978-0-262-31050-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/M8B4RKRC/lehman et al 2012 - rewarding reactivity to evolve robust controllers without multiple trials or noise - OPTISEVIL.pdf}
}

@article{lepczykOntologyLandscapes2008,
  title = {An Ontology for Landscapes},
  author = {Lepczyk, Christopher A. and Lortie, Christopher J. and Anderson, Laurel J.},
  year = {2008},
  month = sep,
  journal = {Ecological Complexity},
  volume = {5},
  number = {3},
  pages = {272--279},
  issn = {1476945X},
  doi = {10.1016/j.ecocom.2008.04.001},
  abstract = {As ecological data increases in breadth, depth, and complexity, the discipline of ecology is increasingly influenced by information science. While this influence provides many opportunities for ecologists, it also necessitates a change in how we manage and share data, and perhaps more fundamentally, define concepts in ecology. Specifically, the information technology process of automated data integration entirely depends upon consistent concept definition. A common tool used in computer science and engineering to specify meanings, which is both novel and offers significant potential to ecology, is an ontology. An ontology is a formal representation of knowledge in which concepts are described by their meaning and their relationship to each other. Ontologies are a tool that can be used to `explicitly specify a concept' (Gruber, 1993) and this approach is uncommon in ecology. In this paper, we develop an ontology for the concept of `landscape' that captures the most general definitions and usages of this term. We selected the concept of landscape because it is often used in very different ways by investigators and hence generates linguistic uncertainty. A graphic theoretic (i.e., visual) model is provided which describes the set of structuring rules we used to define the relationships between `landscape' and appropriately related terms. Based upon these rules, a landscape necessarily contains a spatial component (i.e., area), structure and function (i.e., ecosystems), and is scale independent. This approach provides the set of necessary conditions for landscape studies to reduce linguistic uncertainty, and facilitate interoperability of data, i.e., in a manner that promotes data linkages and quantitative synthesis particularly by automatic data synthesis programs that are likely to become an important part of ecology in the future. Simply put, we use an ontology, a technique novel to ecology but not other disciplines, to define `landscape,' thereby clearly delineating one subset of its potential general usage. As such this ontology can serve as both a checklist for landscape studies and a blueprint for additional ecological ontologies.},
  langid = {english},
  keywords = {bdpg,landscape ecology,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/C7N8L4B5/ontology for landscapes.pdf}
}

@article{lerouxAccountingSystemDynamics2007,
  title = {Accounting for {{System Dynamics}} in {{Reserve Design}}},
  author = {Leroux, Shawn J. and Schmiegelow, Fiona K. A. and Cumming, Steve G. and Lessard, Robert B. and Nagy, John},
  year = {2007},
  journal = {Ecological Applications},
  volume = {17},
  number = {7},
  pages = {1954--1966},
  abstract = {Systematic conservation plans have only recently considered the dynamic nature of ecosystems. Methods have been developed to incorporate climate change, population dynamics, and uncertainty in reserve design, but few studies have examined how to account for natural disturbance. Considering natural disturbance in reserve design may be especially important for the world's remaining intact areas, which still experience active natural disturbance regimes. We developed a spatially explicit, dynamic simulation model, CONSERV, which simulates patch dynamics and fire, and used it to evaluate the efficacy of hypothetical reserve networks in northern Canada. We designed six networks based on conventional reserve design methods, with different conservation targets for woodland caribou habitat, high-quality wetlands, vegetation, water bodies, and relative connectedness. We input the six reservenetworks into CONSERV and tracked the ability of each to maintain initial conservation targets through time under an active natural disturbance regime. None of the reservenetworks maintained all initial targets, and some over-representedcertain features, suggesting that both effectiveness and efficiency of reserve design could be improved through use of spatially explicit dynamic simulation during the planning process. Spatial simulation models of landscape dynamics are commonly used in natural resource management, but we provide the first illustration of their potential use for reservedesign. Spatial simulation models could be used iteratively to evaluate competing reserve designs and select targets that have a higher likelihood of being maintained through time. Such models could be combined with dynamic planning techniques to develop a general theory for reserve design in an uncertain world.},
  langid = {english},
  keywords = {bdpg,reserve selection},
  file = {/Users/bill/D/Zotero/storage/U3VVTBJS/leroux et al 2007 - accounting for system dynamics in reserve design - ecoapps.pdf}
}

@article{lesageIntroductionSpatialEconometrics2009,
  title = {Introduction to {{Spatial Econometrics}}},
  author = {LeSage, James and Pace, R Kelley},
  year = {2009},
  pages = {331},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F6A2VEY5/lesage pace 2009 - introduction to spatial econometrics - ECONOMICS - ECONOMETRICS - SPATIAL - BOOK.pdf}
}

@article{lesageWhatRegionalScientists2014,
  title = {What {{Regional Scientists Need}} to {{Know About Spatial Econometrics}}},
  author = {LeSage, James P.},
  year = {2014},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2420725},
  abstract = {Regional scientists frequently work with regression relationships involving sample data that is spatial in nature. For example, hedonic house-price regressions relate selling prices of houses located at points in space to characteristics of the homes as well as neighborhood characteristics. Migration, commodity, and transportation flow models relate the size flows between origin and destination regions to the distance between origin and destination as well as characteristics of both origin and destination regions. Regional growth regressions relate growth rates of a region to past period ownand nearby-region resource inputs used in production.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2WHZKUI3/Lesage2014.pdf}
}

@misc{levy-carcienteSimulatingBarterFinancial2005,
  title = {Simulating {{Barter}} and {{Financial Economy}}: (401442008-001)},
  shorttitle = {Simulating {{Barter}} and {{Financial Economy}}},
  author = {{Levy-Carciente}, Sary and Jaffe, Klaus},
  year = {2005},
  publisher = {{American Psychological Association}},
  doi = {10.1037/e401442008-001},
  abstract = {Inspired by Adam Smith and Friedrich Hayek, many economists have postulated the existence of invisible forces that drive economic markets. These market forces interact in complex ways making it difficult to visualize or understand the interactions in every detail. Here I show how these forces can transcend a zero-sum game and become a win-win business interaction, thanks to emergent social synergies triggered by division of labor. Computer simulations with the model Sociodynamica show here the detailed dynamics underlying this phenomenon in a simple virtual economy. In these simulations, independent agents act in an economy exploiting and trading two different goods in a heterogeneous environment. All and each of the various forces and individuals were tracked continuously, allowing to unveil a synergistic effect on economic output produced by the division of labor between agents. Running simulations in a homogeneous environment, for example, eliminated all benefits of division of labor. The simulations showed that the synergies unleashed by division of labor arise if: Economies work in a heterogeneous environment; agents engage in complementary activities whose optimization processes diverge; agents have means to synchronize their activities. This insight, although trivial if viewed a posteriori, improve our understanding of the source and nature of synergies in real economic markets and might render economic and natural sciences more consilient.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VPYI9KGV/jaffe 2015 - Agent based simulations visualize Adam Smith's invisible hand by solving Friedrich Hayek's Economic Calculus - OPTISEVIL - ECONOMICS - AGENT-BASED.pdf}
}

@article{leymanHowCanWe,
  title = {How Can We Do Worthwhile Metaheuristic Research?},
  author = {Leyman, Pieter and Causmaecker, Patrick De},
  pages = {2},
  abstract = {We present a metaheuristic development framework, to deal with problems typical of the design of new metaheuristic implementations. We discuss the framework with a component-based view in mind, meaning that we believe added value can be created by doing research on operators instead of some ``novel'' metaheuristic framework. With our framework, we hope to make it easier for researchers to determine where the added value of a speci c metaheuristic implementation comes from.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SIS8E77X/Abstract_ORBEL33_PieterLeyman.pdf}
}

@incollection{leyton-brownBoostingMetaphorAlgorithm2003,
  title = {Boosting as a {{Metaphor}} for {{Algorithm Design}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} \textendash{} {{CP}} 2003},
  author = {{Leyton-Brown}, Kevin and Nudelman, Eugene and Andrew, Galen and McFadden, Jim and Shoham, Yoav},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Rossi, Francesca},
  year = {2003},
  volume = {2833},
  pages = {899--903},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45193-8_75},
  isbn = {978-3-540-20202-8 978-3-540-45193-8},
  langid = {english},
  keywords = {guppy,problem difficulty,problem difficulty guppy},
  file = {/Users/bill/D/Zotero/storage/PM4SY2V2/leyton-brown et al 2003 - Boosting as a Metaphor for Algorithm Design.pdf}
}

@article{leyton-brownEmpiricalHardnessModels2009,
  title = {Empirical Hardness Models: {{Methodology}} and a Case Study on Combinatorial Auctions},
  shorttitle = {Empirical Hardness Models},
  author = {{Leyton-Brown}, Kevin and Nudelman, Eugene and Shoham, Yoav},
  year = {2009},
  month = jun,
  journal = {Journal of the ACM},
  volume = {56},
  number = {4},
  pages = {1--52},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/1538902.1538906},
  abstract = {Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/C7SU63L6/a22-leyton-brown.pdf}
}

@article{leyton-brownIncentiveMechanismsSmoothing2003,
  title = {Incentive Mechanisms for Smoothing out a Focused Demand for Network Resources},
  author = {{Leyton-Brown}, Kevin and Porter, Ryan and Prabhakar, Balaji and Shoham, Yoav and Venkataraman, Shobha},
  year = {2003},
  month = feb,
  journal = {Computer Communications},
  volume = {26},
  number = {3},
  pages = {237--250},
  issn = {01403664},
  doi = {10.1016/S0140-3664(02)00139-1},
  abstract = {We explore the problem of sharing network resources when users' preferences lead to temporally concentrated loads, resulting in an inefficient use of the network. In such cases external incentives can be supplied to smooth out demand, obviating the need for expensive technological mechanisms. Taking a game-theoretic approach, we consider a setting in which bandwidth or access to service is available during different time slots at a fixed cost, but all agents have a natural preference for choosing the same time slot. We present four mechanisms that motivate users to distribute the load by probabilistically waiving the cost for each time slot, and analyze the equilibria that arise under these mechanisms.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/FIDGI83P/03incentivemechanisms.pdf}
}

@incollection{leyton-brownLearningEmpiricalHardness2002,
  title = {Learning the {{Empirical Hardness}} of {{Optimization Problems}}: {{The Case}} of {{Combinatorial Auctions}}},
  shorttitle = {Learning the {{Empirical Hardness}} of {{Optimization Problems}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} - {{CP}} 2002},
  author = {{Leyton-Brown}, Kevin and Nudelman, Eugene and Shoham, Yoav},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Van Hentenryck, Pascal},
  year = {2002},
  volume = {2470},
  pages = {556--572},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-46135-3_37},
  abstract = {We propose a new approach for understanding the algorithm-specific empirical hardness of N P-Hard problems. In this work we focus on the empirical hardness of the winner determination problem\textemdash an optimization problem arising in combinatorial auctions\textemdash when solved by ILOG's CPLEX software. We consider nine widely-used problem distributions and sample randomly from a continuum of parameter settings for each distribution. We identify a large number of distribution-nonspecific features of data instances and use statistical regression techniques to learn, evaluate and interpret a function from these features to the predicted hardness of an instance.},
  isbn = {978-3-540-44120-5 978-3-540-46135-7},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/AP37K6R7/leyton-brown et al 2002 - Learning the Empirical Hardness of Optimization Problems - The case of combinatorial auctions.pdf}
}

@article{leyton-brownLearningSpaceAlgorithm,
  title = {Learning in the {{Space}} of {{Algorithm Designs}}},
  author = {{Leyton-Brown}, Kevin and Hutter, Frank},
  pages = {201},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/J2992FSN/2019-ICML-Tutorial_on_Algorithm_Configuration.pdf}
}

@inproceedings{leyton-brownLocalEffectGames2005,
  title = {Local-{{Effect Games}}},
  booktitle = {Dagstuhl {{Seminar Proceedings}} 05011 {{Computing}} and {{Markets}}},
  author = {{Leyton-Brown}, Kevin and Tennenholtz, Moshe},
  year = {2005},
  abstract = {We present a new class of games, local-effect games (LEGs), which exploit structure in a different way from other compact game representations studied in AI. We show both theoretically and empirically that these games often (but not always) have pure-strategy Nash equilibria. Finding a potential function is a good technique for finding such equilibria. We give a complete characterization of which LEGs have potential functions and provide the functions in each case; we also show a general case where pure-strategy equilibria exist in the absence of potential functions. In experiments, we show that myopic best-response dynamics converge quickly to pure strategy equilibria in games not covered by our positive theoretical results.},
  file = {/Users/bill/D/Zotero/storage/7AHLDTED/05011.LeytonBrownKevin.Paper.219.pdf}
}

@article{leyton-brownMechanismDesignAuctions,
  title = {Mechanism {{Design}} and {{Auctions}}},
  author = {{Leyton-Brown}, Kevin},
  pages = {176},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/X5CB8R5C/Lecture-2_KLeyton-Brown.pdf}
}

@article{leyton-brownPortfolioApproachAlgorithm,
  title = {A {{Portfolio Approach}} to {{Algorithm Selection}}},
  author = {{Leyton-Brown}, Kevin and Nudelman, Eugene and Andrew, Galen and McFadden, Jim and Shoham, Yoav},
  pages = {2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/D8F8ZRHN/leyton-brown et al 2003 - A Portfolio Approach to Algorithm Selection - IJCAI03 - PROBLEM DIFFICULTY.pdf}
}

@article{leyton-brownUnderstandingEmpiricalHardness2014,
  title = {Understanding the Empirical Hardness of {{{\emph{NP}}}} -Complete Problems},
  author = {{Leyton-Brown}, Kevin and Hoos, Holger H. and Hutter, Frank and Xu, Lin},
  year = {2014},
  month = may,
  journal = {Communications of the ACM},
  volume = {57},
  number = {5},
  pages = {98--107},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2594413.2594424},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/YPXIA7QG/2014-CACM-EHMs.pdf}
}

@inproceedings{leyton-brownUniversalTestSuite2000,
  title = {Towards a Universal Test Suite for Combinatorial Auction Algorithms},
  booktitle = {Proceedings of the 2nd {{ACM}} Conference on {{Electronic}} Commerce  - {{EC}} '00},
  author = {{Leyton-Brown}, Kevin and Pearson, Mark and Shoham, Yoav},
  year = {2000},
  pages = {66--76},
  publisher = {{ACM Press}},
  address = {{Minneapolis, Minnesota, United States}},
  doi = {10.1145/352871.352879},
  abstract = {General combinatorial auctions\textemdash auctions in which bidders place unrestricted bids for bundles of goods\textemdash are the subject of increasing study. Much of this work has focused on algorithms for finding an optimal or approximately optimal set of winning bids. Comparatively little attention has been paid to methodical evaluation and comparison of these algorithms. In particular, there has not been a systematic discussion of appropriate data sets that can serve as universally accepted and well motivated benchmarks. In this paper we present a suite of distribution families for generating realistic, economically motivated combinatorial bids in five broad real-world domains. We hope that this work will yield many comments, criticisms and extensions, bringing the community closer to a universal combinatorial auction test suite.},
  isbn = {978-1-58113-272-4},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/MQNKTZVI/p66-leyton-brown.pdf}
}

@article{li2013m&m,
  title = {Variable-{{Centered Consistency}} in {{Model RB}}},
  author = {Li, Liang and Liu, Tian and Xu, Ke},
  year = {2013},
  month = mar,
  journal = {Minds and Machines},
  volume = {23},
  number = {1},
  pages = {95--103},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-012-9270-6},
  abstract = {Model RB is a model of random constraint satisfaction problems, which exhibits exact satisfiability phase transition and many hard instances, both experimentally and theoretically. Benchmarks based on Model RB have been successfully used by various international algorithm competitions and many research papers. In a previous work, Xu and Li defined two notions called i-constraint assignment tuple and flawed i-constraint assignment tuple to show an exponential resolution complexity of Model RB. These two notions are similar to some kind of consistency in constraint satisfaction problems, but seem different from all kinds of consistency so far known in literatures. In this paper, we explicitly define this kind of consistency, called variable-centered consistency, and show an upper bound on a parameter in Model RB, such that up to this bound the typical instances of Model RB are variable-centered consistent.},
  langid = {english},
  keywords = {bdpg,CSP,model RB,phase transition,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/SIESVTI9/li liu xu 2013 - Variable-Centered Consistency in Model RB - BDPG.pdf}
}

@article{likhachevAnytimeSearchDynamic2008,
  title = {Anytime Search in Dynamic Graphs},
  author = {Likhachev, Maxim and Ferguson, Dave and Gordon, Geoff and Stentz, Anthony and Thrun, Sebastian},
  year = {2008},
  month = sep,
  journal = {Artificial Intelligence},
  volume = {172},
  number = {14},
  pages = {1613--1643},
  issn = {00043702},
  doi = {10.1016/j.artint.2007.11.009},
  abstract = {Agents operating in the real world often have limited time available for planning their next actions. Producing optimal plans is infeasible in these scenarios. Instead, agents must be satisfied with the best plans they can generate within the time available. One class of planners well-suited to this task are anytime planners, which quickly find an initial, highly suboptimal plan, and then improve this plan until time runs out.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V8TWP7KF/likhachev ... thrun 2008 - anytime search in dynamic graphs.pdf}
}

@article{liLearningClusterbasedStructure2010,
  title = {Learning Cluster-Based Structure to Solve Constraint Satisfaction Problems},
  author = {Li, Xingjian and Epstein, Susan L.},
  year = {2010},
  month = oct,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {60},
  number = {1-2},
  pages = {91--117},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-010-9212-z},
  abstract = {The hybrid search algorithm for constraint satisfaction problems described here first uses local search to detect crucial substructures and then applies that knowledge to solve the problem. This paper shows the difficulties encountered by traditional and state-of-the-art learning heuristics when these substructures are overlooked. It introduces a new algorithm, Foretell, to detect dense and tight substructures called clusters with local search. It also develops two ways to use clusters during global search: one supports variable-ordering heuristics and the other makes inferences adapted to them. Together they improve performance on both benchmark and real-world problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QFHH6SBU/li epstein 2010 - learning cluster-based structure to solve constraint satisfaction problems.pdf}
}

@article{liOneSolution3satisfiability2009,
  title = {From One Solution of a 3-Satisfiability Formula to a Solution Cluster: {{Frozen}} Variables and Entropy},
  shorttitle = {From One Solution of a 3-Satisfiability Formula to a Solution Cluster},
  author = {Li, Kang and Ma, Hui and Zhou, Haijun},
  year = {2009},
  month = mar,
  journal = {Physical Review E},
  volume = {79},
  number = {3},
  eprint = {0809.4332},
  eprinttype = {arxiv},
  pages = {031102},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.79.031102},
  abstract = {A solution to a 3-satisfiability (3-SAT) formula can be expanded into a cluster, all other solutions of which are reachable from this one through a sequence of single-spin flips. Some variables in the solution cluster are frozen to the same spin values by one of two different mechanisms: frozen-core formation and long-range frustrations. While frozen cores are identified by a local whitening algorithm, long-range frustrations are very difficult to trace, and they make an entropic belief-propagation (BP) algorithm fail to converge. For BP to reach a fixed point the spin values of a tiny fraction of variables (chosen according to the whitening algorithm) are externally fixed during the iteration. From the calculated entropy values, we infer that, for a large random 3-SAT formula with constraint density close to the satisfiability threshold, the solutions obtained by the survey-propagation or the walksat algorithm belong neither to the most dominating clusters of the formula nor to the most abundant clusters. This work indicates that a single solution cluster of a random 3-SAT formula may have further community structures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/Users/bill/D/Zotero/storage/AH5N4G9K/0809.4332.pdf}
}

@article{liouiOptimalBenchmarkingActive2013,
  title = {Optimal Benchmarking for Active Portfolio Managers},
  author = {Lioui, Abraham and Poncet, Patrice},
  year = {2013},
  month = apr,
  journal = {European Journal of Operational Research},
  volume = {226},
  number = {2},
  pages = {268--276},
  issn = {03772217},
  doi = {10.1016/j.ejor.2012.10.043},
  abstract = {Within an agency theoretic framework adapted to the portfolio delegation issue, we show how to construct optimal benchmarks. In accordance with US regulations, the benchmark-adjusted compensation scheme is taken to be symmetric. The investor's control consists in forcing the manager to adopt the appropriate benchmark so that his first-best optimum is attained. Solving simultaneously the manager's and the investor's dynamic optimization programs in a fairly general framework, we characterize the optimal benchmark. We then provide completely explicit solutions when the investor's and the manager's utility functions exhibit different CRRA parameters. We find that, even under optimal benchmarking, it is never optimal for the manager, and therefore for the investor, to follow exactly the benchmark, except in a very restrictive case. We finally assess by simulation the practical importance, in particular in terms of the investor's welfare, of selecting a sub-optimal benchmark.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KFLKPSPW/lioui poncet 2013 - Optimal benchmarking for active portfolio managers - FINANCE - BENCHMARKING.pdf}
}

@article{lipsonEmpiricallyEvaluatingMultiagent,
  title = {Empirically {{Evaluating Multiagent Reinforcement Learning Algorithms}}},
  author = {Lipson, Asher and {Leyton-Brown}, Kevin},
  pages = {40},
  abstract = {This article makes two contributions. First, we present a platform for running and analyzing multiagent reinforcement learning experiments. Second, to demonstrate this platform we undertook and evaluated an empirical test of multiagent reinforcement learning algorithms from the literature, which to our knowledge is the largest such test ever conducted. We summarize some conclusions from our experiments, comparing algorithms on a variety of metrics including reward, regret, convergence to a Nash equilibrium and behavior in self play1.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/MC2H3Q9Z/malt.pdf}
}

@article{liuMeasuringComparingAccuracy2011,
  title = {Measuring and Comparing the Accuracy of Species Distribution Models with Presence-Absence Data},
  author = {Liu, Canran and White, Matt and Newell, Graeme},
  year = {2011},
  month = apr,
  journal = {Ecography},
  volume = {34},
  number = {2},
  pages = {232--243},
  issn = {09067590},
  doi = {10.1111/j.1600-0587.2010.06354.x},
  langid = {english},
  keywords = {accuracy,EF,EF EFs,guppy,sdm},
  file = {/Users/bill/D/Zotero/storage/NL2QUW7P/liu white newell 2011 - Measuring and comparing the accuracy of species distribution models with presence absence data - GUPPY SDM EVALUATION.pdf}
}

@article{loiselleAvoidingPitfallsUsing2003,
  title = {Avoiding {{Pitfalls}} of {{Using Species Distribution Models}} in {{Conservation Planning}}},
  author = {Loiselle, Bette A. and Howell, Christine A. and Graham, Catherine H. and Goerck, Jaqueline M. and Brooks, Thomas and Smith, Kimberly G. and Williams, Paul H.},
  year = {2003},
  month = dec,
  journal = {Conservation Biology},
  volume = {17},
  number = {6},
  pages = {1591--1600},
  issn = {08888892, 15231739},
  doi = {10.1111/j.1523-1739.2003.00233.x},
  abstract = {Museum records have great potential to provide valuable insights into the vulnerability, historic distribution, and conservation of species, especially when coupled with species-distribution models used to predict species' ranges. Yet, the increasing dependence on species-distribution models in identifying conservation priorities calls for a more critical evaluation of model robustness. We used 11 bird species of conservation concern in Brazil's highly fragmented Atlantic Forest and data on environmental conditions in the region to predict species distributions. These predictions were repeated for five different model types for each of the 11 bird species. We then combined these species distributions for each model separately and applied a reserveselection algorithm to identify priority sites. We compared the potential outcomes from the reserve selection among the models. Although similarity in identification of conservation reserve networks occurred among models, models differed markedly in geographic scope and flexibility of reserve networks. It is essential for planners to evaluate the conservation implications of false-positive and false-negative errors for their specific management scenario before beginning the modeling process. Reserve networks selected by models that minimized false-positive errors provided a better match with priority areas identified by specialists. Thus, we urge caution in the use of models that overestimate species' occurrences because they may misdirect conservation action. Our approach further demonstrates the great potential value of museum records to biodiversity studies and the utility of species-distribution models to conservation decision-making. Our results also demonstrate, however, that these models must be applied critically and cautiously.},
  langid = {english},
  keywords = {bdpg,reserve selection,sdm,uncertainty},
  file = {/Users/bill/D/Zotero/storage/VVZA64LI/loiselle et al 2003 - avoiding pitfalls of using species distribution models in conservation planning - consbio.pdf}
}

@article{lonsingAnecdoteEvaluatingQBF,
  title = {An {{Anecdote}} on {{Evaluating QBF Solvers}} and {{Quantifier Alternations}}},
  author = {Lonsing, Florian and Egly, Uwe},
  pages = {5},
  abstract = {On the occasion of the 25th anniversary of the International Conference on Principles and Practice of Constraint Programming (CP), we are glad to present the history of our paper entitled Evaluating QBF Solvers: Quantifier Alternations Matter that was presented at CP 2018. Our paper was finally accepted at CP 2018 after an 18-month odyssey, where it was rejected three times (in different versions) from other top-tier conferences.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZYEKRI85/Lonsing-Egly-25CP-anniversary-commentary-2019.pdf}
}

@incollection{lopesPitfallsInstanceGeneration2010,
  title = {Pitfalls in {{Instance Generation}} for {{Udine Timetabling}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Lopes, Leo and {Smith-Miles}, Kate},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Blum, Christian and Battiti, Roberto},
  year = {2010},
  volume = {6073},
  pages = {299--302},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-13800-3_31},
  abstract = {In many randomly generated instances for Udine timetabling very different solvers achieved the same objective value (with different solutions). This paper summarises observations concerning the structure of the instances and their consequences for effective test instance generation and reporting of computational results in Udine Timetabling problems.},
  isbn = {978-3-642-13799-0 978-3-642-13800-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FSK93F3C/lopes smith-miles 2010 - pitfalls in instance generation for udine timetabling.pdf}
}

@article{lorenaDataComplexityMetafeatures2018,
  title = {Data Complexity Meta-Features for Regression Problems},
  author = {Lorena, Ana C. and Maciel, Aron I. and {de Miranda}, P{\'e}ricles B. C. and Costa, Ivan G. and Prud{\^e}ncio, Ricardo B. C.},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {209--246},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5681-1},
  abstract = {In meta-learning, classification problems can be described by a variety of features, including complexity measures. These measures allow capturing the complexity of the frontier that separates the classes. For regression problems, on the other hand, there is a lack of such type of measures. This paper presents and analyses measures devoted to estimate the complexity of the function that should fitted to the data in regression problems. As case studies, they are employed as meta-features in three meta-learning setups: (i) the first one predicts the regression function type of some synthetic datasets; (ii) the second one is designed to tune the parameter values of support vector regressors; and (iii) the third one aims to predict the performance of various regressors for a given dataset. The results show the suitability of the new measures to describe the regression datasets and their utility in the meta-learning tasks considered. In cases (ii) and (iii) the achieved results are also similar or better than those obtained by the use of classical meta-features in meta-learning.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JIJPSZU3/lorena et al 2018 - Data complexity meta-features for regression problems - BDPG.pdf}
}

@article{lorenaHowComplexYour2019,
  title = {How {{Complex Is Your Classification Problem}}?: {{A Survey}} on {{Measuring Classification Complexity}}},
  shorttitle = {How {{Complex Is Your Classification Problem}}?},
  author = {Lorena, Ana C. and Garcia, Lu{\'i}s P. F. and Lehmann, Jens and Souto, Marcilio C. P. and Ho, Tin Kam},
  year = {2019},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {5},
  pages = {1--34},
  issn = {03600300},
  doi = {10.1145/3347711},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IRP6CHIS/a107-lorena.pdf}
}

@inproceedings{louAccurateIntelligibleModels2013,
  title = {Accurate Intelligible Models with Pairwise Interactions},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '13},
  author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
  year = {2013},
  pages = {623},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/2487575.2487579},
  abstract = {Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions.},
  isbn = {978-1-4503-2174-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9VIWNAF8/10.1.1.352.7682.pdf}
}

@article{louConstructingAlternativeBenchmark2019,
  title = {On Constructing Alternative Benchmark Suite for Evolutionary Algorithms},
  author = {Lou, Yang and Yuen, Shiu Yin},
  year = {2019},
  month = feb,
  journal = {Swarm and Evolutionary Computation},
  volume = {44},
  pages = {287--292},
  issn = {22106502},
  doi = {10.1016/j.swevo.2018.04.005},
  abstract = {Benchmark testing offers performance measurement for an evolutionary algorithm before it is put into applications. In this paper, a systematic method to construct a benchmark test suite is proposed. A set of established algorithms are employed. For each algorithm, a uniquely easy problem instance is generated by evolution. The resulting instances consist of a novel benchmark test suite. Each problem instance is favorable (uniquely easy) to one algorithm only. A hierarchical fitness assignment method, which is based on statistical test results, is designed to generate uniquely easy (or hard) problem instances for an algorithm. Experimental results show that each algorithm performs the best robustly on its uniquely favorable problem. The testing results are repeatable. The distribution of algorithm performance in the suite is unbiased (or uniform), which mimics any subset of real-world problems that is uniformly distributed. The resulting suite offers 1) an alternative benchmark suite to evolutionary algorithms; 2) a novel method of accessing novel algorithms; and 3) meaningful training and testing problems for evolutionary algorithm selectors and portfolios.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DXM3S642/sec_2018.pdf}
}

@inbook{louEvolvingBenchmarkFunctions2019,
  title = {Evolving {{Benchmark Functions}} for {{Optimization Algorithms}}},
  booktitle = {From {{Parallel}} to {{Emergent Computing}}},
  author = {Lou, Yang and Yuen, Shiu Yin and Chen, Guanrong},
  year = {2019},
  month = mar,
  edition = {First},
  pages = {239--260},
  publisher = {{CRC Press}},
  address = {{Boca Raton, Florida : CRC Press, [2019] | Produced in celebration of the 25th anniversary of the International Journal of Parallel, Emergent, and Distributed Systems.}},
  doi = {10.1201/9781315167084-11},
  collaborator = {Adamatzky, Andrew and Akl, Selim G. and Sirakoulis, Georgios Ch.},
  isbn = {978-1-315-16708-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IVPH4A5J/chp11.pdf}
}

@inproceedings{louIntelligibleModelsClassification2012,
  title = {Intelligible Models for Classification and Regression},
  booktitle = {Proceedings of the 18th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '12},
  author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes},
  year = {2012},
  pages = {150},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/2339530.2339556},
  abstract = {Complex models for regression and classification have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users.},
  isbn = {978-1-4503-1462-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XIUZ7VCV/10.1.1.433.8241.pdf}
}

@article{lundyAllocationSocialGood,
  title = {Allocation for {{Social Good}}},
  author = {Lundy, Taylor},
  pages = {24},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KHRMLDYZ/2019-EC-Slides-AllocationForSocialGood.pdf}
}

@article{lutheIntroducingAdaptiveWaves2015,
  title = {Introducing Adaptive Waves as a Concept to Inform Mental Models of Resilience},
  author = {Luthe, Tobias and Wyss, Romano},
  year = {2015},
  month = oct,
  journal = {Sustainability Science},
  volume = {10},
  number = {4},
  pages = {673--685},
  issn = {1862-4065, 1862-4057},
  doi = {10.1007/s11625-015-0316-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/97WK5W3U/luthe wyss 2015 - introducing adaptive waves as a concept to inform mental models of resilience - RESILIENCE - OPTISEVIL.pdf}
}

@article{maciaUCIMindfulRepository2014,
  title = {Towards {{UCI}}+: {{A}} Mindful Repository Design},
  shorttitle = {Towards {{UCI}}+},
  author = {Maci{\`a}, N{\'u}ria and {Bernad{\'o}-Mansilla}, Ester},
  year = {2014},
  month = mar,
  journal = {Information Sciences},
  volume = {261},
  pages = {237--262},
  issn = {00200255},
  doi = {10.1016/j.ins.2013.08.059},
  abstract = {Public repositories have contributed to the maturation of experimental methodology in machine learning. Publicly available data sets have allowed researchers to empirically assess their learners and, jointly with open source machine learning software, they have favoured the emergence of comparative analyses of learners' performance over a common framework. These studies have brought standard procedures to evaluate machine learning techniques. However, current claims\textemdash such as the superiority of enhanced algorithms\textemdash are biased by unsustained assumptions made throughout some praxes.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YP5B3EWN/macia bernado-mansilla 2014 - towards uci plus - a mindful repository design - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{macleodHOWSPECIESABUNDANCE,
  title = {{{HOW DO SPECIES ABUNDANCE DISTRIBUTIONS INFLUENCE PLANT}} \textendash{} {{POLLINATOR NETWORKS}}?},
  author = {MacLEOD, MOLLY KATHERINE},
  pages = {94},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4JDA5E25/ETD-2015-6873.pdf}
}

@inproceedings{makarychevSortingNoisyData2013,
  title = {Sorting Noisy Data with Partial Information},
  booktitle = {Proceedings of the 4th Conference on {{Innovations}} in {{Theoretical Computer Science}} - {{ITCS}} '13},
  author = {Makarychev, Konstantin and Makarychev, Yury and Vijayaraghavan, Aravindan},
  year = {2013},
  pages = {515},
  publisher = {{ACM Press}},
  address = {{Berkeley, California, USA}},
  doi = {10.1145/2422436.2422492},
  abstract = {In this paper, we propose two semi-random models for the Minimum Feedback Arc Set Problem and present approximation algorithms for them. In the first model, which we call the Random Edge Flipping model, an instance is generated as follows. We start with an arbitrary acyclic directed graph and then randomly flip its edges (the adversary may later un-flip some of them). In the second model, which we call the Random Backward Edge model, again we start with an arbitrary acyclic graph but now add new random backward edges (the adversary may delete some of them). For the first model, we give an approximation algorithm that finds a solution of cost (1 + {$\delta$}) opt-cost +n polylog n, where opt-cost is the cost of the optimal solution. For the second model, we give an approximation algorithm that finds a solution of cost O(planted-cost)+n polylog n, where planted-cost is the cost of the planted solution.},
  isbn = {978-1-4503-1859-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NVJGYUWM/noisy_FAS.pdf}
}

@article{malanAlgorithmUsingSupport,
  title = {An Algorithm Using Support Vector Classification Based Constraint Approximations},
  author = {Malan, Maria Magdalena},
  pages = {101},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8IGCSS93/malan_particle_2019.pdf}
}

@article{mammola2021s,
  title = {Impact of the Reference List Features on the Number of Citations},
  author = {Mammola, Stefano and Fontaneto, Diego and Mart{\'i}nez, Alejandro and Chichorro, Filipe},
  year = {2021},
  month = jan,
  journal = {Scientometrics},
  volume = {126},
  number = {1},
  pages = {785--799},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-020-03759-0},
  abstract = {Many believe that the quality of a scientific publication is as good as the science it cites. However, quantifications of how features of reference lists affect citations remain sparse. We examined seven numerical characteristics of reference lists of 50,878 research articles published in 17 ecological journals between 1997 and 2017. Over this period, significant changes occurred in reference lists' features. On average, more recent papers have longer reference lists and cite more high Impact Factor papers and fewer non-journal publications. We also show that highly cited articles across the ecological literature have longer reference lists, cite more recent and impactful references, and include more self-citations. Conversely, the proportion of `classic' papers and non-journal publications cited, as well as the temporal span of the reference list, have no significant influence on articles' citations. From this analysis, we distill a recipe for crafting impactful reference lists, at least in ecology.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RBRM5WAP/mammola et al 2021 - Impact of the reference list features on the number of citations - ECOLOGY - BDPG.pdf}
}

@article{maPlantFunctionalDiversity2014,
  title = {Plant Functional Diversity in Agricultural Margins and Fallow Fields Varies with Landscape Complexity Level: {{Conservation}} Implications},
  shorttitle = {Plant Functional Diversity in Agricultural Margins and Fallow Fields Varies with Landscape Complexity Level},
  author = {Ma, Maohua and Herzon, Irina},
  year = {2014},
  month = dec,
  journal = {Journal for Nature Conservation},
  volume = {22},
  number = {6},
  pages = {525--531},
  issn = {16171381},
  doi = {10.1016/j.jnc.2014.08.006},
  abstract = {A consensus has been established that functional traits rather than taxonomic diversity play a fundamental role in linking biodiversity with ecosystem processes and associated services. This study from Finland addressed an issue of relative values of fallow and field margin biotopes in conservation of plant functional diversity (based on six functional traits of relevance to ecosystem services, and diversity of multiple traits) in agricultural landscapes differing in their structural complexity. Relative covers of plant species were surveyed in sampling plots located in perennial fallow fields and three types of perennial margins (margins between crop fields, along forest edges and by river) in three types of landscape context (simple, intermediate and complex). Fallow fields significantly contributed to the total functional diversity only in simple landscapes. The river margins provided the greatest functional diversity, especially in reproduction and regeneration traits while crop margins were consistently characterised by the lowest functional diversity. Substantial functional diversity of fallow patches in simple landscapes was due to high abundance of functional species, while that of river margins stemmed from presence of unique species. The plant functional diversity progressively declined with agricultural landscapes becoming simplified. The study indicates non-cropped biotopes having complementary roles in ensuring multifunctionality of agro-landscapes and confirms importance of biotope mosaic for functional diversity.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5FN8KJUT/1-s2.0-S1617138114000909-main.pdf}
}

@article{margules2000n,
  title = {Systematic Conservation Planning},
  author = {Margules, C R and Pressey, R L},
  year = {2000},
  journal = {Nature},
  volume = {405},
  pages = {243--253},
  doi = {10.1038/35012251},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QY49C6AN/margules pressey 2000 - systematic conservation planning - RESERVE SELECTION - SCP - BDPG.pdf}
}

@article{marshallTweakMakesNukes2017,
  title = {Tweak Makes {{U}}.{{S}}. Nukes More Precise\textemdash and Deadlier},
  author = {Marshall, Eliot},
  year = {2017},
  month = mar,
  journal = {Science},
  volume = {355},
  number = {6331},
  pages = {1252--1253},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.355.6331.1252},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JQKHPRM9/marshall 2017 - tweak makes US nukes more precise and deadlier - OPTISEVIL.pdf}
}

@article{matchettDetectingInfluenceRare2015,
  title = {Detecting the Influence of Rare Stressors on Rare Species in {{Yosemite National Park}} Using a Novel Stratified Permutation Test},
  author = {Matchett, J. R. and Stark, Philip B. and Ostoja, Steven M. and Knapp, Roland A. and McKenny, Heather C. and Brooks, Matthew L. and Langford, William T. and Joppa, Lucas N. and Berlow, Eric L.},
  year = {2015},
  month = sep,
  journal = {Scientific Reports},
  volume = {5},
  number = {1},
  pages = {10702},
  issn = {2045-2322},
  doi = {10.1038/srep10702},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NV2ZDMSY/matchett ... langford ... berlow 2015 - Detecting the influence of rare stressors on rare species in Yosemite National Park using a novel stratified permutation test - proofs.pdf}
}

@article{matthewsREVIEWSpeciesAbundance2015,
  title = {{{REVIEW}}: {{On}} the Species Abundance Distribution in Applied Ecology and Biodiversity Management},
  shorttitle = {{{REVIEW}}},
  author = {Matthews, Thomas J. and Whittaker, Robert J.},
  editor = {Fuller, Richard},
  year = {2015},
  month = apr,
  journal = {Journal of Applied Ecology},
  volume = {52},
  number = {2},
  pages = {443--454},
  issn = {00218901},
  doi = {10.1111/1365-2664.12380},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TEDMEWZN/matthews whittaker 2015 - On the species abundance distribution in applied ecology and biodiversity management - RANK ABUNDANCE DISTRIBUTION - BDPG.pdf}
}

@article{mccreesh2018j,
  title = {When {{Subgraph Isomorphism}} Is {{Really Hard}}, and {{Why This Matters}} for {{Graph Databases}}},
  author = {McCreesh, Ciaran and Prosser, Patrick and Solnon, Christine and Trimble, James},
  year = {2018},
  month = mar,
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {723--759},
  issn = {1076-9757},
  doi = {10.1613/jair.5768},
  abstract = {The subgraph isomorphism problem involves deciding whether a copy of a pattern graph occurs inside a larger target graph. The non-induced version allows extra edges in the target, whilst the induced version does not. Although both variants are NP-complete, algorithms inspired by constraint programming can operate comfortably on many real-world problem instances with thousands of vertices. However, they cannot handle arbitrary instances of this size. We show how to generate ``really hard'' random instances for subgraph isomorphism problems, which are computationally challenging with a couple of hundred vertices in the target, and only twenty pattern vertices. For the non-induced version of the problem, these instances lie on a satisfiable / unsatisfiable phase transition, whose location we can predict; for the induced variant, much richer behaviour is observed, and constrainedness gives a better measure of difficulty than does proximity to a phase transition. These results have practical consequences: we explain why the widely researched ``filter / verify'' indexing technique used in graph databases is founded upon a misunderstanding of the empirical hardness of NP-complete problems, and cannot be beneficial when paired with any reasonable subgraph isomorphism algorithm.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/695TEGR6/mccreesh et al 2018 - When Subgraph Isomorphism is Really Hard, and Why This Matters for Graph Databases.pdf}
}

@article{mccreeshSOLVINGHARDSUBGRAPH,
  title = {{{SOLVING HARD SUBGRAPH PROBLEMS IN PARALLEL}}},
  author = {Mccreesh, Ciaran},
  pages = {264},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XKFDHNFL/mccreesh 2017 - solving hard subgraph problems in paralle - THESIS.pdf}
}

@incollection{mccreeshUnderstandingEmpiricalHardness2019,
  title = {Understanding the {{Empirical Hardness}} of {{Random Optimisation Problems}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {McCreesh, Ciaran and Pettersson, William and Prosser, Patrick},
  editor = {Schiex, Thomas and {de Givry}, Simon},
  year = {2019},
  volume = {11802},
  pages = {333--349},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30048-7_20},
  abstract = {We look at the empirical complexity of the maximum clique problem, the graph colouring problem, and the maximum satisfiability problem, in randomly generated instances. Although each is NP-hard, we encounter exponential behaviour only with certain choices of instance generation parameters. To explain this, we link the difficulty of optimisation to the difficulty of a small number of decision problems, which are already better-understood through phenomena like phase transitions with associated complexity peaks. However, our results show that individual decision problems can interact in very different ways, leading to different behaviour for each optimisation problem. Finally, we uncover a conflict between anytime and overall behaviour in algorithm design, and discuss the implications for the design of experiments and of search strategies such as variable- and value-ordering heuristics.},
  isbn = {978-3-030-30047-0 978-3-030-30048-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NK99QMI3/190408.pdf}
}

@article{mcdonnellMathematicalMethodsSpatially,
  title = {Mathematical Methods for Spatially Cohesive Reserve Design},
  author = {McDonnell, Mark D and Possingham, Hugh P and Ball, Ian R and Cousins, Elizabeth A},
  pages = {8},
  abstract = {The problem of designing spatially cohesive nature reserve systems that meet biodiversity objectives is formulated as a nonlinear integer programming problem. The multiobjective function minimises a combination of boundary length, area and failed representation of the biological attributes we are trying to conserve. The task is to reserve a subset of sites that best meet this objective. We use data on the distribution of habitats in the Northern Territory, Australia, to show how simulated annealing and a greedy heuristic algorithm can be used to generate good solutions to such large reserve design problems, and to compare the effectiveness of these methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PX64DFYZ/McDonnell_JEMA02.pdf}
}

@article{mcgeoch1995ijc,
  title = {Toward an Experimental Method for Algorithm Simulation},
  author = {McGeoch, Catherine},
  year = {1995},
  journal = {INFORMS Journal on Computing},
  volume = {8},
  number = {1},
  keywords = {bdpg,benchmarking,optimization},
  file = {/Users/bill/D/Zotero/storage/PL6F926P/McGeoch 1995 - toward an experimental method for algorithm simulation.pdf}
}

@incollection{mcgeoch2002hogo,
  title = {Experimental Analysis of Algorithms},
  booktitle = {Handbook of Global Optimization},
  author = {McGeoch, Catherine C},
  year = {2002},
  volume = {2},
  pages = {489--513},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XBJ57FMA/mcgeogh 2002 - experimental analysis of algorithms - BOOK CHAPTER - BDPG.pdf}
}

@article{mcgillSpeciesAbundanceDistributions2007,
  title = {Species Abundance Distributions: Moving beyond Single Prediction Theories to Integration within an Ecological Framework},
  shorttitle = {Species Abundance Distributions},
  author = {McGill, Brian J. and Etienne, Rampal S. and Gray, John S. and Alonso, David and Anderson, Marti J. and Benecha, Habtamu Kassa and Dornelas, Maria and Enquist, Brian J. and Green, Jessica L. and He, Fangliang and Hurlbert, Allen H. and Magurran, Anne E. and Marquet, Pablo A. and Maurer, Brian A. and Ostling, Annette and Soykan, Candan U. and Ugland, Karl I. and White, Ethan P.},
  year = {2007},
  month = oct,
  journal = {Ecology Letters},
  volume = {10},
  number = {10},
  pages = {995--1015},
  issn = {1461-023X, 1461-0248},
  doi = {10.1111/j.1461-0248.2007.01094.x},
  abstract = {Species abundance distributions (SADs) follow one of ecology\~Os oldest and most universal laws \textendash{} every community shows a hollow curve or hyperbolic shape on a histogram with many rare species and just a few common species. Here, we review theoretical, empirical and statistical developments in the study of SADs. Several key points emerge. (i) Literally dozens of models have been proposed to explain the hollow curve. Unfortunately, very few models are ever rejected, primarily because few theories make any predictions beyond the hollow-curve SAD itself. (ii) Interesting work has been performed both empirically and theoretically, which goes beyond the hollow-curve prediction to provide a rich variety of information about how SADs behave. These include the study of SADs along environmental gradients and theories that integrate SADs with other biodiversity patterns. Central to this body of work is an effort to move beyond treating the SAD in isolation and to integrate the SAD into its ecological context to enable making many predictions. (iii) Moving forward will entail understanding how sampling and scale affect SADs and developing statistical tools for describing and comparing SADs. We are optimistic that SADs can provide significant insights into basic and applied ecological science.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2YJEV94V/mcgill et al 2007 - species abundance distributions - moving beyond single prediction theories to integration within an ecological framework - BDPG.pdf}
}

@article{mcrtonMatthewEffectM0,
  title = {The {{Matthew Effect}} M0 {{Science}}},
  author = {Mcrton, K},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JECYNPF9/matthew1.pdf}
}

@article{megiddoMaximumCoverageLocation1983,
  title = {The {{Maximum Coverage Location Problem}}},
  author = {Megiddo, Nimrod and Zemel, Eitan and Hakimi, S. Louis},
  year = {1983},
  month = jun,
  journal = {SIAM Journal on Algebraic Discrete Methods},
  volume = {4},
  number = {2},
  pages = {253--261},
  issn = {0196-5212, 2168-345X},
  doi = {10.1137/0604028},
  abstract = {In this paper we define and discuss the following problem which we call the maximum coverage location problem. A transportation network is given together with the locations of customers and facilities. Thus, for each customer i, a radius ri is known such that customer i can currently be served by a facility which is located within a distance of r, from the location of customer i. We consider the problem from the point of view of a new company which is interested in establishing new facilities on the network so as to maximize the company's "share of the market." Specifically, assume that the company gains an amount of wi in case customer i decides to switch over to one of the new facilities. Moreover, we assume that the decision to switch over is based on proximity only, i.e., customer i switches over to a new facility only if the latter is located at a distance less than ri from i. The problem is to locate p new facilities so as to maximize the total gain.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ECKVN7TW/megiddo-zemel-hakimi.pdf}
}

@article{meirDoesConservationPlanning2004,
  title = {Does Conservation Planning Matter in a Dynamic and Uncertain World?},
  author = {Meir, Eli and Andelman, Sandy and Possingham, Hugh P.},
  year = {2004},
  month = aug,
  journal = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {615--622},
  issn = {1461-023X, 1461-0248},
  doi = {10.1111/j.1461-0248.2004.00624.x},
  abstract = {Loss of biodiversity is one of the world's overriding environmental challenges. Reducing those losses by creating reserve networks is a cornerstone of global conservation and resource management. Historically, assembly of reserve networks has been ad hoc, but recently the focus has shifted to identifying optimal reserve networks. We show that while comprehensive reserve network design is best when the entire network can be implemented immediately, when conservation investments must be staged over years, such solutions actually may be sub-optimal in the context of biodiversity loss and uncertainty. Simple decision rules, such as protecting the available site with the highest irreplaceability or with the highest species richness, may be more effective when implementation occurs over many years.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9XHKRMNB/ele6241.pdf}
}

@article{meiyappanSpatialModelingAgricultural2014,
  title = {Spatial Modeling of Agricultural Land Use Change at Global Scale},
  author = {Meiyappan, Prasanth and Dalton, Michael and O'Neill, Brian C. and Jain, Atul K.},
  year = {2014},
  month = nov,
  journal = {Ecological Modelling},
  volume = {291},
  pages = {152--174},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.07.027},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GU52A42Z/1-s2.0-S0304380014003640-main.pdf}
}

@article{mezardAnalyticAlgorithmicSolution2002,
  title = {Analytic and {{Algorithmic Solution}} of {{Random Satisfiability Problems}}},
  author = {Mezard, M.},
  year = {2002},
  month = aug,
  journal = {Science},
  volume = {297},
  number = {5582},
  pages = {812--815},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1073287},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AUVWLJPQ/Mezard.Science.297_812.pdf}
}

@article{michaelsHowIndeterminismShapes2012,
  title = {How Indeterminism Shapes Ecologists' Contributions to Managing Socio-Ecological Systems: {{Indeterminism}} in {{SES}}},
  shorttitle = {How Indeterminism Shapes Ecologists' Contributions to Managing Socio-Ecological Systems},
  author = {Michaels, Sarah and Tyre, Andrew J.},
  year = {2012},
  month = aug,
  journal = {Conservation Letters},
  volume = {5},
  number = {4},
  pages = {289--295},
  issn = {1755263X},
  doi = {10.1111/j.1755-263X.2012.00241.x},
  abstract = {To make a difference in policy making about socio-ecological systems, ecologists must grasp when decision makers are amenable to acting on ecological expertise and when they are not. To enable them to do so we present a matrix for classifying a socio-ecological system by the extent of what we don't know about its natural components and the social interactions that affects them. We use four examples, Midcontinent Mallards, Laysan Ducks, Pallid Sturgeon, and Rocky Mountain Grey Wolves to illustrate how the combination of natural and social source of indeterminism matters. Where social indeterminism is high, ecologists can expand the range of possible science-based options decision makers might consider even while recognizing societal-based concerns rather than science will dominate decision making. In contrast, where natural indeterminism is low, ecologists can offer reasonably accurate predictions that may well serve as inputs into decision making. Depending on the combination of natural and social indeterminism characterizing a particular circumstance, ecologists have different roles to play in informing socio-ecological system management.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9PA8JWRK/Michaels and Tyre - 2012 - How indeterminism shapes ecologistsâ contributions.pdf}
}

@article{michaelsHowIndeterminismShapes2012a,
  title = {How Indeterminism Shapes Ecologists' Contributions to Managing Socio-Ecological Systems: {{Indeterminism}} in {{SES}}},
  shorttitle = {How Indeterminism Shapes Ecologists' Contributions to Managing Socio-Ecological Systems},
  author = {Michaels, Sarah and Tyre, Andrew J.},
  year = {2012},
  month = aug,
  journal = {Conservation Letters},
  volume = {5},
  number = {4},
  pages = {289--295},
  issn = {1755263X},
  doi = {10.1111/j.1755-263X.2012.00241.x},
  abstract = {To make a difference in policy making about socio-ecological systems, ecologists must grasp when decision makers are amenable to acting on ecological expertise and when they are not. To enable them to do so we present a matrix for classifying a socio-ecological system by the extent of what we don't know about its natural components and the social interactions that affects them. We use four examples, Midcontinent Mallards, Laysan Ducks, Pallid Sturgeon, and Rocky Mountain Grey Wolves to illustrate how the combination of natural and social source of indeterminism matters. Where social indeterminism is high, ecologists can expand the range of possible science-based options decision makers might consider even while recognizing societal-based concerns rather than science will dominate decision making. In contrast, where natural indeterminism is low, ecologists can offer reasonably accurate predictions that may well serve as inputs into decision making. Depending on the combination of natural and social indeterminism characterizing a particular circumstance, ecologists have different roles to play in informing socio-ecological system management.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/993PGBL2/Michaels_et_al-2012-Conservation_Letters.pdf}
}

@article{millerIncorporatingSpatialDependence2007,
  title = {Incorporating Spatial Dependence in Predictive Vegetation Models},
  author = {Miller, Jennifer and Franklin, Janet and Aspinall, Richard},
  year = {2007},
  month = apr,
  journal = {Ecological Modelling},
  volume = {202},
  number = {3-4},
  pages = {225--242},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.12.012},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2EMQ9JU4/Miller et al_2007_Incorporating spatial dependence in predictive vegetation models.pdf}
}

@inproceedings{minhSDASimpleUnifying2010,
  title = {{{SDA}}*: {{A Simple}} and {{Unifying Solution}} to {{Recent Bioinformatic Challenges}} for {{Conservation Genetics}}},
  shorttitle = {{{SDA}}*},
  booktitle = {2010 {{Second International Conference}} on {{Knowledge}} and {{Systems Engineering}}},
  author = {Minh, Bui Quang and Klaere, Steffen and {von Haeseler}, Arndt},
  year = {2010},
  month = oct,
  pages = {33--37},
  publisher = {{IEEE}},
  address = {{Hanoi, Vietnam}},
  doi = {10.1109/KSE.2010.24},
  abstract = {Recently, several algorithms have been proposed to tackle different conservation questions under phylogenetic diversity. Such questions are variants of the more general problem of budgeted reserve selection under split diversity, an NP-hard problem. Here, we present a novel framework, Split Diversity Algorithm* (SDA*), to unify all these attempts. More specifically, SDA* transforms the budgeted reserve selection problem into a binary linear programming (BLP), that can be solved by available linear optimization techniques. SDA* guarantees to find optimal solutions in reasonable time.},
  isbn = {978-1-4244-8334-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FJEPJPQB/05632157.pdf}
}

@inproceedings{mitchell1992ptncaia,
  title = {Hard and {{Easy Distributions}} of {{SAT Problems}}},
  booktitle = {Proceedings of the {{Tenth National Conference}} on {{Artificial Intelligence}} ({{AAAI-92}})},
  author = {Mitchell, David and Selman, Bart and Levesque, Hector},
  year = {1992},
  pages = {459--465},
  address = {{San Jose, CA}},
  abstract = {We report results from large-scale experiments in satisability testing. As has been observed by others, testing the satis ability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satis ability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PCJ6WPNJ/mitchell et al 1992 - Hard and Easy Distributions of SAT Problems - BDPG.pdf}
}

@article{moilanen2001,
  title = {On the Use of Connecti7ity Measures in Spatial Ecology},
  author = {Moilanen, Atte and Hanski, Ilkka},
  year = {2001},
  pages = {5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DCQ8T26L/Moilanen et al. - 2001 - On the use of connectivity measures in spatial ecology - Oikos.pdf}
}

@article{moilanen2002,
  title = {{{SIMPLE CONNECTIVITY MEASURES IN SPATIAL ECOLOGY}}},
  author = {Moilanen, Atte and Nieminen, Marko},
  year = {2002},
  volume = {83},
  number = {4},
  pages = {15},
  abstract = {Connectivity is a fundamental concept that is widely utilized in spatial ecology. The majority of connectivity measures used in the recent ecological literature only consider the nearest neighbor patch/population, or patches within a limited neighborhood of the focal patch (a buffer). Meta-analysis suggests that studies using nearest neighbor connectivity measures are much less likely to find statistically significant effects of connectivity than studies that use more complex measures. Here we compare simple connectivity measures in their ability to predict colonization events in two large and good-quality empirical data sets. The nearest neighbor distance to an occupied patch is found to be an inferior measure. Buffer measures do much better, but their performance is found to be sensitive to the estimate of the buffer radius. For highly fragmented habitats, the best and most consistent performance is found for a measure that takes into account the size of the focal patch and the sizes of and distances to all potential source populations. When experimenting with reduced data sets, it was discovered that nearest neighbor measures fail to find a statistically significant effect of connectivity for a large range of data set sizes for which the more complex measures still detect a highly significant effect. We conclude that the simplicity of a nearest neighbor measure is not an adequate compensation for poor performance.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MNFNZX2N/Moilanen, Nieminen - 2002 - Simple Connectivity Measures in Spatial Ecology - Ecology.pdf}
}

@article{moilanen2005cb,
  title = {Variance and {{Uncertainty}} in the {{Expected Number}} of {{Occurrences}} in {{Reserve Selection}}},
  author = {Moilanen, Atte and Cabeza, Mar},
  year = {2005},
  month = oct,
  journal = {Conservation Biology},
  volume = {19},
  number = {5},
  pages = {1663--1667},
  issn = {0888-8892, 1523-1739},
  doi = {10.1111/j.1523-1739.2005.00203.x},
  abstract = {Reserve selection often concerns the design of reserve networks for the long-term maintenance of biodiversity. We considered uncertainty in the context of three common reserve-selection formulations, the expected number of populations, proportional coverage of land-cover types, and the probability of having at least one population. By uncertainty, we mean variance in the outcome of any probability-based reserve selection formulation. A typical reserve-selection formulation might ask for the least expensive set of sites that contains n populations per species. It is implicit here that this requirement concerns the expected number of populations, which actually is obtained only with a 50\% chance. If the requirement is changed to select the least expensive set of sites that gives n populations per species with a 95\% probability, the number of sites required in the solution increases and the identity of the sites is changed toward sites that have high probabilities of persistence (or occurrence) and low associated binomial variance. Anthropogenic threat is one factor that may cause probabilistic uncertainty in the context of proportional area coverage.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/83YU9U5X/MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf}
}

@article{moilanen2005cba,
  title = {Variance and {{Uncertainty}} in the {{Expected Number}} of {{Occurrences}} in {{Reserve Selection}}},
  author = {Moilanen, Atte and Cabeza, Mar},
  year = {2005},
  month = oct,
  journal = {Conservation Biology},
  volume = {19},
  number = {5},
  pages = {1663--1667},
  issn = {0888-8892, 1523-1739},
  doi = {10.1111/j.1523-1739.2005.00203.x},
  abstract = {Reserve selection often concerns the design of reserve networks for the long-term maintenance of biodiversity. We considered uncertainty in the context of three common reserve-selection formulations, the expected number of populations, proportional coverage of land-cover types, and the probability of having at least one population. By uncertainty, we mean variance in the outcome of any probability-based reserve selection formulation. A typical reserve-selection formulation might ask for the least expensive set of sites that contains n populations per species. It is implicit here that this requirement concerns the expected number of populations, which actually is obtained only with a 50\% chance. If the requirement is changed to select the least expensive set of sites that gives n populations per species with a 95\% probability, the number of sites required in the solution increases and the identity of the sites is changed toward sites that have high probabilities of persistence (or occurrence) and low associated binomial variance. Anthropogenic threat is one factor that may cause probabilistic uncertainty in the context of proportional area coverage.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/23WNK5V7/MOILANEN, CABEZA - 2005 - Variance and Uncertainty in the Expected Number of Occurrences in Reserve Selection - Conservation Biology.pdf}
}

@article{moilanen2006bc,
  title = {Uncertainty Analysis Favours Selection of Spatially Aggregated Reserve Networks},
  author = {Moilanen, Atte and Wintle, Brendan A.},
  year = {2006},
  month = may,
  journal = {Biological Conservation},
  volume = {129},
  number = {3},
  pages = {427--434},
  issn = {00063207},
  doi = {10.1016/j.biocon.2005.11.006},
  abstract = {It has been widely argued that habitat fragmentation is bad for (meta)population persistence and that a high level of fragmentation is a similarly undesirable characteristic for a reserve network. However, modelling the effects of fragmentation for many species is very difficult due to high data demands and uncertainty concerning its effect on particular species. Hence, several reserve selection methods employ qualitative heuristics such as boundary length penalties that aggregate reserve network structures. This aggregation usually comes at a cost because low quality habitats will be included for the sake of increased connectivity. Here a biologically justified method for designing aggregated reserve networks based on a technique called distribution smoothing is investigated. As with the boundary length penalty, its use incurs an apparent biological cost. However, taking a step further, potential negative effects of fragmentation on individual species are evaluated using a decision-theoretic uncertainty analysis approach. This analysis shows that the aggregated reserve network (based on smoothed distributions) is likely to be biologically more valuable than a more fragmented one (based on habitat model predictions). The method is illustrated with a reserve design case study in the Hunter Valley of south-eastern Australia. The uncertainty analysis method, based on information-gap decision theory, provides a systematic framework for making robust decisions under severe uncertainty, making it particularly well adapted to reserve design problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MULCV9PZ/Moilanen, Wintle - 2006 - Uncertainty analysis favours selection of spatially aggregated reserve networks - Biological Conservation.pdf}
}

@article{moilanen2006cb,
  title = {Uncertainty {{Analysis}} for {{Regional-Scale Reserve Selection}}},
  author = {Moilanen, Atte and Wintle, Brendan A. and Elith, Jane and Burgman, Mark},
  year = {2006},
  month = dec,
  journal = {Conservation Biology},
  volume = {20},
  number = {6},
  pages = {1688--1697},
  issn = {0888-8892, 1523-1739},
  doi = {10.1111/j.1523-1739.2006.00560.x},
  abstract = {Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error. We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, ``distribution discounting,'' in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CP7NM73V/Moilanen et al. - 2006 - Uncertainty Analysis for Regional-Scale Reserve Selection - Conservation Biology.pdf}
}

@article{moilanen2007bc,
  title = {Landscape {{Zonation}}, Benefit Functions and Target-Based Planning: {{Unifying}} Reserve Selection Strategies},
  shorttitle = {Landscape {{Zonation}}, Benefit Functions and Target-Based Planning},
  author = {Moilanen, Atte},
  year = {2007},
  month = feb,
  journal = {Biological Conservation},
  volume = {134},
  number = {4},
  pages = {571--579},
  issn = {00063207},
  doi = {10.1016/j.biocon.2006.09.008},
  abstract = {The most widespread reserve selection strategy is target-based planning, as specified under the framework of systematic conservation planning. Targets are given for the representation levels of biodiversity features, and site selection algorithms are employed to either meet the targets with least cost (the minimum set formulation) or to maximize the number of targets met with a given resource (maximum coverage). Benefit functions are another recent approach to reserve selection. In the benefit function framework the objective is to maximize the value of the reserve network, however value is defined. In one benefit function formulation value is a sum over species-specific values, and species-specific value is an increasing function of representation. This benefit function approach is computationally convenient, but because it allows free tradeoffs between species, it essentially makes the assumption that species are acting as surrogates, or samples from a larger regional species pool. The Zonation algorithm is a recent computational method that produces a hierarchy of conservation priority through the landscape. This hierarchy is produced via iterative removal of selection units (cells) using the criterion of least marginal loss of conservation value to decide which cell to remove next. The first variant of Zonation, here called core-area Zonation, has a characteristic of emphasizing coreareas of all species. Here I separate the Zonation meta-algorithm from the cell removal rule, the definition of marginal loss of conservation value utilized inside the algorithm. I show how additive benefit functions and target-based planning can be implemented into the Zonation framework via the use of particular kinds of cell removal rules. The core-area, additive benefit function and targeting benefit function variants of Zonation have interesting conceptual differences in how they treat and trade off between species in the planning process.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W6WK3WIZ/Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf}
}

@article{moilanen2007bca,
  title = {Landscape {{Zonation}}, Benefit Functions and Target-Based Planning: {{Unifying}} Reserve Selection Strategies},
  shorttitle = {Landscape {{Zonation}}, Benefit Functions and Target-Based Planning},
  author = {Moilanen, Atte},
  year = {2007},
  month = feb,
  journal = {Biological Conservation},
  volume = {134},
  number = {4},
  pages = {571--579},
  issn = {00063207},
  doi = {10.1016/j.biocon.2006.09.008},
  abstract = {The most widespread reserve selection strategy is target-based planning, as specified under the framework of systematic conservation planning. Targets are given for the representation levels of biodiversity features, and site selection algorithms are employed to either meet the targets with least cost (the minimum set formulation) or to maximize the number of targets met with a given resource (maximum coverage). Benefit functions are another recent approach to reserve selection. In the benefit function framework the objective is to maximize the value of the reserve network, however value is defined. In one benefit function formulation value is a sum over species-specific values, and species-specific value is an increasing function of representation. This benefit function approach is computationally convenient, but because it allows free tradeoffs between species, it essentially makes the assumption that species are acting as surrogates, or samples from a larger regional species pool. The Zonation algorithm is a recent computational method that produces a hierarchy of conservation priority through the landscape. This hierarchy is produced via iterative removal of selection units (cells) using the criterion of least marginal loss of conservation value to decide which cell to remove next. The first variant of Zonation, here called core-area Zonation, has a characteristic of emphasizing coreareas of all species. Here I separate the Zonation meta-algorithm from the cell removal rule, the definition of marginal loss of conservation value utilized inside the algorithm. I show how additive benefit functions and target-based planning can be implemented into the Zonation framework via the use of particular kinds of cell removal rules. The core-area, additive benefit function and targeting benefit function variants of Zonation have interesting conceptual differences in how they treat and trade off between species in the planning process.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DD3TZ94J/Moilanen - Unknown - Landscape Zonation, benefit functions and target-based planning Unifying reserve selection strategies - Biological Conservation.pdf}
}

@article{moilanen2008bc,
  title = {Two Paths to a Suboptimal Solution \textendash{} Once More about Optimality in Reserve Selection},
  author = {Moilanen, Atte},
  year = {2008},
  month = jul,
  journal = {Biological Conservation},
  volume = {141},
  number = {7},
  pages = {1919--1923},
  issn = {00063207},
  doi = {10.1016/j.biocon.2008.04.018},
  abstract = {Several studies have compared the performances of exact algorithms (integer programming) and heuristic methods in the solution of conservation resource allocation problems, with the conclusion that exact methods are always preferable. Here, I summarize a potentially major deficiency in how the relationship between exact and heuristic methods has been presented: the above comparisons have all been done using relatively simple (linear) maximum coverage or minimum set models that are by definition solvable using integer programming. In contrast, heuristic or meta-heuristic algorithms can be applied to less simplified nonlinear and/or stochastic problems. The focus of this study is two kinds of suboptimality, first-stage suboptimality caused by model simplification and second-stage suboptimality caused by inexact solution. Evidence from comparisons between integer programming and heuristic solution methods suggests a suboptimality level of around 3\%\textendash 10\% for well-chosen heuristics, much depending on the problem and data. There is also largely anecdotal evidence from a few studies that have evaluated results from simplified conservation resource allocation problems using more complicated (nonlinear) models. These studies have found that dropping components such as habitat loss rates or connectivity effects from the model can lead to suboptimality from 5\% to 50\%. Consequently, I suggest that more attention should be given to two topics, first, how the performance of a conservation plan should be evaluated, and second, what are the consequences of simplifying the ideal conservation resource allocation model? Factors that may lead to relatively complicated problem formulations include connectivity and evaluation of long-term persistence, stochastic habitat loss and availability, species interactions, and distributions that shift due to climate change.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8IWAH845/Moilanen - 2008 - Two paths to a suboptimal solution â once more about optimality in reserve selection - Biological Conservation.pdf}
}

@article{moilanen2009bc,
  title = {Assessing Replacement Cost of Conservation Areas: {{How}} Does Habitat Loss Influence Priorities?},
  shorttitle = {Assessing Replacement Cost of Conservation Areas},
  author = {Moilanen, Atte and Arponen, Anni and Stokland, Jogeir N. and Cabeza, Mar},
  year = {2009},
  month = mar,
  journal = {Biological Conservation},
  volume = {142},
  number = {3},
  pages = {575--585},
  issn = {00063207},
  doi = {10.1016/j.biocon.2008.11.011},
  abstract = {Replacement cost refers to the loss incurred if the ideal set of conservation areas cannot be protected due to compulsory inclusion or exclusion of some area candidates. This cost can be defined either in terms of loss of conservation value or in terms of extra acquisition cost, and it has a clear mathematical definition as a difference between the value of the unconstrained optimal solution and a constrained suboptimal solution. In this work we for the first time show how replacement cost can be calculated in the context of sequential reserve selection, where a reserve network is developed over a longer time period and ongoing habitat loss influences retention and availability of sites. In case of site exclusion, a question that can be asked is, ``if a site belonging to the ideal (optimal) solution cannot be obtained, what expected loss in reserve network value does this entail by the end of the planning period given that the rest of the solution is re-organized in the most advantageous manner?'' Heuristically, the proposed method achieves the ambit of combining irreplaceability and vulnerability into one score of site importance. We applied replacement cost analysis to conservation prioritization for wood-inhabiting fungi in Norway, identifying factors that influence replacement cost and urgency of site acquisition. Among other things we find that the reliability of loss rate information is important, because the optimal site acquisition order may be strongly influenced by underestimated loss rates.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N9Q7F5MM/Moilanen et al. - 2009 - Assessing replacement cost of conservation areas How does habitat loss influence priorities - Biological Conservation.pdf}
}

@article{moilanenPlanningRobustReserve2006,
  title = {Planning for Robust Reserve Networks Using Uncertainty Analysis},
  author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and {Ben-Haim}, Yakov},
  year = {2006},
  month = nov,
  journal = {Ecological Modelling},
  volume = {199},
  number = {1},
  pages = {115--124},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.07.004},
  abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence\textendash absence in sites, or on species-specific distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data\textemdash erroneous species presence\textendash absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modifications to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TVWFHK9M/Moilanen et al. - 2006 - Planning for robust reserve networks using uncerta.pdf}
}

@article{moilanenPlanningRobustReserve2006a,
  title = {Planning for Robust Reserve Networks Using Uncertainty Analysis},
  author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and {Ben-Haim}, Yakov},
  year = {2006},
  month = nov,
  journal = {Ecological Modelling},
  volume = {199},
  number = {1},
  pages = {115--124},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.07.004},
  abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence\textendash absence in sites, or on species-specific distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data\textemdash erroneous species presence\textendash absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modifications to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LAFZDKZ5/moilanen et al 2006 - Planning for robust reserve networks usinguncertainty analysis - BDPG - UNCERTAINTY - RESERVE SELECTION.pdf}
}

@article{moilanenPlanningRobustReserve2006b,
  title = {Planning for Robust Reserve Networks Using Uncertainty Analysis},
  author = {Moilanen, Atte and Runge, Michael C. and Elith, Jane and Tyre, Andrew and Carmel, Yohay and Fegraus, Eric and Wintle, Brendan A. and Burgman, Mark and {Ben-Haim}, Yakov},
  year = {2006},
  month = nov,
  journal = {Ecological Modelling},
  volume = {199},
  number = {1},
  pages = {115--124},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.07.004},
  abstract = {Planning land-use for biodiversity conservation frequently involves computer-assisted reserve selection algorithms. Typically such algorithms operate on matrices of species presence\textendash absence in sites, or on species-specific distributions of model predicted probabilities of occurrence in grid cells. There are practically always errors in input data\textemdash erroneous species presence\textendash absence data, structural and parametric uncertainty in predictive habitat models, and lack of correspondence between temporal presence and long-run persistence. Despite these uncertainties, typical reserve selection methods proceed as if there is no uncertainty in the data or models. Having two conservation options of apparently equal biological value, one would prefer the option whose value is relatively insensitive to errors in planning inputs. In this work we show how uncertainty analysis for reserve planning can be implemented within a framework of information-gap decision theory, generating reserve designs that are robust to uncertainty. Consideration of uncertainty involves modifications to the typical objective functions used in reserve selection. Search for robust-optimal reserve structures can still be implemented via typical reserve selection optimization techniques, including stepwise heuristics, integer-programming and stochastic global search.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K54VZ54G/fulltext(5).pdf}
}

@article{monassonDeterminingComputationalComplexity1999,
  title = {Determining Computational Complexity from Characteristic `Phase Transitions'},
  author = {Monasson, R{\'e}mi and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
  year = {1999},
  month = jul,
  journal = {Nature},
  volume = {400},
  number = {6740},
  pages = {133--137},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/22055},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CPWF7QBG/400133a0.pdf}
}

@article{moorePhaseTransitionsNPcomplete,
  title = {Phase Transitions in {{NP-complete}} Problems: A Challenge for Probability, Combinatorics, and Computer Science},
  author = {Moore, Cristopher},
  pages = {36},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3KGC8M7J/moore 2010 - Phase transitions in NP-complete problems - a challenge for probability, combinatorics, and computer science.pdf}
}

@techreport{moraPymfinderToolMotif2018,
  type = {Preprint},
  title = {Pymfinder: A Tool for the Motif Analysis of Binary and Quantitative Complex Networks},
  shorttitle = {Pymfinder},
  author = {Mora, Bernat Bramon and Cirtwill, Alyssa R. and Stouffer, Daniel B.},
  year = {2018},
  month = jul,
  institution = {{Ecology}},
  doi = {10.1101/364703},
  abstract = {Abstract                        We developed             pymfinder             , a new software to analyze multiple aspects of the so-called network motifs\textemdash distinct             n             -node patterns of interaction\textemdash for any directed, undirected, unipartite or bipartite network. Unlike existing software for the study of network motifs,             pymfinder             allows the computation of node- and link-specific motif profiles as well as the analysis of weighted motifs. Beyond the overall characterization of networks, the tools presented in this work therefore allow for the comparison of the ``roles'' of either nodes or links of a network. Examples include the study of the roles of different species and/or their trophic/mutualistic interactions in ecological networks or the roles of specific proteins and/or their activation/inhibition relationships in protein-protein interaction networks. Here, we show how to apply the main tools from             pymfinder             using a predator-prey interaction network from a marine food web.             pymfinder             is open source software that can be freely and anonymously downloaded from             https://github.com/stoufferlab/pymfinder             , distributed under the MIT License (2018).},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WHMWKXYY/364703.full.pdf}
}

@article{moreno-scott2016cian,
  title = {Experimental {{Matching}} of {{Instances}} to {{Heuristics}} for {{Constraint Satisfaction Problems}}},
  author = {{Moreno-Scott}, Jorge Humberto and {Ortiz-Bayliss}, Jos{\'e} Carlos and {Terashima-Mar{\'i}n}, Hugo and {Conant-Pablos}, Santiago Enrique},
  year = {2016},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2016},
  pages = {1--15},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2016/7349070},
  abstract = {Constraint satisfaction problems are of special interest for the artificial intelligence and operations research community due to their many applications. Although heuristics involved in solving these problems have largely been studied in the past, little is known about the relation between instances and the respective performance of the heuristics used to solve them. This paper focuses on both the exploration of the instance space to identify relations between instances and good performing heuristics and how to use such relations to improve the search. Firstly, the document describes a methodology to explore the instance space of constraint satisfaction problems and evaluate the corresponding performance of six variable ordering heuristics for such instances in order to find regions on the instance space where some heuristics outperform the others. Analyzing such regions favors the understanding of how these heuristics work and contribute to their improvement. Secondly, we use the information gathered from the first stage to predict the most suitable heuristic to use according to the features of the instance currently being solved. This approach proved to be competitive when compared against the heuristics applied in isolation on both randomly generated and structured instances of constraint satisfaction problems.},
  langid = {english},
  keywords = {bdpg,csp,phase transition,problem difficulty,problem difficulty prediction},
  file = {/Users/bill/D/Zotero/storage/PWDJQS45/moreno-scott et al 2016 - experimental matching of instances to heuristics for constraint satisfaction problems - BDPG - PROBLEM DIFFICULTY PREDICTION.pdf}
}

@article{moretLearningPredictChannel2006,
  title = {Learning to Predict Channel Stability Using Biogeomorphic Features},
  author = {Moret, Stephanie L. and Langford, William T. and Margineantu, Dragos D.},
  year = {2006},
  month = jan,
  journal = {Ecological Modelling},
  volume = {191},
  number = {1},
  pages = {47--57},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2005.08.011},
  abstract = {Current human land use activities are altering many components of the river landscape, resulting in unstable channels. Instability may have serious negative consequences for water quality, aquatic and riparian habitat, and for river-related human infrastructure such as bridges and roads. Resource management agencies have developed rapid bioassessment surveys to help assess stability in a fast and cost-effective way. While this assessment can be done for a single site fairly rapidly, it is still time-consuming to apply over large watersheds and assessment activities must be prioritized. We constructed a system that employs commonly available map data as inputs to cost-sensitive variants of decision tree algorithms to predict the relative channel stability of different sites. In particular, we use bagged lazy option trees (LOTs) and bagged probability estimation trees (PETs) to identify all unstable channels while making the smallest number of errors of classifying stable channels as unstable, thereby minimizing cost and maximizing safety. We measured the performance of the classifiers using ROC curves and found that the PETs performed better than the LOTs in situations where the number of instances of the stable and unstable classes were relatively balanced, but the LOTs did better where unstable examples were relatively rare compared to stable, perhaps due to the LOTs' ability to focus on individual examples.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VVI3CUA4/moret langford margineantu 2006 - Learning to predict channel stability using biogeomorphic features.pdf}
}

@incollection{mouretNoveltybasedMultiobjectivization2011,
  title = {Novelty-Based Multiobjectivization},
  booktitle = {New Horizons in Evolutionary Robotics},
  author = {Mouret, Jean-Baptiste},
  year = {2011},
  pages = {139--154},
  publisher = {{Springer}},
  file = {/Users/bill/D/Zotero/storage/7EDWRSB3/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf}
}

@incollection{mouretNoveltyBasedMultiobjectivization2011,
  title = {Novelty-{{Based Multiobjectivization}}},
  booktitle = {New {{Horizons}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste},
  editor = {Kacprzyk, Janusz and Doncieux, St{\'e}phane and Bred{\`e}che, Nicolas and Mouret, Jean-Baptiste},
  year = {2011},
  volume = {341},
  pages = {139--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18272-3_10},
  abstract = {Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Paretobased multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant ``Novelty + Fitness'' is better at fine-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.},
  isbn = {978-3-642-18271-6 978-3-642-18272-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F73HNQQG/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf}
}

@incollection{mouretNoveltyBasedMultiobjectivization2011a,
  title = {Novelty-{{Based Multiobjectivization}}},
  booktitle = {New {{Horizons}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste},
  editor = {Kacprzyk, Janusz and Doncieux, St{\'e}phane and Bred{\`e}che, Nicolas and Mouret, Jean-Baptiste},
  year = {2011},
  volume = {341},
  pages = {139--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18272-3_10},
  abstract = {Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Paretobased multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant ``Novelty + Fitness'' is better at fine-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.},
  isbn = {978-3-642-18271-6 978-3-642-18272-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8IW8HAUX/mouret 2011 - novelty-based multiobjectivization - OPTISEVIL.pdf}
}

@article{munozAlgorithmSelectionBlackbox2015,
  title = {Algorithm Selection for Black-Box Continuous Optimization Problems: {{A}} Survey on Methods and Challenges},
  shorttitle = {Algorithm Selection for Black-Box Continuous Optimization Problems},
  author = {Mu{\~n}oz, Mario A. and Sun, Yuan and Kirley, Michael and Halgamuge, Saman K.},
  year = {2015},
  month = oct,
  journal = {Information Sciences},
  volume = {317},
  pages = {224--245},
  issn = {00200255},
  doi = {10.1016/j.ins.2015.05.010},
  abstract = {Selecting the most appropriate algorithm to use when attempting to solve a black-box continuous optimization problem is a challenging task. Such problems typically lack algebraic expressions. It is not possible to calculate derivative information, and the problem may exhibit uncertainty or noise. In many cases, the input and output variables are analyzed without considering the internal details of the problem. Algorithm selection requires expert knowledge of search algorithm efficacy and skills in algorithm engineering and statistics. Even with the necessary knowledge and skills, success is not guaranteed.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/G5CJHNQN/munoz acosta et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{munozAlgorithmSelectionBlackbox2015a,
  title = {Algorithm Selection for Black-Box Continuous Optimization Problems: {{A}} Survey on Methods and Challenges},
  shorttitle = {Algorithm Selection for Black-Box Continuous Optimization Problems},
  author = {Mu{\~n}oz, Mario A. and Sun, Yuan and Kirley, Michael and Halgamuge, Saman K.},
  year = {2015},
  month = oct,
  journal = {Information Sciences},
  volume = {317},
  pages = {224--245},
  issn = {00200255},
  doi = {10.1016/j.ins.2015.05.010},
  abstract = {Selecting the most appropriate algorithm to use when attempting to solve a black-box continuous optimization problem is a challenging task. Such problems typically lack algebraic expressions. It is not possible to calculate derivative information, and the problem may exhibit uncertainty or noise. In many cases, the input and output variables are analyzed without considering the internal details of the problem. Algorithm selection requires expert knowledge of search algorithm efficacy and skills in algorithm engineering and statistics. Even with the necessary knowledge and skills, success is not guaranteed.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QQHX84U8/munoz et al 2015 - Algorithm selection for black-box continuous optimization problems - A survey on methods and challenges - PROBLEM DIFFICULTY - BDPG.pdf}
}

@inproceedings{munozEffectsFunctionTranslation2015,
  title = {Effects of Function Translation and Dimensionality Reduction on Landscape Analysis},
  booktitle = {2015 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Munoz, Mario A. and {Smith-Miles}, Kate},
  year = {2015},
  month = may,
  pages = {1336--1342},
  publisher = {{IEEE}},
  address = {{Sendai, Japan}},
  doi = {10.1109/CEC.2015.7257043},
  abstract = {Exploratory Landscape Analysis (ELA) measures have been shown to predict algorithm performance; hence, they are being applied on critical tasks such as automatic algorithm selection and problem generation. This paper provides a cautionary examination on their use in black-box continuous optimization. We explore the effect that translations have on the measures, when the cost function is defined within a boundconstrained region. Furthermore, we examine the robustness of the neighborhood structure after dimensionality reduction. The results demonstrate that a measure may transition abruptly due a translation. Therefore, we should not generalize the measures of an instance nor report average values of a measure as belonging to the generating function. Moreover, dimensionality reduction could alter the neighborhood structure, such that the regions corresponding to significantly different functions overlap.},
  isbn = {978-1-4799-7492-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GLKRUSTH/munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis copy.pdf}
}

@inproceedings{munozEffectsFunctionTranslation2015a,
  title = {Effects of Function Translation and Dimensionality Reduction on Landscape Analysis},
  booktitle = {2015 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Munoz, Mario A. and {Smith-Miles}, Kate},
  year = {2015},
  month = may,
  pages = {1336--1342},
  publisher = {{IEEE}},
  address = {{Sendai, Japan}},
  doi = {10.1109/CEC.2015.7257043},
  abstract = {Exploratory Landscape Analysis (ELA) measures have been shown to predict algorithm performance; hence, they are being applied on critical tasks such as automatic algorithm selection and problem generation. This paper provides a cautionary examination on their use in black-box continuous optimization. We explore the effect that translations have on the measures, when the cost function is defined within a boundconstrained region. Furthermore, we examine the robustness of the neighborhood structure after dimensionality reduction. The results demonstrate that a measure may transition abruptly due a translation. Therefore, we should not generalize the measures of an instance nor report average values of a measure as belonging to the generating function. Moreover, dimensionality reduction could alter the neighborhood structure, such that the regions corresponding to significantly different functions overlap.},
  isbn = {978-1-4799-7492-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Q6VR4WYL/munoz smith-miles 2015 - Effects of function translation and dimensionality reduction on landscape analysis.pdf}
}

@inproceedings{munozGeneratingCustomClassification2017,
  title = {Generating Custom Classification Datasets by Targeting the Instance Space},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}} on - {{GECCO}} '17},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  pages = {1582--1588},
  publisher = {{ACM Press}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3067695.3082532},
  abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LBGZDNQM/p1582-munoz.pdf}
}

@inproceedings{munozGeneratingCustomClassification2017a,
  title = {Generating Custom Classification Datasets by Targeting the Instance Space},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}} on - {{GECCO}} '17},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  pages = {1582--1588},
  publisher = {{ACM Press}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3067695.3082532},
  abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GMQX32QB/W_ClassProblemGenerator.pdf}
}

@inproceedings{munozGeneratingCustomClassification2017b,
  title = {Generating Custom Classification Datasets by Targeting the Instance Space},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  month = jul,
  pages = {1582--1588},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3067695.3082532},
  abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EEL27K8Z/p1582-munoz.pdf}
}

@inproceedings{munozGeneratingCustomClassification2017c,
  title = {Generating Custom Classification Datasets by Targeting the Instance Space},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  month = jul,
  pages = {1582--1588},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3067695.3082532},
  abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/M4K459LS/W_ClassProblemGenerator.pdf}
}

@inproceedings{munozGeneratingCustomClassification2017d,
  title = {Generating Custom Classification Datasets by Targeting the Instance Space},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  month = jul,
  pages = {1582--1588},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3067695.3082532},
  abstract = {While machine learning has evolved at a fast pace in the last decades, the testing procedure of new methods may be not keeping pace. It o en relies on well-studied collections of classi cation datasets such as the UCI repository. However, a meta-analysis through features has showed that most datasets from UCI are not su ciently challenging to expose unique weaknesses of algorithms. In this paper we present a method to generate datasets with continuous, binary and categorical a ributes, through the ing of a Gaussian Mixture Model and a set of generalized Bernoulli distributions. By targeting empty areas of the instance space, this method has the potential to generate datasets with more diverse feature values.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HC8DY2NP/munoz smith-miles 2017 - Generating custom classification datasets by targeting the instance space - PROBLEM DIFFICULTY - BDPG - ANNP.pdf}
}

@article{munozGeneratingNewSpaceFilling2019,
  title = {Generating {{New Space-Filling Test Instances}} for {{Continuous Black-Box Optimization}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2019},
  month = jul,
  journal = {Evolutionary Computation},
  pages = {1--26},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco_a_00262},
  abstract = {This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a twodimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AQAKKYTJ/evco_a_00262.pdf}
}

@article{munozGeneratingNewSpaceFilling2020,
  title = {Generating {{New Space-Filling Test Instances}} for {{Continuous Black-Box Optimization}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2020},
  month = sep,
  journal = {Evolutionary Computation},
  volume = {28},
  number = {3},
  pages = {379--404},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco_a_00262},
  abstract = {This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a twodimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P3XI3U4V/evco_a_00262.pdf}
}

@inproceedings{munozICARUSIdentificationComplementary2016,
  title = {{{ICARUS}}: {{Identification}} of Complementary Algorithms by Uncovered Sets},
  shorttitle = {{{ICARUS}}},
  booktitle = {2016 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Munoz, Mario A. and Kirley, Michael},
  year = {2016},
  month = jul,
  pages = {2427--2432},
  publisher = {{IEEE}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1109/CEC.2016.7744089},
  abstract = {Since there is no single best performing algorithm for all problems, an algorithm portfolio would leverage the strengths of complementary algorithms to achieve the best performance. In this paper, we present and evaluate a new technique for designing algorithm portfolios for continuous black-box optimization problems, based on social choice and voting theory concepts. Our technique, which we call ICARUS, models the portfolio design task as an election, in which each problem `votes' for a subset of preferred algorithms guided by a performance metric such as the number of fitness evaluations. The resulting `uncovered set' of algorithms forms the portfolio. We demonstrate the efficacy of ICARUS using a suite of stateof-the-art evolutionary algorithms and benchmark continuous optimization problems. Our analysis confirms that ICARUS creates an algorithm portfolio where the expected performance is superior to a manually constructed portfolio.},
  isbn = {978-1-5090-0623-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HIKEDIM3/munoz kirley 2016 - ICARUS - Identification of Complementary algoRithms by Uncovered Sets - PROBLEM DIFFICULTY - BDPG - ANNO.pdf}
}

@article{munozInstanceSpacesMachine2018,
  title = {Instance Spaces for Machine Learning Classification},
  author = {Mu{\~n}oz, Mario A. and Villanova, Laura and Baatar, Davaatseren and {Smith-Miles}, Kate},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {109--147},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5629-5},
  abstract = {This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/B5TCX2BR/munoz acosta ... smith-miles 2017 - Instance Spaces for Machine Learning Classification - ML - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{munozInstanceSpacesMachine2018a,
  title = {Instance Spaces for Machine Learning Classification},
  author = {Mu{\~n}oz, Mario A. and Villanova, Laura and Baatar, Davaatseren and {Smith-Miles}, Kate},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {109--147},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5629-5},
  abstract = {This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3FK4GIVA/munoz villanova baatar miles-smith 2018 - instance spaces for machine learning classification - ML - PROBLEM DIFFICULTY - BDPG - GUPPY.pdf}
}

@incollection{munozMetalearningPredictionModel2012,
  title = {A {{Meta-learning Prediction Model}} of {{Algorithm Performance}} for {{Continuous Optimization Problems}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} - {{PPSN XII}}},
  author = {Mu{\~n}oz, Mario A. and Kirley, Michael and Halgamuge, Saman K.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Coello, Carlos A. Coello and Cutello, Vincenzo and Deb, Kalyanmoy and Forrest, Stephanie and Nicosia, Giuseppe and Pavone, Mario},
  year = {2012},
  volume = {7491},
  pages = {226--235},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32937-1_23},
  abstract = {Algorithm selection and configuration is a challenging problem in the continuous optimization domain. An approach to tackle this problem is to develop a model that links landscape analysis measures and algorithm parameters to performance. This model can be then used to predict algorithm performance when a new optimization problem is presented. In this paper, we investigate the use of a machine learning framework to build such a model. We demonstrate the effectiveness of our technique using CMA-ES as a representative algorithm and a feed-forward backpropagation neural network as the learning strategy. Our experimental results show that we can build sufficiently accurate predictions of an algorithm's expected performance. This information is used to rank the algorithm parameter settings based on the current problem instance, hence increasing the probability of selecting the best configuration for a new problem.},
  isbn = {978-3-642-32936-4 978-3-642-32937-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/S2384QC8/munoz acosta et al 20012 - A Meta-Learning Prediction Model of Algorithm Performance for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf}
}

@incollection{munozMetalearningPredictionModel2012a,
  title = {A {{Meta-learning Prediction Model}} of {{Algorithm Performance}} for {{Continuous Optimization Problems}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} - {{PPSN XII}}},
  author = {Mu{\~n}oz, Mario A. and Kirley, Michael and Halgamuge, Saman K.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Coello, Carlos A. Coello and Cutello, Vincenzo and Deb, Kalyanmoy and Forrest, Stephanie and Nicosia, Giuseppe and Pavone, Mario},
  year = {2012},
  volume = {7491},
  pages = {226--235},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32937-1_23},
  abstract = {Algorithm selection and configuration is a challenging problem in the continuous optimization domain. An approach to tackle this problem is to develop a model that links landscape analysis measures and algorithm parameters to performance. This model can be then used to predict algorithm performance when a new optimization problem is presented. In this paper, we investigate the use of a machine learning framework to build such a model. We demonstrate the effectiveness of our technique using CMA-ES as a representative algorithm and a feed-forward backpropagation neural network as the learning strategy. Our experimental results show that we can build sufficiently accurate predictions of an algorithm's expected performance. This information is used to rank the algorithm parameter settings based on the current problem instance, hence increasing the probability of selecting the best configuration for a new problem.},
  isbn = {978-3-642-32936-4 978-3-642-32937-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6ISQHAPF/munoz et al 2012 - A Meta-Learning Prediction Model of Algorithm Performance  or Continuous Optimization Problems - BDPG - GUPPY.pdf}
}

@inproceedings{munozNonparametricModelSpace2017,
  title = {Non-Parametric Model of the Space of Continuous Black-Box Optimization Problems},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}} on   - {{GECCO}} '17},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  pages = {175--176},
  publisher = {{ACM Press}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3067695.3075971},
  abstract = {Exploratory Landscape Analysis are data driven methods used for automated algorithm selection in continuous black-box optimization. Most of these methods follow strong assumptions that limit their characterization power, or loose information by compressing the data into a few scalar features. A more exible approach is to avoid explicit measuring and comparing of speci c structures. In this paper we present a proof-of-concept for a more general method, which produces non-parametric models of the space of problems. Using non-metric multidimensional scaling, we generate synthetic features for each problem, which could replace or complement the existing ones. We demonstrate approaches to produce algorithm recommendations and visual representations of the space. To validate the model, we compare our results with those obtained through existing methods, which show that our models have competitive performance.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AM2E7F7X/p175-munoz.pdf}
}

@inproceedings{munozNonparametricModelSpace2017a,
  title = {Non-Parametric Model of the Space of Continuous Black-Box Optimization Problems},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate},
  year = {2017},
  month = jul,
  pages = {175--176},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3067695.3075971},
  abstract = {Exploratory Landscape Analysis are data driven methods used for automated algorithm selection in continuous black-box optimization. Most of these methods follow strong assumptions that limit their characterization power, or loose information by compressing the data into a few scalar features. A more exible approach is to avoid explicit measuring and comparing of speci c structures. In this paper we present a proof-of-concept for a more general method, which produces non-parametric models of the space of problems. Using non-metric multidimensional scaling, we generate synthetic features for each problem, which could replace or complement the existing ones. We demonstrate approaches to produce algorithm recommendations and visual representations of the space. To validate the model, we compare our results with those obtained through existing methods, which show that our models have competitive performance.},
  isbn = {978-1-4503-4939-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CUUZAJE7/p175-munoz.pdf}
}

@article{munozPerformanceAnalysisContinuous2017,
  title = {Performance {{Analysis}} of {{Continuous Black-Box Optimization Algorithms}} via {{Footprints}} in {{Instance Space}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate A.},
  year = {2017},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {25},
  number = {4},
  pages = {529--554},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco_a_00194},
  abstract = {This article presents a method for the objective assessment of an algorithm's strengths and weaknesses. Instead of examining the performance of only one or more algorithms on a benchmark set, or generating custom problems that maximize the performance difference between two algorithms, our method quantifies both the nature of the test instances and the algorithm performance. Our aim is to gather information about possible phase transitions in performance, that is, the points in which a small change in problem structure produces algorithm failure. The method is based on the accurate estimation and characterization of the algorithm footprints, that is, the regions of instance space in which good or exceptional performance is expected from an algorithm. A footprint can be estimated for each algorithm and for the overall portfolio. Therefore, we select a set of features to generate a common instance space, which we validate by constructing a sufficiently accurate prediction model. We characterize the footprints by their area and density. Our method identifies complementary performance between algorithms, quantifies the common features of hard problems, and locates regions where a phase transition may lie.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6A45ZAER/evco_a_00194.pdf}
}

@article{munozPerformanceAnalysisContinuous2017a,
  title = {Performance {{Analysis}} of {{Continuous Black-Box Optimization Algorithms}} via {{Footprints}} in {{Instance Space}}},
  author = {Mu{\~n}oz, Mario A. and {Smith-Miles}, Kate A.},
  year = {2017},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {25},
  number = {4},
  pages = {529--554},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco_a_00194},
  abstract = {This article presents a method for the objective assessment of an algorithm's strengths and weaknesses. Instead of examining the performance of only one or more algorithms on a benchmark set, or generating custom problems that maximize the performance difference between two algorithms, our method quantifies both the nature of the test instances and the algorithm performance. Our aim is to gather information about possible phase transitions in performance, that is, the points in which a small change in problem structure produces algorithm failure. The method is based on the accurate estimation and characterization of the algorithm footprints, that is, the regions of instance space in which good or exceptional performance is expected from an algorithm. A footprint can be estimated for each algorithm and for the overall portfolio. Therefore, we select a set of features to generate a common instance space, which we validate by constructing a sufficiently accurate prediction model. We characterize the footprints by their area and density. Our method identifies complementary performance between algorithms, quantifies the common features of hard problems, and locates regions where a phase transition may lie.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FVW6VZKH/evco_a_00194.pdf}
}

@article{munozReliabilityExploratoryLandscape,
  title = {Reliability of {{Exploratory Landscape Analysis}}},
  author = {Munoz, Mario A and Kirley, Michael and {Smith-Miles}, Kate},
  pages = {21},
  abstract = {The inherent difficulty of solving a black-box optimization problem depends on the characteristics of the problem's fitness landscape and the algorithm being used. Exploratory Landscape Analysis (ELA) methods can be used to describe the complexities of the problem using numerical features generated via a sampling process of the search space. Despite their success in a number of applications, ELA methods have significant limitations typically related with the computational costs associated with generating accurate features. Consequently, only approximate features are available in practice which may be unreliable, leading to systemic errors. The overarching aim of this paper, is to evaluate the reliability of landscape features generated by well-known ELA methods. We describe a comprehensive evaluative framework combining exploratory and statistical validation stages. The results show that particular landscape features are highly volatile. In addition, instances of the same function can have feature values that are significantly different. This implies that the fitness landscapes may be statistically anisotropic. Finally, the results show evidence of a curse of the modality, meaning that the sample size should increase with the number of local optima.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y7N6Z4WR/J_RobustnessAnalysisELA_R5_preprint.pdf}
}

@article{munozReliabilityExploratoryLandscapea,
  title = {Reliability of {{Exploratory Landscape Analysis}}},
  author = {Munoz, Mario A and Kirley, Michael and {Smith-Miles}, Kate},
  pages = {21},
  abstract = {The inherent difficulty of solving a black-box optimization problem depends on the characteristics of the problem's fitness landscape and the algorithm being used. Exploratory Landscape Analysis (ELA) methods can be used to describe the complexities of the problem using numerical features generated via a sampling process of the search space. Despite their success in a number of applications, ELA methods have significant limitations typically related with the computational costs associated with generating accurate features. Consequently, only approximate features are available in practice which may be unreliable, leading to systemic errors. The overarching aim of this paper, is to evaluate the reliability of landscape features generated by well-known ELA methods. We describe a comprehensive evaluative framework combining exploratory and statistical validation stages. The results show that particular landscape features are highly volatile. In addition, instances of the same function can have feature values that are significantly different. This implies that the fitness landscapes may be statistically anisotropic. Finally, the results show evidence of a curse of the modality, meaning that the sample size should increase with the number of local optima.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XNZH85SL/J_RobustnessAnalysisELA_R5_preprint.pdf}
}

@article{munsonClusterEnsemblesNetwork,
  title = {Cluster {{Ensembles}} for {{Network Anomaly Detection}}},
  author = {Munson, Art and Caruana, Rich},
  pages = {9},
  abstract = {Cluster ensembles aim to find better, more natural clusterings by combining multiple clusterings. We apply ensemble clustering to anomaly detection, hypothesizing that multiple views of the data will improve the detection of attacks. Each clustering rates how anomalous a point is; ratings are combined by averaging or taking either the minimum, the maximum, or median score. The evaluation shows that taking the median prediction from the cluster ensemble results in better performance than single clusterings. Surprisingly, averaging the individual predictions a) leads to worse performance than that of individual clusterings, and b) performs identically to taking the minimum prediction from the ensemble. This counter-intuitive result stems from asymmetric prediction distributions.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SV9W36KE/TR2006-2047.pdf}
}

@incollection{munsonFeatureSelectionBiasVariance2009,
  title = {On {{Feature Selection}}, {{Bias-Variance}}, and {{Bagging}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Munson, M. Arthur and Caruana, Rich},
  editor = {Buntine, Wray and Grobelnik, Marko and Mladeni{\'c}, Dunja and {Shawe-Taylor}, John},
  year = {2009},
  volume = {5782},
  pages = {144--159},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-04174-7_10},
  abstract = {We examine the mechanism by which feature selection improves the accuracy of supervised learning. An empirical bias/variance analysis as feature selection progresses indicates that the most accurate feature set corresponds to the best bias-variance trade-off point for the learning algorithm. Often, this is not the point separating relevant from irrelevant features, but where increasing variance outweighs the gains from adding more (weakly) relevant features. In other words, feature selection can be viewed as a variance reduction method that trades off the benefits of decreased variance (from the reduction in dimensionality) with the harm of increased bias (from eliminating some of the relevant features). If a variance reduction method like bagging is used, more (weakly) relevant features can be exploited and the most accurate feature set is usually larger. In many cases, the best performance is obtained by using all available features.},
  isbn = {978-3-642-04173-0 978-3-642-04174-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PB2E9N27/54eaa0310cf2f7aa4d57f900.pdf}
}

@article{naimiStellaRSoftwareTranslate2012,
  title = {{{StellaR}}: {{A}} Software to Translate {{Stella}} Models into {{R}} Open-Source Environment},
  shorttitle = {{{StellaR}}},
  author = {Naimi, Babak and Voinov, Alexey},
  year = {2012},
  month = dec,
  journal = {Environmental Modelling \& Software},
  volume = {38},
  pages = {117--118},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2012.05.012},
  abstract = {Stella is a popular system dynamics modeling tool, which helps to put together conceptual diagrams and converts them into numeric computer models. Although it can be very useful, especially in participatory modeling, it lacks the power and flexibility of a programming language. This paper presents the StellaR software which translates a Stella model into a model in R, an open source high level programming language. This allows using conceptual modeling tools provided in Stella, together with computational functionality and programming flexibility provided in R. It also opens access to powerful software libraries available in R, which is especially useful for spatial modeling.},
  langid = {english},
  keywords = {problem attributes},
  file = {/Users/bill/D/Zotero/storage/IXKAE245/naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf}
}

@article{naimiStellaRSoftwareTranslate2012a,
  title = {{{StellaR}}: {{A}} Software to Translate {{Stella}} Models into {{R}} Open-Source Environment},
  shorttitle = {{{StellaR}}},
  author = {Naimi, Babak and Voinov, Alexey},
  year = {2012},
  month = dec,
  journal = {Environmental Modelling \& Software},
  volume = {38},
  pages = {117--118},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2012.05.012},
  abstract = {Stella is a popular system dynamics modeling tool, which helps to put together conceptual diagrams and converts them into numeric computer models. Although it can be very useful, especially in participatory modeling, it lacks the power and flexibility of a programming language. This paper presents the StellaR software which translates a Stella model into a model in R, an open source high level programming language. This allows using conceptual modeling tools provided in Stella, together with computational functionality and programming flexibility provided in R. It also opens access to powerful software libraries available in R, which is especially useful for spatial modeling.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/M6BMIF94/naimi voinov 2012 - StellaR - A software to translate Stella models into R open-source environment.pdf}
}

@article{nat,
  title = {Appendix {{B}} from {{A}}. {{Moilanen}}, ``{{Reserve Selection Using Nonlinear Species Distribution Models}}''},
  author = {Nat, Am},
  pages = {2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CICSKJRE/moilanen 2005 - reserve selection using nonlinear species distribution models - appdx b - amnat.pdf}
}

@article{naudtsComparisonPredictiveMeasures2000,
  title = {A Comparison of Predictive Measures of Problem Difficulty in Evolutionary Algorithms},
  author = {Naudts, B. and Kallel, L.},
  year = {2000},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {4},
  number = {1},
  pages = {1--15},
  issn = {1089778X},
  doi = {10.1109/4235.843491},
  abstract = {This paper studies a number of predictive measures of problem difficulty, among which epistasis variance and fitness distance correlation are the most widely known. Our approach is based on comparing the reference class of a measure to a number of known easy function classes. First, we generalize the reference classes of fitness distance correlation and epistasis variance, and construct a new predictive measure that is insensitive to nonlinear fitness scaling. We then investigate the relations between the reference classes of the measures and a number of intuitively easy classes such as the steepest ascent optimizable functions. Within the latter class, functions that fool the predictive quality of all of the measures are easily found. This points out the need to further identify which functions are easy for a given class of evolutionary algorithms in order to design more efficient hardness indicators for them. We finally restrict attention to the genetic algorithm (GA), and consider both GA-easy and GA-hard fitness functions, and give experimental evidence that the values of the measures, based on random samples, can be completely unreliable and entirely uncorrelated to the convergence quality and convergence speed of GA instances using either proportional or ranking selection.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Q6I3KVA7/naudts and kallel 2000 - a comparison of predictive measures of problem difficulty in evolutionary algorithms - ieeeevocomp.pdf}
}

@article{neelyPerformanceMeasurementSystem,
  title = {Performance Measurement System Design: {{A}} Literature Review and Research Agenda},
  author = {Neely, Andy and Gregory, Mike and Platts, Ken},
  pages = {36},
  langid = {english},
  keywords = {EF EFs,EFs},
  file = {/Users/bill/D/Zotero/storage/ITDIVIAM/performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf}
}

@article{neumannEvolutionaryDiversityOptimization2018,
  title = {Evolutionary {{Diversity Optimization Using Multi-Objective Indicators}}},
  author = {Neumann, Aneta and Gao, Wanru and Wagner, Markus and Neumann, Frank},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.06804 [cs]},
  eprint = {1811.06804},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Evolutionary diversity optimization aims to compute a diverse set of solutions where all solutions meet a given quality criterion. With this paper, we bridge the areas of evolutionary diversity optimization and evolutionary multi-objective optimization. We show how popular indicators frequently used in the area of multi-objective optimization can be used for evolutionary diversity optimization. Our experimental investigations for evolving diverse sets of TSP instances and images according to various features show that two of the most prominent multi-objective indicators, namely the hypervolume indicator and the inverted generational distance, provide excellent results in terms of visualization and various diversity indicators.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/bill/D/Zotero/storage/Q5XJ66N7/1811.06804.pdf}
}

@article{neuteboomSilicoSamplingReveals2005,
  title = {In Silico Sampling Reveals the Effect of Clustering and Shows That the Log-Normal Rank Abundance Curve Is an Artefact},
  author = {Neuteboom, J.H. and Struik, P.C.},
  year = {2005},
  journal = {NJAS - Wageningen Journal of Life Sciences},
  volume = {53},
  number = {2},
  pages = {223--245},
  issn = {15735214},
  doi = {10.1016/S1573-5214(05)80006-5},
  abstract = {The impact of clustering on rank abundance, species-individual (5-N) and species-area curves was investigated using a computer programme for in silico sampling. In a rank abundance curve the abundances of species are plotted on log-scale against species sequence. In an 5-N curve the number of species (5) is plotted against the log of the total number of individuals (N) in the sample, in a species-area curve 5 is plotted against log-area. The results from in silico sampling confirm the general shape of 5-N and speciesarea curves for communities with clustering, i.e., a curve that starts with a smaller slope but that later is temporarily steeper than the curve expected for Poisson-distributed species. Extrapolation of 5-N and species-area curves could therefore be misleading. The output furthermore shows that sigmoid rank abundance curves (curves of the type of a log-normal or broken stick) can be an artefact of the standard procedure of first sorting the species in sequence of abundance in combination with clustering in the low abundant and rare species. This makes the usual explanation given to the log-normal rank abundance curve dubious. An extension of the negative-binomial rank abundance curve-fit model is discussed to make it suitable for also fitting sigmoid rank abundance curves.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WUXW8RUG/neuteboom struik 2005 - in silica sampling reveals the effect of clustering and shows that the log-normal rank abundance curve is an artefact - BDPG.pdf}
}

@article{neuteboomVariationRankAbundance2005,
  title = {Variation in Rank Abundance Replicate Samples and Impact of Clustering},
  author = {Neuteboom, J.H. and Struik, P.C},
  year = {2005},
  journal = {NJAS - Wageningen Journal of Life Sciences},
  volume = {53},
  number = {2},
  pages = {199--222},
  issn = {15735214},
  doi = {10.1016/S1573-5214(05)80005-3},
  abstract = {Calculating a single-sample rank abundance curve by using the negative-binomial distribution provides a way to investigate the variability within rank abundance replicate samples and yields a measure of the degree of heterogeneity of the sampled community. The calculation of the single-sample rank abundance curve is used in combination with the negative-binomial rank abundance curve-fit model to analyse the principal effect of clustering on the species-individual (S-N) curve and the species-area curve. With the usual plotting of S against log N or log area, assuming that N is proportional to area, S-N curves and species-area curves are the same curves with only a shifted horizontal axis. Clustering results in a lower recorded number of species in a sample and stretches the S-N curve and species-area curve over the horizontal axis to the right. In contrast to what is suggested in the literature, we surmise that the effect of clustering on both curves will gradually fade away with increasing sample size. Since the slopes of the curves are not constant, they cannot be used as species diversity indices or site discriminant. S-N curves and species-area curves cannot be extrapolated.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/59F236BD/1-s2.0-S1573521405800053-main.pdf}
}

@article{newChallengesUsingProbabilistic2007,
  title = {Challenges in Using Probabilistic Climate Change Information for Impact Assessments: An Example from the Water Sector},
  shorttitle = {Challenges in Using Probabilistic Climate Change Information for Impact Assessments},
  author = {New, Mark and Lopez, Ana and Dessai, Suraje and Wilby, Rob},
  year = {2007},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {365},
  number = {1857},
  pages = {2117--2131},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2007.2080},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KJAVL8EX/NEWETA~1.PDF}
}

@inproceedings{newmanDesigningEvolvingElectronic2018,
  title = {Designing and {{Evolving}} an {{Electronic Agricultural Marketplace}} in {{Uganda}}},
  booktitle = {Proceedings of the 1st {{ACM SIGCAS Conference}} on {{Computing}} and {{Sustainable Societies}} ({{COMPASS}}) - {{COMPASS}} '18},
  author = {Newman, Neil and Bergquist, Lauren Falcao and Immorlica, Nicole and {Leyton-Brown}, Kevin and Lucier, Brendan and McIntosh, Craig and Quinn, John and Ssekibuule, Richard},
  year = {2018},
  pages = {1--11},
  publisher = {{ACM Press}},
  address = {{Menlo Park and San Jose, CA, USA}},
  doi = {10.1145/3209811.3209862},
  isbn = {978-1-4503-5816-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ESUM4YGE/2018-electronic-agricultural-marketplace.pdf}
}

@inproceedings{nguyenClassificationPartialLabels2008,
  title = {Classification with Partial Labels},
  booktitle = {Proceeding of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} 08},
  author = {Nguyen, Nam and Caruana, Rich},
  year = {2008},
  pages = {551},
  publisher = {{ACM Press}},
  address = {{Las Vegas, Nevada, USA}},
  doi = {10.1145/1401890.1401958},
  abstract = {In this paper, we address the problem of learning when some cases are fully labeled while other cases are only partially labeled, in the form of partial labels. Partial labels are represented as a set of possible labels for each training example, one of which is the correct label. We introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework. The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm regularized empirical risk using hinge loss. We also present an efficient algorithm for classification in the presence of partial labels. Experiments with different data sets show that partial label information improves the performance of classification when there is traditional fully-labeled data, and also yields reasonable performance in the absence of any fully labeled data.},
  isbn = {978-1-60558-193-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8AU75BX2/10.1.1.448.9323.pdf}
}

@article{niculescu-mizilLearningStructureRelated,
  title = {Learning the {{Structure}} of {{Related Tasks}}},
  author = {{Niculescu-Mizil}, Alexandru and Caruana, Rich},
  pages = {10},
  abstract = {We consider the problem of learning Bayes Net structures for related tasks. We present a formalism for learning related Bayes Net structures that takes advantage of the similarity between tasks by biasing toward learning similar structures for each task. Heuristic search is used to find a high scoring set of structures (one for each task), where the score for a set of structures is computed in a principled way. Experiments on synthetic problems generated from the ALARM and INSURANCE networks show that learning the structures for related tasks using the proposed method yields better results than learning the structures independently.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/55QH5L5Q/MizilCaruana.pdf}
}

@inproceedings{niculescu-mizilPredictingGoodProbabilities2005,
  title = {Predicting Good Probabilities with Supervised Learning},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {{Niculescu-Mizil}, Alexandru and Caruana, Rich},
  year = {2005},
  pages = {625--632},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102430},
  abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  isbn = {978-1-59593-180-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YRXHYUDM/calibration.icml05.crc.rev3.pdf}
}

@article{nielsenPrinciplesOptimalMetabolic2007,
  title = {Principles of Optimal Metabolic Network Operation},
  author = {Nielsen, Jens},
  year = {2007},
  month = jan,
  journal = {Molecular Systems Biology},
  volume = {3},
  number = {1},
  pages = {126},
  issn = {1744-4292, 1744-4292},
  doi = {10.1038/msb4100169},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MKAHNM6J/nielsen 2007 - Principles of optimal metabolic network operation - OPTISEVIL - EFs - SYSTEMS BIOLOGY.pdf}
}

@article{nishikawaSensitiveDependenceOptimal2017,
  title = {Sensitive {{Dependence}} of {{Optimal Network Dynamics}} on {{Network Structure}}},
  author = {Nishikawa, Takashi and Sun, Jie and Motter, Adilson E.},
  year = {2017},
  month = nov,
  journal = {Physical Review X},
  volume = {7},
  number = {4},
  pages = {041044},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.7.041044},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/82EH7883/nishikawa et al 2017 - Sensitive Dependence of Optimal Network Dynamics on Network Structure - OPTISEVIL - NETWORKS.pdf}
}

@article{nishimoriComparativeStudyPerformance2015,
  title = {Comparative {{Study}} of the {{Performance}} of {{Quantum Annealing}} and {{Simulated Annealing}}},
  author = {Nishimori, Hidetoshi and Tsuda, Junichi and Knysh, Sergey},
  year = {2015},
  month = jan,
  journal = {Physical Review E},
  volume = {91},
  number = {1},
  eprint = {1409.6386},
  eprinttype = {arxiv},
  pages = {012104},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.91.012104},
  abstract = {Relations of simulated annealing and quantum annealing are studied by a mapping from the transition matrix of classical Markovian dynamics of the Ising model to a quantum Hamiltonian and vice versa. It is shown that these two operators, the transition matrix and the Hamiltonian, share the eigenvalue spectrum. Thus, if simulated annealing with slow temperature change does not encounter a difficulty caused by an exponentially long relaxation time at a first-order phase transition, the same is true for the corresponding process of quantum annealing in the adiabatic limit. One of the important differences between the classical-to-quantum mapping and the converse quantum-to-classical mapping is that the Markovian dynamics of a short-range Ising model is mapped to a short-range quantum system, but the converse mapping from a short-range quantum system to a classical one results in long-range interactions. This leads to a difference in efficiencies that simulated annealing can be efficiently simulated by quantum annealing but the converse is not necessarily true. We conclude that quantum annealing is easier to implement and is more flexible than simulated annealing. We also point out that the present mapping can be extended to accommodate explicit time dependence of temperature, which is used to justify the quantum-mechanical analysis of simulated annealing by Somma, Batista, and Ortiz. Additionally, an alternative method to solve the non-equilibrium dynamics of the one-dimensional Ising model is provided through the classical-to-quantum mapping.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Quantum Physics},
  file = {/Users/bill/D/Zotero/storage/HIH7HZL5/Nishimori Tsuda 2014 - Comparative Study of the Performance of Quantum Annealing and Simulated Annealing.pdf}
}

@article{norbergComprehensiveEvaluationPredictive2019,
  title = {A Comprehensive Evaluation of Predictive Performance of 33 Species Distribution Models at Species and Community Levels},
  author = {Norberg, Anna and Abrego, Nerea and Blanchet, F. Guillaume and Adler, Frederick R. and Anderson, Barbara J. and Anttila, Jani and Ara{\'u}jo, Miguel B. and Dallas, Tad and Dunson, David and Elith, Jane and Foster, Scott D. and Fox, Richard and Franklin, Janet and Godsoe, William and Guisan, Antoine and O'Hara, Bob and Hill, Nicole A. and Holt, Robert D. and Hui, Francis K. C. and Husby, Magne and K{\aa}l{\aa}s, John Atle and Lehikoinen, Aleksi and Luoto, Miska and Mod, Heidi K. and Newell, Graeme and Renner, Ian and Roslin, Tomas and Soininen, Janne and Thuiller, Wilfried and Vanhatalo, Jarno and Warton, David and White, Matt and Zimmermann, Niklaus E. and Gravel, Dominique and Ovaskainen, Otso},
  year = {2019},
  month = aug,
  journal = {Ecological Monographs},
  volume = {89},
  number = {3},
  issn = {0012-9615, 1557-7015},
  doi = {10.1002/ecm.1370},
  abstract = {A large array of species distribution model (SDM) approaches has been developed for explaining and predicting the occurrences of individual species or species assemblages. Given the wealth of existing models, it is unclear which models perform best for interpolation or extrapolation of existing data sets, particularly when one is concerned with species assemblages. We compared the predictive performance of 33 variants of 15 widely applied and recently emerged SDMs in the context of multispecies data, including both joint SDMs that model multiple species together, and stacked SDMs that model each species individually combining the predictions afterward. We offer a comprehensive evaluation of these SDM approaches by examining their performance in predicting withheld empirical validation data of different sizes representing five different taxonomic groups, and for prediction tasks related to both interpolation and extrapolation. We measure predictive performance by 12 measures of accuracy, discrimination power, calibration, and precision of predictions, for the biological levels of species occurrence, species richness, and community composition. Our results show large variation among the models in their predictive performance, especially for communities comprising many species that are rare. The results do not reveal any major trade-offs among measures of model performance; the same models performed generally well in terms of accuracy, discrimination, and calibration, and for the biological levels of individual species, species richness, and community composition. In contrast, the models that gave the most precise predictions were not well calibrated, suggesting that poorly performing models can make overconfident predictions. However, none of the models performed well for all prediction tasks. As a general strategy, we therefore propose that researchers fit a small set of models showing complementary performance, and then apply a cross-validation procedure involving separate data to establish which of these models performs best for the goal of the study.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7RCAGYZ3/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY - ANNO.pdf;/Users/bill/D/Zotero/storage/B3925UT9/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S3.pdf;/Users/bill/D/Zotero/storage/FVPNNYJE/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S2.pdf;/Users/bill/D/Zotero/storage/KIZLVNRK/norberg et al 2019 - A  comprehensive  evaluation  of  predictive  performance  of  33  species  distribution  models  at  species and community levels - GUPPY - BDPG - SDM - UNCERTAINTY.appendix S1.pdf}
}

@article{nudelmanUnderstandingGameTheoreticAlgorithms,
  title = {Understanding {{Game-Theoretic Algorithms}}:},
  author = {Nudelman, Eugene and Wortman, Jennifer and Shoham, Yoav and {Leyton-Brown}, Kevin},
  pages = {28},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/73Z6DVXK/GAMUT (GAMES 2004).pdf}
}

@incollection{nudelmanUnderstandingRandomSAT2004a,
  title = {Understanding {{Random SAT}}: {{Beyond}} the {{Clauses-to-Variables Ratio}}},
  shorttitle = {Understanding {{Random SAT}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} \textendash{} {{CP}} 2004},
  author = {Nudelman, Eugene and {Leyton-Brown}, Kevin and Hoos, Holger H. and Devkar, Alex and Shoham, Yoav},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Wallace, Mark},
  year = {2004},
  volume = {3258},
  pages = {438--452},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30201-8_33},
  isbn = {978-3-540-23241-4 978-3-540-30201-8},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/TM3KPB4P/nudelman ... leyton-brown - understanding SAT - beyond the clauses-to-variables ratio.pdf}
}

@article{odonnellLectureDefinitionsGreedy,
  title = {Lecture 1: {{Definitions}}; Greedy Algorithm for {{Set-Cover}} \& {{Max-Coverage}}},
  author = {O'Donnell, Ryan and Shahaf, Dafna},
  pages = {5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VZJ3NEN6/lecture1.pdf}
}

@article{oliveiraThesisMappingEffectiveness,
  title = {Thesis: {{Mapping}} the {{Effectiveness}} of {{Automated Test Suite Generation Techniques}}},
  author = {Oliveira, Carlos},
  pages = {167},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DMJC4JD2/OLIVEIRA-Carlos Thesis.pdf}
}

@article{olson2017bm,
  title = {{{PMLB}}: A Large Benchmark Suite for Machine Learning Evaluation and Comparison},
  shorttitle = {{{PMLB}}},
  author = {Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
  year = {2017},
  month = dec,
  journal = {BioData Mining},
  volume = {10},
  number = {1},
  pages = {36},
  issn = {1756-0381},
  doi = {10.1186/s13040-017-0154-4},
  abstract = {The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.},
  langid = {english},
  keywords = {bdpg,benchmarking,machine learning},
  file = {/Users/bill/D/Zotero/storage/GD37UKDE/olson et al 2017 - PMLB - A Large Benchmark Suite for Machine Learning Evaluation and Comparison - EVALUATION - ANNO.pdf}
}

@incollection{onsjoSimpleMessagePassing2006,
  title = {A {{Simple Message Passing Algorithm}} for {{Graph Partitioning Problems}}},
  booktitle = {Algorithms and {{Computation}}},
  author = {Onsj{\"o}, Mikael and Watanabe, Osamu},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Asano, Tetsuo},
  year = {2006},
  volume = {4288},
  pages = {507--516},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11940128_51},
  abstract = {Motivated by the belief propagation, we propose a simple and deterministic message passing algorithm for the Graph Bisection problem and related problems. The running time of the main algorithm is linear w.r.t. the number of vertices and edges. For evaluating its average-case correctness, planted solution models are used. For the Graph Bisection problem under the standard planted solution model with probability parameters p and r, we prove that our algorithm yields a planted solution with probability {$>$} 1 - {$\delta$} if p - r = {$\Omega$}(n-1/2 log(n/{$\delta$})).},
  isbn = {978-3-540-49694-6 978-3-540-49696-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/I44BLF2D/chp%3A10.1007%2F11940128_51.pdf}
}

@article{pagano2013hp,
  title = {Ensemble Dressing for Hydrological Applications: {{ENSEMBLE DRESSING FOR HYDROLOGICAL APPLICATIONS}}},
  shorttitle = {Ensemble Dressing for Hydrological Applications},
  author = {Pagano, Thomas C. and Shrestha, Durga Lal and Wang, Q. J. and Robertson, David and Hapuarachchi, Prasantha},
  year = {2013},
  month = jan,
  journal = {Hydrological Processes},
  volume = {27},
  number = {1},
  pages = {106--116},
  issn = {08856087},
  doi = {10.1002/hyp.9313},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YVG7L4PK/pagano shrestha et al 2012 - Ensemble dressing for hydrological applications - GUPPY - ERROR - ENSEMBLES.pdf}
}

@article{palubeckisGeneratingHardTest,
  title = {Generating {{Hard Test Instances}} with {{Known Optimal Solution}} for the {{Rectilinear Quadratic Assignment Problem}}},
  author = {Palubeckis, G},
  pages = {30},
  abstract = {In this paper we consider the rectilinear version of the quadratic assignment problem (QAP). We define a class of edge-weighted graphs with nonnegatively valued bisections. For one important type of such graphs we provide a characterization of point sets on the plane for which the optimal value of the related QAP is zero. These graphs are used in the algorithms for generating rectilinear QAP instances with known provably optimal solutions. The basic algorithm of such type uses only triangles. Making a reduction from 3-dimensional matching, it is shown that the set of instances which can be generated by this algorithm is hard. The basic algorithm is extended to process graphs larger than triangles. We give implementation details of this extension and of four other variations of the basic algorithm. We compare these five and also two existing generators experimentally employing multi-start descent heuristic for the QAP as an examiner. The graphs with nonnegatively valued bisections can also be used in the construction of lower bounds on the optimal value for the rectilinear QAP.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UTKLJL8P/fulltext.pdf}
}

@article{papathanasiouEquilibriumPointBased,
  title = {An {{Equilibrium Point Based Humanoids Control Model}}},
  author = {Papathanasiou, Anthanasios},
  pages = {16},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/28MKX2LU/sanchis obituary - university of rochester newsletter-2006 - woman who generated NP-complet problems with known solutions - PROBLEM DIFFICULTY.pdf}
}

@misc{pappas,
  title = {Pappas et al 2009 - {{A Comparison}} of {{Heuristic}}, {{Meta-Heuristic}} and {{Optimal Approaches}} to the {{Selection}} of {{Conservation Area Networks}} - {{BDPG}} - {{RESERVE SELECTION}} - {{TABU SEARCH}} - {{ANNO}}.Pdf},
  author = {Pappas, Christopher},
  file = {/Users/bill/D/Zotero/storage/RJI2GVNQ/pappas et al 2009 - A Comparison of Heuristic, Meta-Heuristic and Optimal Approaches to the Selection of Conservation Area Networks - BDPG - RESERVE SELECTION - TABU SEARCH - ANNO.pdf}
}

@article{paredesLectureExploratorySpatial,
  title = {Lecture 3 - {{Exploratory Spatial Data Analysis}}},
  author = {Paredes, Dusan},
  pages = {33},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZR6JILIQ/d51cf1_4551851a74ebef487541c485b31c8de9.pdf}
}

@article{paredesLectureIntroductionSpatial,
  title = {Lecture 1 - {{Introduction}} to the {{Spatial Dependence}}   (from {{LeSage}} and {{Peace}}, 2009)},
  author = {Paredes, Dusan},
  pages = {29},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EQYPNTXP/d51cf1_106e5f17e7191445064c6ef65e8054f2.pdf}
}

@article{paredesLectureMakingRegression,
  title = {Lecture 0 - {{Making Regression Sense}}   (from {{Mostly Harmless Econometrics}})},
  author = {Paredes, Dusan},
  pages = {34},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WQ443PL6/d51cf1_2eef13b1432afc1f6b6d30d29c18ed63.pdf}
}

@article{paredesLectureMotivatingInterpreting,
  title = {Lecture 2 - {{Motivating}} and {{Interpreting Spatial Econometric Models}}  (from {{LeSage}} and {{Peace}}, 2009)},
  author = {Paredes, Dusan},
  pages = {26},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W8KF2KGI/d51cf1_a47ea74e132b7ceff52032ce20779bda.pdf}
}

@article{pearsonModelbasedUncertaintySpecies2006,
  title = {Model-Based Uncertainty in Species Range Prediction},
  author = {Pearson, Richard G. and Thuiller, Wilfried and Ara{\'u}jo, Miguel B. and {Martinez-Meyer}, Enrique and Brotons, Llu{\'i}s and McClean, Colin and Miles, Lera and Segurado, Pedro and Dawson, Terence P. and Lees, David C.},
  year = {2006},
  month = oct,
  journal = {Journal of Biogeography},
  volume = {33},
  number = {10},
  pages = {1704--1711},
  issn = {0305-0270, 1365-2699},
  doi = {10.1111/j.1365-2699.2006.01460.x},
  abstract = {Aim Many attempts to predict the potential range of species rely on environmental niche (or `bioclimate envelope') modelling, yet the effects of using different niche-based methodologies require further investigation. Here we investigate the impact that the choice of model can have on predictions, identify key reasons why model output may differ and discuss the implications that model uncertainty has for policy-guiding applications.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RXLKPVRZ/pearson et al 2006 - model-based uncertainty in species range prediction - BDPG - SDM - GUPPY.pdf}
}

@article{pembertonEpsilontransformationExploitingPhase1996,
  title = {Epsilon-Transformation: Exploiting Phase Transitions to Solve Combinatorial Optimization Problems},
  shorttitle = {Epsilon-Transformation},
  author = {Pemberton, Joseph C. and Zhang, Weixiong},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {297--325},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00057-7},
  abstract = {It has been shown that there exists a transition in the average-case complexity of tree search problems, from exponential to polynomial in the search depth. We develop a new method, called E-transformation, which makes use of this complexity transition, to find a suboptimal solution. With a random tree model, we show that, as the tree depth approaches infinity, the expected number of nodes expanded by branch-and-bound (BnB) using stransformation is at most cubic in the search depth, and that the relative error of the solution cost found with respect to the optimal solution cost is bounded above by a small constant. We also present an iterative version of c-transformation that can be used to find both suboptimal and optimal goal nodes. Depth-first BnB (DFBnB) using iterative Hransformation significantly improves upon DPBnB on random trees with large branching factors and deep goal nodes, finding a better solution sooner. We then present experimental results for ctransformation and iterative e-transformation on the asymmetric traveling salesman problem ( ATSP) and tbe maximum boolean satisfiability problem, and identify the conditions under which these two methods are effective. On the ATSP, DFBnB using Etransformation outperforms a well-known local search method, and DFBnB using iterative Etransformation improves upon DFBnB.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V97NIGIE/1-s2.0-0004370295000577-main.pdf}
}

@article{petersen2021esae,
  title = {Species Data for Understanding Biodiversity Dynamics: {{The}} What, Where and When of Species Occurrence Data Collection},
  shorttitle = {Species Data for Understanding Biodiversity Dynamics},
  author = {Petersen, Tanja K. and Speed, James D. M. and Gr{\o}tan, Vidar and Austrheim, Gunnar},
  year = {2021},
  month = jan,
  journal = {Ecological Solutions and Evidence},
  volume = {2},
  number = {1},
  issn = {2688-8319, 2688-8319},
  doi = {10.1002/2688-8319.12048},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QW7K9CRN/petersen et al 2021 - Species data for understanding biodiversity dynamics - The what, where and when of species occurrence data collection - GUPPY - BDPG - SDM - DATA BIAS - KOALA. - ANNO.pdf}
}

@article{phillipsSampleSelectionBias2009,
  title = {Sample Selection Bias and Presence-Only Distribution Models: Implications for Background and Pseudo-Absence Data},
  shorttitle = {Sample Selection Bias and Presence-Only Distribution Models},
  author = {Phillips, Steven J. and Dud{\'i}k, Miroslav and Elith, Jane and Graham, Catherine H. and Lehmann, Anthony and Leathwick, John and Ferrier, Simon},
  year = {2009},
  journal = {Ecological Applications},
  volume = {19},
  number = {1},
  pages = {181--197},
  file = {/Users/bill/D/Zotero/storage/3HTESF67/Phillips et al_2009_Sample selection bias and presence-only distribution models.pdf}
}

@article{phippsImprovingSamplingbasedUncertainty,
  title = {Improving {{Sampling-based Uncertainty Quantification Performance Through Embedded Ensemble Propagation}}},
  author = {Phipps, E and D'Elia, M and Ebeida, M and Rushdi, A},
  pages = {1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/95PMKJGV/phipps d elia et al 2017 - Improving Sampling-based Uncertainty Quantification Performance Through Embedded Ensemble Propagation - PROBLEM DIFFICULTY - BDPG.pdf}
}

@inproceedings{phuaAdaptiveCommunalDetection2007,
  title = {Adaptive Communal Detection in Search of Adversarial Identity Crime},
  booktitle = {Proceedings of the 2007 International Workshop on {{Domain}} Driven Data Mining  - {{DDDM}} '07},
  author = {Phua, Clifton and Lee, Vincent and {Smith-Miles}, Kate and Gayler, Ross},
  year = {2007},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{San Jose, California}},
  doi = {10.1145/1288552.1288553},
  abstract = {This paper is on adaptive real-time searching of credit application data streams for identity crime with many search parameters. Specifically, we concentrated on handling our domain-specific adversarial activity problem with the adaptive Communal Analysis Suspicion Scoring (CASS) algorithm. CASS's main novel theoretical contribution is in the formulation of State-ofAlert (SoA) which sets the condition of reduced, same, or heightened watchfulness; and Parameter-of-Change (PoC) which improves detection ability with pre-defined parameter values for each SoA. With pre-configured SoA policy and PoC strategy, CASS determines when, what, and how much to adapt its search parameters to ongoing adversarial activity. The above approach is validated with three sets of experiments, where each experiment is conducted on several million real credit applications and measured with three appropriate performance metrics. Significant improvements are achieved over previous work, with the discovery of some practical insights of adaptivity into our domain.},
  isbn = {978-1-59593-846-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/98959G55/p1-phua.pdf}
}

@inproceedings{phuaAdaptiveCommunalDetection2007a,
  title = {Adaptive Communal Detection in Search of Adversarial Identity Crime},
  booktitle = {Proceedings of the 2007 International Workshop on {{Domain}} Driven Data Mining  - {{DDDM}} '07},
  author = {Phua, Clifton and Lee, Vincent and {Smith-Miles}, Kate and Gayler, Ross},
  year = {2007},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{San Jose, California}},
  doi = {10.1145/1288552.1288553},
  abstract = {This paper is on adaptive real-time searching of credit application data streams for identity crime with many search parameters. Specifically, we concentrated on handling our domain-specific adversarial activity problem with the adaptive Communal Analysis Suspicion Scoring (CASS) algorithm. CASS's main novel theoretical contribution is in the formulation of State-ofAlert (SoA) which sets the condition of reduced, same, or heightened watchfulness; and Parameter-of-Change (PoC) which improves detection ability with pre-defined parameter values for each SoA. With pre-configured SoA policy and PoC strategy, CASS determines when, what, and how much to adapt its search parameters to ongoing adversarial activity. The above approach is validated with three sets of experiments, where each experiment is conducted on several million real credit applications and measured with three appropriate performance metrics. Significant improvements are achieved over previous work, with the discovery of some practical insights of adaptivity into our domain.},
  isbn = {978-1-59593-846-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RB8ZSDSA/p1-phua.pdf}
}

@techreport{poisotSpeciesWhyEcological2014,
  type = {Preprint},
  title = {Beyond Species: Why Ecological Interaction Networks Vary through Space and Time},
  shorttitle = {Beyond Species},
  author = {Poisot, Timoth{\'e}e and Stouffer, Daniel B and Gravel, Dominique},
  year = {2014},
  month = jan,
  institution = {{Ecology}},
  doi = {10.1101/001677},
  abstract = {Community ecology is tasked with the considerable challenge of predicting the structure, and properties, of emerging ecosystems. It requires the ability to understand how and why species interact, as this will allow the development of mechanism-based predictive models, and as such to better characterize how ecological mechanisms act locally on the existence of inter-specific interactions. Here we argue that the current conceptualization of species interaction networks is ill-suited for this task. Instead, we propose that future research must start to account for the intrinsic variability of species interactions, then scale up from here onto complex networks. This can be accomplished simply by recognizing that there exists intra-specific variability, in traits or properties related to the establishment of species interactions. By shifting the scale towards population-based processes, we show that this new approach will improve our predictive ability and mechanistic understanding of how species interact over large spatial or temporal scales.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LMN9E5XR/001677.full.pdf}
}

@article{poisotStructureProbabilisticNetworks2016,
  title = {The Structure of Probabilistic Networks},
  author = {Poisot, Timoth{\'e}e and Cirtwill, Alyssa R. and Cazelles, K{\'e}vin and Gravel, Dominique and Fortin, Marie-Jos{\'e}e and Stouffer, Daniel B.},
  editor = {Vamosi, Jana},
  year = {2016},
  month = mar,
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {3},
  pages = {303--312},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.12468},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EH3RFBDG/2041-210X.12468.pdf}
}

@article{poisotSyntheticDatasetsCommunity2016,
  title = {Synthetic Datasets and Community Tools for the Rapid Testing of Ecological Hypotheses},
  author = {Poisot, Timoth{\'e}e and Gravel, Dominique and Leroux, Shawn and Wood, Spencer A. and Fortin, Marie-Jos{\'e}e and Baiser, Benjamin and Cirtwill, Alyssa R. and Ara{\'u}jo, Miguel B. and Stouffer, Daniel B.},
  year = {2016},
  month = apr,
  journal = {Ecography},
  volume = {39},
  number = {4},
  pages = {402--408},
  issn = {09067590},
  doi = {10.1111/ecog.01941},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DT855GPJ/ecog.01941.pdf}
}

@article{polhillWhatEveryAgentbased2006,
  title = {What Every Agent-Based Modeller Should Know about Floating Point Arithmetic},
  author = {Polhill, J. Gary and Izquierdo, Luis R. and Gotts, Nicholas M.},
  year = {2006},
  month = mar,
  journal = {Environmental Modelling \& Software},
  volume = {21},
  number = {3},
  pages = {283--309},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2004.10.011},
  abstract = {Floating point arithmetic is a subject all too often ignored, yet, for agent-based models in particular, it has the potential to create misleading results, and even to influence emergent outcomes of the model. Using a simple demonstration model, this paper illustrates the problems that accumulated floating point errors can cause, and compares a number of techniques that might be used to address them. We show that inexact representation of parameter values, imprecision in calculation results, and differing implementations of mathematical expressions can significantly influence the behaviour of the model, and create issues for replicating results, though they do not necessarily do so. None of the techniques offer a failsafe approach that can be applied in any situation, though interval arithmetic is the most promising.},
  langid = {english},
  keywords = {agent-based,floating point,guppy},
  file = {/Users/bill/D/Zotero/storage/I9B2EG2H/polhill izquierdo gotts 2006 - what every agent-based modeller should know about floating point arithmetic - envmodsoft.pdf}
}

@article{postvanderburgRoleBudgetSufficiency2014,
  title = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management: {{Robust Species Management}}},
  shorttitle = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management},
  author = {{Post van der Burg}, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
  year = {2014},
  month = jan,
  journal = {The Journal of Wildlife Management},
  volume = {78},
  number = {1},
  pages = {153--163},
  issn = {0022541X},
  doi = {10.1002/jwmg.638},
  abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used field data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efficient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufficient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufficient budget. Below this budget, the costefficient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufficient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one's budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. \'O 2013 The Wildlife Society.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/66BU9ZN6/Post van der Burg et al. - 2014 - On the role of budget sufficiency, cost efficiency.pdf}
}

@article{postvanderburgRoleBudgetSufficiency2014a,
  title = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management: {{Robust Species Management}}},
  shorttitle = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management},
  author = {{Post van der Burg}, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
  year = {2014},
  month = jan,
  journal = {The Journal of Wildlife Management},
  volume = {78},
  number = {1},
  pages = {153--163},
  issn = {0022541X},
  doi = {10.1002/jwmg.638},
  abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used field data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efficient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufficient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufficient budget. Below this budget, the costefficient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufficient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one's budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. \'O 2013 The Wildlife Society.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RJA5UU5D/burg et al 2014 - On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management - BDPG.pdf}
}

@article{postvanderburgRoleBudgetSufficiency2014b,
  title = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management: {{Robust Species Management}}},
  shorttitle = {On the Role of Budget Sufficiency, Cost Efficiency, and Uncertainty in Species Management},
  author = {{Post van der Burg}, Max and Bly, Bartholomew B. and Vercauteren, Tammy and Grand, James B. and Tyre, Andrew J.},
  year = {2014},
  month = jan,
  journal = {The Journal of Wildlife Management},
  volume = {78},
  number = {1},
  pages = {153--163},
  issn = {0022541X},
  doi = {10.1002/jwmg.638},
  abstract = {Many conservation planning frameworks rely on the assumption that one should prioritize locations for management actions based on the highest predicted conservation value (i.e., abundance, occupancy). This strategy may underperform relative to the expected outcome if one is working with a limited budget or the predicted responses are uncertain. Yet, cost and tolerance to uncertainty rarely become part of species management plans. We used field data and predictive models to simulate a decision problem involving western burrowing owls (Athene cunicularia hypugaea) using prairie dog colonies (Cynomys ludovicianus) in western Nebraska. We considered 2 species management strategies: one maximized abundance and the other maximized abundance in a cost-efficient way. We then used heuristic decision algorithms to compare the 2 strategies in terms of how well they met a hypothetical conservation objective. Finally, we performed an infogap decision analysis to determine how these strategies performed under different budget constraints and uncertainty about owl response. Our results suggested that when budgets were sufficient to manage all sites, the maximizing strategy was optimal and suggested investing more in expensive actions. This pattern persisted for restricted budgets up to approximately 50\% of the sufficient budget. Below this budget, the costefficient strategy was optimal and suggested investing in cheaper actions. When uncertainty in the expected responses was introduced, the strategy that maximized abundance remained robust under a sufficient budget. Reducing the budget induced a slight trade-off between expected performance and robustness, which suggested that the most robust strategy depended both on one's budget and tolerance to uncertainty. Our results suggest that wildlife managers should explicitly account for budget limitations and be realistic about their expected levels of performance. \'O 2013 The Wildlife Society.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ADQMHC2Q/fulltext(1).pdf}
}

@article{powellTurningStudentsProblem,
  title = {Turning {{Students}} into {{Problem Solvers}}},
  author = {Powell, Larkin A and Tyre, Andrew J and Conroy, Michael J and Peterson, James T and Williams, B Ken},
  pages = {4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2K3QMFWV/Powell et al. - Turning Students into Problem Solvers.pdf}
}

@article{powellTurningStudentsProblema,
  title = {Turning {{Students}} into {{Problem Solvers}}},
  author = {Powell, Larkin A and Tyre, Andrew J and Conroy, Michael J and Peterson, James T and Williams, B Ken},
  pages = {4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DGRD6E6Z/fulltext(3).pdf}
}

@article{powlesNewYorkCity,
  title = {New {{York City}}'s {{Bold}}, {{Flawed Attempt}} to {{Make Algorithms Accountable}}},
  author = {Powles, Julia},
  journal = {New York City},
  pages = {6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FKD8QPR3/powles 2017 - New York Citys Bold Flawed Attempt to Make Algorithms Accountable - OPTISEVIL - EFs.pdf}
}

@article{pressey2002pipgeae,
  title = {Classics in Physical Geography Revisited},
  author = {Pressey, R. L.},
  year = {2002},
  month = sep,
  journal = {Progress in Physical Geography: Earth and Environment},
  volume = {26},
  number = {3},
  pages = {434--441},
  issn = {0309-1333, 1477-0296},
  doi = {10.1191/0309133302pp347xx},
  langid = {english},
  keywords = {reserve selection},
  file = {/Users/bill/D/Zotero/storage/B4YCH27Z/pressey 2002 - the first reserve selection algorithm - ANNO - BDPG - RESERVE SELECTION.pdf}
}

@article{pressey2017bc,
  title = {From Displacement Activities to Evidence-Informed Decisions in Conservation},
  author = {Pressey, Robert L. and Weeks, Rebecca and Gurney, Georgina G.},
  year = {2017},
  month = aug,
  journal = {Biological Conservation},
  volume = {212},
  pages = {337--348},
  issn = {00063207},
  doi = {10.1016/j.biocon.2017.06.009},
  abstract = {This paper highlights a disjunction between the basic motivation of conservation planners, policy-makers, and managers, which is to make a positive difference for biodiversity, and many of our day-to-day activities, which are tangential (at best) to the goal of avoiding biodiversity loss. At the core of this problem is the use of conservation measures (inputs, outputs, and outcomes) that do not explicitly address conservation impact, and thus risk undermining its achievement. These measures are used to formulate policy targets and operational objectives, gauge progress towards them, and identify priorities for action. In particular, the pervasive use of representation of biodiversity features as a sole basis for identifying priorities, and the considerable global effort directed towards increasing protected-area extent and assessing protected-area management effectiveness, exemplify that much conservation decision-making is founded more on belief systems than evidence. Measures such as the extent or representativeness of protected areas risk misdirecting conservation actions towards areas of low impact and misleading decision-makers and the public about conservation progress. To promote more effective, evidence-informed decision-making, analytical evidence can and should be used to test and refine decision-makers' implicit models of the world, focusing on predicting conservation impact - the future difference made by our future actions - to increase our effectiveness and accountability.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FMN4GYGR/pressey et al 2017 - From displacement activities to evidence-informed decisions in conservation - EFs - PREDICTION - BDPG - ANNO.pdf}
}

@article{presseyEffectivenessAlternativeHeuristic1997,
  title = {Effectiveness of Alternative Heuristic Algorithms for Identifying Indicative Minimum Requirements for Conservation Reserves},
  author = {Pressey, R.L. and Possingham, H.P. and Day, J.R.},
  year = {1997},
  month = may,
  journal = {Biological Conservation},
  volume = {80},
  number = {2},
  pages = {207--219},
  issn = {00063207},
  doi = {10.1016/S0006-3207(96)00045-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/26TMVFPE/Effectiveness_of.pdf}
}

@article{presseyEffectsDataCharacteristics1999,
  title = {Effects of Data Characteristics on the Results of Reserve Selection Algorithms},
  author = {Pressey, R. L. and Possingham, H. P. and Logan, V. S. and Day, J. R. and Williams, P. H.},
  year = {1999},
  month = jan,
  journal = {Journal of Biogeography},
  volume = {26},
  number = {1},
  pages = {179--191},
  issn = {0305-0270, 1365-2699},
  doi = {10.1046/j.1365-2699.1999.00258.x},
  abstract = {We tested the effects of four data characteristics on the results of reserve selection algorithms. The data characteristics were nestedness of features (land types in this case), rarity of features, size variation of sites (potential reserves) and size of data sets (numbers of sites and features). We manipulated data sets to produce three levels, with replication, of each of these data characteristics while holding the other three characteristics constant. We then used an optimizing algorithm and three heuristic algorithms to select sites to solve several reservation problems. We measured efficiency as the number or total area of selected sites, indicating the relative cost of a reserve system. Higher nestedness increased the efficiency of all algorithms (reduced the total cost of new reserves). Higher rarity reduced the efficiency of all algorithms (increased the total cost of new reserves). More variation in site size increased the efficiency of all algorithms expressed in terms of total area of selected sites. We measured the suboptimality of heuristic algorithms as the percentage increase of their results over optimal (minimum possible) results. Suboptimality is a measure of the reliability of heuristics as indicative costing analyses. Higher rarity reduced the suboptimality of heuristics (increased their reliability) and there is some evidence that more size variation did the same for the total area of selected sites. We discuss the implications of these results for the use of reserve selection algorithms as indicative and real-world planning tools.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JAXE6PB3/Pressey et al. - 1999 - Effects of data characteristics on the results of reserve selection algorithms - BDPG -  PROBLEM DIFFICULTY - ANNO.pdf}
}

@article{presseyOptimalityReserveSelection1996,
  title = {Optimality in Reserve Selection Algorithms: {{When}} Does It Matter and How Much?},
  shorttitle = {Optimality in Reserve Selection Algorithms},
  author = {Pressey, R.L. and Possingham, H.P. and Margules, C.R.},
  year = {1996},
  journal = {Biological Conservation},
  volume = {76},
  number = {3},
  pages = {259--267},
  issn = {00063207},
  doi = {10.1016/0006-3207(95)00120-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MDLUTMMH/hp_bc_76_96.pdf}
}

@article{prosserEmpiricalStudyPhase1996,
  title = {An Empirical Study of Phase Transitions in Binary Constraint Satisfaction Problems},
  author = {Prosser, Patrick},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {81--109},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00048-8},
  abstract = {An empirical study of randomly generated binary constraint satisfaction problems reveals that for problems with a given number of variables, domain size, and connectivity there is a critical level of constraint tightness at which a phase transition occurs. At the phase transition, problems change from being soluble to insoluble, and the difficulty of problems increases dramatically. A theory developed by Williams and Hogg [44], and independently developed by Smith [37], predicts where the hardest problems should occur. It is shown that the theory is in close agreement with the empirical results, except when constraint graphs are sparse.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/B2VCWB7W/1-s2.0-0004370295000488-main.pdf}
}

@article{pullinConservationManagersUse2004,
  title = {Do Conservation Managers Use Scientific Evidence to Support Their Decision-Making?},
  author = {Pullin, Andrew S and Knight, Teri M and Stone, David A and Charman, Kevin},
  year = {2004},
  month = sep,
  journal = {Biological Conservation},
  volume = {119},
  number = {2},
  pages = {245--252},
  issn = {00063207},
  doi = {10.1016/j.biocon.2003.11.007},
  abstract = {Conservation involves making decisions on appropriate action from a wide range of options. For conservation to be effective, decision-makers need to know what actions do and do not work. Ideally, decisions should be based on effectiveness as demonstrated by scientific experiment or systematic review of evidence. Can decision-makers get this kind of information? We undertook a formal assessment of the extent to which scientific evidence is being used in conservation practice by conducting a survey of management plans and their compilers from major conservation organizations within the UK. Data collected suggest that the majority of conservation actions remain experience-based and rely heavily on traditional land management practices because, many management interventions remain unevaluated and, although some evidence exists, much is not readily accessible in a suitable form. We argue that nature conservation along with other fields of applied ecology, should exploit the concept of evidence-based practice developed and used in medicine and public health that aims to provide the best available evidence to the decision-maker(s) on the likely outcomes of alternative actions. Through critical evaluation, we present the challenges and benefits of adopting evidence based practice from the decision-makers point of view and identify the process to be followed to make it work.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7LDMFU93/8 - pullin et al 2004 - Do conservation managers use scientific evidence to support their decision-making.pdf}
}

@article{pullinDoingMoreGood2009,
  title = {Doing More Good than Harm \textendash{} {{Building}} an Evidence-Base for Conservation and Environmental Management},
  author = {Pullin, Andrew S. and Knight, Teri M.},
  year = {2009},
  month = may,
  journal = {Biological Conservation},
  volume = {142},
  number = {5},
  pages = {931--934},
  issn = {00063207},
  doi = {10.1016/j.biocon.2009.01.010},
  abstract = {The problems of environmental change and biodiversity loss have entered the mainstream political agenda. Given the call from an increasingly influential environmental lobby for government and wider society to make both financial and personal sacrifices to address these problems, it seems likely that conservation biologists and environmental managers will be asked tough questions of the general form `are conservation interventions effective?' and, `are they doing more good than harm?' Science constantly advances and must remain open to challenge, but managers and policy formers require an interim product (an evidence-base) to underpin their current decision-making. The health services have been using the objective and transparent methodology of systematic review to summarise the evidence-base relating to the effectiveness of interventions. Environmental management has, up until now, had no formal shared evidence-base of this kind. Reviewing recent developments in evidence-based practice, this paper introduces a `systematic review' section for this journal and argues that constructing an evidence-based framework for environment management is possible, the challenge is scaling it up to engage the global scientific community. We draw on the history of evidence-based healthcare, but also on the differences between healthcare and conservation, to set out the challenges in creating a Collaboration for Environmental Evidence that develops a library of systematic reviews on the effectiveness of conservation and environmental interventions.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/H5G8EU92/12 - pullin knight 2009 - Doing more good than harm â Building an evidence-base for conservation and environmental management.pdf}
}

@inproceedings{qiaoTakingTooMany2010,
  title = {Taking {{Too Many Destinations Can Be Bad}} for {{Traceroute Sampling}}},
  booktitle = {2010 {{Proceedings}} of 19th {{International Conference}} on {{Computer Communications}} and {{Networks}}},
  author = {Qiao, Zhongliang and Chen, Mingming and Xu, Ke},
  year = {2010},
  month = aug,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Zurich, Switzerland}},
  doi = {10.1109/ICCCN.2010.5560162},
  abstract = {Considerable effort has been spent on collecting all the information of routers by using traceroute-like probes in the router-level topology measurements. This method has been argued to introduce uncontrolled sampling biases on statistical properties of the sample graph and heavy load to the network being measured. In order to improve the quality of the maps induced by the method, researchers are starting to investigate the deployment of large-scale distributed systems. But the lack of sources, the additional load introduced to the network and the potential uncontrolled scale in the IPv6 network cast a shadow over this direction. In this paper, we study traceroute sampling to represent the topology. Instead of finding a general strategy that would match all the graph properties, we focus on testing the impact of the proportion of destinations and sources on a single or several properties of the graph. We argue that, in order to obtain a more accurate sample graph, the general method of taking a small set of sources to perform traceroute-like probes to a large set of destinations is very unreasonable. Our results obtained from simulated experiments show that as the proportion of destinations and sources increases, the resulting properties of the sample graph can differ sharply from the underlying graph. The results also show that there is no single perfect proportion answer to meet all the graph properties but the small often perform better than the large overall. When we do the same measurement on several real-world networks, we find strong evidence for sampling bias because of taking so many destinations.},
  isbn = {978-1-4244-7114-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WMBEM6K3/ICCCN2010.pdf}
}

@article{quinnApplicationDetectabilityUse2011,
  title = {Application of Detectability in the Use of Indicator Species: {{A}} Case Study with Birds},
  shorttitle = {Application of Detectability in the Use of Indicator Species},
  author = {Quinn, John E. and Brandle, James R. and Johnson, Ron J. and Tyre, Andrew J.},
  year = {2011},
  month = sep,
  journal = {Ecological Indicators},
  volume = {11},
  number = {5},
  pages = {1413--1418},
  issn = {1470160X},
  doi = {10.1016/j.ecolind.2011.03.003},
  abstract = {The use of indicator species is popular in ecological monitoring and management. In recent years, new methods to improve the quality and application of indicator data have been proposed and developed. Here we propose the use of detection probability in the selection and application of indicator species. We evaluated environmental and observer factors believed to affect detection of potential species. Observer effects were the most evident factor and may necessitate the greatest consideration in the use of indicator species. Our results call attention to the fact that raw counts are far from accurate and that the use of detection probability can and should be incorporated into sampling protocols, species selection, and the allocation of effort for projects that use indicator species as part of monitoring and management programs.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3CUY68WZ/Quinn et al. - 2011 - Application of detectability in the use of indicat.pdf}
}

@article{quinnApplicationDetectabilityUse2011a,
  title = {Application of Detectability in the Use of Indicator Species: {{A}} Case Study with Birds},
  shorttitle = {Application of Detectability in the Use of Indicator Species},
  author = {Quinn, John E. and Brandle, James R. and Johnson, Ron J. and Tyre, Andrew J.},
  year = {2011},
  month = sep,
  journal = {Ecological Indicators},
  volume = {11},
  number = {5},
  pages = {1413--1418},
  issn = {1470160X},
  doi = {10.1016/j.ecolind.2011.03.003},
  abstract = {The use of indicator species is popular in ecological monitoring and management. In recent years, new methods to improve the quality and application of indicator data have been proposed and developed. Here we propose the use of detection probability in the selection and application of indicator species. We evaluated environmental and observer factors believed to affect detection of potential species. Observer effects were the most evident factor and may necessitate the greatest consideration in the use of indicator species. Our results call attention to the fact that raw counts are far from accurate and that the use of detection probability can and should be incorporated into sampling protocols, species selection, and the allocation of effort for projects that use indicator species as part of monitoring and management programs.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4QEJIX4U/Quinn et al 2011 Application of detectability.pdf}
}

@article{rardinExperimentalEvaluationHeuristic,
  title = {Experimental {{Evaluation}} of {{Heuristic Optimization Algorithms}}: {{A Tutorial}}},
  author = {Rardin, Ronald L and Uzsoy, Reha},
  pages = {44},
  abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically\textemdash by applying procedures to a collection of specific instances and comparing the observed solution quality and computational burden.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2ATTQTGK/fulltext(8).pdf}
}

@article{rardinExperimentalEvaluationHeuristica,
  title = {Experimental {{Evaluation}} of {{Heuristic Optimization Algorithms}}: {{A Tutorial}}},
  author = {Rardin, Ronald L and Uzsoy, Reha},
  pages = {44},
  abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically\textemdash by applying procedures to a collection of specific instances and comparing the observed solution quality and computational burden.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LXXVE6YL/Rardin Uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{rardinExperimentalEvaluationHeuristicb,
  title = {Experimental {{Evaluation}} of {{Heuristic Optimization Algorithms}}: {{A Tutorial}}},
  author = {Rardin, Ronald L and Uzsoy, Reha},
  pages = {44},
  abstract = {Heuristic optimization algorithms seek good feasible solutions to optimization problems in circumstances where the complexities of the problem or the limited time available for solution do not allow exact solution. Although worst case and probabilistic analysis of algorithms have produced insight on some classic models, most of the heuristics developed for large optimization problem must be evaluated empirically\textemdash by applying procedures to a collection of specific instances and comparing the observed solution quality and computational burden.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FM7J3R4T/rardin uzsoy 2001 - Experimental Evaluation of Heuristic Optimization Algorithms - A Tutorial.pdf}
}

@techreport{rcoreteam2019,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2019},
  address = {{Vienna, Austria}},
  institution = {{R Foundation for Statistical Computing}}
}

@article{reganROBUSTDECISIONMAKINGSEVERE2005,
  title = {{{ROBUST DECISION-MAKING UNDER SEVERE UNCERTAINTY FOR CONSERVATION MANAGEMENT}}},
  author = {Regan, Helen M. and {Ben-Haim}, Yakov and Langford, Bill and Wilson, William G. and Lundberg, Per and Andelman, Sandy J. and Burgman, Mark A.},
  year = {2005},
  month = aug,
  journal = {Ecological Applications},
  volume = {15},
  number = {4},
  pages = {1471--1477},
  issn = {1051-0761},
  doi = {10.1890/03-5419},
  abstract = {In conservation biology it is necessary to make management decisions for endangered and threatened species under severe uncertainty. Failure to acknowledge and treat uncertainty can lead to poor decisions. To illustrate the importance of considering uncertainty, we reanalyze a decision problem for the Sumatran rhino, Dicerorhinus sumatrensis, using information-gap theory to propagate uncertainties and to rank management options. Rather than requiring information about the extent of parameter uncertainty at the outset, information-gap theory addresses the question of how much uncertainty can be tolerated before our decision would change. It assesses the robustness of decisions in the face of severe uncertainty. We show that different management decisions may result when uncertainty in utilities and probabilities are considered in decision-making problems. We highlight the importance of a full assessment of uncertainty in conservation management decisions to avoid, as much as possible, undesirable outcomes.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/C22Q7IRR/regan ben-haim langford et al 2005 - ROBUST DECISION-MAKING UNDER SEVERE UNCERTAINTY FOR CONSERVATION MANAGEMENT.pdf}
}

@article{reichardtDetectableClusterStructure2008,
  title = {({{Un}})Detectable Cluster Structure in Sparse Networks},
  author = {Reichardt, Joerg and Leone, Michele},
  year = {2008},
  month = aug,
  journal = {Physical Review Letters},
  volume = {101},
  number = {7},
  eprint = {0711.1452},
  eprinttype = {arxiv},
  pages = {078701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.101.078701},
  abstract = {We study the problem of recovering a known cluster structure in a sparse network, also known as the planted partitioning problem, by means of statistical mechanics. We find a sharp transition from un-recoverable to recoverable structure as a function of the separation of the clusters. For multivariate data, such transitions have been observed frequently, but always as a function of the number of data points provided, i.e. given a large enough data set, two point clouds can always be recognized as different clusters, as long as their separation is non-zero. In contrast, for the sparse networks studied here, a cluster structure remains undetectable even in an infinitely large network if a critical separation is not exceeded. We give analytic formulas for this critical separation as a function of the degree distribution of the network and calculate the shape of the recoverability-transition. Our findings have implications for unsupervised learning and data-mining in relational data bases and provide bounds on the achievable performance of graph clustering algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/bill/D/Zotero/storage/AR9WC8NB/0711.1452.pdf}
}

@article{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/bill/D/Zotero/storage/QRCQ9SPS/ribeiro singh guestrin 2016 - why should i trust you - explaining the predictions of any classifier - ML - SUBMODULAR - BDPG - GUPPY.pdf}
}

@techreport{rice1975,
  title = {The Algorithm Selection Problem},
  author = {Rice, John R.},
  year = {1975},
  number = {CSD-TR 152},
  address = {{West Lafayette, Indiana}},
  institution = {{Purdue University}},
  abstract = {Publisher Summary The problem of selecting an effective algorithm arises in a wide variety of situations. This chapter starts with a discussion on abstract models: the basic model and associated problems, the model with selection based on features, and the model with variable performance criteria. One objective of this chapter is to explore the applicability of the approximation theory to the algorithm selection problem. There is an intimate relationship here and that the approximation theory forms an appropriate base upon which to develop a theory of algorithm selection methods. The approximation theory currently lacks much of the necessary machinery for the algorithm selection problem. There is a need to develop new results and apply known techniques to these new circumstances. The final pages of this chapter form a sort of appendix, which lists 15 specific open problems and questions in this area. There is a close relationship between the algorithm selection problem and the general optimization theory. This is not surprising since the approximation problem is a special form of the optimization problem. Most realistic algorithm selection problems are of moderate to high dimensionality and thus one should expect them to be quite complex. One consequence of this is that most straightforward approaches (even well-conceived ones) are likely to lead to enormous computations for the best selection. The single most important part of the solution of a selection problem is the appropriate choice of the form for selection mapping. It is here that theories give the least guidance and that the art of problem solving is most crucial.},
  annotation = {published later but can't find a copy yet: Advances in Computers Volume 15, 1976, Pages 65-118},
  file = {/Users/bill/D/Zotero/storage/J64BYEQR/rice 1975 - the algorithm selection problem - tech report - TR 75-152.pdf}
}

@article{rice1976ac,
  title = {The {{Algorithm Selection Problem}}},
  author = {Rice, John R.},
  year = {1976},
  journal = {Advances in Computers},
  volume = {15},
  pages = {65--118},
  doi = {10.1016/S0065-2458(08)60520-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PQHUFAVQ/rice 1976 - the algorithm selection problem - BDPG - PROBLEM DIFFICULTY - BENCHMARKING - ALGORITHM SELECTION.pdf}
}

@article{rivera-hutinelEffectsSamplingCompleteness2012,
  title = {Effects of Sampling Completeness on the Structure of Plant\textendash Pollinator Networks},
  author = {{Rivera-Hutinel}, A. and Bustamante, R. O. and Mar{\'i}n, V. H. and Medel, R.},
  year = {2012},
  month = jul,
  journal = {Ecology},
  volume = {93},
  number = {7},
  pages = {1593--1603},
  issn = {0012-9658},
  doi = {10.1890/11-1803.1},
  abstract = {Plant\textendash animal interaction networks provide important information on community organization. One of the most critical assumptions of network analysis is that the observed interaction patterns constitute an adequate sample of the set of interactions present in plant\textendash animal communities. In spite of its importance, few studies have evaluated this assumption, and in consequence, there is no consensus on the sensitivity of network metrics to sampling methodological shortcomings. In this study we examined how variation in sampling completeness influences the estimation of six network metrics frequently used in the literature (connectance, nestedness, modularity, robustness to species loss, path length, and centralization). We analyzed data of 186 flowering plants and 336 pollinator species in 10 networks from a forest-fragmented system in central Chile. Using species-based accumulation curves, we estimated the deviation of network metrics in undersampled communities with respect to exhaustively sampled communities and the effect of network size and sampling evenness on network metrics. Our results indicate that: (1) most metrics were affected by sampling completeness but differed in their sensitivity to sampling effort; (2) nestedness, modularity, and robustness to species loss were less influenced by insufficient sampling than connectance, path length, and centralization; (3) robustness was mildly influenced by sampling evenness. These results caution studies that summarize information from databases with high, or unknown, heterogeneity in sampling effort per species and should stimulate researchers to report sampling intensity to standardize its effects in the search for broad patterns in plant\textendash pollinator networks.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/T3PZKRTH/Effects_of_sampling_completeness_on_the_structure_.pdf}
}

@article{robillardAssessingShelfLife2017,
  title = {Assessing the Shelf Life of Cost-Efficient Conservation Plans for Species at Risk across Gradients of Agricultural Land Use: {{Cost-Efficient Conservation}}},
  shorttitle = {Assessing the Shelf Life of Cost-Efficient Conservation Plans for Species at Risk across Gradients of Agricultural Land Use},
  author = {Robillard, Cassandra M. and Kerr, Jeremy T.},
  year = {2017},
  month = aug,
  journal = {Conservation Biology},
  volume = {31},
  number = {4},
  pages = {837--847},
  issn = {08888892},
  doi = {10.1111/cobi.12886},
  abstract = {High costs of land in agricultural regions warrant spatial prioritization approaches to conservation that explicitly consider land prices to produce protected-area networks that accomplish targets efficiently. However, land-use changes in such regions and delays between plan design and implementation may render optimized plans obsolete before implementation occurs. To measure the shelf life of cost-efficient conservation plans, we simulated a land-acquisition and restoration initiative aimed at conserving species at risk in Canada's farmlands. We accounted for observed changes in land-acquisition costs and in agricultural intensity based on censuses of agriculture taken from 1986 to 2011. For each year of data, we mapped costs and areas of conservation priority designated using Marxan. We compared plans to test for changes through time in the arrangement of high-priority sites and in the total cost of each plan. For acquisition costs, we measured the savings from accounting for prices during site selection. Land-acquisition costs and land-use intensity generally rose over time independent of inflation (24\textendash 78\%), although rates of change were heterogeneous through space and decreased in some areas. Accounting for spatial variation in land price lowered the cost of conservation plans by 1.73\textendash 13.9\%, decreased the range of costs by 19\textendash 82\%, and created unique solutions from which to choose. Despite the rise in plan costs over time, the high conservation priority of particular areas remained consistent. Delaying conservation in these critical areas may compromise what optimized conservation plans can achieve. In the case of Canadian farmland, rapid conservation action is cost-effective, even with moderate levels of uncertainty in how to implement restoration goals.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZAVSPH98/robillard kerr 2017 - Assessing the shelf life of cost-efï¬cient conservation plans for species at risk across gradients of agricultural land use - BDPG - RESERVE SELECTION - COST.pdf}
}

@article{rodewaldTradeoffsValueBiodiversity2019,
  title = {Tradeoffs in the Value of Biodiversity Feature and Cost Data in Conservation Prioritization},
  author = {Rodewald, Amanda D. and {Strimas-Mackey}, Matt and Schuster, Richard and Arcese, Peter},
  year = {2019},
  month = dec,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {15921},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-52241-2},
  abstract = {Abstract                            Decision-support tools are commonly used to maximize return on investments (ROI) in conservation. We evaluated how the relative value of information on biodiversity features and land cost varied with data structure and variability, attributes of focal species and conservation targets, and habitat suitability thresholds for contrasting bird communities in the Pacific Northwest of North America. Specifically, we used spatial distribution maps for 20 bird species, land values, and an integer linear programming model to prioritize land units (1 km               2               ) that met conservation targets at the lowest estimated cost (hereafter `efficiency'). Across scenarios, the relative value of biodiversity data increased with conservation targets, as higher thresholds for suitable habitat were applied, and when focal species occurred disproportionately on land of high assessed value. Incorporating land cost generally improved planning efficiency, but at diminishing rates as spatial variance in biodiversity features relative to land cost increased. Our results offer a precise, empirical demonstration of how spatially-optimized planning solutions are influenced by spatial variation in underlying feature layers. We also provide guidance to planners seeking to maximize efficiency in data acquisition and resolve potential trade-offs when setting targets and thresholds in financially-constrained, spatial planning efforts aimed at maximizing ROI in biodiversity conservation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WX5K62BN/rodewald strimas-mackey et al 2019 - Tradeoffs in the value of biodiversity feature and cost data in conservation prioritization - BDPG - RESERVE SELECTION - COST ERROR - VOI - ANNO.pdf}
}

@article{rodriguesFlexibilityEfficiencyAccountability2000,
  title = {Flexibility, Efficiency, and Accountability: Adapting Reserve Selection Algorithms to More Complex Conservation Problems},
  author = {Rodrigues, Ana S and Cerdeira, J Orestes and Gaston, Kevin J},
  year = {2000},
  pages = {10},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8R2L6NVX/rodrigues et al 2000 - flexibility, efficiency, and accountability - adapting reserve selection algorithms to more complex conservation problems - RESERVE SELECTION - ANNO.pdf}
}

@article{rodriguesOptimisationReserveSelection2002,
  title = {Optimisation in Reserve Selection Procedures\textemdash Why Not?},
  author = {Rodrigues, Ana S.L. and Gaston, Kevin J.},
  year = {2002},
  month = sep,
  journal = {Biological Conservation},
  volume = {107},
  number = {1},
  pages = {123--129},
  issn = {00063207},
  doi = {10.1016/S0006-3207(02)00042-3},
  abstract = {Linear programming techniques provide an appropriate tool for solving reserve selection problems. Although this has long been known, most published analyses persist in the use of intuitive heuristics, which cannot guarantee the optimality of the solutions found. Here, we dispute two of the most common justifications for the use of intuitive heuristics, namely that optimisation techniques are too slow and cannot solve the most realistic selection problems. By presenting an overview of processing times obtained when solving a diversity of reserve selection problems, we demonstrate that most of those published could almost certainly be solved very quickly by standard optimisation software using current widely available computing technology. Even for those problems that take longer to solve, solutions with low levels of sub-optimality can be obtained quite quickly, presenting a better alternative to intuitive heuristics. \# 2002 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/DIXKU65K/rodrigues gaston 2002 - optimisation in reserve selection procedures - why not - BDPG - RESERVE SELECTION - ANNO.pdf}
}

@article{romeijnGeneratingExperimentalData2001,
  title = {Generating {{Experimental Data}} for the {{Generalized Assignment Problem}}},
  author = {Romeijn, H. Edwin and Romero Morales, Dolores},
  year = {2001},
  month = dec,
  journal = {Operations Research},
  volume = {49},
  number = {6},
  pages = {866--878},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.49.6.866.10021},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TVJUSSZJ/romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf}
}

@article{romeijnGeneratingExperimentalData2001a,
  title = {Generating {{Experimental Data}} for the {{Generalized Assignment Problem}}},
  author = {Romeijn, H. Edwin and Romero Morales, Dolores},
  year = {2001},
  month = dec,
  journal = {Operations Research},
  volume = {49},
  number = {6},
  pages = {866--878},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.49.6.866.10021},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XTTVXTVB/romeijn morales 2001 - Generating Experimental Data for the Generalized Assignment Problem.pdf}
}

@article{rondininiTradeoffsDifferentTypes2006,
  title = {Tradeoffs of Different Types of Species Occurrence Data for Use in Systematic Conservation Planning: {{Species}} Data for Conservation Planning},
  shorttitle = {Tradeoffs of Different Types of Species Occurrence Data for Use in Systematic Conservation Planning},
  author = {Rondinini, Carlo and Wilson, Kerrie A. and Boitani, Luigi and Grantham, Hedley and Possingham, Hugh P.},
  year = {2006},
  month = oct,
  journal = {Ecology Letters},
  volume = {9},
  number = {10},
  pages = {1136--1145},
  issn = {1461023X},
  doi = {10.1111/j.1461-0248.2006.00970.x},
  abstract = {Data on the occurrence of species are widely used to inform the design of reserve networks. These data contain commission errors (when a species is mistakenly thought to be present) and omission errors (when a species is mistakenly thought to be absent), and the rates of the two types of error are inversely related. Point locality data can minimize commission errors, but those obtained from museum collections are generally sparse, suffer from substantial spatial bias and contain large omission errors. Geographic ranges generate large commission errors because they assume homogenous species distributions. Predicted distribution data make explicit inferences on species occurrence and their commission and omission errors depend on model structure, on the omission of variables that determine species distribution and on data resolution. Omission errors lead to identifying networks of areas for conservation action that are smaller than required and centred on known species occurrences, thus affecting the comprehensiveness, representativeness and efficiency of selected areas. Commission errors lead to selecting areas not relevant to conservation, thus affecting the representativeness and adequacy of reserve networks. Conservation plans should include an estimation of commission and omission errors in underlying species data and explicitly use this information to influence conservation planning outcomes.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PW3EMZHJ/Rondinini et al. - 2006 - Tradeoffs of different types of species occurrence data for use in systematic conservation planning. - Ecology letters.pdf}
}

@article{rothFunctionalEcologyImperfect2018,
  title = {Functional Ecology and Imperfect Detection of Species},
  author = {Roth, Tobias and Allan, Eric and Pearman, Peter B. and Amrhein, Valentin},
  editor = {Carvalheiro, Lu{\'i}sa},
  year = {2018},
  month = apr,
  journal = {Methods in Ecology and Evolution},
  volume = {9},
  number = {4},
  pages = {917--928},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.12950},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EZSVA4DK/roth et al 2017 - Functional ecology and imperfect detection of species - DETECTABILITY - BDPG - GUPPY.pdf}
}

@article{roughgardenCS369NWorstCaseAnalysis,
  title = {{{CS369N}}: {{Beyond Worst-Case Analysis Lecture}} \#4: {{Probabilistic}} and {{Semirandom Models}} for {{Clustering}} and {{Graph Partitioning}}},
  author = {Roughgarden, Tim},
  pages = {12},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5HDX7YT3/l4.pdf}
}

@article{rougierClimateSimulatorsClimate2014,
  title = {Climate {{Simulators}} and {{Climate Projections}}},
  author = {Rougier, Jonathan and Goldstein, Michael},
  year = {2014},
  month = jan,
  journal = {Annual Review of Statistics and Its Application},
  volume = {1},
  number = {1},
  pages = {103--123},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-022513-115652},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4G7GWRBP/Rougier_Goldstein_2014_Climate Simulators and Climate Projections.pdf}
}

@article{rudinOptimizedScoringSystems,
  title = {Optimized {{Scoring Systems}}: {{Towards Trust}} in {{Machine Learning}} for {{Healthcare}} and {{Criminal Justice}}},
  author = {Rudin, Cynthia and Ustun, Berk},
  pages = {58},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EA5QBEHH/rudin ustun 2018 - Optimized Scoring Systems - Towards Trust in Machine Learning for Healthcare and Criminal Justice - OPTISEVIL - EFs - ML - POST-RMIT.pdf}
}

@article{rungeDetectingFailureClimate2016,
  title = {Detecting Failure of Climate Predictions},
  author = {Runge, Michael C. and Stroeve, Julienne C. and Barrett, Andrew P. and {McDonald-Madden}, Eve},
  year = {2016},
  month = may,
  journal = {Nature Clim. Change},
  volume = {advance online publication},
  issn = {1758-6798}
}

@article{rungeDetectingFailureClimate2016a,
  title = {Detecting Failure of Climate Predictions},
  author = {Runge, Michael C. and Stroeve, Julienne C. and Barrett, Andrew P. and {McDonald-Madden}, Eve},
  year = {2016},
  month = may,
  journal = {Nature Clim. Change},
  volume = {advance online publication},
  issn = {1758-6798}
}

@article{runtingReducingRiskReserve2018,
  title = {Reducing Risk in Reserve Selection Using {{Modern Portfolio Theory}}: {{Coastal}} Planning under Sea-Level Rise},
  shorttitle = {Reducing Risk in Reserve Selection Using {{Modern Portfolio Theory}}},
  author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
  editor = {Magrach, Ainhoa},
  year = {2018},
  month = sep,
  journal = {Journal of Applied Ecology},
  volume = {55},
  number = {5},
  pages = {2193--2203},
  issn = {00218901},
  doi = {10.1111/1365-2664.13190},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PDRS9AGV/runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise.pdf}
}

@article{runtingReducingRiskReserve2018a,
  title = {Reducing Risk in Reserve Selection Using Modern Portfolio Theory: Coastal Planning under Sea-Level Rise},
  shorttitle = {Reducing Risk in Reserve Selection Using Modern Portfolio Theory},
  author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
  year = {2018},
  month = sep,
  journal = {Journal of applied ecology},
  volume = {55},
  number = {5},
  pages = {2193--2203},
  issn = {0021-8901, 1365-2664},
  doi = {10.1111/1365-2664.13190},
  copyright = {2018, The Authors, British Ecological Society},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KRMFAQGW/Runting et al. - 2018 - Reducing risk in reserve selection using modern po.pdf}
}

@article{runtingReducingRiskReserve2018b,
  title = {Reducing Risk in Reserve Selection Using {{Modern Portfolio Theory}}: {{Coastal}} Planning under Sea-Level Rise},
  shorttitle = {Reducing Risk in Reserve Selection Using {{Modern Portfolio Theory}}},
  author = {Runting, Rebecca K. and Beyer, Hawthorne L. and Dujardin, Yann and Lovelock, Catherine E. and Bryan, Brett A. and Rhodes, Jonathan R.},
  editor = {Magrach, Ainhoa},
  year = {2018},
  month = sep,
  journal = {Journal of Applied Ecology},
  volume = {55},
  number = {5},
  pages = {2193--2203},
  issn = {00218901},
  doi = {10.1111/1365-2664.13190},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BF4IE58B/runting beyer et al 2018 - reducing risk in reserve selection using modern porfolio theory - coastal planning under sea-level rise - RESERVE SELECTION - PORTFOLIO - ANNO.pdf}
}

@article{ruppeinerEnsembleApproachSimulated1991,
  title = {Ensemble Approach to Simulated Annealing},
  author = {Ruppeiner, George and Pedersen, Jacob M{\dbend}rch and Salamon, Peter},
  year = {1991},
  month = apr,
  journal = {Journal de Physique I},
  volume = {1},
  number = {4},
  pages = {455--470},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1991146},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LQZQ2DBD/ruppeiner et al 1991 - ensemble approach to simulated annealing.pdf}
}

@article{rushdiVPSVORONOIPIECEWISE2017,
  title = {{{VPS}}: {{VORONOI PIECEWISE SURROGATE MODELS FOR HIGH-DIMENSIONAL DATA FITTING}}},
  shorttitle = {{{VPS}}},
  author = {Rushdi, Ahmad A. and Swiler, Laura P. and Phipps, Eric T. and D'Elia, Marta and Ebeida, Mohamed S.},
  year = {2017},
  journal = {International Journal for Uncertainty Quantification},
  volume = {7},
  number = {1},
  pages = {1--21},
  issn = {2152-5080},
  doi = {10.1615/Int.J.UncertaintyQuantification.2016018697},
  abstract = {Global surrogate models (metamodels) are indispensable for numerical simulations over high-dimensional spaces. They typically use well-selected samples of the expensive code runs to produce a cheap-to-evaluate model. We introduce a new method to construct credible global surrogates with local accuracy without dictating where to sample: Voronoi Piecewise Surrogate (VPS) models. The key component in our method is to decompose the high-dimensional parameter space using an implicit Voronoi tessellation around the sample points as seeds. VPS assigns samples to cells using a simple nearest seed search, and finds cell neighbors via local hyperplane sampling, without constructing an explicit mesh. To avoid the intractable complexity of high-dimensional Voronoi cells, we construct an approximate dual Delaunay graph to establish a neighborhood network between cells. Each cell then uses information at its neighbors to build its own local piece of the global surrogate. This approach breaks down the high-order approximation problem into a set of piecewise low-order problems in the neighborhood of each function evaluation. The one-to-one mapping between the number of function evaluations and the number of Voronoi cells, regardless of the number of dimensions, eliminates the curse of dimensionality associated with standard domain decompositions. Furthermore, the Voronoi tessellation is naturally updated with the addition of new function evaluations. Due to its piecewise nature, VPS accurately handles smooth functions as well as functions with discontinuities, and can adopt a parallel implementation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3MBSY338/rushdi swiler phipps d elia ebida 2016 - vps - voronoi piecewise surrogate models for high-dimensional data fitting - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{ryanTutorialRationalGenerating,
  title = {A {{Tutorial}} on {{Rational Generating Functions}}},
  author = {Ryan, Christopher Thomas and Jiang, Albert Xin and {Leyton-Brown}, Kevin},
  pages = {6},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/V8238PZS/gf_tutorial.pdf}
}

@article{sakataStatisticalMechanicsDictionary2013,
  title = {Statistical Mechanics of Dictionary Learning},
  author = {Sakata, Ayaka and Kabashima, Yoshiyuki},
  year = {2013},
  month = jul,
  journal = {EPL (Europhysics Letters)},
  volume = {103},
  number = {2},
  pages = {28008},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/0295-5075/103/28008},
  abstract = {Finding a basis matrix (dictionary) by which objective signals are represented sparsely is of major relevance in various scientific and technological fields. We consider a problem to learn a dictionary from a set of training signals. We employ techniques of statistical mechanics of disordered systems to evaluate the size of the training set necessary to typically succeed in the dictionary learning. The results indicate that the necessary size is much smaller than previously estimated, which theoretically supports and/or encourages the use of dictionary learning in practical situations.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TSLEMIL4/0295-5075_103_2_28008.pdf}
}

@article{salomonPopulationViabilityEcological2006,
  title = {Population Viability, Ecological Processes and Biodiversity: {{Valuing}} Sites for Reserve Selection},
  shorttitle = {Population Viability, Ecological Processes and Biodiversity},
  author = {Salomon, Anne K. and Ruesink, Jennifer L. and DeWreede, Robert E.},
  year = {2006},
  month = feb,
  journal = {Biological Conservation},
  volume = {128},
  number = {1},
  pages = {79--92},
  issn = {00063207},
  doi = {10.1016/j.biocon.2005.09.018},
  abstract = {No-take reserves constitute one tool to improve conservation of marine ecosystems, yet criteria for their placement, size, and arrangement remain uncertain. Representation of biodiversity is necessary in reserve planning, but will ultimately fail for conservation unless factors affecting species' persistence are also incorporated. This study presents an empirical example of the divergent relationships among multiple metrics used to quantify a site's conservation value, including those that address representation (habitat type, species richness, species diversity), and others that address ecological processes and viability (density and reproductive capacity of a keystone species, in this case, the black chiton, Katharina tunicata). We characterized 10 rocky intertidal sites across two habitats in Barkley Sound, British Columbia, Canada, according to these site metrics. High-richness and high-production sites for K. tunicata were present in both habitat types, but high richness and high-production sites did not overlap. Across sites, species richness ranged from 29 to 46, and adult K. tunicata varied from 6 to 22 individuals m\`A2. Adult density was negatively correlated with species richness, a pattern that likely occurs due to post-recruitment growth and survival because no correlation was evident with non-reproductive juveniles. Sites with high adult density also contributed disproportionately greater potential reproductive output (PRO), defined by total gonad mass. PRO varied by a factor of five across sites and was also negatively correlated with species richness. Compromise or relative weighting would be necessary to select valuable sites for conservation because of inherent contradictions among some reserve selection criteria. We suspect that this inconsistency among site metrics will occur more generally in other ecosystems and emphasize the importance of population viability of strongly interacting species.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WXND6WXG/1 - Salomon, Ruesink, Dewreede - 2006 - Population viability, ecological processes and biodiversity Valuing sites for reserve selection - Biological Conservation.pdf}
}

@article{saltelliWhatWrongEvidence2017,
  title = {What Is Wrong with Evidence Based Policy, and How Can It Be Improved?},
  author = {Saltelli, Andrea and Giampietro, Mario},
  year = {2017},
  month = aug,
  journal = {Futures},
  volume = {91},
  pages = {62--71},
  issn = {00163287},
  doi = {10.1016/j.futures.2016.11.012},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UDPG98DE/saltelli giampietro 2017 - What is wrong with evidence based policy, and how can it be improved - ANNO - OPTISEVIL - SENSITIVITY - UNCERTAINTY - DISCOUNTING - ECONOMICS - UNKNOWN UNKNOWNS.pdf}
}

@article{sanchezAnalysisHowTraining2007a,
  title = {An Analysis of How Training Data Complexity Affects the Nearest Neighbor Classifiers},
  author = {S{\'a}nchez, J. S. and Mollineda, R. A. and Sotoca, J. M.},
  year = {2007},
  month = jul,
  journal = {Pattern Analysis and Applications},
  volume = {10},
  number = {3},
  pages = {189--201},
  issn = {1433-7541, 1433-755X},
  doi = {10.1007/s10044-007-0061-2},
  abstract = {The k-nearest neighbors (k-NN) classifier is one of the most popular supervised classification methods. It is very simple, intuitive and accurate in a great variety of real-world domains. Nonetheless, despite its simplicity and effectiveness, practical use of this rule has been historically limited due to its high storage requirements and the computational costs involved. On the other hand, the performance of this classifier appears strongly sensitive to training data complexity. In this context, by means of several problem difficulty measures, we try to characterize the behavior of the k-NN rule when working under certain situations. More specifically, the present analysis focuses on the use of some data complexity measures to describe class overlapping, feature space dimensionality and class density, and discover their relation with the practical accuracy of this classifier.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y8TNM63P/sanchez et al 2007 - an analysis of how training data complexity affects the nearest neighbor classifiers - KNN - PROBLEM DIFFICULTY.pdf}
}

@techreport{sanchis1988,
  type = {{{TR}}},
  title = {Test {{Instance Construction}} for {{NP-hard Problems}}},
  author = {Sanchis, Laura A},
  year = {1988},
  number = {206},
  pages = {22},
  address = {{Rochester, NY}},
  institution = {{Computer Science Dept, University of Rochester}},
  abstract = {The performance of heuristic approximation algorithms for NP-hard problems can often only be determined by experimentation. This paper explores some of the issues involved in the efficient generation of useful test sets for such problems, i.e. test sets consisting of instances of the problem for which the answer is known and having other properties useful for testing the performance of approximation algorithms. Some theoretical results are given concerning what kinds of test sets can and cannot be generated; these are derived by examining the complexity of length-restricted instance generation for languages in NP and co-NP. Also examples of test set generation procedures are presented.},
  langid = {english},
  keywords = {bdpg,NP-hard problems,test generation},
  file = {/Users/bill/D/Zotero/storage/D27MKTC2/sanchis 1988 - test instance construction for NP-hard problems - technical report - BDPG - PROBLEM DIFFICULTY.pdf}
}

@techreport{sanchis1989,
  type = {Dissertation},
  title = {Language Instance Generation and Test Case Construction for {{NP-hard}} Problems},
  author = {Sanchis, Laura A.},
  year = {1989},
  month = may,
  number = {TR 296},
  address = {{University of Rochester}},
  institution = {{Computer Science Dept, University of Rochester}},
  file = {/Users/bill/D/Zotero/storage/S5EUPCT3/sanchis 1989 - language instance generation and test case construction for np-hard problems - PROBLEM DIFFICULTY.pdf}
}

@article{sanchisComplexityTestCase1990,
  title = {On the Complexity of Test Case Generation for {{NP-hard}} Problems},
  author = {Sanchis, Laura A.},
  year = {1990},
  month = nov,
  journal = {Information Processing Letters},
  volume = {36},
  number = {3},
  pages = {135--140},
  issn = {00200190},
  doi = {10.1016/0020-0190(90)90082-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/G6BCT84F/sanchis 1990 - on the complexity of test case generation for NP-hard problems - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{sanchisExperimentalAnalysisHeuristic2002,
  title = {Experimental {{Analysis}} of {{Heuristic Algorithms}} for the {{Dominating Set Problem}}},
  author = {Sanchis, L. A.},
  year = {2002},
  month = may,
  journal = {Algorithmica},
  volume = {33},
  number = {1},
  pages = {3--18},
  issn = {0178-4617, 1432-0541},
  doi = {10.1007/s00453-001-0101-z},
  abstract = {We say a vertex v in a graph G covers a vertex w if v = w or if v and w are adjacent. A subset of vertices of G is a dominating set if it collectively covers all vertices in the graph. The dominating set problem, which is NP-hard, consists of finding a smallest possible dominating set for a graph. The straightforward greedy strategy for finding a small dominating set in a graph consists of successively choosing vertices which cover the largest possible number of previously uncovered vertices. Several variations on this greedy heuristic are described and the results of extensive testing of these variations is presented. A more sophisticated procedure for choosing vertices, which takes into account the number of ways in which an uncovered vertex may be covered, appears to be the most successful of the algorithms which are analyzed. For our experimental testing, we used both random graphs and graphs constructed by test case generators which produce graphs with a given density and a specified size for the smallest dominating set. We found that these generators were able to produce challenging graphs for the algorithms, thus helping to discriminate among them, and allowing a greater variety of graphs to be used in the experiments.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YT539468/Sanchis 2002 - Experimental Analysis Of Heuristic algorithms for the dominating set problem.pdf}
}

@article{sanchisExperimentalTheoreticalResults1996a,
  title = {Some {{Experimental}} and {{Theoretical Results}} on {{Test Case Generators}} for the {{Maximum Clique Problem}}},
  author = {Sanchis, Laura A. and Jagota, Arun},
  year = {1996},
  month = may,
  journal = {INFORMS Journal on Computing},
  volume = {8},
  number = {2},
  pages = {87--102},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.8.2.87},
  abstract = {We describe and analyze test case generators for the maximum clique problem or equivalently for the maximum independent set or vertex cover problems. The generators produce graphs with speci ed number of vertices and edges, and known maximum clique size. The experimental hardness of the test cases is evaluated in relation to several heuristics for the maximum clique problem, based on neural networks, and derived from the work of A. Jagota.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RMMG3UIE/sanchis jagota 1995 - Some Experimental and Theoretical Results on Test Case Generators for the Maximum Clique Problem - BDPG.pdf}
}

@article{sanchisGeneratingHardDiverse1995a,
  title = {Generating Hard and Diverse Test Sets for {{NP-hard}} Graph Problems},
  author = {Sanchis, Laura A.},
  year = {1995},
  month = mar,
  journal = {Discrete Applied Mathematics},
  volume = {58},
  number = {1},
  pages = {35--66},
  issn = {0166218X},
  doi = {10.1016/0166-218X(93)E0140-T},
  abstract = {In evaluating the performance of approximation algorithms for NP-hard problems, it is often necessary to resort to empirical testing. In order to do such testing it is useful to have test instances of the problem for which the correct answer is known. We present algorithms for efficiently generating test instances for some NP-hard graph problems in such a way that the sets of instances generated can be shown to be both diverse and computationally hard. The techniques used involve combining extremal graph theory results with NP-hardness reductions.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WYG5FVD9/sanchis 1995 - generating hard and diverse test sets for NP-hard graph problems - BDPG - PROBLEM DIFFICULTY - VERTEX COVER.pdf}
}

@article{sanchisRelatingSizeConnected2004,
  title = {Relating the Size of a Connected Graph to Its Total and Restricted Domination Numbers},
  author = {Sanchis, Laura A.},
  year = {2004},
  month = jun,
  journal = {Discrete Mathematics},
  volume = {283},
  number = {1-3},
  pages = {205--216},
  issn = {0012365X},
  doi = {10.1016/j.disc.2003.11.011},
  abstract = {A dominating set for a graph G = (V; E) is a subset of vertices D {$\subseteq$} V such that for all v {$\in$} V - D there exists some u {$\in$} D adjacent to v. The domination number of G is the size of its smallest dominating set. A dominating set D is a total dominating set if every vertex in D has a neighbor in D. We give a tight upper bound on the number of edges that a connected graph with a given total domination number can have, and characterize the extremal graphs attaining the bound. We do the same for the k-restricted domination number, which is the smallest number d, such that for any subset U {$\subseteq$} V where |U | = k there exists a dominating set for G of size at most d, and containing all vertices in U .},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UJZBTUEM/sanchis 2004 - relating the size of a connected graph to its total and restricted domination numbers.pdf}
}

@article{sarkarBiodiversityConservationPlanning2006,
  title = {Biodiversity {{Conservation Planning Tools}}: {{Present Status}} and {{Challenges}} for the {{Future}}},
  shorttitle = {Biodiversity {{Conservation Planning Tools}}},
  author = {Sarkar, Sahotra and Pressey, Robert L. and Faith, Daniel P. and Margules, Christopher R. and Fuller, Trevon and Stoms, David M. and Moffett, Alexander and Wilson, Kerrie A. and Williams, Kristen J. and Williams, Paul H. and Andelman, Sandy},
  year = {2006},
  month = nov,
  journal = {Annual Review of Environment and Resources},
  volume = {31},
  number = {1},
  pages = {123--159},
  issn = {1543-5938, 1545-2050},
  doi = {10.1146/annurev.energy.31.042606.085844},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Z4K5QMWR/2006_Sarkar_etal_BiodiversityConservationPlanning.pdf}
}

@article{sarkarPlacePrioritizationBiodiversity2004,
  title = {Place Prioritization for Biodiversity Conservation Using Probabilistic Surrogate Distribution Data: {{Prioritization}} Using Distribution Data},
  shorttitle = {Place Prioritization for Biodiversity Conservation Using Probabilistic Surrogate Distribution Data},
  author = {Sarkar, Sahotra and Pappas, Christopher and Garson, Justin and Aggarwal, Anshu and Cameron, Susan},
  year = {2004},
  month = feb,
  journal = {Diversity and Distributions},
  volume = {10},
  number = {2},
  pages = {125--133},
  issn = {13669516, 14724642},
  doi = {10.1111/j.1366-9516.2004.00060.x},
  abstract = {We analyse optimal and heuristic place prioritization algorithms for biodiversity conservation area network design which can use probabilistic data on the distribution of surrogates for biodiversity. We show how an Expected Surrogate Set Covering Problem (ESSCP) and a Maximal Expected Surrogate Covering Problem (MESCP) can be linearized for computationally efficient solution. For the ESSCP, we study the performance of two optimization software packages (XPRESS and CPLEX) and five heuristic algorithms based on traditional measures of complementarity and rarity as well as the Shannon and Simpson indices of {$\alpha$}-diversity which are being used in this context for the first time. On small artificial data sets the optimal place prioritization algorithms often produced more economical solutions than the heuristic algorithms, though not always ones guaranteed to be optimal. However, with large data sets, the optimal algorithms often required long computation times and produced no better results than heuristic ones. Thus there is generally little reason to prefer optimal to heuristic algorithms with probabilistic data sets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/M6Y4JJH7/sarkar et al 2004 - Place prioritization for biodiversity conservation using probabilistic surrogate distribution data - - ANNO.pdf}
}

@article{ScenariomodelparameterNewMethod2003,
  title = {Scenario-Model-Parameter: A New Method of Cumulative Risk Uncertainty Analysis},
  shorttitle = {Scenario-Model-Parameter},
  year = {2003},
  month = may,
  journal = {Fuel and Energy Abstracts},
  volume = {44},
  number = {3},
  pages = {184},
  issn = {01406701},
  doi = {10.1016/S0140-6701(03)82016-7},
  abstract = {The recently developed concepts of aggregate risk and cumulative risk rectify two limitations associated with the classical risk assessment paradigm established in the early 1980s. Aggregate exposure denotes the amount of one pollutant available at the biological exchange boundaries from multiple routes of exposure. Cumulative risk assessment is defined as an assessment of risk from the accumulation of a common toxic effect from all routes of exposure to multiple chemicals sharing a common mechanism of toxicity. Thus, cumulative risk constitutes an improvement over the classical risk paradigm, which treats exposures from multiple routes as independent events associated with each specific route. Risk assessors formulate complex models and identify many realistic scenarios of exposure that enable them to estimate risks from exposures to multiple pollutants and multiple routes. The increase in complexity of the risk assessment process is likely to increase risk uncertainty. Despite evidence that scenario and model uncertainty contribute to the overall uncertainty of cumulative risk estimates, present uncertainty analysis of risk estimates accounts only for parameter uncertainty and excludes model and scenario uncertainties. This paper provides a synopsis of the risk assessment evolution and associated uncertainty analysis methods. This evolution leads to the concept of the scenario \textendash{} model \textendash{} parameter (SMP) cumulative risk uncertainty analysis method. The SMP uncertainty analysis is a multiple step procedure that assesses uncertainty associated with the use of judiciously selected scenarios and models of exposure and risk. Ultimately, the SMP uncertainty analysis method compares risk uncertainty estimates determined using all three sources of uncertainty with conventional risk uncertainty estimates obtained using only the parameter source. An example of applying the SMP uncertainty analysis to cumulative risk estimates from exposures to two pesticides indicates that inclusion of scenario and model sources increases uncertainty of risk estimates relative to those estimated using only the parameter source. Changes in uncertainty magnitude may affect decisions made by risk managers.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5235J5WN/moschandreas karuchit - scenario-model-parameter -  a new method of cumulative risk uncertainty analysis.pdf}
}

@article{schapaughAccountingParametricUncertainty2013,
  title = {Accounting for Parametric Uncertainty in {{Markov}} Decision Processes},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2013},
  month = apr,
  journal = {Ecological Modelling},
  volume = {254},
  pages = {15--21},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2013.01.003},
  abstract = {Markov decision processes have become the standard tool for modeling sequential decision-making problems in conservation. In many real-world applications, however, it is practically infeasible to accurately parameterize the state transition function. In this study, we introduce a new way of dealing with ambiguity in the state transition function. In contrast to existing methods, we explore the effects of uncertainty at the level of the policy, rather than at the level of decisions within states. We use information-gap decision theory to ask the question of how much uncertainty in the state transition function can be tolerated while still delivering a specified expected value given by the objective function. Accordingly, the goal of the optimization problem is no longer to maximize expected value, but to maximize local robustness to uncertainty (while still meeting the desired level of performance). We analyze a simple land acquisition problem, using info-gap decision theory to propagate uncertainties and rank alternative policies. Rather than requiring information about the extent of parameter uncertainty at the outset, info-gap addresses the question of how much uncertainty is permissible in the state transition function before the optimal policy would change.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GHSFK8Q8/Schapaugh and Tyre - 2013 - Accounting for parametric uncertainty in Markov de.pdf}
}

@article{schapaughAccountingParametricUncertainty2013a,
  title = {Accounting for Parametric Uncertainty in {{Markov}} Decision Processes},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2013},
  month = apr,
  journal = {Ecological Modelling},
  volume = {254},
  pages = {15--21},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2013.01.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FVZ3MJCW/Accounting-for-parametric-uncertainty-in.pdf}
}

@article{schapaughBayesianNetworksQuest2012,
  title = {Bayesian Networks and the Quest for Reserve Adequacy},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2012},
  month = aug,
  journal = {Biological Conservation},
  volume = {152},
  pages = {178--186},
  issn = {00063207},
  doi = {10.1016/j.biocon.2012.03.014},
  abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. We describe a method that integrates correlates of persistence for multiple species into a single currency \textendash{} site quality. Site quality is, in turn, an explicit measure of performance used in optimization. We develop a Bayesian network to assess site quality, which assigns an expected value to a property based on criteria arrayed into a causal diagram. We then use stochastic dynamic programming to determine whether an organization should acquire or reject a site placed on the public market. Our framework for assessing sites and making land acquisition decisions represents a compromise between the use of generic spatial design criteria and more intensive computational tools, like spatially-explicit population models. There is certainly a loss of precision by using site quality as a surrogate for more direct measures of persistence. However, we believe this simplification is defensible when sufficient data, expertise, or other resources are lacking.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9DKID3LU/Schapaugh and Tyre - 2012 - Bayesian networks and the quest for reserve adequa.pdf}
}

@article{schapaughBayesianNetworksQuest2012a,
  title = {Bayesian Networks and the Quest for Reserve Adequacy},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2012},
  month = aug,
  journal = {Biological Conservation},
  volume = {152},
  pages = {178--186},
  issn = {00063207},
  doi = {10.1016/j.biocon.2012.03.014},
  abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. We describe a method that integrates correlates of persistence for multiple species into a single currency \textendash{} site quality. Site quality is, in turn, an explicit measure of performance used in optimization. We develop a Bayesian network to assess site quality, which assigns an expected value to a property based on criteria arrayed into a causal diagram. We then use stochastic dynamic programming to determine whether an organization should acquire or reject a site placed on the public market. Our framework for assessing sites and making land acquisition decisions represents a compromise between the use of generic spatial design criteria and more intensive computational tools, like spatially-explicit population models. There is certainly a loss of precision by using site quality as a surrogate for more direct measures of persistence. However, we believe this simplification is defensible when sufficient data, expertise, or other resources are lacking.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LTLQHG3Y/1-s2.0-S0006320712001577-main.pdf}
}

@article{schapaughMaximizingNewQuantity2014,
  title = {Maximizing a New Quantity in Sequential Reserve Selection},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2014},
  month = jun,
  journal = {Environmental Conservation},
  volume = {41},
  number = {2},
  pages = {198--205},
  issn = {0376-8929, 1469-4387},
  doi = {10.1017/S0376892913000544},
  abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. Numerous empirical studies support the notion that defining and measuring objectives in terms of species richness (where the value of a site is equal to the number of species it contains, or contributes to an existing reserve network) can be inadequate for maintaining biodiversity in the longterm. An existing site-assessment framework that implicitly maximized the persistence probability of multiple species was integrated with a dynamic optimization model. The problem of sequential reserve selection as a Markov decision process was combined with stochastic dynamic programming to find the optimal solution. The approach represents a compromise between representation-based approaches (maximizing occurrences) and more complex tools, like spatially-explicit population models. The method, the inherent problems and interesting conclusions are illustrated with a land acquisition case study on the central Platte River.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W7ZNKCNE/schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf}
}

@article{schapaughMaximizingNewQuantity2014a,
  title = {Maximizing a New Quantity in Sequential Reserve Selection},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  year = {2014},
  month = jun,
  journal = {Environmental Conservation},
  volume = {41},
  number = {2},
  pages = {198--205},
  issn = {0376-8929, 1469-4387},
  doi = {10.1017/S0376892913000544},
  abstract = {The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. Numerous empirical studies support the notion that defining and measuring objectives in terms of species richness (where the value of a site is equal to the number of species it contains, or contributes to an existing reserve network) can be inadequate for maintaining biodiversity in the longterm. An existing site-assessment framework that implicitly maximized the persistence probability of multiple species was integrated with a dynamic optimization model. The problem of sequential reserve selection as a Markov decision process was combined with stochastic dynamic programming to find the optimal solution. The approach represents a compromise between representation-based approaches (maximizing occurrences) and more complex tools, like spatially-explicit population models. The method, the inherent problems and interesting conclusions are illustrated with a land acquisition case study on the central Platte River.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K32NK5J7/schapaugh  tyre 2013 - maximizing a new quantity in sequential reserve selection - RESERVE SELECTION - EFs - GUPPY - BDPG.pdf}
}

@article{schapaughSimpleMethodDealing2012,
  title = {A Simple Method for Dealing with Large State Spaces},
  author = {Schapaugh, Adam W. and Tyre, Andrew J.},
  editor = {Freckleton, Robert},
  year = {2012},
  month = dec,
  journal = {Methods in Ecology and Evolution},
  volume = {3},
  number = {6},
  pages = {949--957},
  issn = {2041210X},
  doi = {10.1111/j.2041-210X.2012.00242.x},
  abstract = {STATE SPACE Constructing an abstract MDP requires that we identify the state variables that must be retained in the problem. We first identify a set of immediately relevant (IR) state variables. This set is formed by examining the reward structure of the problem and selecting the state variables that have the greatest impact on the reward for each state. The larger this set is, the more accurate the abstraction will be. Thus, by varying the size of the IR set, we can examine the balance between the quality of the abstraction and the feasibility of the specification. Specifically, we examine each state variable that appears in the reward function and calculate the maximum range of the reward function for each of its values. In general, state variables with smaller ranges have greater overall effect on reward than variables with larger ranges; these should be retained first.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WYQH3X2S/Schapaugh and Tyre - 2012 - A simple method for dealing with large state space.pdf}
}

@article{schmidhuber2015nn,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  langid = {english},
  keywords = {credit assignment,deep learning,neural networks},
  file = {/Users/bill/D/Zotero/storage/L6F3SHVW/schmidhuber 2015 - Deep Learning In Neural Networks Overview - DEEP LEARNING - NEURAL NETWORKS - CREDIT ASSIGNMENT.pdf}
}

@article{schmolkeEcologicalModelsSupporting2010,
  title = {Ecological Models Supporting Environmental Decision Making: A Strategy for the Future},
  shorttitle = {Ecological Models Supporting Environmental Decision Making},
  author = {Schmolke, Amelie and Thorbek, Pernille and DeAngelis, Donald L. and Grimm, Volker},
  year = {2010},
  month = aug,
  journal = {Trends in Ecology \& Evolution},
  volume = {25},
  number = {8},
  pages = {479--486},
  issn = {01695347},
  doi = {10.1016/j.tree.2010.05.001},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Q2ZD9M2K/schmolke et al 2010 - ecological models supporting environmental decision making - a strategy for the future - tree.pdf}
}

@article{schobel,
  title = {Mitglieder Der {{Pru}}\textasciidieresis fungskommission},
  author = {Schobel, Dr Anita},
  pages = {187},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VX27QIFW/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf}
}

@article{schobela,
  title = {Mitglieder Der {{Pru}}\textasciidieresis fungskommission},
  author = {Schobel, Dr Anita},
  pages = {187},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EY93XUFV/ide 2014 - concepts of robustness for uncertain multi-objective optimization - DISSERTATION - ROBUST OPTIMIZATION - MULTI-OBJECTIVE - BDPG -GUPPY.pdf}
}

@article{schotterDECISIONMAKINGNAIVE,
  title = {{{DECISION MAKING WITH NA\"IVE ADVICE}}},
  author = {Schotter, Andrew},
  pages = {26},
  abstract = {In many of the decisions we make we rely on the advice of others who have preceded us. For example, before we buy a car, choose a dentist, choose a spouse, find a school for our children, sign on to a retirement plan, etc. we usually ask the advice of others who have experience with such decisions. The same is true when we make major financial decisions. Here people easily take advice from their fellow workers or relatives as to how to choose stock, balance a portfolio, or save for their child's education. Although some advice we get is from experts, most of the time we make our decisions relying only on the rather uninformed word-of-mouth advice we get from our friends or neighbors. We call this ?aive advice? In this paper I will outline a set of experimental results that indicate that word-of-mouth advice is a very powerful force in shaping the decisions that people make and tends to push those decisions in the direction of the predictions of the rational theory.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/39HLDWC4/schotter - decision making with naive advice.pdf}
}

@article{schuster2020p,
  title = {Exact Integer Linear Programming Solvers Outperform Simulated Annealing for Solving Conservation Planning Problems},
  author = {Schuster, Richard and Hanson, Jeffrey O. and {Strimas-Mackey}, Matthew and Bennett, Joseph R.},
  year = {2020},
  month = may,
  journal = {PeerJ},
  volume = {8},
  pages = {e9258},
  issn = {2167-8359},
  doi = {10.7717/peerj.9258},
  abstract = {The resources available for conserving biodiversity are limited, and so protected areas need to be established in places that will achieve objectives for minimal cost. Two of the main algorithms for solving systematic conservation planning problems are Simulated Annealing (SA) and exact integer linear programing (EILP) solvers. Using a case study in BC, Canada, we compare the cost-effectiveness and processing times of SA used in Marxan versus EILP using both commercial and open-source algorithms. Plans for expanding protected area systems based on EILP algorithms were 12\textendash 30\% cheaper than plans using SA, due to EILP's ability to find optimal solutions as opposed to approximations. The best EILP solver we examined was on average 1,071 times faster than the SA algorithm tested. The performance advantages of EILP solvers were also observed when we aimed for spatially compact solutions by including a boundary penalty. One practical advantage of using EILP over SA is that the analysis does not require calibration, saving even more time. Given the performance of EILP solvers, they can be used to generate conservation plans in real-time during stakeholder meetings and can facilitate rapid sensitivity analysis, and contribute to a more transparent, inclusive, and defensible decision-making process.},
  langid = {english},
  keywords = {bdpg},
  file = {/Users/bill/D/Zotero/storage/GMEHCEGQ/schuster et al 2020 - exact integer linear programming solvers outperform simulated annealing for solving conservation planning problems - BDPG.pdf}
}

@article{schusterRevivalLandscapeParadigm2012,
  title = {A Revival of the Landscape Paradigm: {{Large}} Scale Data Harvesting Provides Access to Fitness Landscapes},
  shorttitle = {A Revival of the Landscape Paradigm},
  author = {Schuster, Peter},
  year = {2012},
  month = may,
  journal = {Complexity},
  volume = {17},
  number = {5},
  pages = {6--10},
  issn = {10762787},
  doi = {10.1002/cplx.21401},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/496HFVLZ/schuster 2012 - Solving problems involving the distribution of a species of unknown distribution via ecological niche modeling - GUPPY - EFs - PROBLEM DIFFICULTY.pdf}
}

@article{selmanGeneratingHardSatisfiability1996,
  title = {Generating Hard Satisfiability Problems},
  author = {Selman, Bart and Mitchell, David G. and Levesque, Hector J.},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {17--29},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00045-3},
  abstract = {We report results from large-scale experiments in satisfiabilitytesting. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability testing procedures.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P6VKHJHV/selman mitchell levesque 1996 - generating hard satisfiability problems.pdf}
}

@article{selwood2019cl,
  title = {Collaborative Conservation Planning: {{Quantifying}} the Contribution of Expert Engagement to Identify Spatial Conservation Priorities},
  shorttitle = {Collaborative Conservation Planning},
  author = {Selwood, Katherine E. and Wintle, Brendan A. and Kujala, Heini},
  year = {2019},
  month = nov,
  journal = {Conservation Letters},
  volume = {12},
  number = {6},
  issn = {1755-263X, 1755-263X},
  doi = {10.1111/conl.12673},
  abstract = {The importance of expert input to spatial conservation prioritization outcomes is poorly understood. We quantified the impacts of refinements made during consultation with experts on spatial conservation prioritization of Christmas Island. There was just 0.57 correlation between the spatial conservation priorities before and after consultation, bottom ranked areas being most sensitive to changes. The inclusion of a landscape condition layer was the most significant individual influence. Changes (addition, removal, modification) to biodiversity layers resulted in a combined 0.2 reduction in correlation between initial and final solutions. Representation of rare species in top ranked areas was much greater after expert consultation; representation of widely distributed species changed relatively little. Our results show how different inputs have notably different impacts on the final plan. Understanding these differences helps plan time and resources for expert consultation.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4NLDBW6Y/selwood wintle kujala 2019 - Collaborative conservation planning - Quantifying the contribution of expert engagement to identify spatial conservation priorities - BDPG - RESERVE SELECTION - UNCERTAINTY - ERRO.pdf}
}

@article{sempleNatureReserveSelection1992,
  title = {Nature {{Reserve Selection}} and {{Other Problems}} in {{Phylogenetic Diversity}}},
  author = {Semple, Charles},
  year = {1992},
  journal = {Conservation Biology},
  pages = {42},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZGE3RZI6/2007090611301.pdf}
}

@article{seoanePhaseTransitionsPareto2015,
  title = {Phase Transitions in {{Pareto}} Optimal Complex Networks},
  author = {Seoane, Lu{\'i}s F. and Sol{\'e}, Ricard},
  year = {2015},
  month = sep,
  journal = {Physical Review E},
  volume = {92},
  number = {3},
  eprint = {1505.06937},
  eprinttype = {arxiv},
  pages = {032807},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.92.032807},
  abstract = {The organization of interactions in complex systems can be described by networks connecting different units. These graphs are useful representations of the local and global complexity of the underlying systems. The origin of their topological structure can be diverse, resulting from different mechanisms including multiplicative processes and optimization. In spatial networks or in graphs where cost constraints are at work, as it occurs in a plethora of situations from power grids to the wiring of neurons in the brain, optimization plays an important part in shaping their organization. In this paper we study network designs resulting from a Pareto optimization process, where different simultaneous constraints are the targets of selection. We analyze three variations on a problem finding phase transitions of different kinds. Distinct phases are associated to different arrangements of the connections; but the need of drastic topological changes does not determine the presence, nor the nature of the phase transitions encountered. Instead, the functions under optimization do play a determinant role. This reinforces the view that phase transitions do not arise from intrinsic properties of a system alone, but from the interplay of that system with its external constraints.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Physics and Society},
  file = {/Users/bill/D/Zotero/storage/VBA4N7GS/seoane sole 2015 - Phase transitions in Pareto optimal complex networks - NETWORKS - OPTISEVIL.pdf}
}

@inproceedings{shandEvolvingControllablyDifficult2019,
  title = {Evolving Controllably Difficult Datasets for Clustering},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Shand, Cameron and Allmendinger, Richard and Handl, Julia and Webb, Andrew and Keane, John},
  year = {2019},
  month = jul,
  pages = {463--471},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321761},
  abstract = {Synthetic datasets play an important role in evaluating clustering algorithms, as they can help shed light on consistent biases, strengths, and weaknesses of particular techniques, thereby supporting sound conclusions. Despite this, there is a surprisingly small set of established clustering benchmark data, and many of these are currently handcrafted. Even then, their difficulty is typically not quantified or considered, limiting the ability to interpret algorithmic performance on these datasets. Here, we introduce HAWKS, a new data generator that uses an evolutionary algorithm to evolve cluster structure of a synthetic data set. We demonstrate how such an approach can be used to produce datasets of a pre-specified difficulty, to trade off different aspects of problem difficulty, and how these interventions directly translate into changes in the clustering performance of established algorithms.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P4JRUZI3/ClusterGen_GECCO2019_Deposit_nonacm.pdf}
}

@article{shaparenkoIdentifyingTemporalPatterns,
  title = {Identifying {{Temporal Patterns}} and {{Key Players}} in {{Document Collections}}},
  author = {Shaparenko, Benyah and Caruana, Rich and Gehrke, Johannes and Joachims, Thorsten},
  pages = {10},
  abstract = {This paper considers the problem of analyzing the development of a document collection over time without requiring meaningful citation data. Given a collection of timestamped documents, we formulate and explore the following two questions. First, what are the main topics and how do these topics develop over time? Second, to gain insight into the dynamics driving this development, what are the documents and who are the authors that are most influential in this process? Unlike prior work in citation analysis, we propose methods addressing these questions without requiring the availability of citation data. The methods use only the text of the documents as input. Consequentially, they are applicable to a much wider range of document collections (email, blogs, etc.), most of which lack meaningful citation data. We evaluate our methods on the proceedings of the Neural Information Processing Systems (NIPS) conference. Even with the preliminary methods that we implemented, the results show that the methods are effective and that addressing the questions based on the text alone is feasible. In fact, the text-based methods sometimes even identify influential papers that are missed by citation analysis.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JP5DUR3R/shaparenko_etal_05a.pdf}
}

@article{shen2016jco,
  title = {Bounding the Scaling Window of Random Constraint Satisfaction Problems},
  author = {Shen, Jing and Ren, Yaofeng},
  year = {2016},
  month = feb,
  journal = {Journal of Combinatorial Optimization},
  volume = {31},
  number = {2},
  pages = {786--801},
  issn = {1382-6905, 1573-2886},
  doi = {10.1007/s10878-014-9789-y},
  abstract = {The model k-CSP is a random CSP model with moderately growing arity k of constraints. By incorporating certain linear structure, k-CSP is revised to a random linear CSP, named k-hyper-F-linear CSP. It had been shown theoretically that the two models exhibit exact satisfiability phase transitions when the constraint density r is varied accordingly. In this paper, we use finite-size scaling analysis to characterize the threshold behaviors of the two models with finite problem size n. A series of experimental studies are carried out to illustrate the scaling window of the model k-CSP.},
  langid = {english},
  keywords = {bdpg,csp,model RB,phase transition,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/45UWC8UG/shen ren 2016 - Bounding the scaling window of random constraintsatisfaction problems - MODEL RB - XU - BDPG - PHASE TRANSITION - PROBLEM DIFFICULTY.pdf}
}

@article{shrestha,
  title = {{{PREDICTING HYDROLOGICAL MODELS UNCERTAINTY}}: {{USE OF MACHINE LEARNING}}},
  author = {Shrestha, Durga Lal and Solomatine, Dimitri},
  pages = {11},
  abstract = {This paper presents a methodology for assessing total model uncertainty using machine learning techniques. Historical model errors are assumed to be indicator of total model uncertainty. The model uncertainty is measured in the form of the model errors quantiles or prediction intervals (PIs) and such expression of uncertainty comprises all sources of uncertainty (e.g. model structure, model parameters, input data and output data etc.) without attempting to separate the contribution given by the individual sources of uncertainties. The method consists of partition of the model input data into different clusters. The data belonging to the same cluster have similar values of model errors (at least mean and variance). This is done by building a data matrix by combining (some of the) historical model inputs and corresponding model errors; partitioning this calibration data using clustering techniques such as crisp cluster or fuzzy clustering. PIs are constructed for each cluster by constructing empirical distribution of the model errors. The estimation of PIs for unseen test (or validation) data can be done by i) ``eager'' supervised classification, ii) instance-based (prototype) learning, and iii) supervised regression method. In classification method classifiers are built from the cluster labels and input data matrix and this classifier classifies the unseen input data. Estimation of PIs for the given input data consists of query of lookup table between cluster labels and PIs. In instance-based learning instead of building classifier, distance function is used to identify the cluster for the given validation input data, and represent it by its prototype (typically, its center). In regression method, PIs to each input in calibration data set are computed. Two regression models that estimate upper and lower PIs independently are trained from the input data matrix. The trained regression models are applied to estimate PIs in the unseen validation data set. The third approach was applied by Shrestha and Solomatine (2006) to estimate uncertainty of river flows. This paper presents the instance based approach to estimate the total model uncertainty of simulated river flows by HBV model of the case study of Brue catchment in United Kingdom.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8TVUS36I/shrestha solomatine 2007 - PREDICTING HYDROLOGICAL MODELS UNCERTAINTY - USE OF MACHINE LEARNING - ProcIAHR2007.pdf}
}

@inproceedings{shrestha2005p2iijcnn2,
  title = {Estimation of Prediction Intervals for the Model Outputs Using Machine Learning},
  booktitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  author = {Shrestha, D.L. and Solomatine, D.P.},
  year = {2005},
  volume = {5},
  pages = {2700--2705},
  publisher = {{IEEE}},
  address = {{MOntreal, QC, Canada}},
  doi = {10.1109/IJCNN.2005.1556351},
  abstract = {A new method for estimating prediction intervals for a model output using machine learning is presented. In it, first the prediction intervals for insample data using clustering techniques to identify the distinguishable regions in input space with similar distributions of model errors are constructed. Then regression model is built for in-sample data using computed prediction intervals as targets, and, finally, this model is applied to estimate the prediction intervals for out-of-sample data. The method was tested on artificial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction intervals. A new method for evaluating performance for estimating prediction intervals is proposed as well.},
  isbn = {978-0-7803-9048-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EMYK7GIG/Shrestha,Solomatine,Estimation,PI,ProcIJCNN,2005.pdf}
}

@article{shrestha2006,
  title = {{{ASSESSING MODEL PREDICTION LIMITS USING FUZZY CLUSTERING AND MACHINE LEARNING}}},
  author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
  year = {2006},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y8YZ75CJ/shrestha et al 2006 - assessing model prediction limits using fuzzy clustering and machine learning.pdf}
}

@article{shrestha2006a,
  title = {{{ASSESSING MODEL PREDICTION LIMITS USING FUZZY CLUSTERING AND MACHINE LEARNING}}},
  author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
  year = {2006},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KRD2NNJJ/Shrestha_and_Solomatine,_2006c.pdf}
}

@article{shrestha2006b,
  title = {{{ASSESSING MODEL PREDICTION LIMITS USING FUZZY CLUSTERING AND MACHINE LEARNING}}},
  author = {Shrestha, D L and Rodriguez, J and Price, R K and Solomatine, D P},
  year = {2006},
  pages = {8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KPJK522H/Shrestha,Rodriguez,Price,Solomatine,AssesingModelPrediction,ProcHI,2006.pdf}
}

@article{shrestha2006nc,
  title = {Experiments with {{AdaBoost}}.{{RT}}, an {{Improved Boosting Scheme}} for {{Regression}}},
  author = {Shrestha, D. L. and Solomatine, D. P.},
  year = {2006},
  month = jul,
  journal = {Neural Computation},
  volume = {18},
  number = {7},
  pages = {1678--1710},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1678},
  abstract = {The application of boosting technique to regression problems has received relatively little attention in contrast to research aimed at classification problems. This letter describes a new boosting algorithm, AdaBoost.RT, for regression problems. Its idea is in filtering out the examples with the relative estimation error that is higher than the preset threshold value, and then following the AdaBoost procedure. Thus, it requires selecting the suboptimal value of the error threshold to demarcate examples as poorly or well predicted. Some experimental results using the M5 model tree as a weak learning machine for several benchmark data sets are reported. The results are compared to other boosting methods, bagging, artificial neural networks, and a single M5 model tree. The preliminary empirical comparisons show higher performance of AdaBoost.RT for most of the considered data sets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MZTB7GV7/Shrestha and Solomatine 2006 - Experiments with AdaBoost.RT - an Improved Boosting Scheme for Regression.pdf}
}

@article{shrestha2006nn,
  title = {Machine Learning Approaches for Estimation of Prediction Interval for the Model Output},
  author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
  year = {2006},
  month = mar,
  journal = {Neural Networks},
  volume = {19},
  number = {2},
  pages = {225--235},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.01.012},
  abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and finally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artificial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6LVLY9JB/Shrestha and Solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - GUPPY - UNCERTAINTY - ML - PROBLEM DIFFICULTY - BIODIVPROBGEN.pdf}
}

@article{shrestha2006nna,
  title = {Machine Learning Approaches for Estimation of Prediction Interval for the Model Output},
  author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
  year = {2006},
  month = mar,
  journal = {Neural Networks},
  volume = {19},
  number = {2},
  pages = {225--235},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.01.012},
  abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and finally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artificial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2Z4N3366/shrestha solomatine 2006 - Machine learning approaches for estimation of prediction interval for the model output - PREDICTION INTERVALS - BDPG - GUPPY.pdf}
}

@article{shrestha2006nnb,
  title = {Machine Learning Approaches for Estimation of Prediction Interval for the Model Output},
  author = {Shrestha, Durga L. and Solomatine, Dimitri P.},
  year = {2006},
  month = mar,
  journal = {Neural Networks},
  volume = {19},
  number = {2},
  pages = {225--235},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.01.012},
  abstract = {A novel method for estimating prediction uncertainty using machine learning techniques is presented. Uncertainty is expressed in the form of the two quantiles (constituting the prediction interval) of the underlying distribution of prediction errors. The idea is to partition the input space into different zones or clusters having similar model errors using fuzzy c-means clustering. The prediction interval is constructed for each cluster on the basis of empirical distributions of the errors associated with all instances belonging to the cluster under consideration and propagated from each cluster to the examples according to their membership grades in each cluster. Then a regression model is built for in-sample data using computed prediction limits as targets, and finally, this model is applied to estimate the prediction intervals (limits) for out-of-sample data. The method was tested on artificial and real hydrologic data sets using various machine learning techniques. Preliminary results show that the method is superior to other methods estimating the prediction interval. A new method for evaluating performance for estimating prediction interval is proposed as well.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3GWPIKHF/Shrestha,Sol,MachLearnEstimUncertanty,NNJ,2006.pdf}
}

@article{shrestha2009hess,
  title = {A Novel Approach to Parameter Uncertainty Analysis of Hydrological Models Using Neural Networks},
  author = {Shrestha, D L and Kayastha, N and Solomatine, D P},
  year = {2009},
  journal = {Hydrol. Earth Syst. Sci.},
  pages = {14},
  abstract = {In this study, a methodology has been developed to emulate a time consuming Monte Carlo (MC) simulation by using an Artificial Neural Network (ANN) for the assessment of model parametric uncertainty. First, MC simulation of a given process model is run. Then an ANN is trained to approximate the functional relationships between the input variables of the process model and the synthetic uncertainty descriptors estimated from the MC realizations. The trained ANN model encapsulates the underlying characteristics of the parameter uncertainty and can be used to predict uncertainty descriptors for the new data vectors. This approach was validated by comparing the uncertainty descriptors in the verification data set with those obtained by the MC simulation. The method is applied to estimate the parameter uncertainty of a lumped conceptual hydrological model, HBV, for the Brue catchment in the United Kingdom. The results are quite promising as the prediction intervals estimated by the ANN are reasonably accurate. The proposed techniques could be useful in real time applications when it is not practicable to run a large number of simulations for complex hydrological models and when the forecast lead time is very short.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PRZ27GB5/shrestha et al 2009  A novel approach to parameter uncertainty analysis of hydrological models using neural networks.pdf}
}

@article{shrestha2014jh,
  title = {Encapsulation of Parametric Uncertainty Statistics by Various Predictive Machine Learning Models: {{MLUE}} Method},
  shorttitle = {Encapsulation of Parametric Uncertainty Statistics by Various Predictive Machine Learning Models},
  author = {Shrestha, Durga L. and Kayastha, Nagendra and Solomatine, Dimitri and Price, Roland},
  year = {2014},
  month = jan,
  journal = {Journal of Hydroinformatics},
  volume = {16},
  number = {1},
  pages = {95--113},
  issn = {1464-7141, 1465-1734},
  doi = {10.2166/hydro.2013.242},
  abstract = {Monte Carlo simulation-based uncertainty analysis techniques have been applied successfully in hydrology for quantification of the model output uncertainty. They are flexible, conceptually simple and straightforward, but provide only average measures of uncertainty based on past data. However, if one needs to estimate uncertainty of a model in a particular hydro-meteorological situation in real time application of complex models, Monte Carlo simulation becomes impractical because of the large number of model runs required. This paper presents a novel approach to encapsulating and predicting parameter uncertainty of hydrological models using machine learning techniques.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PHQ2UQK7/shrestha et al 2014 - Encapsulation of parametric uncertainty statistics by various predictive machine learning models - MLUE method - GUPPY - ERROR.pdf}
}

@article{shresthaa,
  title = {{{ENCAPSULATION OF MONTE-CARLO UNCERTAINTY ANALYSIS RESULTS IN A PREDICTIVE MACHINE LEARNING}}},
  author = {Shrestha, Durga Lal and Kayastha, Nagendra and Solomatine, Dimitri},
  pages = {10},
  abstract = {Monte Carlo (MC) simulation is widely used to quantify the parameter uncertainty of hydrological and other models because of its flexibility and robustness. However, MC simulation is not always practical for real time flow forecasting when computationally intensive models are used. Here we present an approach for assessment of model parametric uncertainty using machine learning techniques to replicate time consuming MC simulation. In this approach, firstly MC simulation is performed by sampling parameters form the given probability distribution. Secondly, the uncertainty descriptors such as quantiles or prediction intervals are estimated from the realizations of MC simulation. Thirdly, the machine learning models are trained to approximate the functional relationships between the input variables and the synthetic uncertainty descriptors estimated from the realizations. The trained models encapsulate the underlying dynamics of the parameter uncertainty and can be used to predict uncertainty descriptors for the new data vectors. This approach was validated by comparing the uncertainty descriptors in the verification data set with those obtained by MC simulation. The method is applied to estimate parameter uncertainty of lumped conceptual hydrological model, HBV, for the Brue catchment in UK. The results are quite promising as the prediction intervals estimated by the machine learning techniques are reasonably accurate. The proposed techniques could be useful in real time application to replicate MC simulation when it is not practicable to run large number of simulations for time consuming hydrological models and when the forecast lead time is very short.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N7QWG9EU/Shrestha,Kayastha,Solomatine,MonteCarloSimulation,ProcHI,2009.pdf}
}

@article{shresthab,
  title = {A {{Novel Method}} to {{Estimate}} the {{Model Uncertainty Based}} on the {{Model Errors}}},
  author = {Shrestha, Durga Lal and Solomatine, Dimitri P},
  pages = {6},
  abstract = {This paper presents a novel method for estimating ``total'' predictive uncertainty using machine learning techniques. By the term ``total'' we mean that all sources of uncertainty are taken into account, including that of the input and observed data, model parameters and structure, without attempting to separate the contribution given by these different sources. We assume that the model error, which is mismatch between the observed and modelled value reflects all sources of uncertainty. Fuzzy c-means clustering was employed to cluster the input space into different zones or clusters assuming that the all the examples those belong to the particular cluster have similar model errors. The prediction interval is constructed for each cluster on the basis of empirical distributions of the historical model errors associated with all examples of the particular cluster. Prediction interval for the individual example is derived from cluster based prediction interval according to their membership grades in each cluster. Linear or non-linear regression model is then built in calibration data that approximates an underlying functional relationship between an input vector and the computed prediction intervals. Finally, this model is applied to estimate the prediction intervals in verification data. The method was tested on hydrologic datasets using various machine learning techniques. Preliminary results show that the method has certain advantage if compared to other methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZNZPREBF/Shrestha,Solomatine,NovelMethodEstimation,ProcIEMS,2006.pdf}
}

@article{shyloRestartStrategiesOptimization2011,
  title = {Restart Strategies in Optimization: Parallel and Serial Cases},
  shorttitle = {Restart Strategies in Optimization},
  author = {Shylo, Oleg V. and Middelkoop, Timothy and Pardalos, Panos M.},
  year = {2011},
  month = jan,
  journal = {Parallel Computing},
  volume = {37},
  number = {1},
  pages = {60--68},
  issn = {01678191},
  doi = {10.1016/j.parco.2010.08.004},
  abstract = {This paper addresses the problem of minimizing the average running time of the Las Vegas type algorithm, both in serial and parallel setups. The necessary conditions for the existence of an effective restart strategy are presented. We clarify the counter-intuitive empirical observations of super linear speedup and relate parallel speedup with the restart properties of serial algorithms. The general property of restart distributions is derived. The computational experiments involving the state-of-the-art optimization algorithm are provided.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GP9RGEGE/1-s2.0-S0167819110001110-main.pdf}
}

@article{sierra-altamiranda2020em,
  title = {Spatial Conservation Planning under Uncertainty Using Modern Portfolio Theory and {{Nash}} Bargaining Solution},
  author = {{Sierra-Altamiranda}, Alvaro and Charkhgard, Hadi and Eaton, Mitchell and Martin, Julien and Yurek, Simeon and Udell, Bradley J.},
  year = {2020},
  month = may,
  journal = {Ecological Modelling},
  volume = {423},
  pages = {109016},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2020.109016},
  abstract = {In recent years, researchers from interdisciplinary teams involving ecologists, economists and operations researchers collaborated to provide decision support tools to address the challenges of preserving biodiversity by optimizing the design of reserves. The goal of this paper is to further advance this area of research and provide new solutions to solve complex Spatial Conservation Planning (SCP) problems under uncertainty that consider risk preferences of decision makers. Our approach employs modern portfolio theory to address uncertainties in SCP problems, and involves two conflicting objectives: maximizing return and minimizing risk. We apply concepts from game theory such as the Nash bargaining solution to directly compute a desirable Pareto-optimal solution for the proposed bi-objective optimization formulation in natural resource management problems. We demonstrate with numerical examples that by directly computing a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. We show that our approach (implementable with commercial solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature. Optimal solutions for problems with less than 400 parcels can be computed within a minute. Near optimal solutions (within at most 0.2\% gap from an optimal solution) for high-dimensional problems involving up to 800 parcels can be computed within 8 h on a standard computer. We have presented a new approach to solve SCP optimization problems while considering uncertainty and risk tolerance of decision makers. Our new approach expands considerably the applicability of such SCP optimization methods to address real conservation problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/KIJ867QG/sierra-altamiranda et al 2020 - Spatial conservation planning under uncertainty using modern portfolio theory and Nash bargaining solution - BDPG - PORTFOLIO THEORY - RESERVE SELECTION.pdf}
}

@article{simmons2019mee,
  title = {Bmotif: {{A}} Package for Motif Analyses of Bipartite Networks},
  author = {Simmons, Benno I. and Sweering, Michelle J. M. and Schillinger, Maybritt and Dicks, Lynn V. and Sutherland, William J. and Di Clemente, Riccardo},
  editor = {Matthiopoulos, Jason},
  year = {2019},
  month = may,
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {5},
  pages = {695--701},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13149},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XGYWS24H/simmons et al 2019 - bmotif - a package for motif analyses of bipartite networks - BDPG - BIPARTITE NETWORKS.pdf}
}

@article{simmonsMotifsBipartiteEcological2019,
  title = {Motifs in Bipartite Ecological Networks: Uncovering Indirect Interactions},
  shorttitle = {Motifs in Bipartite Ecological Networks},
  author = {Simmons, Benno I. and Cirtwill, Alyssa R. and Baker, Nick J. and Wauchope, Hannah S. and Dicks, Lynn V. and Stouffer, Daniel B. and Sutherland, William J.},
  year = {2019},
  month = jan,
  journal = {Oikos},
  volume = {128},
  number = {2},
  pages = {154--170},
  issn = {00301299},
  doi = {10.1111/oik.05670},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8KKSW3ED/oik.05670.pdf}
}

@inproceedings{simonciniCentricSelectionWay2009,
  title = {Centric Selection: A Way to Tune the Exploration/Exploitation Trade-Off},
  shorttitle = {Centric Selection},
  booktitle = {Proceedings of the 11th {{Annual}} Conference on {{Genetic}} and Evolutionary Computation},
  author = {Simoncini, David and Verel, S{\'e}bastien and Collard, Philippe and Clergue, Manuel},
  year = {2009},
  pages = {891--898},
  publisher = {{ACM}},
  keywords = {exploration/exploitation tradeoff,GA,genetic algorithms},
  file = {/Users/bill/D/Zotero/storage/7TINSXNV/Simoncini et al. - 2009 - Centric selection a way to tune the exploratione.pdf}
}

@incollection{skalakClassifierLossMetric2007,
  title = {Classifier {{Loss Under Metric Uncertainty}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2007},
  author = {Skalak, David B. and {Niculescu-Mizil}, Alexandru and Caruana, Rich},
  editor = {Kok, Joost N. and Koronacki, Jacek and de Mantaras, Raomon Lopez and Matwin, Stan and Mladeni{\v c}, Dunja and Skowron, Andrzej},
  year = {2007},
  volume = {4701},
  pages = {310--322},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-74958-5_30},
  abstract = {Classifiers that are deployed in the field can be used and evaluated in ways that were not anticipated when the model was trained. The ultimate evaluation metric may not have been known to the modeler at training time, additional performance criteria may have been added, the evaluation metric may have changed over time, or the real-world evaluation procedure may have been impossible to simulate. But unforeseen ways of measuring model utility can degrade performance. Our objective is to provide experimental support for modelers who face potential ``cross-metric'' performance deterioration. First, to identify model-selection metrics that lead to stronger cross-metric performance, we characterize the expected loss where the selection metric is held fixed and the evaluation metric is varied. Second, we show that the number of data points evaluated by a selection metric has a substantial effect on the optimal evaluation. In trying to address both these issues, we hypothesize that whether classifiers are calibrated to output probabilities may influence these issues. In our consideration of the role of calibration, we show that our experiments demonstrate that cross-entropy is the highestperforming selection metric where little data is available for selection. With these experiments, modelers may be in a better position to choose selection metrics that are robust where it is uncertain what evaluation metric will be applied.},
  isbn = {978-3-540-74957-8 978-3-540-74958-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EVYMTSAG/skalak ... ccaruana 2007 - classifier loss under metric uncertainty - GUPPY - UNCERTAINTY - ENSEMBLE.pdf}
}

@article{sluban2015n,
  title = {Relating Ensemble Diversity and Performance: {{A}} Study in Class Noise Detection},
  shorttitle = {Relating Ensemble Diversity and Performance},
  author = {Sluban, Borut and Lavra{\v c}, Nada},
  year = {2015},
  month = jul,
  journal = {Neurocomputing},
  volume = {160},
  pages = {120--131},
  issn = {09252312},
  doi = {10.1016/j.neucom.2014.10.086},
  abstract = {The advantage of ensemble methods over single methods is their ability to correct the errors of individual ensemble members and thereby improve the overall ensemble performance. This paper explores the relation between ensemble diversity and noise detection performance in the context of ensemble-based class noise detection by studying different diversity measures on a range of heterogeneous noise detection ensembles. In the empirical analysis the majority and the consensus ensemble voting schemes are studied. It is shown that increased diversity of ensembles using the majority voting scheme does not lead to better noise detection performance and may even degrade the performance of heterogeneous noise detection ensembles. On the other hand, for consensus-based noise detection ensembles the results show that more diverse ensembles achieve higher precision of class noise detection, whereas less diverse ensembles lead to higher recall of noise detection and higher F-scores. \& 2015 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {bdpg,ensemble diversity,ensembles,label noise,noise,uncertainty},
  file = {/Users/bill/D/Zotero/storage/HXDTIG7D/sluban lavrac 2015 - Relating ensemble diversity and performance - A study in class noise detection - BDPG - UNCERTAINTY - NOISE - ENSEMBLE DIVERSITY.pdf}
}

@article{smith-miles2011amai,
  title = {Discovering the Suitability of Optimisation Algorithms by Learning from Evolved Instances},
  author = {{Smith-Miles}, Kate and {van Hemert}, Jano},
  year = {2011},
  month = feb,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {2},
  pages = {87--104},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9230-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IG5EQX24/smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{smith-miles2021c&or,
  title = {Revisiting Where Are the Hard Knapsack Problems? Via {{Instance Space Analysis}}},
  shorttitle = {Revisiting Where Are the Hard Knapsack Problems?},
  author = {{Smith-Miles}, Kate and Christiansen, Jeffrey and Mu{\~n}oz, Mario Andr{\'e}s},
  year = {2021},
  month = apr,
  journal = {Computers \& Operations Research},
  volume = {128},
  pages = {105184},
  issn = {03050548},
  doi = {10.1016/j.cor.2020.105184},
  abstract = {In 2005, David Pisinger asked the question ``where are the hard knapsack problems?''. Noting that the classical benchmark test instances were limited in difficulty due to their selected structure, he proposed a set of new test instances for the 0-1 knapsack problem with characteristics that made them more challenging for dynamic programming and branch-and-bound algorithms. This important work highlighted the influence of diversity in test instances to draw reliable conclusions about algorithm performance. In this paper, we revisit the question in light of recent methodological advances \textendash{} in the form of Instance Space Analysis \textendash enabling the strengths and weaknesses of algorithms to be visualised and assessed across the broadest possible space of test instances. We show where the hard instances lie, and objectively assess algorithm performance across the instance space to articulate the strengths and weaknesses of algorithms. Furthermore, we propose a method to fill the instance space with diverse and challenging new test instances with controllable properties to support greater insights into algorithm selection, and drive future algorithmic innovations.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/E6TCQ9KW/smith-miles et al 2021 - revisiting where are the hard knapsack problems - via instance space analysis - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2008},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {03600300},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  keywords = {bdpg,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/7DWGI3PN/performance measurement system design - a literature review and research agenda - neely et al - intjopandprodman 2005.pdf;/Users/bill/D/Zotero/storage/EH3M2958/a6-smith-miles.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008a,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2008},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {03600300},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EZ874QSU/a6-smith-miles.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2008b,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2008},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {03600300},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/74UUCAPD/a6-smith-miles.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2009},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N228KTGV/smith-miles 2008 - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009a,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2009},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/R4VRBDRW/smith-miles - cross-disciplinary perspectives on meta-learning for algorithm selection.pdf}
}

@article{smith-milesCrossdisciplinaryPerspectivesMetalearning2009b,
  title = {Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection},
  author = {{Smith-Miles}, Kate A.},
  year = {2009},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {1},
  pages = {1--25},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1456650.1456656},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K7RQZAFU/a6-smith-miles.pdf}
}

@article{smith-milesDiscoveringSuitabilityOptimisation2011,
  title = {Discovering the Suitability of Optimisation Algorithms by Learning from Evolved Instances},
  author = {{Smith-Miles}, Kate and {van Hemert}, Jano},
  year = {2011},
  month = feb,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {2},
  pages = {87--104},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9230-5},
  abstract = {The suitability of an optimisation algorithm selected from within an algorithm portfolio depends upon the features of the particular instance to be solved. Understanding the relative strengths and weaknesses of different algorithms in the portfolio is crucial for effective performance prediction, automated algorithm selection, and to generate knowledge about the ideal conditions for each algorithm to influence better algorithm design. Relying on well-studied benchmark instances, or randomly generated instances, limits our ability to truly challenge each of the algorithms in a portfolio and determine these ideal conditions. Instead we use an evolutionary algorithm to evolve instances that are uniquely easy or hard for each algorithm, thus providing a more direct method for studying the relative strengths and weaknesses of each algorithm. The proposed methodology ensures that the meta-data is sufficient to be able to learn the features of the instances that uniquely characterise the ideal conditions for each algorithm. A case study is presented based on a comprehensive study of the performance of two heuristics on the Travelling Salesman Problem. The results show that prediction of search effort as well as the best performing algorithm for a given instance can be achieved with high accuracy.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RBX5XNJP/smith-miles van hemert 2011 - discovering the suitability of optimisation algorithms by learning from evolved instances.pdf}
}

@article{smith-milesDiscoveringSuitabilityOptimisation2011a,
  title = {Discovering the Suitability of Optimisation Algorithms by Learning from Evolved Instances},
  author = {{Smith-Miles}, Kate and {van Hemert}, Jano},
  year = {2011},
  month = feb,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {2},
  pages = {87--104},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9230-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VGFRKLI7/smith-miles lopes 2011 - generalising algorithm performance in instance space - SLIDES - BDPG - PROBLEM DIFFICULTY.pdf}
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011,
  title = {Generalising {{Algorithm Performance}} in {{Instance Space}}: {{A Timetabling Case Study}}},
  shorttitle = {Generalising {{Algorithm Performance}} in {{Instance Space}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  editor = {Coello, Carlos A. Coello},
  year = {2011},
  volume = {6683},
  pages = {524--538},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25566-3_41},
  abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a ``footprint'' in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the differences between the instance generation methods, and the suitability of different algorithms.},
  isbn = {978-3-642-25565-6 978-3-642-25566-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5DRPSJ6D/smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf}
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011a,
  title = {Generalising {{Algorithm Performance}} in {{Instance Space}}: {{A Timetabling Case Study}}},
  shorttitle = {Generalising {{Algorithm Performance}} in {{Instance Space}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  editor = {Coello, Carlos A. Coello},
  year = {2011},
  volume = {6683},
  pages = {524--538},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25566-3_41},
  abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a ``footprint'' in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the differences between the instance generation methods, and the suitability of different algorithms.},
  isbn = {978-3-642-25565-6 978-3-642-25566-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RHL3FW4V/smith-miles lopes 2011 - generalising algorithm performance in instance space - a timetabling case study.pdf}
}

@incollection{smith-milesGeneralisingAlgorithmPerformance2011b,
  title = {Generalising {{Algorithm Performance}} in {{Instance Space}}: {{A Timetabling Case Study}}},
  shorttitle = {Generalising {{Algorithm Performance}} in {{Instance Space}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  editor = {Coello, Carlos A. Coello},
  year = {2011},
  volume = {6683},
  pages = {524--538},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25566-3_41},
  abstract = {The ability to visualise how algorithm performance varies across the feature space of possible instance, both real and synthetic, is critical to algorithm selection. Generalising algorithm performance, based on learning from a subset of instances, creates a ``footprint'' in instance space. This paper shows how self-organising maps can be used to visualise the footprint of algorithm performance, and illustrates the approach using a case study from university course timetabling. The properties of the timetabling instances, viewed from this instance space, are revealing of the differences between the instance generation methods, and the suitability of different algorithms.},
  isbn = {978-3-642-25565-6 978-3-642-25566-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X49UWU9A/smith-miles lopes 2011 - Generalising Algorithm Performance in InstanceSpace - A Timetabling Case Study - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{smith-milesIntelligentOptimization,
  title = {= {{Intelligent Optimization}}},
  author = {{Smith-Miles}, Professor Kate},
  pages = {51},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BDDFQ835/smith-miles - how can data mining help to understand what makes an optimization problem hard, which algorithm will perform best, and why - talk.PDF}
}

@incollection{smith-milesKnowledgeDiscoveryApproach2009,
  title = {A {{Knowledge Discovery Approach}} to {{Understanding Relationships}} between {{Scheduling Problem Structure}} and {{Heuristic Performance}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {{Smith-Miles}, Kate A. and James, Ross J. W. and Giffin, John W. and Tu, Yiqing},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and St{\"u}tzle, Thomas},
  year = {2009},
  volume = {5851},
  pages = {89--103},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-11169-3_7},
  abstract = {Using a knowledge discovery approach, we seek insights into the relationships between problem structure and the effectiveness of scheduling heuristics. A large collection of 75,000 instances of the single machine early/tardy scheduling problem is generated, characterized by six features, and used to explore the performance of two common scheduling heuristics. The best heuristic is selected using rules from a decision tree with accuracy exceeding 97\%. A self-organizing map is used to visualize the feature space and generate insights into heuristic performance. This paper argues for such a knowledge discovery approach to be applied to other optimization problems, to contribute to automation of algorithm selection as well as insightful algorithm design.},
  isbn = {978-3-642-11168-6 978-3-642-11169-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7JWRVFSN/smith-miles et al 2009 - a knowledge discovery approach to understanding relationships between scheduling problem structure and heuristic performance.pdf}
}

@inproceedings{smith-milesMeasuringAlgorithmFootprints2012,
  title = {Measuring Algorithm Footprints in Instance Space},
  booktitle = {2012 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {{Smith-Miles}, Kate and Tan, Thomas T.},
  year = {2012},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Brisbane, Australia}},
  doi = {10.1109/CEC.2012.6252992},
  abstract = {This paper proposes a new methodology to determine the relative performance of optimization algorithms across various classes of instances. Rather than reporting performance based on a chosen test set of benchmark instances, we aim to develop metrics for an algorithm's performance generalized across a diverse set of instances. Instances are summarized by a set of features that correlate with difficulty, and we propose methods for visualizing instances and algorithm performance in this high-dimensional feature space. The footprint of an algorithm is where good performance can be expected, and we propose new metrics to measure the relative size of an algorithm's footprint in instance space. The methodology is demonstrated using the Traveling Salesman Problem as a case study.},
  isbn = {978-1-4673-1509-8 978-1-4673-1510-4 978-1-4673-1508-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AIRWLG2N/smith-miles tan 2012 - measuring algorithm footprints in instance space.pdf}
}

@article{smith-milesMeasuringInstanceDifficulty2012,
  title = {Measuring Instance Difficulty for Combinatorial Optimization Problems},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  year = {2012},
  month = may,
  journal = {Computers \& Operations Research},
  volume = {39},
  number = {5},
  pages = {875--889},
  issn = {03050548},
  doi = {10.1016/j.cor.2011.07.006},
  abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies \textendash{} studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances \textendash{} provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difficulty in terms of algorithmic performance, and how such features can be defined and measured for various optimization problems. We provide a comprehensive survey of the research field with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems \textendash{} which are important abstractions of many real-world problems \textendash{} we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6FCV7WIL/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf}
}

@article{smith-milesMeasuringInstanceDifficulty2012a,
  title = {Measuring Instance Difficulty for Combinatorial Optimization Problems},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  year = {2012},
  month = may,
  journal = {Computers \& Operations Research},
  volume = {39},
  number = {5},
  pages = {875--889},
  issn = {03050548},
  doi = {10.1016/j.cor.2011.07.006},
  abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies \textendash{} studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances \textendash{} provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difficulty in terms of algorithmic performance, and how such features can be defined and measured for various optimization problems. We provide a comprehensive survey of the research field with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems \textendash{} which are important abstractions of many real-world problems \textendash{} we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7Z9NLMMM/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf}
}

@article{smith-milesMeasuringInstanceDifficulty2012b,
  title = {Measuring Instance Difficulty for Combinatorial Optimization Problems},
  author = {{Smith-Miles}, Kate and Lopes, Leo},
  year = {2012},
  month = may,
  journal = {Computers \& Operations Research},
  volume = {39},
  number = {5},
  pages = {875--889},
  issn = {03050548},
  doi = {10.1016/j.cor.2011.07.006},
  abstract = {Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies \textendash{} studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances \textendash{} provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difficulty in terms of algorithmic performance, and how such features can be defined and measured for various optimization problems. We provide a comprehensive survey of the research field with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems \textendash{} which are important abstractions of many real-world problems \textendash{} we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting similar structures.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CRBN3D2H/smith-myles lopes 2012 - Measuring instance difficulty for combinatorial optimization problems.pdf}
}

@inproceedings{smith-milesMetalearningDataSummarization2010,
  title = {Meta-Learning for Data Summarization Based on Instance Selection Method},
  booktitle = {{{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {{Smith-Miles}, Kate and Islam, Rafiqul},
  year = {2010},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/CEC.2010.5585986},
  abstract = {The purpose of instance selection is to identify which instances (examples, patterns) in a large dataset should be selected as representatives of the entire dataset, without significant loss of information. When a machine learning method is applied to the reduced dataset, the accuracy of the model should not be significantly worse than if the same method were applied to the entire dataset. The reducibility of any dataset, and hence the success of instance selection methods, surely depends on the characteristics of the dataset, as well as the machine learning method. This paper adopts a meta-learning approach, via an empirical study of 112 classification datasets from the UCI Repository [1], to explore the relationship between data characteristics, machine learning methods, and the success of instance selection method.},
  isbn = {978-1-4244-6909-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VRJ4VPQW/smith-miles islam 2010 - meta-learning for data summarization based on instance selection method.pdf}
}

@incollection{smith-milesMetaLearningInstanceSelection2011,
  title = {Meta-{{Learning}} of {{Instance Selection}} for {{Data Summarization}}},
  booktitle = {Meta-{{Learning}} in {{Computational Intelligence}}},
  author = {{Smith-Miles}, Kate A. and Islam, Rafiqul M. D.},
  editor = {Kacprzyk, Janusz and Jankowski, Norbert and Duch, W{\l}odzis{\l}aw and Gr{\c a}bczewski, Krzysztof},
  year = {2011},
  volume = {358},
  pages = {77--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-20980-2_2},
  abstract = {The goal of instance selection is to identify which instances (examples, patterns) in a large dataset should be selected as representatives of the entire dataset, without significant loss of information. When a machine learning method is applied to the reduced dataset, the accuracy of the model should not be significantly worse than if the same method were applied to the entire dataset. The reducibility of any dataset, and hence the success of instance selection methods, surely depends on the characteristics of the dataset. However the relationship between data characteristics and the reducibility achieved by instance selection methods has not been extensively tested. This chapter adopts a meta-learning approach, via an empirical study of 112 classification datasets, to explore the relationship between data characteristics and the success of a na\"ive instance selection method. The approach can be readily extended to explore how the data characteristics influence the performance of many more sophisticated instance selection methods.},
  isbn = {978-3-642-20979-6 978-3-642-20980-2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AM83HNHR/smith-miles islam 2011 - meta-learning of instance selection for data summarization.pdf}
}

@article{smith-milesUnderstandingRelationshipScheduling,
  title = {Understanding the {{Relationship}} between {{Scheduling Problem Structure}} and {{Heuristic Performance}} Using {{Knowledge Discovery}}},
  author = {{Smith-Miles}, Kate A and James, Ross J W and Giffin, John W and Tu, Yiqing},
  pages = {15},
  abstract = {Using a knowledge discovery approach, we seek insights into the relationships between problem structure and the effectiveness of scheduling heuristics. A large collection of 75,000 instances of the single machine early/tardy scheduling problem is generated, characterized by six features, and used to explore the performance of two common scheduling heuristics. The best heuristic is selected using rules from a decision tree with accuracy exceeding 97\%. A self-organizing map is used to visualize the feature space and generate insights into heuristic performance. This paper argues for such a knowledge discovery approach to be applied to other optimization problems, to contribute to automation of algorithm selection as well as insightful algorithm design.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/A7D2PKJC/smith-miles et al - understanding the relationship between scheduling problem structure and heuristic performance using knowledge discovery.pdf}
}

@article{smith2001tcs,
  title = {Constructing an Asymptotic Phase Transition in Random Binary Constraint Satisfaction Problems},
  author = {Smith, Barbara M.},
  year = {2001},
  month = aug,
  journal = {Theoretical Computer Science},
  volume = {265},
  number = {1-2},
  pages = {265--283},
  issn = {03043975},
  doi = {10.1016/S0304-3975(01)00166-9},
  abstract = {The standard models used to generate random binary constraint satisfaction problems are described. At the problem sizes studied experimentally, a phase transition is seen as the constraint tightness is varied. However, Achlioptas et al. showed that if the problem size (number of variables) increases while the remaining parameters are kept constant, asymptotically almost all instances are unsatis\"yable. In this paper, an alternative scheme for one of the standard models is proposed in which both the number of values in each variable's domain and the average degree of the constraint graph are increased with problem size. It is shown that with this scheme there is asymptotically a range of values of the constraint tightness in which instances are trivially satis\"yable with probability at least 0.5 and a range in which instances are almost all unsatis\"yable; hence there is a crossover point at some value of the constraint tightness between these two ranges. This scheme is compared to a similar scheme due to Xu and Li. c 2001 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/JYM9UBIA/smith 2001 - Constructing an asymptotic phase transition in random binary constraint satisfaction problems - BDPG - ANNO.pdf}
}

@article{smith2006ms,
  title = {The {{Optimizer}}'s {{Curse}}: {{Skepticism}} and {{Postdecision Surprise}} in {{Decision Analysis}}},
  shorttitle = {The {{Optimizer}}'s {{Curse}}},
  author = {Smith, James E. and Winkler, Robert L.},
  year = {2006},
  month = mar,
  journal = {Management Science},
  volume = {52},
  number = {3},
  pages = {311--322},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1050.0451},
  langid = {english},
  keywords = {bdpg,guppy,optimization,surprise,uncertainty},
  file = {/Users/bill/D/Zotero/storage/KZTXU87Y/smith et al 2006 - the optimizers curse - skepticism and postdecision surprise in decision analysis.talks about negative surprise - BDPG.pdf}
}

@article{smithLocatingPhaseTransition1996,
  title = {Locating the Phase Transition in Binary Constraint Satisfaction Problems},
  author = {Smith, Barbara M. and Dyer, Martin E.},
  year = {1996},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {81},
  number = {1-2},
  pages = {155--181},
  issn = {00043702},
  doi = {10.1016/0004-3702(95)00052-6},
  abstract = {The phase transition in binary constraint satisfaction problems, i.e. the transition from a region in which almost all problems have many solutions to a region in which almost all problems have no solutions, as the constraints become tighter, is investigated by examining the behaviour of samples of randomly-generated problems. In contrast to theoretical work, which is concerned with the asymptotic behaviour of problems as the number of variables becomes larger, this paper is concerned with the location of the phase transition in finite problems. The accuracy of a prediction based on the expected number of solutions is discussed; it is shown that the variance of the number of solutions can be used to set bounds on the phase transition and to indicate the accuracy of the prediction. A class of sparse problems, for which the prediction is known to be inaccurate, is considered in detail; it is shown that, for these problems, the phase transition depends on the topology of the constraint graph as well as on the tightness of the constraints.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/K4L3DEWL/smith dyer 1996 - locating the phase transition in binary constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf}
}

@article{sofaerMisleadingPrioritizationsModelling2018,
  title = {Misleading Prioritizations from Modelling Range Shifts under Climate Change},
  author = {Sofaer, Helen R. and Jarnevich, Catherine S. and Flather, Curtis H.},
  year = {2018},
  month = jun,
  journal = {Global Ecology and Biogeography},
  volume = {27},
  number = {6},
  pages = {658--666},
  issn = {1466822X},
  doi = {10.1111/geb.12726},
  abstract = {Aim: Conservation planning requires the prioritization of a subset of taxa and geographical locations to focus monitoring and management efforts. Integration of the threats and opportunities posed by climate change often relies on predictions from species distribution models, particularly for assessments of vulnerability or invasion risk for multiple taxa. We evaluated whether species distribution models could reliably rank changes in species range size under climate and land use change.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7IYPF63P/sofaer et al 2018 - Misleading prioritizations from modelling range shifts underclimate change - BDPG - GUPPY - SDM - UNCERTAINTY.pdf}
}

@article{solomatine2009wrr,
  title = {A Novel Method to Estimate Model Uncertainty Using Machine Learning Techniques: {{NOVEL METHOD TO ESTIMATE UNCERTAINTY}}},
  shorttitle = {A Novel Method to Estimate Model Uncertainty Using Machine Learning Techniques},
  author = {Solomatine, Dimitri P. and Shrestha, Durga Lal},
  year = {2009},
  month = dec,
  journal = {Water Resources Research},
  volume = {45},
  number = {12},
  issn = {00431397},
  doi = {10.1029/2008WR006839},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YSYIHXLW/Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf}
}

@article{solomatine2009wrra,
  title = {A Novel Method to Estimate Model Uncertainty Using Machine Learning Techniques: {{NOVEL METHOD TO ESTIMATE UNCERTAINTY}}},
  shorttitle = {A Novel Method to Estimate Model Uncertainty Using Machine Learning Techniques},
  author = {Solomatine, Dimitri P. and Shrestha, Durga Lal},
  year = {2009},
  month = dec,
  journal = {Water Resources Research},
  volume = {45},
  number = {12},
  issn = {00431397},
  doi = {10.1029/2008WR006839},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XEPHWCUM/Solomatine and Shrestha  2009 - A novel method to estimate model uncertainty using machine learning techniques.pdf.pdf}
}

@incollection{solomatine2013eaonn,
  title = {Learning {{Errors}} of {{Environmental Mathematical Models}}},
  booktitle = {Engineering {{Applications}} of {{Neural Networks}}},
  author = {Solomatine, Dimitri and Kuzmin, Vadim and Shrestha, Durga Lal},
  editor = {Iliadis, Lazaros and Papadopoulos, Harris and Jayne, Chrisina},
  year = {2013},
  volume = {383},
  pages = {466--473},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41013-0_48},
  abstract = {In solving civil engineering problems the use of various models for forecasting environmental variables (for example, water levels in a river during flooding) is a must. Mathematical models of environmental processes inevitably contain errors (even if models are calibrated on accurate data) which can be represented as realizations of a stochastic process. Parameters of this process vary in time and cannot be reliably estimated without making (unrealistic) assumptions. However the model errors depend on various factors characterizing environmental conditions (for example, for extreme events errors are typically higher), and such dependencies can be reconstructed based on data. We present a unifying approach allowing for building machine learning models (in particular ANN and Local weighted regression) able to predict such errors as well as the properties of their distributions. Examples in modelling hydrological processes are considered.},
  isbn = {978-3-642-41012-3 978-3-642-41013-0},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FA7SLMRH/solomatine kuzmin shrestha 2013 - learning errors of environmental mathematical models - GUPPY - ERROR.pdf}
}

@article{solomonGenomeEditingAnimals2020,
  title = {Genome Editing in Animals: Why {{FDA}} Regulation Matters},
  shorttitle = {Genome Editing in Animals},
  author = {Solomon, Steven M.},
  year = {2020},
  month = feb,
  journal = {Nature Biotechnology},
  volume = {38},
  number = {2},
  pages = {142--143},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-020-0413-7},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/N3X45WS7/solomon 2020 - Genome editing in animals - why FDA regulation matters - OPTISEVIL - GENETIC ENGINEERING - GMO.pdf}
}

@incollection{sorokinaAdditiveGrovesRegression2007,
  title = {Additive {{Groves}} of {{Regression Trees}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2007},
  author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek},
  editor = {Kok, Joost N. and Koronacki, Jacek and de Mantaras, Raomon Lopez and Matwin, Stan and Mladeni{\v c}, Dunja and Skowron, Andrzej},
  year = {2007},
  volume = {4701},
  pages = {323--334},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-74958-5_31},
  abstract = {We present a new regression algorithm called Additive Groves and show empirically that it is superior in performance to a number of other established regression methods. A single Grove is an additive model containing a small number of large trees. Trees added to a Grove are trained on the residual error of other trees already in the model. We begin the training process with a single small tree and gradually increase both the number of trees in the Grove and their size. This procedure ensures that the resulting model captures the additive structure of the response. A single Grove may still overfit to the training set, so we further decrease the variance of the final predictions with bagging. We show that in addition to exhibiting superior performance on a suite of regression test problems, Additive Groves are very resistant to overfitting.},
  isbn = {978-3-540-74957-8 978-3-540-74958-5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8366VP5Y/groves.pdf}
}

@inproceedings{sorokinaDetectingInterpretingVariable2009,
  title = {Detecting and {{Interpreting Variable Interactions}} in {{Observational Ornithology Data}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Hochachka, Wesley and Kelling, Steve},
  year = {2009},
  month = dec,
  pages = {64--69},
  publisher = {{IEEE}},
  address = {{Miami, FL, USA}},
  doi = {10.1109/ICDMW.2009.84},
  abstract = {In this paper we demonstrate a practical approach to interaction detection on real data describing the abundance of different species of birds in the prairies east of the southern Rocky Mountains. This data is very noisy - predictive models built from this data perform only slightly better than baseline. Previous approaches for interaction detection, including recently proposed algorithm based on Additive Groves, might not work ideally on such noisy data for a number of reasons. We describe the issues that appear when working with such data sets and suggest solutions to them. We further demonstrate that with our improvements to the interaction detection algorithm it is possible to detect interactions between important features and the response function, even when the data is this noisy. In the end, we show and interpret the results of our analysis for several bird species.},
  isbn = {978-1-4244-5384-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WNFX9V29/10.1.1.205.4865.pdf}
}

@inproceedings{sorokinaDetectingStatisticalInteractions2008,
  title = {Detecting Statistical Interactions with Additive Groves of Trees},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
  year = {2008},
  pages = {1000--1007},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390282},
  abstract = {Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.},
  isbn = {978-1-60558-205-4},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5QYDL5JB/2008-ICML-Interactions.pdf}
}

@article{spence2010ajea,
  title = {Sgen1: {{A}} Generator of Small but Difficult Satisfiability Benchmarks},
  shorttitle = {Sgen1},
  author = {Spence, Ivor},
  year = {2010},
  month = mar,
  journal = {ACM Journal of Experimental Algorithmics},
  volume = {15},
  issn = {1084-6654, 1084-6654},
  doi = {10.1145/1671970.1671972},
  langid = {english},
  keywords = {bdpg,problem difficulty,problem generator},
  file = {/Users/bill/D/Zotero/storage/NJVCWTSY/spence 2010 - sgen1 - A Generator of Small but Difficult Satisfiability Benchmarks - PROBLEM GENERATOR - SAT - BDPG - PROBLEM DIFFICULTY.pdf}
}

@article{stainforthConfidenceUncertaintyDecisionsupport2007,
  title = {Confidence, Uncertainty and Decision-Support Relevance in Climate Predictions},
  author = {Stainforth, D.A and Allen, M.R and Tredger, E.R and Smith, L.A},
  year = {2007},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {365},
  number = {1857},
  pages = {2145--2161},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2007.2074},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WFBCH6QE/STAINF~1.PDF}
}

@article{staniczenkoGhostNestednessEcological2013,
  title = {The Ghost of Nestedness in Ecological Networks},
  author = {Staniczenko, Phillip P. A. and Kopp, Jason C. and Allesina, Stefano},
  year = {2013},
  month = jun,
  journal = {Nature Communications},
  volume = {4},
  number = {1},
  pages = {1391},
  issn = {2041-1723},
  doi = {10.1038/ncomms2422},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/9LUFU7C8/ncomms2422-s1.pdf}
}

@article{sternWhatSetCover,
  title = {What Is the Set Cover Problem},
  author = {Stern, Tamara I},
  pages = {5},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HZX2NB9W/setcover-tamara.pdf}
}

@book{stevens2009,
  title = {A {{Primer}} of {{Ecology}} with {{R}}},
  author = {Stevens, M. H. H.},
  year = {2009},
  series = {Use {{R}}!},
  number = {6991},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-89881-0},
  keywords = {bdpg,bdpg_P1,lognormal}
}

@article{stillmanDerivingSimplePredictions2015,
  title = {Deriving Simple Predictions from Complex Models to Support Environmental Decision-Making},
  author = {Stillman, Richard A. and Wood, Kevin A. and {Goss-Custard}, John D.},
  year = {2015},
  month = may,
  journal = {Ecological Modelling},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2015.04.014},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TCJ26JZC/stillman et al 2015 - Deriving simple predictions from complex models to support environmental decision-making - GUPPY - THRESHOLDS.pdf}
}

@article{submissionMachineLearningThat,
  title = {Machine {{Learning}} That {{Matters}}},
  author = {Submission, Anonymous},
  pages = {6},
  abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QAXUBL9F/wagstaff 2012 - Machine Learning that Matters.pdf}
}

@book{sugiyamaMachineLearningNonStationary2012,
  title = {Machine {{Learning}} in {{Non-Stationary Environments}}: {{Introduction}} to {{Covariate Shift Adaptation}}},
  shorttitle = {Machine {{Learning}} in {{Non-Stationary Environments}}},
  author = {Sugiyama, Masashi and Kawanabe, Motoaki},
  year = {2012},
  month = mar,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262017091.001.0001},
  abstract = {Most active learning methods avoid model selection by training models of one type (SVMs, boosted trees, etc.) using one pre-defined set of model hyperparameters. We propose an algorithm that actively samples data to simultaneously train a set of candidate models (different model types and/or different hyperparameters) and also select the best model from this set. The algorithm actively samples points for training that are most likely to improve the accuracy of the more promising candidate models, and also samples points for model selection\textemdash all samples count against the same labeling budget. This exposes a natural trade-off between the focused active sampling that is most effective for training models, and the unbiased sampling that is better for model selection. We empirically demonstrate on six test problems that this algorithm is nearly as effective as an active learning oracle that knows the optimal model in advance.},
  isbn = {978-0-262-01709-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QBD3GZ7M/AAAI2014.pdf}
}

@inproceedings{sunSelectionFitnessLandscape2014,
  title = {On the Selection of Fitness Landscape Analysis Metrics for Continuous Optimization Problems},
  booktitle = {7th {{International Conference}} on {{Information}} and {{Automation}} for {{Sustainability}}},
  author = {Sun, Yuan and Halgamuge, Saman K. and Kirley, Michael and Munoz, Mario A.},
  year = {2014},
  month = dec,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Colombo, Sri Lanka}},
  doi = {10.1109/ICIAFS.2014.7069635},
  abstract = {Selecting the best algorithm for a given optimization problem is non-trivial due to large number of existing algorithms and high complexity of problems. A possible way to tackle this challenge is to attempt to understand the problem complexity. Fitness Landscape Analysis (FLA) metrics are widely used techniques to extract characteristics from problems. Based on the extracted characteristics, machine learning methods are employed to select the optimal algorithm for a given problem. Therefore, the accuracy of the algorithm selection framework heavily relies on the choice of FLA metrics. Although researchers have paid great attention to designing FLA metrics to quantify the problem characteristics, there is still no agreement on which combination of FLA metrics should be employed. In this paper, we present some well-performed FLA metrics, discuss their contributions and limitations in detail, and map each FLA metric to the captured problem characteristics. Moreover, computational complexity of each FLA metric is carefully analysed. We propose two criteria to follow when selecting FLA metrics. We hope our work can help researchers identify the best combination of FLA metrics.},
  isbn = {978-1-4799-4598-6},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UB2DP49J/sun ... kirley munoz 2014 - On the Selection of Fitness Landscape Analysis Metrics for Continuous Optimization Problems - PROBLEM DIFFICULTY - BDPG.pdf}
}

@misc{sutton,
  title = {Towards the {{Rigorous Analysis}} of {{Evolutionary Algorithms}} on {{Random}} K-{{Satisfiability Formulas}}},
  author = {Sutton, Andrew M},
  langid = {english},
  keywords = {slides},
  file = {/Users/bill/D/Zotero/storage/EY5RSBT9/AMS_220812.pdf}
}

@inproceedings{suttonApproximatingDistributionFitness2011,
  title = {Approximating the Distribution of Fitness over Hamming Regions},
  booktitle = {Proceedings of the 11th Workshop Proceedings on {{Foundations}} of Genetic Algorithms - {{FOGA}} '11},
  author = {Sutton, Andrew M. and Whitley, Darrell and Howe, Adele E.},
  year = {2011},
  pages = {93},
  publisher = {{ACM Press}},
  address = {{Schwarzenberg, Austria}},
  doi = {10.1145/1967654.1967663},
  abstract = {The distribution of fitness values across a set of states sharply influences the dynamics of evolutionary processes and heuristic search in combinatorial optimization. In this paper we present a method for approximating the distribution of fitness values over Hamming regions by solving a linear programming problem that incorporates low order moments of the target function. These moments can be retrieved in polynomial time for select problems such as MAX-k-SAT using Walsh analysis. The method is applicable to any real function on binary strings that is epistatically bounded and discrete with asymptotic bounds on the cardinality of its codomain.},
  isbn = {978-1-4503-0633-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/FASPKNTK/sutton whitley howe 2011 - approximating the distribution of fitness over hamming regions.pdf}
}

@article{svancaraPolicydrivenEvidencebasedConservation2005,
  title = {Policy-Driven versus {{Evidence-based Conservation}}: {{A Review}} of {{Political Targets}} and {{Biological Needs}}},
  shorttitle = {Policy-Driven versus {{Evidence-based Conservation}}},
  author = {Svancara, Leona K. and Brannon J., Ree and Scott, Michael and Groves, Craig R. and Noss, Reed F. and Pressey, Robert L.},
  year = {2005},
  journal = {BioScience},
  volume = {55},
  number = {11},
  pages = {989},
  issn = {0006-3568},
  doi = {10.1641/0006-3568(2005)055[0989:PVECAR]2.0.CO;2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/W544VFSS/svancara et al 2005 - policy-driven versus evidence-based conservation - a review of political targets and biological needs - GUPPY - TARGETS - MARXAN - BDPG.pdf}
}

@techreport{swilerEfficientAlgorithmsMixed2009,
  title = {Efficient Algorithms for Mixed Aleatory-Epistemic Uncertainty Quantification with Application to Radiation-Hardened Electronics. {{Part I}}, Algorithms and Benchmark Results.},
  author = {Swiler, Laura Painton and Eldred, Michael Scott},
  year = {2009},
  month = sep,
  number = {SAND2009-5805, 972887},
  pages = {SAND2009-5805, 972887},
  doi = {10.2172/972887},
  abstract = {This report documents the results of an FY09 ASC V\&V Methods level 2 milestone demonstrating new algorithmic capabilities for mixed aleatory-epistemic uncertainty quantification. Through the combination of stochastic expansions for computing aleatory statistics and interval optimization for computing epistemic bounds, mixed uncertainty analysis studies are shown to be more accurate and efficient than previously achievable. Part I of the report describes the algorithms and presents benchmark performance results. Part II applies these new algorithms to UQ analysis of radiation effects in electronic devices and circuits for the QASPR program.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/37KAPNZX/eldred swiler 2009 - Efficient algorithms for mixed aleatory-epistemic uncertainty quantification with application to radiation-hardened electronics - Part I - algorithms and benchmark results - PROBLEM DIFFI.pdf}
}

@article{talagalaFFORMPPFeaturebasedForecast2019,
  title = {{{FFORMPP}}: {{Feature-based}} Forecast Model Performance Prediction},
  shorttitle = {{{FFORMPP}}},
  author = {Talagala, Thiyanga S. and Li, Feng and Kang, Yanfei},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.11500 [stat]},
  eprint = {1908.11500},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {This paper introduces a novel meta-learning algorithm for time series forecasting. The efficient Bayesian multivariate surface regression approach is used to model forecast error as a function of features calculated from the time series. The minimum predicted forecast error is then used to identify an individual model or combination of models to produce forecasts. In general, the performance of any meta-learner strongly depends on the reference dataset used to train the model. We further examine the feasibility of using GRATIS (a feature-based time series simulation approach) in generating a realistic time series collection to obtain a diverse collection of time series for our reference set. The proposed framework is tested using the M4 competition data and is compared against several benchmarks and other commonly used forecasting approaches. The new approach obtains performance comparable to the second and the third rankings of the M4 competition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Applications},
  file = {/Users/bill/D/Zotero/storage/8XW8IVU8/talagala et al 2019 - FFORMPP - Feature-based forecast model performance prediction.pdf}
}

@article{taleghan2015jmlr,
  title = {{{PAC Optimal MDP Planning}} with {{Application}} to {{Invasive Species Management}}},
  author = {Taleghan, Majid Alkaee},
  year = {2015},
  journal = {Journal of Machine Learning Research},
  volume = {16},
  pages = {3877--3903},
  abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8\% and 47\% in the number of simulator calls required to reach near-optimal policies.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6LKQZHVE/taleghan DIETTERICH ... ALBERS 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management.pdf}
}

@article{tanResearchAdvanceSwarm2013,
  title = {Research {{Advance}} in {{Swarm Robotics}}},
  author = {Tan, Ying and Zheng, Zhong-yang},
  year = {2013},
  month = mar,
  journal = {Defence Technology},
  volume = {9},
  number = {1},
  pages = {18--39},
  issn = {22149147},
  doi = {10.1016/j.dt.2013.03.001},
  abstract = {The research progress of swarm robotics is reviewed in details. The swarm robotics inspired from nature is a combination of swarm intelligence and robotics, which shows a great potential in several aspects. First of all, the cooperation of nature swarm and swarm intelligence are briefly introduced, and the special features of the swarm robotics are summarized compared to a single robot and other multi-individual systems. Then the modeling methods for swarm robotics are described, followed by a list of several widely used swarm robotics entity projects and simulation platforms. Finally, as a main part of this paper, the current research on the swarm robotic algorithms are presented in detail, including cooperative control mechanisms in swarm robotics for flocking, navigating and searching applications.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LJQI5MII/1-s2.0-S221491471300024X-main.pdf}
}

@article{thompsonEmpiricalAnalysisPlurality,
  title = {Empirical {{Analysis}} of {{Plurality Election Equilibria}}},
  author = {Thompson, David R M and Lev, Omer},
  pages = {8},
  abstract = {Voting is widely used to aggregate the different preferences of agents, even though these agents are often able to manipulate the outcome through strategic voting. Most research on manipulation of voting methods studies (1) limited solution concepts, (2) limited preferences, or (3) scenarios with a few manipulators that have a common goal. In contrast, we study voting in plurality elections through the lens of Nash equilibrium, which allows for the possibility that any number of agents, with arbitrary different goals, could all be manipulators. This is possible thanks to recent advances in (Bayes-)Nash equilibrium computation for large games. Although plurality has numerous pure-strategy Nash equilibria, we demonstrate how a simple equilibrium refinement\textemdash assuming that agents only deviate from truthfulness when it will change the outcome\textemdash dramatically reduces this set. We also use symmetric Bayes-Nash equilibria to investigate the case where voters are uncertain of each others' preferences. This refinement does not completely eliminate the problem of multiple equilibria. However, it does show that even when agents manipulate, plurality still tends to lead to good outcomes (e.g., Condorcet winners, candidates that would win if voters were truthful, outcomes with high social welfare).},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P8AKHDVQ/p391.pdf}
}

@article{thompsonValuationUncertaintyImperfect,
  title = {Valuation {{Uncertainty}} and {{Imperfect Introspection}} in {{Second-Price Auctions}}},
  author = {Thompson, David R M},
  pages = {6},
  abstract = {In auction theory, agents are typically presumed to have perfect knowledge of their valuations. In practice, though, they may face barriers to this knowledge due to transaction costs or bounded rationality. Modeling and analyzing such settings has been the focus of much recent work, though a canonical model of such domains has not yet emerged. We begin by proposing a taxonomy of auction models with valuation uncertainty and showing how it categorizes previous work. We then restrict ourselves to single-good sealed-bid auctions, in which agents have (uncertain) independent private values and can introspect about their own (but not others') valuations through possibly costly and imperfect queries. We investigate second-price auctions, performing equilibrium analysis for cases with both discrete and continuous valuation distributions. We identify cases where every equilibrium involves either randomized or asymmetric introspection. We contrast the revenue properties of different equilibria, discuss steps the seller can take to improve revenue, and identify a form of revenue equivalence across mechanisms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ZI9FZGUC/AAAI07-022.pdf}
}

@article{thomson2020bc,
  title = {Spatial Conservation Action Planning in Heterogeneous Landscapes},
  author = {Thomson, Jim and Regan, Tracey J. and Hollings, Tracey and Amos, Nevil and Geary, William L. and Parkes, David and Hauser, Cindy E. and White, Matthew},
  year = {2020},
  month = oct,
  journal = {Biological Conservation},
  volume = {250},
  pages = {108735},
  issn = {00063207},
  doi = {10.1016/j.biocon.2020.108735},
  abstract = {A key challenge in conservation is the efficient allocation of limited resources to maximise benefits for biodi\- versity. Decision-support tools that account for landscape heterogeneity are needed to identify spatially-explicit actions that will achieve the greatest biodiversity benefits with available resources. We developed a raster-based, landscape-scale, spatial conservation action planning tool (SCAP) that offers significant advances for prioritising local and regional scale conservation actions in heterogenous landscapes. The SCAP tool was developed for the state of Victoria, Australia, to integrate heterogeneity of landscapes, species distributions, threats, and man\- agement costs and benefits across the state. We used empirical data to derive current and pre-European set\- tlement distributions for 4400 native terrestrial species, and developed spatially explicit models of 19 threats to biodiversity. We coupled structured expert-elicitation techniques with machine learning to map the expected benefits to species, and the implementation costs, of 17 management actions \textendash{} alone and in combination. We then ranked location-specific actions by their cost-effective contribution to an overall objective of minimizing the risk of species loss in Victoria over the next 50 years, using a modified implementation of the Zonation conservation planning framework. The SCAP tool provides decision makers with a transparent decision-support tool for identifying the cost-effective management actions at scales relevant to management.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WVE28TFB/thomson regan ... white 2020 - spatial conservation action planning in heterogeneous landscapes - GUPPY - BDPG - ARI - RESERVE SELECTION - UNCERTAINTY.pdf}
}

@article{thorntonAutoWEKACombinedSelection2013,
  title = {Auto-{{WEKA}}: {{Combined Selection}} and {{Hyperparameter Optimization}} of {{Classification Algorithms}}},
  shorttitle = {Auto-{{WEKA}}},
  author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2013},
  month = mar,
  journal = {arXiv:1208.3719 [cs]},
  eprint = {1208.3719},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/NJUKHPT9/1208.3719.pdf}
}

@article{tierneyRealisticGuideMaking2020,
  title = {A {{Realistic Guide}} to {{Making Data Available Alongside Code}} to {{Improve Reproducibility}}},
  author = {Tierney, Nicholas J. and Ram, Karthik},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.11626 [cs]},
  eprint = {2002.11626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data makes science possible. Sharing data improves visibility, and makes the research process transparent. This increases trust in the work, and allows for independent reproduction of results. However, a large proportion of data from published research is often only available to the original authors. Despite the obvious benefits of sharing data, and scientists' advocating for the importance of sharing data, most advice on sharing data discusses its broader benefits, rather than the practical considerations of sharing. This paper provides practical, actionable advice on how to actually share data alongside research. The key message is sharing data falls on a continuum, and entering it should come with minimal barriers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries},
  file = {/Users/bill/D/Zotero/storage/QJC66JG4/tierney ram 2020 - A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility - REPRODUCIBILITY - BDPG - ANNO.pdf}
}

@article{tinkamhoComplexityMeasuresSupervised2002,
  title = {Complexity Measures of Supervised Classification Problems},
  author = {{Tin Kam Ho} and Basu, M.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {3},
  pages = {289--300},
  issn = {01628828},
  doi = {10.1109/34.990132},
  abstract = {\DH We studied a number of measures that characterize the difficulty of a classification problem, focusing on the geometrical complexity of the class boundary. We compared a set of real-world problems to random labelings of points and found that real problems contain structures in this measurement space that are significantly different from the random sets. Distributions of problems in this space show that there exist at least two independent factors affecting a problem's difficulty. We suggest using this space to describe a classifier's domain of competence. This can guide static and dynamic selection of classifiers for specific problems as well as subproblems formed by confinement, projection, and transformations of the feature vectors.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VP3BMADZ/ho basu 2002 - complexity measures of supervised classification problems - PROBLEM DIFFICULTY.pdf}
}

@article{toonenIfLarvaeWere2007,
  title = {If Larvae Were Smart: A Simple Model for Optimal Settlement Behavior of Competent Larvae},
  shorttitle = {If Larvae Were Smart},
  author = {Toonen, Rj and Tyre, Aj},
  year = {2007},
  month = nov,
  journal = {Marine Ecology Progress Series},
  volume = {349},
  pages = {43--61},
  issn = {0171-8630, 1616-1599},
  doi = {10.3354/meps06963},
  abstract = {Much research has been done on larval settlement cues. Rather than having simple fixed responses to constant environmental stimuli, it seems likely that settlement decisions made by individual larvae should vary depending on the individual and the conditions under which it encounters that cue. Here, we present a simple stochastic dynamic programming model that explores the conditions under which larvae may maximize their lifetime fitness by accepting lower quality habitat rather than continuing to search for superior habitat. Our model predicts that there is a relatively narrow range of parameter values over which larval selectivity among habitat types changes dramatically from 1 (larvae accept only optimal substrata) to 0 (indiscriminant settlement). This narrow range coincides with our best estimate of parameter values gleaned from empirical studies, and the model output matches data for the polychaete worm Hydroides dianthus remarkably well. The relative availability of habitats and the total time available to search for high quality habitat (i.e. the ability to delay metamorphosis) had the greatest effects on larval selectivity. In contrast, intuitive factors, including larval energetics and mortality, showed little effect on larval habitat preference, but could still alter the proportion of larvae settling in different habitats by reducing search time. Our model predicts that a given larva may behave differently depending on where it falls in the optimality decision matrix at the instant in which it locates substrata. This model provides a conceptual framework in which to conduct future studies involving variability in settlement decisions among individual larvae, and in which to consider the selective forces driving the evolution of specific larval settlement cues. Our results suggest that a combination of the maximum search period and the relative frequency and quality of optimal habitat likely exert the greatest influence on the evolution of larval selectivity in the field.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7S9CG6C8/Toonen and Tyre - 2007 - If larvae were smart a simple model for optimal s.pdf}
}

@article{toonenIfLarvaeWere2007a,
  title = {If Larvae Were Smart: A Simple Model for Optimal Settlement Behavior of Competent Larvae},
  shorttitle = {If Larvae Were Smart},
  author = {Toonen, Rj and Tyre, Aj},
  year = {2007},
  month = nov,
  journal = {Marine Ecology Progress Series},
  volume = {349},
  pages = {43--61},
  issn = {0171-8630, 1616-1599},
  doi = {10.3354/meps06963},
  abstract = {Much research has been done on larval settlement cues. Rather than having simple fixed responses to constant environmental stimuli, it seems likely that settlement decisions made by individual larvae should vary depending on the individual and the conditions under which it encounters that cue. Here, we present a simple stochastic dynamic programming model that explores the conditions under which larvae may maximize their lifetime fitness by accepting lower quality habitat rather than continuing to search for superior habitat. Our model predicts that there is a relatively narrow range of parameter values over which larval selectivity among habitat types changes dramatically from 1 (larvae accept only optimal substrata) to 0 (indiscriminant settlement). This narrow range coincides with our best estimate of parameter values gleaned from empirical studies, and the model output matches data for the polychaete worm Hydroides dianthus remarkably well. The relative availability of habitats and the total time available to search for high quality habitat (i.e. the ability to delay metamorphosis) had the greatest effects on larval selectivity. In contrast, intuitive factors, including larval energetics and mortality, showed little effect on larval habitat preference, but could still alter the proportion of larvae settling in different habitats by reducing search time. Our model predicts that a given larva may behave differently depending on where it falls in the optimality decision matrix at the instant in which it locates substrata. This model provides a conceptual framework in which to conduct future studies involving variability in settlement decisions among individual larvae, and in which to consider the selective forces driving the evolution of specific larval settlement cues. Our results suggest that a combination of the maximum search period and the relative frequency and quality of optimal habitat likely exert the greatest influence on the evolution of larval selectivity in the field.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CGQQ2YUT/m349p043.pdf}
}

@article{troupinConservationPlanningUncertainty2018,
  title = {Conservation Planning under Uncertainty in Urban Development and Vegetation Dynamics},
  author = {Troupin, David and Carmel, Yohay},
  editor = {Zipp, Katherine},
  year = {2018},
  month = apr,
  journal = {PLOS ONE},
  volume = {13},
  number = {4},
  pages = {e0195429},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0195429},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8Z7NY39M/248Lechner.doc;/Users/bill/D/Zotero/storage/944RSCEZ/file.pdf}
}

@book{tsang1993,
  title = {Foundations of Constraint Satisfaction},
  author = {Tsang, Edward},
  year = {1993},
  series = {Computation in Cognitive Science},
  publisher = {{Academic Press}},
  address = {{London}},
  isbn = {978-0-12-701610-8},
  langid = {english},
  annotation = {OCLC: 636781070},
  file = {/Users/bill/D/Zotero/storage/UD9UPBAM/tsang 1993 - Foundations of Constraint Satisfaction - CSP - BDPG_.pdf}
}

@article{tullerDiscoveringLocalPatterns2010,
  title = {Discovering Local Patterns of Co - Evolution: Computational Aspects and Biological Examples},
  shorttitle = {Discovering Local Patterns of Co - Evolution},
  author = {Tuller, Tamir and Felder, Yifat and Kupiec, Martin},
  year = {2010},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {43},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-43},
  abstract = {Background: Co-evolution is the process in which two (or more) sets of orthologs exhibit a similar or correlative pattern of evolution. Co-evolution is a powerful way to learn about the functional interdependencies between sets of genes and cellular functions and to predict physical interactions. More generally, it can be used for answering fundamental questions about the evolution of biological systems. Orthologs that exhibit a strong signal of co-evolution in a certain part of the evolutionary tree may show a mild signal of co-evolution in other branches of the tree. The major reasons for this phenomenon are noise in the biological input, genes that gain or lose functions, and the fact that some measures of co-evolution relate to rare events such as positive selection. Previous publications in the field dealt with the problem of finding sets of genes that co-evolved along an entire underlying phylogenetic tree, without considering the fact that often co-evolution is local. Results: In this work, we describe a new set of biological problems that are related to finding patterns of local coevolution. We discuss their computational complexity and design algorithms for solving them. These algorithms outperform other bi-clustering methods as they are designed specifically for solving the set of problems mentioned above. We use our approach to trace the co-evolution of fungal, eukaryotic, and mammalian genes at high resolution across the different parts of the corresponding phylogenetic trees. Specifically, we discover regions in the fungi tree that are enriched with positive evolution. We show that metabolic genes exhibit a remarkable level of co-evolution and different patterns of co-evolution in various biological datasets. In addition, we find that protein complexes that are related to gene expression exhibit non-homogenous levels of co-evolution across different parts of the fungi evolutionary line. In the case of mammalian evolution, signaling pathways that are related to neurotransmission exhibit a relatively higher level of co-evolution along the primate subtree. Conclusions: We show that finding local patterns of co-evolution is a computationally challenging task and we offer novel algorithms that allow us to solve this problem, thus opening a new approach for analyzing the evolution of biological systems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CQEU26YC/1471-2105-11-43.pdf}
}

@article{tullochConservationPlannersTend2016,
  title = {Conservation Planners Tend to Ignore Improved Accuracy of Modelled Species Distributions to Focus on Multiple Threats and Ecological Processes},
  author = {Tulloch, Ayesha I.T. and Sutcliffe, Patricia and {Naujokaitis-Lewis}, Ilona and Tingley, Reid and Brotons, Lluis and Ferraz, Katia Maria P.M.B. and Possingham, Hugh and Guisan, Antoine and Rhodes, Jonathan R.},
  year = {2016},
  month = jul,
  journal = {Biological Conservation},
  volume = {199},
  pages = {157--171},
  issn = {00063207},
  doi = {10.1016/j.biocon.2016.04.023},
  abstract = {Limited conservation resources mean that management decisions are often made on the basis of scarce biological information. Species distribution models (SDMs) are increasingly proposed as a way to improve the representation of biodiversity features in conservation planning, but the extent to which SDMs are used in conservation planning is unclear. We reviewed the peer-reviewed and grey conservation planning literature to explore if and how SDMs are used in conservation prioritisations. We use text mining to analyse 641 peer-reviewed conservation prioritisation articles published between 2006 and 2012 and find that only 10\% of articles specifically mention SDMs in the abstract, title, and/or keywords. We use topic modelling of all peer-reviewed articles plus a detailed review of a random sample of 40 peer-reviewed and grey literature plans to evaluate factors that might influence whether decision-makers use SDMs to inform prioritisations. Our results reveal that habitat maps, expert-elicited species distributions, or metrics representing landscape processes (e.g. connectivity surfaces) are used more often than SDMs as biodiversity surrogates in prioritisations. We find four main reasons for using such alternatives in place of SDMs: (i) insufficient species occurrence data (particularly for threatened species); (ii) lack of biologically-meaningful predictor data relevant to the spatial scale of planning; (iii) low concern about uncertainty in biodiversity data; and (iv) a focus on accounting for ecological, evolutionary, and cumulative threatening processes that requires alternative data to be collected. Our results suggest that SDMs are perceived as best-suited to dealing with traditional reserve selection objectives and accounting for uncertainties such as future climate change or mapping accuracy. The majority of planners in both the grey and peer-reviewed literature appear to trade off the benefits of using SDMs for the benefits of including information on multiple threats and processes. We suggest that increasing the complexity of species distribution modelling methods might have little impact on their use in conservation planning without a corresponding increase in research aiming at better incorporation of a range of ecological, evolutionary, and threatening processes.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V2VKXFJN/tulloch et al 2016 - Conservation planners tend to ignore improved accuracy of modelled species distributions to focus on multiple threats and ecological processes - GUPPY - BDPG - RESERVE SELECTION - TOPIC M.pdf}
}

@article{tullochEffectRiskAversion2015,
  title = {Effect of Risk Aversion on Prioritizing Conservation Projects: {{Risk-Averse Species Prioritization}}},
  shorttitle = {Effect of Risk Aversion on Prioritizing Conservation Projects},
  author = {Tulloch, Ayesha I.T. and Maloney, Richard F. and Joseph, Liana N. and Bennett, Joseph R. and Di Fonzo, Martina M.I. and Probert, William J.M. and O'Connor, Shaun M. and Densem, Jodie P. and Possingham, Hugh P.},
  year = {2015},
  month = apr,
  journal = {Conservation Biology},
  volume = {29},
  number = {2},
  pages = {513--524},
  issn = {08888892},
  doi = {10.1111/cobi.12386},
  abstract = {Conservation outcomes are uncertain. Agencies making decisions about what threat mitigation actions to take to save which species frequently face the dilemma of whether to invest in actions with high probability of success and guaranteed benefits or to choose projects with a greater risk of failure that might provide higher benefits if they succeed. The answer to this dilemma lies in the decision maker's aversion to risk\textemdash their unwillingness to accept uncertain outcomes. Little guidance exists on how risk preferences affect conservation investment priorities. Using a prioritization approach based on cost effectiveness, we compared 2 approaches: a conservative probability threshold approach that excludes investment in projects with a risk of management failure greater than a fixed level, and a variance-discounting heuristic used in economics that explicitly accounts for risk tolerance and the probabilities of management success and failure. We applied both approaches to prioritizing projects for 700 of New Zealand's threatened species across 8303 management actions. Both decision makers' risk tolerance and our choice of approach to dealing with risk preferences drove the prioritization solution (i.e., the species selected for management). Use of a probability threshold minimized uncertainty, but more expensive projects were selected than with variance discounting, which maximized expected benefits by selecting the management of species with higher extinction risk and higher conservation value. Explicitly incorporating risk preferences within the decision making process reduced the number of species expected to be safe from extinction because lower risk tolerance resulted in more species being excluded from management, but the approach allowed decision makers to choose a level of acceptable risk that fit with their ability to accommodate failure. We argue for transparency in risk tolerance and recommend that decision makers accept risk in an adaptive management framework to maximize benefits and avoid potential extinctions due to inefficient allocation of limited resources.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8RA6JRSL/tulloch et al 2014 - Effect of risk aversion on prioritizing conservation projects - BDPG - RESERVE SELECTION - UNCERTAINTY - RISK.pdf}
}

@article{tullochIncorporatingUncertaintyAssociated2013,
  title = {Incorporating Uncertainty Associated with Habitat Data in Marine Reserve Design},
  author = {Tulloch, Vivitskaia J. and Possingham, Hugh P. and Jupiter, Stacy D. and Roelfsema, Chris and Tulloch, Ayesha I.T. and Klein, Carissa J.},
  year = {2013},
  month = jun,
  journal = {Biological Conservation},
  volume = {162},
  pages = {41--51},
  issn = {00063207},
  doi = {10.1016/j.biocon.2013.03.003},
  abstract = {One of the most pervasive forms of uncertainty in data used to make conservation decisions is error associated with mapping of conservation features. Whilst conservation planners should consider uncertainty associated with ecological data to make informed decisions, mapping error is rarely, if ever, accommodated in the planning process. Here, we develop a spatial conservation prioritization approach that accounts for the uncertainty inherent in coral reef habitat maps and apply it in the Kubulau District fisheries management area, Fiji. We use accuracy information describing the probability of occurrence of each habitat type, derived from remote sensing data validated by field surveys, to design a marine reserve network that has a high probability of protecting a fixed percentage (10\textendash 90\%) of every habitat type. We compare the outcomes of our approach to those of standard reserve design approaches, where habitatmapping errors are not known or ignored. We show that the locations of priority areas change between the standard and probabilistic approaches, with errors of omission and commission likely to occur if reserve design does not accommodate mapping accuracy. Although consideration of habitat mapping accuracy leads to bigger reserve networks, they are unlikely to miss habitat conservation targets. We explore the trade-off between conservation feature representation and reserve network area, with smaller reserve networks possible if we give up on trying to meet targets for habitats mapped with a low accuracy. The approach can be used with any habitat type at any scale to inform more robust and defensible conservation decisions in marine or terrestrial environments.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/35K7BKXA/tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - ANNO.pdf}
}

@article{tullochIncorporatingUncertaintyAssociated2013a,
  title = {Incorporating Uncertainty Associated with Habitat Data in Marine Reserve Design},
  author = {Tulloch, Vivitskaia J. and Possingham, Hugh P. and Jupiter, Stacy D. and Roelfsema, Chris and Tulloch, Ayesha I.T. and Klein, Carissa J.},
  year = {2013},
  month = jun,
  journal = {Biological Conservation},
  volume = {162},
  pages = {41--51},
  issn = {00063207},
  doi = {10.1016/j.biocon.2013.03.003},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BP4VWNU3/tulloch et al 2013 - Incorporating uncertainty associated with habitat data in marine reserve design - GUPPY - BDPG - RESERVE SELECTION - UNCERTAINTY - APPENDIX.pdf}
}

@article{tulowieckiUsingVegetationData2014,
  title = {Using Vegetation Data within Presettlement Land Survey Records for Species Distribution Modeling: {{A}} Tale of Two Datasets},
  shorttitle = {Using Vegetation Data within Presettlement Land Survey Records for Species Distribution Modeling},
  author = {Tulowiecki, Stephen J.},
  year = {2014},
  month = nov,
  journal = {Ecological Modelling},
  volume = {291},
  pages = {109--120},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2014.07.025},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NVJJ4QZ8/1-s2.0-S0304380014003627-main.pdf}
}

@article{tyreIdentifyingLandscapeScale2006,
  title = {Identifying Landscape Scale Patterns from Individual Scale Processes},
  author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Michael Bull, C.},
  year = {2006},
  month = dec,
  journal = {Ecological Modelling},
  volume = {199},
  number = {4},
  pages = {442--450},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2005.12.001},
  abstract = {Extrapolating across scales is a critical problem in ecology. Explicit mechanistic models of ecological systems provide a bridge from measurements of processes at small and short scales to larger scales; spatial patterns at large scales can be used to test the outcomes of these models. However, it is necessary to identify patterns that are not dependent on initial conditions, because small scale initial conditions will not normally be measured at large scales. We examined one possible pattern that could meet these conditions, the relationship between mean and variance in abundance of a parasitic tick in an individual based model of a lizard tick interaction. We scaled discrepancies between the observed and simulated patterns with a transformation of the variance\textendash covariance matrix of the observed pattern to objectively identify patterns that are ``close''.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/HB4NDJF8/Tyre et al. - 2006 - Identifying landscape scale patterns from individu.pdf}
}

@article{tyreIdentifyingLandscapeScale2006a,
  title = {Identifying Landscape Scale Patterns from Individual Scale Processes},
  author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Michael Bull, C.},
  year = {2006},
  month = dec,
  journal = {Ecological Modelling},
  volume = {199},
  number = {4},
  pages = {442--450},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2005.12.001},
  abstract = {Extrapolating across scales is a critical problem in ecology. Explicit mechanistic models of ecological systems provide a bridge from measurements of processes at small and short scales to larger scales; spatial patterns at large scales can be used to test the outcomes of these models. However, it is necessary to identify patterns that are not dependent on initial conditions, because small scale initial conditions will not normally be measured at large scales. We examined one possible pattern that could meet these conditions, the relationship between mean and variance in abundance of a parasitic tick in an individual based model of a lizard tick interaction. We scaled discrepancies between the observed and simulated patterns with a transformation of the variance\textendash covariance matrix of the observed pattern to objectively identify patterns that are ``close''.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Y5K2LKVF/Identifying landscape scale patterns from individual scale proces.pdf}
}

@article{tyreIdentifyingMechanisticModels2007,
  title = {Identifying Mechanistic Models of Spatial Behaviour Using Pattern-Based Modelling: {{An}} Example from Lizard Home Ranges},
  shorttitle = {Identifying Mechanistic Models of Spatial Behaviour Using Pattern-Based Modelling},
  author = {Tyre, Andrew and Kerr, Gregory D. and Tenhumberg, Brigitte and Bull, C. Michael},
  year = {2007},
  month = nov,
  journal = {Ecological Modelling},
  volume = {208},
  number = {2-4},
  pages = {307--316},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2007.06.004},
  abstract = {Landscape and population level patterns form through the aggregation of responses of individual organisms to heterogeneity. Spatial organization within a population can range from random overlap of individual home ranges, to completely exclusive territories, with most populations falling somewhere between these two extremes. A fundamental question in behavioral ecology concerns the factors that influence the degree of spatial overlap of home ranges, and the processes that determine how likely it is that an individual will access resources over its home range. However, traditional experimental methods are not always practical or possible. Pattern-based modeling is an alternative, non-intrusive technique for explaining observed patterns. We explored behavioral mechanisms for home range overlap in a Scincid lizard, Tiliqua rugosa, by constructing a spatially explicit individual based model. We tested two mechanisms, one that used refuge sites randomly and one that included a behavioral component. The random use model, the fixed total range model, incorporated all refuge sites within a circle of radius h. The behavioral model, the variable total range model, probabilistically incorporated refuge sites based on nearest neighbor distances and use by conspecifics. Comparisons between the simulated patterns and the observed patterns of range overlap provided evidence that the variable total range model was a better approximation of lizard space use than the fixed total range model. Pattern-based modeling showed substantial promise as a means for identifying behavioral mechanisms underlying observed patterns.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/C3M2G4GB/Tyre et al. - 2007 - Identifying mechanistic models of spatial behaviou.pdf}
}

@article{tyreIdentifyingMechanisticModels2007a,
  title = {Identifying Mechanistic Models of Spatial Behaviour Using Pattern-Based Modelling: {{An}} Example from Lizard Home Ranges},
  shorttitle = {Identifying Mechanistic Models of Spatial Behaviour Using Pattern-Based Modelling},
  author = {Tyre, Andrew and Kerr, Gregory D. and Tenhumberg, Brigitte and Bull, C. Michael},
  year = {2007},
  month = nov,
  journal = {Ecological Modelling},
  volume = {208},
  number = {2-4},
  pages = {307--316},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2007.06.004},
  abstract = {Landscape and population level patterns form through the aggregation of responses of individual organisms to heterogeneity. Spatial organization within a population can range from random overlap of individual home ranges, to completely exclusive territories, with most populations falling somewhere between these two extremes. A fundamental question in behavioral ecology concerns the factors that influence the degree of spatial overlap of home ranges, and the processes that determine how likely it is that an individual will access resources over its home range. However, traditional experimental methods are not always practical or possible. Pattern-based modeling is an alternative, non-intrusive technique for explaining observed patterns. We explored behavioral mechanisms for home range overlap in a Scincid lizard, Tiliqua rugosa, by constructing a spatially explicit individual based model. We tested two mechanisms, one that used refuge sites randomly and one that included a behavioral component. The random use model, the fixed total range model, incorporated all refuge sites within a circle of radius h. The behavioral model, the variable total range model, probabilistically incorporated refuge sites based on nearest neighbor distances and use by conspecifics. Comparisons between the simulated patterns and the observed patterns of range overlap provided evidence that the variable total range model was a better approximation of lizard space use than the fixed total range model. Pattern-based modeling showed substantial promise as a means for identifying behavioral mechanisms underlying observed patterns.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/R5BXNR5G/Identifying mechanistic models of spatial behavior using pattern-.pdf}
}

@article{tyreIMPROVINGPRECISIONREDUCING2003,
  title = {{{IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS}}: {{ESTIMATING FALSE-NEGATIVE ERROR RATES}}},
  shorttitle = {{{IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS}}},
  author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Field, Scott A. and Niejalke, Darren and Parris, Kirsten and Possingham, Hugh P.},
  year = {2003},
  month = dec,
  journal = {Ecological Applications},
  volume = {13},
  number = {6},
  pages = {1790--1801},
  issn = {1051-0761},
  doi = {10.1890/02-5078},
  abstract = {The use of presence/absence data in wildlife management and biological surveys is widespread. There is a growing interest in quantifying the sources of error associated with these data. We show that false-negative errors (failure to record a species when in fact it is present) can have a significant impact on statistical estimation of habitat models using simulated data. Then we introduce an extension of logistic modeling, the zero-inflated binomial (ZIB) model that permits the estimation of the rate of false-negative errors and the correction of estimates of the probability of occurrence for false-negative errors by using repeated visits to the same site. Our simulations show that even relatively low rates of false negatives bias statistical estimates of habitat effects. The method with three repeated visits eliminates the bias, but estimates are relatively imprecise. Six repeated visits improve precision of estimates to levels comparable to that achieved with conventional statistics in the absence of false-negative errors. In general, when error rates are Õ50\% greater efficiency is gained by adding more sites, whereas when error rates are Ï¾50\% it is better to increase the number of repeated visits. We highlight the flexibility of the method with three case studies, clearly demonstrating the effect of false-negative errors for a range of commonly used survey methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IWSTYW2D/Tyre et al. - 2003 - IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICA.pdf}
}

@article{tyreIMPROVINGPRECISIONREDUCING2003a,
  title = {{{IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS}}: {{ESTIMATING FALSE-NEGATIVE ERROR RATES}}},
  shorttitle = {{{IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS}}},
  author = {Tyre, Andrew J. and Tenhumberg, Brigitte and Field, Scott A. and Niejalke, Darren and Parris, Kirsten and Possingham, Hugh P.},
  year = {2003},
  month = dec,
  journal = {Ecological Applications},
  volume = {13},
  number = {6},
  pages = {1790--1801},
  issn = {1051-0761},
  doi = {10.1890/02-5078},
  abstract = {The use of presence/absence data in wildlife management and biological surveys is widespread. There is a growing interest in quantifying the sources of error associated with these data. We show that false-negative errors (failure to record a species when in fact it is present) can have a significant impact on statistical estimation of habitat models using simulated data. Then we introduce an extension of logistic modeling, the zero-inflated binomial (ZIB) model that permits the estimation of the rate of false-negative errors and the correction of estimates of the probability of occurrence for false-negative errors by using repeated visits to the same site. Our simulations show that even relatively low rates of false negatives bias statistical estimates of habitat effects. The method with three repeated visits eliminates the bias, but estimates are relatively imprecise. Six repeated visits improve precision of estimates to levels comparable to that achieved with conventional statistics in the absence of false-negative errors. In general, when error rates are Õ50\% greater efficiency is gained by adding more sites, whereas when error rates are Ï¾50\% it is better to increase the number of repeated visits. We highlight the flexibility of the method with three case studies, clearly demonstrating the effect of false-negative errors for a range of commonly used survey methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/CYVYIBNS/IMPROVING PRECISION AND REDUCING BIAS IN BIOLOGICAL SURVEYS_ ESTI.pdf}
}

@article{tyreINFERRINGPROCESSPATTERN2001,
  title = {{{INFERRING PROCESS FROM PATTERN}}: {{CAN TERRITORY OCCUPANCY PROVIDE INFORMATION ABOUT LIFE HISTORY PARAMETERS}}?},
  author = {Tyre, Andrew J and Possingham, Hugh P and Lindenmayer, David B},
  year = {2001},
  journal = {Ecological Applications},
  volume = {11},
  number = {6},
  pages = {16},
  abstract = {A significant problem in wildlife management is identifying ``good'' habitat for species within the short time frames demanded by policy makers. Statistical models of the response of species presence/absence to predictor variables are one solution, widely known as habitat modeling. We use a ``virtual ecologist'' to test logistic regression as a means of developing habitat models within a spatially explicit, individual-based simulation that allows habitat quality to influence either fecundity or survival with a continuous scale. The basic question is how good are logistic regression models of habitat quality at identifying habitat where birth rates are high and death rates low (i.e., ``source'' habitat)? We find that, even when all the important variables are perfectly measured, and there is no error in surveying the species of interest, demographic stochasticity and the limiting effect of localized dispersal generally prevent an explanation of much more than half of the variation in territory occupancy as a function of habitat quality. This is true regardless of whether fecundity or survival is influenced by habitat quality. In addition, habitat models only detect a significant effect of habitat on territory occupancy when habitat quality is spatially autocorrelated. We find that habitat models based on logistic regression really measure the ability of the species to reach and colonize areas, not birth or death rates.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SA4S74X8/tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf}
}

@article{tyreINFERRINGPROCESSPATTERN2001a,
  title = {{{INFERRING PROCESS FROM PATTERN}}: {{CAN TERRITORY OCCUPANCY PROVIDE INFORMATION ABOUT LIFE HISTORY PARAMETERS}}?},
  author = {Tyre, Andrew J and Possingham, Hugh P and Lindenmayer, David B},
  year = {2001},
  journal = {Ecological Applications},
  volume = {11},
  number = {6},
  pages = {16},
  abstract = {A significant problem in wildlife management is identifying ``good'' habitat for species within the short time frames demanded by policy makers. Statistical models of the response of species presence/absence to predictor variables are one solution, widely known as habitat modeling. We use a ``virtual ecologist'' to test logistic regression as a means of developing habitat models within a spatially explicit, individual-based simulation that allows habitat quality to influence either fecundity or survival with a continuous scale. The basic question is how good are logistic regression models of habitat quality at identifying habitat where birth rates are high and death rates low (i.e., ``source'' habitat)? We find that, even when all the important variables are perfectly measured, and there is no error in surveying the species of interest, demographic stochasticity and the limiting effect of localized dispersal generally prevent an explanation of much more than half of the variation in territory occupancy as a function of habitat quality. This is true regardless of whether fecundity or survival is influenced by habitat quality. In addition, habitat models only detect a significant effect of habitat on territory occupancy when habitat quality is spatially autocorrelated. We find that habitat models based on logistic regression really measure the ability of the species to reach and colonize areas, not birth or death rates.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IATG5RNT/tyre et al 2001 - Inferring process from pattern - can territory occupancy provide information about life history parameters - BDPG - GUPPY - SDM.pdf}
}

@article{universitatpolitecnicadevalenciaUniversitatPolitecnicaValencia2014,
  title = {Universitat {{Polit\`ecnica}} de {{Val\`encia}}},
  author = {{Universitat Polit{\`e}cnica de Val{\`e}ncia}, Editorial},
  year = {2014},
  month = sep,
  journal = {Ingenier\'ia del agua},
  volume = {18},
  number = {1},
  pages = {ix},
  issn = {1886-4996, 1134-2196},
  doi = {10.4995/ia.2014.3293},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BLQVXXEI/How to measure the difficulty of a Mixed-Linear Integer....pdf}
}

@incollection{vacherLearningEcologicalNetworks2016,
  title = {Learning {{Ecological Networks}} from {{Next-Generation Sequencing Data}}},
  booktitle = {Advances in {{Ecological Research}}},
  author = {Vacher, Corinne and {Tamaddoni-Nezhad}, Alireza and Kamenova, Stefaniya and Peyrard, Nathalie and Moalic, Yann and Sabbadin, R{\'e}gis and Schwaller, Lo{\"i}c and Chiquet, Julien and Smith, M. Alex and Vallance, Jessica and Fievet, Virgil and Jakuschkin, Boris and Bohan, David A.},
  year = {2016},
  volume = {54},
  pages = {1--39},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.aecr.2015.10.004},
  abstract = {Species diversity, and the various interactions that occur between species, supports ecosystems functioning and benefit human societies. Monitoring the response of species interactions to human alterations of the environment is thus crucial for preserving ecosystems. Ecological networks are now the standard method for representing and simultaneously analyzing all the interactions between species. However, deciphering such networks requires considerable time and resources to observe and sample the organisms, to identify them at the species level and to characterize their interactions. Next-generation sequencing (NGS) techniques, combined with network learning and modelling, can help alleviate these constraints. They are essential for observing cryptic interactions involving microbial species, as well as short-term interactions such as those between predator and prey. Here, we present three case studies, in which species associations or interactions have been revealed with NGS. We then review several currently available statistical and machine-learning approaches that could be used for reconstructing networks of direct interactions between species, based on the NGS co-occurrence data. Future developments of these methods may allow us to discover and monitor species interactions cost-effectively, under various environmental conditions and within a replicated experimental design framework.},
  isbn = {978-0-08-100978-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/AQGPU6HB/Vacher_AER_2016.pdf}
}

@article{valsanNatureCorporationTale2016,
  title = {The {{Nature}} of the {{Corporation}}: {{A Tale}} of {{Economic Complexity}}},
  shorttitle = {The {{Nature}} of the {{Corporation}}},
  author = {Valsan, Calin},
  year = {2016},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2710962},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4XMYZKDM/valsan 2016 - The Nature of the Corporation - A Tale of Economic Complexity - Proof062216 - BOOK - OPTISEVIL - ECONOMICS.PDF}
}

@article{vanderkamHeuristicAlgorithmsVs2007,
  title = {Heuristic Algorithms vs. Linear Programs for Designing Efficient Conservation Reserve Networks: {{Evaluation}} of Solution Optimality and Processing Time},
  shorttitle = {Heuristic Algorithms vs. Linear Programs for Designing Efficient Conservation Reserve Networks},
  author = {Vanderkam, Robert P.D. and Wiersma, Yolanda F. and King, Douglas J.},
  year = {2007},
  month = jul,
  journal = {Biological Conservation},
  volume = {137},
  number = {3},
  pages = {349--358},
  issn = {00063207},
  doi = {10.1016/j.biocon.2007.02.018},
  abstract = {Systematic approaches to efficient reserve network design often make use of one of two types of site selection algorithm; linear programs or heuristic algorithms. Unlike with linear programs, heuristic algorithms have been demonstrated to yield suboptimal networks in that more sites are selected in order to meet conservation goals than may be necessary or fewer features are captured than is possible. Although the degree of suboptimality is not known when using heuristics, some researchers have suggested that it is not significant in most cases and that heuristics are preferred since they are more flexible and can yield a solution more quickly. Using eight binary datasets, we demonstrate that suboptimality of numbers of sites selected and biodiversity features protected can occur to various degrees depending on the dataset, the model design, and the type of heuristic applied, and that processing time is not dramatically different between optimal and heuristic algorithms. In choosing an algorithm, the degree of suboptimality may not always be as important to planners as the perception that optimal solvers have feasibility issues, and therefore heuristic algorithms might continue to be a popular tool for conservation planning. We conclude that for many datasets, feasibility of optimal algorithms should not be a concern and that the value of heuristic results can be greatly improved by using optimal algorithms to determine the degree of suboptimality of the results.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3IHKQX4Z/vanderkam et al 2007 - heuristic algorithms vs linear programs for designing efficient conservation reserve networks - evaluation of solution optimality and processing time.pdf}
}

@article{vanhemertEvolvingCombinatorialProblem2006,
  title = {Evolving {{Combinatorial Problem Instances That Are Difficult}} to {{Solve}}},
  author = {{van Hemert}, Jano I.},
  year = {2006},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {14},
  number = {4},
  pages = {433--462},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco.2006.14.4.433},
  abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difficult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisfiability, and the travelling salesman problem. Problem instances acquired through this technique are more difficult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difficulty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/PN7Q6JNG/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf}
}

@article{vanhemertEvolvingCombinatorialProblem2006a,
  title = {Evolving {{Combinatorial Problem Instances That Are Difficult}} to {{Solve}}},
  author = {{van Hemert}, Jano I.},
  year = {2006},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {14},
  number = {4},
  pages = {433--462},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco.2006.14.4.433},
  abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difficult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisfiability, and the travelling salesman problem. Problem instances acquired through this technique are more difficult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difficulty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/58JHUFMR/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf}
}

@article{vanhemertEvolvingCombinatorialProblem2006b,
  title = {Evolving {{Combinatorial Problem Instances That Are Difficult}} to {{Solve}}},
  author = {{van Hemert}, Jano I.},
  year = {2006},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {14},
  number = {4},
  pages = {433--462},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco.2006.14.4.433},
  abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difficult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisfiability, and the travelling salesman problem. Problem instances acquired through this technique are more difficult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difficulty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/SS246XGB/EC-Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf}
}

@article{vanhemertEvolvingCombinatorialProblem2006c,
  title = {Evolving {{Combinatorial Problem Instances That Are Difficult}} to {{Solve}}},
  author = {{van Hemert}, Jano I.},
  year = {2006},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {14},
  number = {4},
  pages = {433--462},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco.2006.14.4.433},
  abstract = {In this paper we demonstrate how evolutionary computation can be used to acquire difficult to solve combinatorial problem instances, thereby stress-testing the corresponding algorithms used to solve these instances. The technique is applied in three important domains of combinatorial optimisation, binary constraint satisfaction, Boolean satisfiability, and the travelling salesman problem. Problem instances acquired through this technique are more difficult than ones found in popular benchmarks. We analyse these evolved instances with the aim to explain their difficulty in terms of structural properties, thereby exposing the weaknesses of corresponding algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V4RLYNBL/van hemert - Evolving_combinatorial_problem_instances_that_are_difficult_to_solve.pdf}
}

@article{vanrijnOnlinePerformanceEstimation2018,
  title = {The Online Performance Estimation Framework: Heterogeneous Ensemble Learning for Data Streams},
  shorttitle = {The Online Performance Estimation Framework},
  author = {{van Rijn}, Jan N. and Holmes, Geoffrey and Pfahringer, Bernhard and Vanschoren, Joaquin},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {149--176},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5686-9},
  abstract = {Ensembles of classifiers are among the best performing classifiers available in many data mining applications, including the mining of data streams. Rather than training one classifier, multiple classifiers are trained, and their predictions are combined according to a given voting schedule. An important prerequisite for ensembles to be successful is that the individual models are diverse. One way to vastly increase the diversity among the models is to build an heterogeneous ensemble, comprised of fundamentally different model types. However, most ensembles developed specifically for the dynamic data stream setting rely on only one type of base-level classifier, most often Hoeffding Trees. We study the use of heterogeneous ensembles for data streams. We introduce the Online Performance Estimation framework, which dynamically weights the votes of individual classifiers in an ensemble. Using an internal evaluation on recent training data, it measures how well ensemble members performed on this and dynamically updates their weights. Experiments over a wide range of data streams show performance that is competitive with state of the art ensemble techniques, including Online Bagging and Leveraging Bagging, while being significantly faster. All experimental results from this work are easily reproducible and publicly available online.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/7PFJELAA/Rijn2018_Article_TheOnlinePerformanceEstimation.pdf}
}

@article{vazeImpactAccuracyResolution2010,
  title = {Impact of {{DEM}} Accuracy and Resolution on Topographic Indices},
  author = {Vaze, Jai and Teng, Jin and Spencer, Georgina},
  year = {2010},
  month = oct,
  journal = {Environmental Modelling \& Software},
  volume = {25},
  number = {10},
  pages = {1086--1098},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2010.03.014},
  abstract = {Topography is an important land-surface characteristic that affects most aspects of the water balance in a catchment, including the generation of surface and sub-surface runoff; the flow paths followed by water as it moves down and through hillslopes and the rate of water movement. All of the spatially explicit fully distributed hydraulic and hydrological models use topography (represented by the DEM of the area modelled) to derive bathymetry. DEM is also used to derive some other key information critical in fully distributed hydraulic and hydrological models.},
  langid = {english},
  keywords = {DEM,koala,uncertainty},
  file = {/Users/bill/D/Zotero/storage/VLA2ZWNZ/vaze et al 2010 - impact of dem accuracy and resolution on topographic indices - envmodsoft.pdf}
}

@article{viscontiBuildingRobustConservation2015,
  title = {Building Robust Conservation Plans: {{Flexible}} and {{Efficient Conservation Planning}}},
  shorttitle = {Building Robust Conservation Plans},
  author = {Visconti, Piero and Joppa, Lucas},
  year = {2015},
  month = apr,
  journal = {Conservation Biology},
  volume = {29},
  number = {2},
  pages = {503--512},
  issn = {08888892},
  doi = {10.1111/cobi.12416},
  abstract = {Systematic conservation planning optimizes trade-offs between biodiversity conservation and human activities by accounting for socioeconomic costs while aiming to achieve prescribed conservation objectives. However, the most cost-efficient conservation plan can be very dissimilar to any other plan achieving the set of conservation objectives. This is problematic under conditions of implementation uncertainty (e.g., if all or part of the plan becomes unattainable). We determined through simulations of parallel implementation of conservation plans and habitat loss the conditions under which optimal plans have limited chances of implementation and where implementation attempts would fail to meet objectives. We then devised a new, flexible method for identifying conservation priorities and scheduling conservation actions. This method entails generating a number of alternative plans, calculating the similarity in site composition among all plans, and selecting the plan with the highest density of neighboring plans in similarity space. We compared our method with the classic method that maximizes cost efficiency with synthetic and real data sets. When implementation was uncertain\textemdash a common reality\textemdash our method provided higher likelihood of achieving conservation targets. We found that {$\chi$} , a measure of the shortfall in objectives achieved by a conservation plan if the plan could not be implemented entirely, was the main factor determining the relative performance of a flexibility enhanced approach to conservation prioritization. Our findings should help planning authorities prioritize conservation efforts in the face of uncertainty about future condition and availability of sites.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/WA3UQFHH/visconti joppa 2014 - Building robust conservation plans - BDPG - RESERVE SELECTION - METHOD - ERROR MODELS - ANNO.pdf}
}

@article{vitonNotesSpatialEconometric,
  title = {Notes on {{Spatial Econometric Models}}},
  author = {Viton, Philip A},
  pages = {23},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/5LHMUULU/spatial.pdf}
}

@article{wangHowLargeSpatiallyexplicit2018,
  title = {How Large Spatially-Explicit Optimal Reserve Design Models Can We Solve Now? {{An}} Exploration of Current Models' Computational Efficiency},
  shorttitle = {How Large Spatially-Explicit Optimal Reserve Design Models Can We Solve Now?},
  author = {Wang, Yicheng and {\"O}nal, Hayri and Fang, Qiaoling},
  year = {2018},
  month = jun,
  journal = {Nature Conservation},
  volume = {27},
  pages = {17--34},
  issn = {1314-3301, 1314-6947},
  doi = {10.3897/natureconservation.27.21642},
  abstract = {Spatially-explicit optimal reserve design models select best sites from a set of candidate sites to assemble nature reserves to protect species (or habitats) and these reserves display certain spatial attributes which are desirable for species. These models are formulated with linear 0\textendash 1 programming and solved using standard optimisation software, but they were run on different platforms, resulting in discrepant or even conflicting messages with regard to their computational efficiency. A fair and accurate comparison of the convenience of these models would be important for conservation planners who use these models. In this article, we considered eight models presented in literature and tested their computational efficiency using randomly generated data sets containing up to 2000 sites. We focused on reserve contiguity and compactness which are considered crucial to species persistence. Our results showed that two of these models, namely Williams (2002) and \"Onal et al. (2016), stand out as the most efficient models. We also found that the relative efficiency of these models depends on the scope of analysis. Specifically, the Williams (2002) model solves more of the test problems when contiguity is the only spatial attribute and a large subset of the candidate sites needs to be selected. When compactness is considered also, the \"Onal et al. (2016) model generally performs better. Large scale models are found to be difficult to solve in a reasonable period of time. We discussed factors that may affect those models' computational efficiency, including model size, share of selected sites, model structure and input data. These results provide useful insight and guidance to conservation practitioners and researchers who focus on spatial aspects and work with large-scale data sets.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2GC9PS2Q/wang onal fang 2018 - how large spatially-explicit optimal reserve design models can we solve now - an exploration of current models computational efficiency - BDPG - ANNO.pdf}
}

@article{warmanSensitivitySystematicReserve2004,
  title = {Sensitivity of {{Systematic Reserve Selection}} to {{Decisions}} about {{Scale}}, {{Biological Data}}, and {{Targets}}: {{Case Study}} from {{Southern British Columbia}}},
  shorttitle = {Sensitivity of {{Systematic Reserve Selection}} to {{Decisions}} about {{Scale}}, {{Biological Data}}, and {{Targets}}},
  author = {Warman, Leanna D. and Sinclair, A. R. E. and Scudder, G. G. E. and Klinkenberg, Brian and Pressey, Robert L.},
  year = {2004},
  month = jun,
  journal = {Conservation Biology},
  volume = {18},
  number = {3},
  pages = {655--666},
  issn = {0888-8892, 1523-1739},
  doi = {10.1111/j.1523-1739.2004.00538.x},
  abstract = {The identification of conservation areas based on systematic reserve-selection algorithms requires decisions related to both spatial and ecological scale. These decisions may affect the distribution and number of sites considered priorities for conservation within a region. We explored the sensitivity of systematic reserve selection by altering values of three essential variables. We used a 1:20,000\textendash scale terrestrial ecosystem map and habitat suitability data for 29 threatened vertebrate species in the Okanagan region of British Columbia, Canada. To these data we applied a reserve-selection algorithm to select conservation sites while altering selection unit size and shape, features of biodiversity (i.e., vertebrate species), and area conservation targets for each biodiversity feature. The spatial similarity, or percentage overlap, of selected sets of conservation sites identified (1) with different selection units was {$\leq$}40\%, (2) with different biodiversity features was 59\%, and (3) with different conservation targets was {$\geq$}94\%. Because any selected set of sites is only one of many possible sets, we also compared the conservation value (irreplaceability) of all sites in the region for each variation of the data. The correlations of irreplaceability were weak for different selection units (0.23 {$\leq$} r {$\leq$} 0.67), strong for different biodiversity features (r = 0.84), and mixed for different conservation targets (r = 0.16; 0.16; 1.00). Because of the low congruence of selected sites and weak correlations of irreplaceability for different selection units, recommendations from studies that have been applied at only one spatial scale must be considered cautiously.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GCGKCNTP/warman ... pressey 2004 - Sensitivity of Systematic Reserve Selection to Decisions about Scale, Biological Data, and Targets - Case Study from Southern British Columbia - BDPG - RESERVE SELECTION - UNCERTAINT.pdf}
}

@article{warrenEvaluatingPresenceOnly2020,
  title = {Evaluating Presence-only Species Distribution Models with Discrimination Accuracy Is Uninformative for Many Applications},
  author = {Warren, Dan L. and Matzke, Nicholas J. and Iglesias, Teresa L.},
  year = {2020},
  month = jan,
  journal = {Journal of Biogeography},
  volume = {47},
  number = {1},
  pages = {167--180},
  issn = {0305-0270, 1365-2699},
  doi = {10.1111/jbi.13705},
  abstract = {Aim: Species distribution models are used across evolution, ecology, conservation and epidemiology to make critical decisions and study biological phenomena, often in cases where experimental approaches are intractable. Choices regarding optimal models, methods and data are typically made based on discrimination accuracy: a model's ability to predict subsets of species occurrence data that were withheld during model construction. However, empirical applications of these models often involve making biological inferences based on continuous estimates of relative habitat suitability as a function of environmental predictor variables. We term the reliability of these biological inferences `functional accuracy.' We explore the link between discrimination accuracy and functional accuracy.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/G5ZNEMBP/warren et al 2019 - Evaluating presenceâonly species distribution models with discrimination accuracy is uninformative for many applications - BDPG - GUPPY.pdf}
}

@article{watanabeAverageCaseAnalysisMAX2SAT,
  title = {Average-{{Case Analysis}} for the {{MAX-2SAT Problem}}},
  author = {Watanabe, Osamu and Yamamoto, Masaki},
  pages = {6},
  abstract = {We propose a ``planted solution model'' for discussing the average-case complexity of the MAX-2SAT problem. We show that for a large range of parameters, the planted solution (more precisely, one of the planted solution pair) is the optimal solution for the generated instance with high probability. We then give a simple linear time algorithm based on a message passing method, and we prove that it solves the MAX-2SAT problem with high probability under our planted solution model.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/VY95BPTW/chp%3A10.1007%2F11814948_27.pdf}
}

@article{watlingPerformanceMetricsVariance2015,
  title = {Performance Metrics and Variance Partitioning Reveal Sources of Uncertainty in Species Distribution Models},
  author = {Watling, James I. and Brandt, Laura A. and Bucklin, David N. and Fujisaki, Ikuko and Mazzotti, Frank J. and Roma{\~n}ach, Stephanie S. and Speroterra, Carolina},
  year = {2015},
  month = aug,
  journal = {Ecological Modelling},
  volume = {309--310},
  pages = {48--59},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2015.03.017},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/3CA7RST8/watling et al 2015 - Performance metrics and variance partitioning reveal sources of uncertainty in species distribution models - SDM - GUPPY - BDPG - UNCERTAINTY.pdf}
}

@article{watsonFocusingIndividualWhy,
  title = {Focusing on the {{Individual}}: {{Why We Need New Empirical Methods}} for {{Characterizing Problem Difficulty}} - {{Position Paper}}},
  author = {Watson, Jean-Paul and Howe, Adele E and Collins, Fort},
  pages = {7},
  abstract = {A large number of empirical methods for characterizing problem difficulty have appeared in the last 15 or so years, including the work on phase transitions, fitness landscape correlation length, analysis of optima distributions, and algorithm run-time distributions. These methods have been successful either in predicting the difficulty of an ensemble of problem instances or providing descriptive characterizations of algorithm performance. However, they are of limited use in explaining and predicting the performance of algorithms on individual problem instances. We argue that the development of empirical methods for characterizing problem difficulty at the instance level is necessary for an advanced understanding of algorithm behavior. Further, the practical benefit is tremendous, enabling 1) the development of more comprehensive benchmarks, 2) problem-sensitive algorithm selection, and 3) intelligent tuning of problem-sensitive algorithm parameters.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NIF2KCTK/watson howe 2001 - focusing on the individual - why we need new empirical methods for characterizing problem difficulty - PROBLEM DIFFICULTY - BDPG.pdf}
}

@article{watts2009em&s,
  title = {Marxan with {{Zones}}: {{Software}} for Optimal Conservation Based Land- and Sea-Use Zoning},
  shorttitle = {Marxan with {{Zones}}},
  author = {Watts, Matthew E. and Ball, Ian R. and Stewart, Romola S. and Klein, Carissa J. and Wilson, Kerrie and Steinback, Charles and Lourival, Reinaldo and Kircher, Lindsay and Possingham, Hugh P.},
  year = {2009},
  month = dec,
  journal = {Environmental Modelling \& Software},
  volume = {24},
  number = {12},
  pages = {1513--1521},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2009.06.005},
  abstract = {Marxan is the most widely used conservation planning software in the world and is designed for solving complex conservation planning problems in landscapes and seascapes. In this paper we describe a substantial extension of Marxan called Marxan with Zones, a decision support tool that provides landuse zoning options in geographical regions for biodiversity conservation. We describe new functions designed to enhance the original Marxan software and expand on its utility as a decision support tool. The major new element in the decision problem is allowing any parcel of land or sea to be allocated to a specific zone, not just reserved or unreserved. Each zone then has the option of its own actions, objectives and constraints, with the flexibility to define the contribution of each zone to achieve targets for pre-specified features (e.g. species or habitats). The objective is to minimize the total cost of implementing the zoning plan while ensuring a variety of conservation and land-use objectives are achieved. We outline the capabilities, limitations and additional data requirements of this new software and perform a comparison with the original version of Marxan. We feature a number of case studies to demonstrate the functionality of the software and highlight its flexibility to address a range of complex spatial planning problems. These studies demonstrate the design of multiple-use marine parks in both Western Australia and California, and the zoning of forest use in East Kalimantan.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BFM7GDQ6/Watts et al. - 2009 - Marxan with Zones Software for optimal conservation based land- and sea-use zoning - Environmental Modelling & Software.PDF}
}

@article{weiIntegratedApproachAddressing2012,
  title = {An Integrated Approach for Addressing Geographic Uncertainty in Spatial Optimization},
  author = {Wei, Ran and Murray, Alan T.},
  year = {2012},
  month = jul,
  journal = {International Journal of Geographical Information Science},
  volume = {26},
  number = {7},
  pages = {1231--1249},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2011.633918},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/IJY6RK2A/wei murray 2012 - An integrated approach for addressing geographic uncertainty in spatial optimization - BDPG - UNCERTAINTY.pdf}
}

@article{whiteMeasuringAccuracySpecies,
  title = {Measuring the Accuracy of Species Distribution Models: A Review},
  author = {White, M and Newell, G},
  pages = {7},
  abstract = {Species distribution models (SDMs) are empirical models relating species occurrence to environmental variables based on statistical or other response surfaces. Species distribution modeling can be used as a tool to solve many theoretical and applied ecological and environmental problems, which include testing biogeographical, ecological and evolutionary hypotheses, assessing species invasion and climate change impact, and supporting conservation planning and reserve selection. The utility of SDM in real world applications requires the knowledge of the model's accuracy. The accuracy of a model includes two aspects: discrimination capacity and reliability. The former is the power of the model to differentiate presences from absences; and the latter refers to the capability of the predicted probabilities to reflect the observed proportion of sites occupied by the subject species.},
  langid = {english},
  keywords = {accuracy,calibration,EF EFs,EFs,guppy,metrics,sdm},
  file = {/Users/bill/D/Zotero/storage/G6JQZM5K/liu white newell 2009 - Measuring the accuracy of species distribution models - a review - GUPPY - SDMs - EF EFs.pdf}
}

@article{whitleyLauraBarbulescuCarnegie,
  title = {Laura {{Barbulescu Carnegie Mellon University}}},
  author = {Whitley, Darrell and Howe, Adele},
  pages = {9},
  abstract = {Test suites for many domains often fail to model features present in real-world problems. For the permutation ow-shop sequencing problem (PFSP), the most popular test suite consists of problems whose features are generated from a single uniform random distribution. Synthetic generation of problems with characteristics present in real-world problems is a viable alternative. We compare the performance of several competitive algorithms on problems produced with such a generator. We nd that, as more realistic characteristics are introduced, the performance of a stateof-the-art algorithm degrades rapidly: faster and less complex stochastic algorithms provide superior performance. Our empirical results show that small changes in problem structure or problem size can in uence algorithm performance. We hypothesize that these performance di erences may be partially due to di erences in search space topologies; we show that structured problems produce topologies with performance plateaus. Algorithm sensitivity to problem characteristics suggests the need to construct test suites more representative of real-world applications.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/8M43X6PP/watson barbulescu howe whitley 1999 - algorithm performance and problem structure for flow-shop scheduling.pdf}
}

@article{wilkinson2020mee,
  title = {Defining and Evaluating Predictions of Joint Species Distribution Models},
  author = {Wilkinson, David P. and Golding, Nick and Guillera-Arroita, Gurutzeta and Tingley, Reid and McCarthy, Michael A.},
  editor = {Freckleton, Robert},
  year = {2020},
  month = nov,
  journal = {Methods in Ecology and Evolution},
  pages = {2041-210X.13518},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13518},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/72LNGFPW/wilkinson ... mccarthy 2020 - defining and evaluating predictions of joint species distribution models - BDPG - GUPPY - SDMs - EVALUATION - EFs.pdf}
}

@inproceedings{williams2003p1ijcai,
  title = {Backdoors {{To Typical Case Complexity}}},
  booktitle = {Proceedings of the 18th International Joint Conference on {{Artificial}} Intelligence},
  author = {Williams, Ryan and Gomes, Carla P and Selman, Bart},
  year = {2003},
  pages = {1173--1178},
  abstract = {There has been significant recent progress in reasoning and constraint processing methods. In areas such as planning and finite model-checking, current solution techniques can handle combinatorial problems with up to a million variables and five million constraints. The good scaling behavior of these methods appears to defy what one would expect based on a worst-case complexity analysis. In order to bridge this gap between theory and practice, we propose a new framework for studying the complexity of these techniques on practical problem instances. In particular, our approach incorporates general structural properties observed in practical problem instances into the formal complexity analysis. We introduce a notion of ``backdoors'', which are small sets of variables that capture the overall combinatorics of the problem instance. We provide empirical results showing the existence of such backdoors in real-world problems. We then present a series of complexity results that explain the good scaling behavior of current reasoning and constraint methods observed on practical problem instances.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MDEGXBKS/williams gomes selman 2003 - Backdoors To Typical Case Complexity - PROBLEM DIFFICULTY - BDPG - CSPs.pdf}
}

@article{wilsonBiodiversitySpeciesInteractions2003,
  title = {Biodiversity and Species Interactions: Extending {{Lotka-Volterra}} Community Theory},
  shorttitle = {Biodiversity and Species Interactions},
  author = {Wilson, W. G. and Lundberg, P. and Vazquez, D. P. and Shurin, J. B. and Smith, M. D. and Langford, W. and Gross, K. L. and Mittelbach, G. G.},
  year = {2003},
  month = oct,
  journal = {Ecology Letters},
  volume = {6},
  number = {10},
  pages = {944--952},
  issn = {1461-023X, 1461-0248},
  doi = {10.1046/j.1461-0248.2003.00521.x},
  abstract = {A new analysis of the nearly century-old Lotka\textendash Volterra theory allows us to link species interactions to biodiversity patterns, including: species abundance distributions, estimates of total community size, patterns of community invasibility, and predicted responses to disturbance. Based on a few restrictive assumptions about species interactions, our calculations require only that the community is sufficiently large to allow a mean-field approximation. We develop this analysis to show how an initial assemblage of species with varying interaction strengths is predicted to sort out into the final community based on the species\~O predicted target densities. The sorting process yields predictions of covarying patterns of species abundance, community size, and species interaction strengths. These predictions can be tested using enrichment experiments, examination of latitudinal and productivity gradients, and features of community assembly.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/LT354Z4M/wilson et al 2003 - Biodiversity and species interactions - extending LotkaâVolterra community theory.pdf}
}

@article{wilsonMethodsFittingDominance1991,
  title = {Methods for Fitting Dominance/Diversity Curves},
  author = {Wilson, J. Bastow},
  year = {1991},
  month = feb,
  journal = {Journal of Vegetation Science},
  volume = {2},
  number = {1},
  pages = {35--46},
  issn = {11009233, 16541103},
  doi = {10.2307/3235896},
  abstract = {Dominance/diversity curves, displaying the relative abundances of the species within a community, have often been constructed from field data. Several ecological and statistical models of dominance/diversity have been proposed, to explain the curves. Yet, rarely have curves of different models been fitted to field data.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/EDSPGMZM/wilson 1991 - Methods for fitting dominance-diversity curves - RANK ABUNDANCE DISTRIBUTION - LOGNORMAL - BDPG.pdf}
}

@article{wilsonSettingConservationPriorities2009,
  title = {Setting {{Conservation Priorities}}},
  author = {Wilson, Kerrie A. and Carwardine, Josie and Possingham, Hugh P.},
  year = {2009},
  month = apr,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1162},
  number = {1},
  pages = {237--264},
  issn = {00778923},
  doi = {10.1111/j.1749-6632.2009.04149.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F9FKQJKP/wilson et al 2009 - setting conservation priorities - annnyacadsci.pdf}
}

@article{woinarskiApplicationTaxonPriority1996,
  title = {Application of a Taxon Priority System for Conservation Planning by Selecting Areas Which Are Most Distinct from Environments Already Reserved},
  author = {Woinarski, J.C.Z. and Price, O. and Faith, D.P.},
  year = {1996},
  journal = {Biological Conservation},
  volume = {76},
  number = {2},
  pages = {147--159},
  issn = {00063207},
  doi = {10.1016/0006-3207(95)00106-9},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/A3DYTNHD/1-s2.0-0006320795001069-main.pdf}
}

@article{wolffClassificationDetectionConsequences2011,
  title = {Classification, {{Detection}} and {{Consequences}} of {{Data Error}}: {{Evidence}} from the {{Human Development Index}}},
  shorttitle = {Classification, {{Detection}} and {{Consequences}} of {{Data Error}}},
  author = {Wolff, Hendrik and Chong, Howard and Auffhammer, Maximilian},
  year = {2011},
  month = jun,
  journal = {The Economic Journal},
  volume = {121},
  number = {553},
  pages = {843--870},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/j.1468-0297.2010.02408.x},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/X7YDJ56Q/wolff et al 2011 - classification detection and consequences of data error - evidence from the human development index - ERROR - UNCERTAINTY - INDICES - ECONOMICS - OPTISEVIL.pdf}
}

@misc{wolpertWHATDOESDINNER,
  title = {{{WHAT DOES DINNER COST}}?},
  author = {Wolpert, David H},
  langid = {english},
  keywords = {NFL theorems},
  file = {/Users/bill/D/Zotero/storage/XPLRC7IK/coev.pdf}
}

@article{wolpertWhatNoFree,
  title = {What the No Free Lunch Theorems Really Mean; How to Improve Search Algorithms},
  author = {Wolpert, David H},
  pages = {14},
  langid = {english},
  keywords = {NFL theorems},
  file = {/Users/bill/D/Zotero/storage/BIP8DMII/12-10-017.pdf}
}

@article{woolley2017mee,
  title = {Characterising Uncertainty in Generalised Dissimilarity Models},
  author = {Woolley, Skipton N.C. and Foster, Scott D. and O'Hara, Timothy D. and Wintle, Brendan A. and Dunstan, Piers K.},
  editor = {Hodgson, David},
  year = {2017},
  month = aug,
  journal = {Methods in Ecology and Evolution},
  volume = {8},
  number = {8},
  pages = {985--995},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.12710},
  langid = {english},
  keywords = {Bayesian Bootstrap,GDM,Generalized dissimilarity models,uncertainty},
  file = {/Users/bill/D/Zotero/storage/9UMPEI4Q/WoolleyFosterDunstanOHaraWintleBBGDM_v3.pdf}
}

@article{wrightEquilibriumPredictingHuman,
  title = {Beyond {{Equilibrium}}: {{Predicting Human Behavior}} in {{Normal-Form Games}}},
  author = {Wright, James R and {Leyton-Brown}, Kevin},
  pages = {7},
  abstract = {It is standard in multiagent settings to assume that agents will adopt Nash equilibrium strategies. However, studies in experimental economics demonstrate that Nash equilibrium is a poor description of human players' initial behavior in normal-form games. In this paper, we consider a wide range of widely-studied models from behavioral game theory. For what we believe is the first time, we evaluate each of these models in a meta-analysis, taking as our data set large-scale and publicly-available experimental data from the literature. We then propose modifications to the best-performing model that we believe make it more suitable for practical prediction of initial play by humans in normal-form games.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/27TQHIBN/1946-8218-1-PB.pdf}
}

@article{wrightFormalSeparationStrategic2020,
  title = {A {{Formal Separation Between Strategic}} and {{Nonstrategic Behavior}}},
  author = {Wright, James R. and {Leyton-Brown}, Kevin},
  year = {2020},
  month = aug,
  journal = {arXiv:1812.11571 [cs]},
  eprint = {1812.11571},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {It is common to make a distinction between ``strategic'' behavior and other forms of intentional but ``nonstrategic'' behavior: typically, that strategic agents model other agents while nonstrategic agents do not. However, a crisp boundary between these concepts has proven elusive. This problem is pervasive throughout the game theoretic literature on bounded rationality. It is particularly critical in parts of the behavioral game theory literature that make an explicit distinction between the behavior of ``nonstrategic'' level-0 agents and ``strategic'' higher-level agents (e.g., the level-k and cognitive hierarchy models). The literature gives no clear guidance on how the rationality of nonstrategic agents must be bounded, instead typically just singling out specific decision rules and informally asserting them to be nonstrategic (e.g., truthfully revealing private information; randomizing uniformly). In this work, we propose a new, formal characterization of nonstrategic behavior. Our main contribution is to show that it satisfies two properties: (1) it is general enough to capture all purportedly ``nonstrategic'' decision rules of which we are aware; (2) behavior that obeys our characterization is distinct from strategic behavior in a precise sense.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/Users/bill/D/Zotero/storage/52YCTYW8/1812.11571.pdf}
}

@article{wrightLevel0ModelsPredicting2019,
  title = {Level-0 {{Models}} for {{Predicting Human Behavior}} in {{Games}}},
  author = {Wright, James R. and {Leyton-Brown}, Kevin},
  year = {2019},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {64},
  pages = {357--383},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11361},
  abstract = {Behavioral game theory seeks to describe the way actual people (as compared to idealized, ``rational'' agents) act in strategic situations. Our own recent work has identified iterative models, such as quantal cognitive hierarchy, as the state of the art for predicting human play in unrepeated, simultaneous-move games. Iterative models predict that agents reason iteratively about their opponents, building up from a specification of nonstrategic behavior called level-0. A modeler is in principle free to choose any description of level-0 behavior that makes sense for a given setting. However, in practice almost all existing work specifies this behavior as a uniform distribution over actions. In most games it is not plausible that even nonstrategic agents would choose an action uniformly at random, nor that other agents would expect them to do so. A more accurate model for level-0 behavior has the potential to dramatically improve predictions of human behavior, since a substantial fraction of agents may play level-0 strategies directly, and furthermore since iterative models ground all higher-level strategies in responses to the level-0 strategy. Our work considers models of the way in which level-0 agents construct a probability distribution over actions, given an arbitrary game. We considered a large space of alternatives and, in the end, recommend a model that achieved excellent performance across the board: a linear weighting of four binary features, each of which is general in the sense that it can be computed from any normal form game. Adding real-valued variants of the same four features yielded further improvements in performance, albeit with a corresponding increase in the number of parameters needing to be estimated. We evaluated the effects of combining these new level-0 models with several iterative models and observed large improvements in predictive accuracy.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/4RIQGLDR/11361-Article (PDF)-21052-1-10-20190219.pdf}
}

@article{wrightPredictingHumanBehavior2017,
  title = {Predicting Human Behavior in Unrepeated, Simultaneous-Move Games},
  author = {Wright, James R. and {Leyton-Brown}, Kevin},
  year = {2017},
  month = nov,
  journal = {Games and Economic Behavior},
  volume = {106},
  pages = {16--37},
  issn = {08998256},
  doi = {10.1016/j.geb.2017.09.009},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F45ZEPQS/2017-GEB-PredictingHumanBehavior.pdf}
}

@article{wu2018acms,
  title = {Statistical {{Problems}} with {{Planted Structures}}: {{Information-Theoretical}} and {{Computational Limits}}},
  shorttitle = {Statistical {{Problems}} with {{Planted Structures}}},
  author = {Wu, Yihong and Xu, Jiaming},
  year = {2018},
  month = aug,
  journal = {arXiv:1806.00118 [cs, math, stat]},
  eprint = {1806.00118},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Over the past few years, insights from computer science, statistical physics, and information theory have revealed phase transitions in a wide array of high-dimensional statistical problems at two distinct thresholds: One is the information-theoretical (IT) threshold below which the observation is too noisy so that inference of the ground truth structure is impossible regardless of the computational cost; the other is the computational threshold above which inference can be performed efficiently, i.e., in time that is polynomial in the input size. In the intermediate regime, inference is information-theoretically possible, but conjectured to be computationally hard.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,Computer Science - Information Theory,Mathematics - Statistics Theory,planted solution},
  file = {/Users/bill/D/Zotero/storage/4X2SKVSH/1806.00118.pdf}
}

@inproceedings{wuApproximateBayesianComputation2013,
  title = {Approximate {{Bayesian}} Computation for Estimating Rate Constants in Biochemical Reaction Systems},
  booktitle = {2013 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}}},
  author = {Wu, Qianqian and {Smith-Miles}, Kate and Tian, Tianhai},
  year = {2013},
  month = dec,
  pages = {416--421},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/BIBM.2013.6732528},
  abstract = {To study the dynamic properties of complex biological systems, mathematical modeling has been used widely in systems biology. Apart from the well-established knowledge for modeling techniques, there are still some difficulties while understanding the dynamics in system biology. One of the major challenges is how to infer unknown parameters in mathematical models based on the experimentally observed data sets. This is extremely difficult when the experimental data are sparse and the biological systems are stochastic. To tackle this problem, in this work we revised one computation method for inference called approximate Bayesian computation (ABC) and conducted extensive computing tests to examine the influence of a number of factors on the performance of ABC. Based on simulation results, we found that the number of stochastic simulations and step size of the observation data have substantial influence on the estimation accuracy. We applied the ABC method to two stochastic systems to test the efficiency and effectiveness of the ABC and obtained promising approximation for the unknown parameters in the systems. This work raised a number of important issues for designing effective inference methods for estimating rate constants in biochemical reaction systems.},
  isbn = {978-1-4799-1309-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/UI3PRRP4/06732528.pdf}
}

@inproceedings{wuApproximateBayesianComputation2013a,
  title = {Approximate {{Bayesian}} Computation for Estimating Rate Constants in Biochemical Reaction Systems},
  booktitle = {2013 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}}},
  author = {Wu, Qianqian and {Smith-Miles}, Kate and Tian, Tianhai},
  year = {2013},
  month = dec,
  pages = {416--421},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/BIBM.2013.6732528},
  abstract = {To study the dynamic properties of complex biological systems, mathematical modeling has been used widely in systems biology. Apart from the well-established knowledge for modeling techniques, there are still some difficulties while understanding the dynamics in system biology. One of the major challenges is how to infer unknown parameters in mathematical models based on the experimentally observed data sets. This is extremely difficult when the experimental data are sparse and the biological systems are stochastic. To tackle this problem, in this work we revised one computation method for inference called approximate Bayesian computation (ABC) and conducted extensive computing tests to examine the influence of a number of factors on the performance of ABC. Based on simulation results, we found that the number of stochastic simulations and step size of the observation data have substantial influence on the estimation accuracy. We applied the ABC method to two stochastic systems to test the efficiency and effectiveness of the ABC and obtained promising approximation for the unknown parameters in the systems. This work raised a number of important issues for designing effective inference methods for estimating rate constants in biochemical reaction systems.},
  isbn = {978-1-4799-1309-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/V4QB5E3N/06732528.pdf}
}

@article{xingengFaceImageModeling2011,
  title = {Face {{Image Modeling}} by {{Multilinear Subspace Analysis With Missing Values}}},
  author = {{Xin Geng} and {Smith-Miles}, K and {Zhi-Hua Zhou} and {Liang Wang}},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {41},
  number = {3},
  pages = {881--892},
  issn = {1083-4419, 1941-0492},
  doi = {10.1109/TSMCB.2010.2097588},
  abstract = {Multilinear subspace analysis (MSA) is a promising methodology for pattern-recognition problems due to its ability in decomposing the data formed from the interaction of multiple factors. The MSA requires a large training set, which is well organized in a single tensor, which consists of data samples with all possible combinations of the contributory factors. However, such a ``complete'' training set is difficult (or impossible) to obtain in many real applications. The missing-value problem is therefore crucial to the practicality of the MSA but has been hardly investigated up to present. To solve the problem, this paper proposes an algorithm named M2SA, which is advantageous in real applications due to the following: 1) it inherits the ability of the MSA to decompose the interlaced semantic factors; 2) it does not depend on any assumptions on the data distribution; and 3) it can deal with a high percentage of missing values. M2SA is evaluated by face image modeling on two typical multifactorial applications, i.e., face recognition and facial age estimation. Experimental results show the effectiveness of M2SA even when the majority of the values in the training tensor are missing.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NTPEIPIT/geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf}
}

@article{xingengFaceImageModeling2011a,
  title = {Face {{Image Modeling}} by {{Multilinear Subspace Analysis With Missing Values}}},
  author = {{Xin Geng} and {Smith-Miles}, K and {Zhi-Hua Zhou} and {Liang Wang}},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {41},
  number = {3},
  pages = {881--892},
  issn = {1083-4419, 1941-0492},
  doi = {10.1109/TSMCB.2010.2097588},
  abstract = {Multilinear subspace analysis (MSA) is a promising methodology for pattern-recognition problems due to its ability in decomposing the data formed from the interaction of multiple factors. The MSA requires a large training set, which is well organized in a single tensor, which consists of data samples with all possible combinations of the contributory factors. However, such a ``complete'' training set is difficult (or impossible) to obtain in many real applications. The missing-value problem is therefore crucial to the practicality of the MSA but has been hardly investigated up to present. To solve the problem, this paper proposes an algorithm named M2SA, which is advantageous in real applications due to the following: 1) it inherits the ability of the MSA to decompose the interlaced semantic factors; 2) it does not depend on any assumptions on the data distribution; and 3) it can deal with a high percentage of missing values. M2SA is evaluated by face image modeling on two typical multifactorial applications, i.e., face recognition and facial age estimation. Experimental results show the effectiveness of M2SA even when the majority of the values in the training tensor are missing.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YD7JGAFQ/geng smith-miles et al 2011 - Face Image Modeling by Multilinear Subspace Analysis With Missing Values.pdf}
}

@article{xu,
  title = {Many {{Hard Examples}} in {{Exact Phase Transitions}} with {{Application}} to {{Generating Hard Satisfiable Instances}}},
  author = {Xu, Ke and Li, Wei},
  pages = {19},
  abstract = {This paper first analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisfiable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.},
  langid = {english},
  keywords = {bdpg,bdpg_P1,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/3MQZ2AWX/xu li - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - BDPG - ANNO.pdf}
}

@article{xu2000j,
  title = {Exact {{Phase Transitions}} in {{Random Constraint Satisfaction Problems}}},
  author = {Xu, K. and Li, W.},
  year = {2000},
  month = mar,
  journal = {Journal of Artificial Intelligence Research},
  volume = {12},
  pages = {93--103},
  issn = {1076-9757},
  doi = {10.1613/jair.696},
  abstract = {In this paper we propose a new type of random CSP model,    called Model RB, which is a revision to the standard Model B. It is    proved that phase transitions from a region where almost all problems    are satisfiable to a region where almost all problems are    unsatisfiable do exist for Model RB as the number of variables    approaches infinity.  Moreover, the critical values at which the phase    transitions occur are also known exactly. By relating the hardness of    Model RB to Model B, it is shown that there exist a lot of hard    instances in Model RB.},
  file = {/Users/bill/D/Zotero/storage/XCZ2GDSW/xu li 2000 - exact phase transitions in random constraint satisfaction problems - PROBLEM DIFFICULTY - BDPG - ANNO.pdf}
}

@inproceedings{xu2005ipnijcai,
  title = {A {{Simple Model}} to {{Generate Hard Satisfiable Instances}}},
  booktitle = {{{IJCAI-05}}, {{Proceedings}} of the {{Nineteenth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xu, Ke and Boussemart, Frederic and Hemery, Fred and Lecoutre, Christophe},
  year = {2005},
  pages = {337--342},
  publisher = {{AAAI Press}},
  address = {{Edinburgh, Scotland, UK}},
  abstract = {In this paper, we try to further demonstrate that the models of random CSP instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.},
  langid = {english},
  keywords = {bdpg,bdpg_P1,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/Z3NISNDA/xu et al 2005 - A Simple Model to Generate Hard Satisfiable Instances - ijcai05 - BDPG - ANNO.pdf}
}

@article{xu2006tcs,
  title = {Many Hard Examples in Exact Phase Transitions},
  author = {Xu, Ke and Li, Wei},
  year = {2006},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {355},
  number = {3},
  pages = {291--302},
  issn = {03043975},
  doi = {10.1016/j.tcs.2006.01.001},
  abstract = {This paper analyzes the resolution complexity of two random constraint satisfaction problem (CSP) models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CSPs and CNF formulas hard to solve, which can be useful in the experimental evaluation of CSP and SAT algorithms, but also propose models with both many hard instances and exact phase transitions. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.},
  langid = {english},
  keywords = {bdpg,bdpg_P1,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/BSRFL4WP/xu li 2006 - many hard examples in exact phase transitions - PROBLEM DIFFICULTY - BDPG - ANNO.pdf}
}

@article{xu2007ai,
  title = {Random Constraint Satisfaction: {{Easy}} Generation of Hard (Satisfiable) Instances},
  shorttitle = {Random Constraint Satisfaction},
  author = {Xu, Ke and Boussemart, Fr{\'e}d{\'e}ric and Hemery, Fred and Lecoutre, Christophe},
  year = {2007},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {171},
  number = {8-9},
  pages = {514--534},
  issn = {00043702},
  doi = {10.1016/j.artint.2007.04.001},
  abstract = {In this paper, we show that the models of random CSP instances proposed by Xu and Li [K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artificial Intelligence Research 12 (2000) 93\textendash 103; K. Xu, W. Li, Many hard examples in exact phase transitions with application to generating hard satisfiable instances, Technical report, CoRR Report cs.CC/0302001, Revised version in Theoretical Computer Science 355 (2006) 291\textendash 302] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.},
  langid = {english},
  keywords = {bdpg,bdpg_P1,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/BVDE8EK9/xu et al 2007 - random constraint satisfaction - easy generation of hard satisfiable problems - BDPG - ANNO.pdf}
}

@incollection{xu2016ioaaoticp,
  title = {A {{New Solver}} for the {{Minimum Weighted Vertex Cover Problem}}},
  booktitle = {Integration of {{AI}} and {{OR Techniques}} in {{Constraint Programming}}},
  author = {Xu, Hong and Kumar, T. K. Satish and Koenig, Sven},
  editor = {Quimper, Claude-Guy},
  year = {2016},
  volume = {9676},
  pages = {392--405},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-33954-2_28},
  isbn = {978-3-319-33953-5 978-3-319-33954-2},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2Y9PVRRW/xu et al 2016 - HONG XU not KE XU - a new solver for the minimum weighted vertex cover problem - SLIDES - BDPG.pdf}
}

@article{xuAverageAnalysisBacktracking,
  title = {An {{Average Analysis}} of {{Backtracking}} on {{Random Constraint Satisfaction Problems}}},
  author = {Xu, Ke and Li, Wei},
  pages = {20},
  abstract = {In this paper we propose a random CSP model, called Model GB, which is a natural generalization of standard Model B. This paper considers Model GB in the case where each constraint is easy to satisfy. In this case Model GB exhibits non-trivial behaviour (not trivially satisfiable or unsatisfiable) as the number of variables approaches infinity. A detailed analysis to obtain an asymptotic estimate (good to 1 + o(1) ) of the average number of nodes in a search tree used by the backtracking algorithm on Model GB is also presented. It is shown that the average number of nodes required for finding all solutions or proving that no solution exists grows exponentially with the number of variables. So this model might be an interesting distribution for studying the nature of hard instances and evaluating the performance of CSP algorithms. In addition, we further investigate the behaviour of the average number of nodes as r (the ratio of constraints to variables) varies. The results indicate that as r increases, random CSP instances get easier and easier to solve, and the base for the average number of nodes that is exponential in n tends to 1 as r approaches infinity. Therefore, although the average number of nodes used by the backtracking algorithm on random CSP is exponential, many CSP instances will be very easy to solve when r is sufficiently large.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/GE6S7DQ8/amai-final.pdf}
}

@article{xuAverageSimilarityDegree2004,
  title = {On the Average Similarity Degree between Solutions of Random K-{{SAT}} and Random {{CSPs}}},
  author = {Xu, Ke and Li, Wei},
  year = {2004},
  month = jan,
  journal = {Discrete Applied Mathematics},
  volume = {136},
  number = {1},
  pages = {125--149},
  issn = {0166218X},
  doi = {10.1016/S0166-218X(03)00204-X},
  abstract = {To study the structure of solutions for random k-SAT and random CSPs, this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other. It is proved that under certain conditions, as r (i.e. the ratio of constraints to variables) increases, the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point, shifting from a smaller value to a larger value abruptly. For random k-SAT this phenomenon will occur when k {$\geq$} 5 . It is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions. Finally, we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/ERJMQWNP/dam-final.pdf}
}

@article{xueSchedulingConservationDesigns,
  title = {Scheduling {{Conservation Designs}} via {{Network Cascade Optimization}}},
  author = {Xue, Shan and Fern, Alan and Sheldon, Daniel},
  pages = {7},
  abstract = {We introduce the problem of scheduling land purchases to conserve an endangered species in a way that achieves maximum population spread but delays purchases as long as possible, so that conservation planners retain maximum flexibility and use available budgets in the most efficient way. We develop the problem formally as a stochastic optimization problem over a network cascade model describing the population spread, and present a solution approach that reduces the stochastic problem to a novel variant of a Steiner tree problem. We give a primal-dual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution. Our experiments, using actual conservation data and a standard diffusion model, show that the approach produces near optimal results and is much more scalable than more generic off-the-shelf optimizers.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/2MW79YX4/xue fern sheldon 2012 - Scheduling Conservation Designs via Network Cascade Optimization - aaai2012 paper by OSU CS authors - GUPPY.pdf}
}

@inproceedings{xuFeaturesSAT,
  title = {Features for {{SAT}}},
  author = {Xu, Lin and Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  file = {/Users/bill/D/Zotero/storage/AY3N77CU/xu hoos leyton-brown - features for SAT.pdf}
}

@incollection{xuHierarchicalHardnessModels2007,
  title = {Hierarchical {{Hardness Models}} for {{SAT}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} \textendash{} {{CP}} 2007},
  author = {Xu, Lin and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  editor = {Bessi{\`e}re, Christian},
  year = {2007},
  volume = {4741},
  pages = {696--711},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74970-7_49},
  abstract = {Empirical hardness models predict a solver's runtime for a given instance of an N P-hard problem based on efficiently computable features. Previous research in the SAT domain has shown that better prediction accuracy and simpler models can be obtained when models are trained separately on satisfiable and unsatisfiable instances. We extend this work by training separate hardness models for each class, predicting the probability that a novel instance belongs to each class, and using these predictions to build a hierarchical hardness model using a mixture-of-experts approach. We describe and analyze classifiers and hardness models for four well-known distributions of SAT instances and nine high-performance solvers. We show that surprisingly accurate classifications can be achieved very efficiently. Our experiments show that hierarchical hardness models achieve higher prediction accuracy than the previous state of the art. Furthermore, the classifier's confidence correlates strongly with prediction error, giving a useful per-instance estimate of prediction error.},
  isbn = {978-3-540-74969-1},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/VVHKUC3Q/XuEtAl07.pdf}
}

@misc{xuke2014,
  title = {Challenging {{Benchmarks}} for {{Maximum Clique}}, {{Maximum Independent Set}}, {{Minimum Vertex Cover}} and {{Vertex Coloring}} - {{Generating Hard Graph Problem Instances}}},
  author = {Xu, Ke},
  year = {2014},
  month = dec,
  howpublished = {http://sites.nlsde.buaa.edu.cn/\textasciitilde kexu/benchmarks/graph-benchmarks.htm},
  file = {/Users/bill/D/Zotero/storage/Q8LDEL4I/graph-benchmarks.html}
}

@article{xuManyHardExamplesb,
  title = {Many {{Hard Examples}} in {{Exact Phase Transitions}} with {{Application}} to {{Generating Hard Satisfiable Instances}}},
  author = {Xu, K},
  pages = {2},
  abstract = {This paper first analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisfiable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/NWZADC6W/xu web page - Many Hard Examples in Exact Phase Transitions with Application to Generating Hard Satisfiable Instances - Resolution Complexity of Random CSP.pdf}
}

@article{xuPredictingSatisfiabilityPhase,
  title = {Predicting {{Satisfiability}} at the {{Phase Transition}}},
  author = {Xu, Lin and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {7},
  abstract = {Uniform random 3-SAT at the solubility phase transition is one of the most widely studied and empirically hardest distributions of SAT instances. For 20 years, this distribution has been used extensively for evaluating and comparing algorithms. In this work, we demonstrate that simple rules can predict the solubility of these instances with surprisingly high accuracy. Specifically, we show how classification accuracies of about 70\% can be obtained based on cheaply (polynomial-time) computable features on a wide range of instance sizes. We argue in two ways that classification accuracy does not decrease with instance size: first, we show that our models' predictive accuracy remains roughly constant across a wide range of problem sizes; second, we show that a classifier trained on small instances is sufficient to achieve very accurate predictions across the entire range of instance sizes currently solvable by complete methods. Finally, we demonstrate that a simple decision tree based on only two features, and again trained only on the smallest instances, achieves predictive accuracies close to those of our most complex model. We conjecture that this twofeature model outperforms random guessing asymptotically; due to the model's extreme simplicity, we believe that this conjecture is a worthwhile direction for future theoretical work.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/JZHIPAJC/2012-PredictingSatisfiability.pdf}
}

@article{xuPredictingSatisfiabilityPhasea,
  title = {Predicting {{Satisfiability}} at the {{Phase Transition}}},
  author = {Xu, Lin and Hoos, Holger H and {Leyton-Brown}, Kevin},
  pages = {7},
  abstract = {Uniform random 3-SAT at the solubility phase transition is one of the most widely studied and empirically hardest distributions of SAT instances. For 20 years, this distribution has been used extensively for evaluating and comparing algorithms. In this work, we demonstrate that simple rules can predict the solubility of these instances with surprisingly high accuracy. Specifically, we show how classification accuracies of about 70\% can be obtained based on cheaply (polynomial-time) computable features on a wide range of instance sizes. We argue in two ways that classification accuracy does not decrease with instance size: first, we show that our models' predictive accuracy remains roughly constant across a wide range of problem sizes; second, we show that a classifier trained on small instances is sufficient to achieve very accurate predictions across the entire range of instance sizes currently solvable by complete methods. Finally, we demonstrate that a simple decision tree based on only two features, and again trained only on the smallest instances, achieves predictive accuracies close to those of our most complex model. We conjecture that this twofeature model outperforms random guessing asymptotically; due to the model's extreme simplicity, we believe that this conjecture is a worthwhile direction for future theoretical work.},
  langid = {english},
  keywords = {Leyton-Brown},
  file = {/Users/bill/D/Zotero/storage/4F9UGZD6/5098-22365-1-PB.pdf}
}

@incollection{xuQuantifyingSimilarityAlgorithm2016,
  title = {Quantifying the {{Similarity}} of {{Algorithm Configurations}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Xu, Lin and KhudaBukhsh, Ashiqur R. and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  editor = {Festa, Paola and Sellmann, Meinolf and Vanschoren, Joaquin},
  year = {2016},
  volume = {10079},
  pages = {203--217},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50349-3_14},
  isbn = {978-3-319-50348-6 978-3-319-50349-3},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/TAZJBSSZ/2016-LION-Visualizing-Configurations.pdf}
}

@article{xuSATPhaseTransition1999,
  title = {The {{SAT Phase Transition}}},
  author = {Xu, Ke and Li, Wei},
  year = {1999},
  journal = {SCIENCE IN CHINA},
  volume = {42},
  number = {5},
  pages = {13},
  abstract = {Phase transition is an important feature of SAT problem. In this paper, for random k SAT model, we prove that as r (ratio of clauses to variables) increases, the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point ( r = rcr ). This phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/Q4S7XEB6/scichina99.pdf}
}

@incollection{xuSATzilla07DesignAnalysis2007,
  title = {{{SATzilla-07}}: {{The Design}} and {{Analysis}} of an {{Algorithm Portfolio}} for {{SAT}}},
  shorttitle = {{{SATzilla-07}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} \textendash{} {{CP}} 2007},
  author = {Xu, Lin and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  editor = {Bessi{\`e}re, Christian},
  year = {2007},
  volume = {4741},
  pages = {712--727},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74970-7_50},
  abstract = {It has been widely observed that there is no ``dominant'' SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe a per-instance solver portfolio for SAT, SATzilla-07, which uses socalled empirical hardness models to choose among its constituent solvers. We leverage new model-building techniques such as censored sampling and hierarchical hardness models, and demonstrate the effectiveness of our techniques by building a portfolio of state-of-the-art SAT solvers and evaluating it on several widely-studied SAT data sets. Overall, we show that our portfolio significantly outperforms its constituent algorithms on every data set. Our approach has also proven itself to be effective in practice: in the 2007 SAT competition, SATzilla-07 won three gold medals, one silver, and one bronze; it is available online at http://www.cs.ubc.ca/labs/beta/Projects/SATzilla .},
  isbn = {978-3-540-74969-1},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/BE2ZI637/XuEtAl07b.pdf}
}

@incollection{yamamoto2007aac,
  title = {A {{Spectral Method}} for {{MAX2SAT}} in the {{Planted Solution Model}}},
  booktitle = {Algorithms and {{Computation}}},
  author = {Yamamoto, Masaki},
  editor = {Tokuyama, Takeshi},
  year = {2007},
  volume = {4835},
  pages = {112--123},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77120-3_12},
  isbn = {978-3-540-77118-0},
  langid = {english},
  keywords = {bdpg,planted solution},
  file = {/Users/bill/D/Zotero/storage/RK3KFAXI/chp%3A10.1007%2F978-3-540-77120-3_12.pdf}
}

@article{yehuda,
  title = {It's {{Not What Machines Can Learn}}, {{It}}'s {{What We Cannot Teach}}},
  author = {Yehuda, Gal and Gabel, Moshe and Schuster, Assaf},
  pages = {11},
  abstract = {Can deep neural networks learn to solve any task, and in particular problems of high complexity? This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability. In this work we offer a different perspective on this question. Given the common assumption that NP = coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem. We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased datasets that lead practitioners to over-estimate model accuracy. Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difficulty of generating sufficiently large and unbiased training sets.},
  langid = {english},
  keywords = {bdpg,deep learning,machine learning,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/VD6AT7PA/yehuda et al 2020 - Its Not What Machines Can Learn, Itâs What We Cannot Teach - BDPG - PROBLEM DIFFICULTY - ML - DEEP LEARNING.pdf}
}

@article{yinQuantifyingDynamicsFailure2019,
  title = {Quantifying the Dynamics of Failure across Science, Startups and Security},
  author = {Yin, Yian and Wang, Yang and Evans, James A. and Wang, Dashun},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7781},
  pages = {190--194},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1725-y},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/QA3SS5RG/yin et al 2019 - quantifying the dynamics of failure across science startups and security.pdf}
}

@inproceedings{you2019anips3acnips2n2d82vbc,
  title = {{{G2SAT}}: {{Learning}} to {{Generate SAT Formulas}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} 2019, {{NeurIPS}} 2019, {{December}} 8-14, 2019, {{Vancouver}}, {{BC}}, {{Canada}}},
  author = {You, Jiaxuan and Wu, Haoze},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {d'Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {10552--10563},
  address = {{Vancouver, Canada}},
  abstract = {The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem and is fundamental to computer science, with a wide array of applications in planning, verification, and theorem proving. Developing and evaluating practical SAT solvers relies on extensive empirical testing on a set of real-world benchmark formulas. However, the availability of such real-world SAT formulas is limited. While these benchmark formulas can be augmented with synthetically generated ones, existing approaches for doing so are heavily hand-crafted and fail to simultaneously capture a wide range of characteristics exhibited by real-world SAT instances. In this work, we present G2SAT, the first deep generative framework that learns to generate SAT formulas from a given set of input formulas. Our key insight is that SAT formulas can be transformed into latent bipartite graph representations which we model using a specialized deep generative neural network. We show that G2SAT can generate SAT formulas that closely resemble given real-world SAT instances, as measured by both graph metrics and SAT solver behavior. Further, we show that our synthetic SAT formulas could be used to improve SAT solver performance on real-world benchmarks, which opens up new opportunities for the continued development of SAT solvers and a deeper understanding of their performance.},
  langid = {english},
  keywords = {bdpg,bipartite network,generative,machine learning,problem generator},
  file = {/Users/bill/D/Zotero/storage/GLKQ4MBQ/you et al 2019 - G2SAT - Learning to Generate SAT Formulas.pdf}
}

@article{youngFirstOrderPhase2010,
  title = {First Order Phase Transition in the {{Quantum Adiabatic Algorithm}}},
  author = {Young, A. P. and Knysh, S. and Smelyanskiy, V. N.},
  year = {2010},
  month = jan,
  journal = {Physical Review Letters},
  volume = {104},
  number = {2},
  eprint = {0910.1378},
  eprinttype = {arxiv},
  pages = {020502},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.104.020502},
  abstract = {We simulate the quantum adiabatic algorithm (QAA) for the exact cover problem for sizes up to N=256 using quantum Monte Carlo simulations incorporating parallel tempering. At large N we find that some instances have a discontinuous (first order) quantum phase transition during the evolution of the QAA. This fraction increases with increasing N and may tend to 1 for N -{$>$} infinity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Quantum Physics},
  file = {/Users/bill/D/Zotero/storage/3GHZGWCM/0910.1378.pdf}
}

@article{zabashtaNDSEMethodClassification,
  title = {{{NDSE}}: {{Method}} for {{Classification Instance Generation Given Meta-Feature Description}}},
  author = {Zabashta, Alexey and Filchenkov, Andrey},
  pages = {8},
  abstract = {This paper addresses the problem of instance generation. We propose a method called NDSE that generates an instance of binary classification problem given its vector description. We solve this problem by reducing it to an optimization task and applying genetic search. We use operators of addition and removal of objects and features to handle crossover and mutation operators over classifications datasets for genetic algorithm. We conducted several experiments, the results of which show that our method outperforms baseline methods.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/XSC8QPQW/e1d129149723eb77faec0f1d60dcf18d18b3.pdf}
}

@article{zamora-marinRoleProtectedAreas,
  title = {The Role of Protected Areas in Representing Aquatic Biodiversity: A Test Using {$\alpha$}, {$\beta$} and {$\gamma$} Diversity of Water Beetles from the {{Segura River Basin}} ({{SE Spain}})},
  author = {{Zamora-Mar{\'i}n}, J M and {Guti{\'e}rrez-C{\'a}novas}, C and Abell{\'a}n, P and Mill{\'a}n, A},
  pages = {14},
  abstract = {The role of protected areas in representing aquatic biodiversity: a test using {$\alpha$}, {$\beta$} and {$\gamma$} diversity of water beetles from the Segura River Basin (SE Spain) Networks of protected areas represent one of the main strategies to reduce the rapid loss of biodiversity. However, most of these protected areas have been designed by considering only charismatic groups of vertebrates and plants, most linked to terrestrial environments. Thus, little is known about how well protected areas perform in representing aquatic biodiversity. This study analyses the suitability of national and European protected area networks (Natural Protected Areas and Natura 2000) in representing such biodiversity. For this purpose, we studied the different components of diversity ({$\alpha$}, {$\beta$} and {$\gamma$}) using water beetles from the Segura River Basin as surrogates of overall macroinvertebrate biodiversity. Our results revealed no significant differences in {$\alpha$}-diversity between protected and non-protected areas. Similarly, we did not find significant differences in {$\beta$} -diversity components (species replacement and nestedness, i.e., differences in among-site richness without species replacement) between protected and non-protected areas. The species replacement contributed more than nestedness to explain overall {$\beta$} -diversity changes. Finally, we found that the {$\gamma$}-diversity component was significantly higher in both protected areas, when compared to an equivalent number of randomly selected locations.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/YWG2BYBJ/13 - zamora-marin et al - the role of protected areas in representing aquatic biodiversity - a test using alpha beta and gamma diversity of water beetles form the segura river basin SE Spain.pdf}
}

@article{zdeborova2011sjdm,
  title = {Quiet {{Planting}} in the {{Locked Constraint Satisfaction Problems}}},
  author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  year = {2011},
  month = jan,
  journal = {SIAM Journal on Discrete Mathematics},
  volume = {25},
  number = {2},
  eprint = {0902.4185},
  eprinttype = {arxiv},
  pages = {750--770},
  issn = {0895-4801, 1095-7146},
  doi = {10.1137/090750755},
  abstract = {We study the planted ensemble of locked constraint satisfaction problems. We describe the connection between the random and planted ensembles. The use of the cavity method is combined with arguments from reconstruction on trees and the first and the second moment considerations. Our main result is the location of the hard region in the planted ensemble. In a part of that hard region instances have with high probability a single satisfying assignment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {bdpg,bdpg_P1,Computer Science - Computational Complexity,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,planted problem,planted solution,problem difficulty},
  file = {/Users/bill/D/Zotero/storage/9S5M9MFH/zdeborova krzakala 2010 - QUIET PLANTING IN THE LOCKED CONSTRAINT SATISFACTION PROBLEMS.pdf}
}

@article{zhangAlgorithmsConnectedSet2009,
  title = {Algorithms for Connected Set Cover Problem and Fault-Tolerant Connected Set Cover Problem},
  author = {Zhang, Zhao and Gao, Xiaofeng and Wu, Weili},
  year = {2009},
  month = mar,
  journal = {Theoretical Computer Science},
  volume = {410},
  number = {8-10},
  pages = {812--817},
  issn = {03043975},
  doi = {10.1016/j.tcs.2008.11.005},
  abstract = {Given a set V of elements, S a family of subsets of V , and G a connected graph on vertex set S,a connected set cover (CSC) is a subfamily R of S such that every element in V is covered by at least one set of R, and the subgraph G[R] of G induced by R is connected. If furthermore G[R] is k-connected and every element in V is covered by at least m sets in R, then R is a (k, m)-CSC. In this paper, we present two approximation algorithms for the minimum CSC problem, and one approximation algorithm for the minimum (2, m)-CSC problem. Performance ratios are analyzed. These are the first approximation algorithms for CSC problems in general graphs with guaranteed performance ratios.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/6ELAH9UR/zhang et al 2009 - Algorithms for connected set cover problem and fault-tolerant connected set cover problem.pdf}
}

@article{zhangConsensusForecastingSpecies2015,
  title = {Consensus {{Forecasting}} of {{Species Distributions}}: {{The Effects}} of {{Niche Model Performance}} and {{Niche Properties}}},
  shorttitle = {Consensus {{Forecasting}} of {{Species Distributions}}},
  author = {Zhang, Lei and Liu, Shirong and Sun, Pengsen and Wang, Tongli and Wang, Guangyu and Zhang, Xudong and Wang, Linlin},
  editor = {L{\"o}tters, Stefan},
  year = {2015},
  month = mar,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0120056},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0120056},
  abstract = {Ensemble forecasting is advocated as a way of reducing uncertainty in species distribution modeling (SDM). This is because it is expected to balance accuracy and robustness of SDM models. However, there are little available data regarding the spatial similarity of the combined distribution maps generated by different consensus approaches. Here, using eight niche-based models, nine split-sample calibration bouts (or nine random model-training subsets), and nine climate change scenarios, the distributions of 32 forest tree species in China were simulated under current and future climate conditions. The forecasting ensembles were combined to determine final consensual prediction maps for target species using three simple consensus approaches (average, frequency, and median [PCA]). Species' geographic ranges changed (area change and shifting distance) in response to climate change, but the three consensual projections did not differ significantly with respect to how much or in which direction, but they did differ with respect to the spatial similarity of the three consensual predictions. Incongruent areas were observed primarily at the edges of species' ranges. Multiple stepwise regression models showed the three factors (niche marginality and specialization, and niche model accuracy) to be related to the observed variations in consensual prediction maps among consensus approaches. Spatial correspondence among prediction maps was the highest when niche model accuracy was high and marginality and specialization were low. The difference in spatial predictions suggested that more attention should be paid to the range of spatial uncertainty before any decisions regarding specialist species can be made based on map outputs. The niche properties and singlemodel predictive performance provide promising insights that may further understanding of uncertainties in SDM.},
  langid = {english},
  keywords = {ensembles,sdm,spatial similarity,uncertainty},
  file = {/Users/bill/D/Zotero/storage/FHM5G2UG/zhang et al 2015 - Consensus Forecasting of Species Distributions -  The Effects of Niche Model Performance and Niche Properties - SDMs - ENSEMBLE - SPATIAL SIMILARITY - UNCERTAINTY.pdf}
}

@article{zhangInfluenceSegmentationFeature1995,
  title = {Influence of Segmentation over Feature Measurement},
  author = {Zhang, Yu Jin},
  year = {1995},
  month = feb,
  journal = {Pattern Recognition Letters},
  volume = {16},
  number = {2},
  pages = {201--206},
  issn = {01678655},
  doi = {10.1016/0167-8655(94)00083-F},
  abstract = {The influence ofimage segmentation over the measurement accuracy of object features is studied. Seven features are examined under different image conditions by using a common segmentation procedure. The results indicate that accurate measures of image properties profoundly depend on the quality of image segmentation.},
  langid = {english},
  keywords = {EFs,Segmentation,Ultimate Measurement Accuracy},
  file = {/Users/bill/D/Zotero/storage/J2W4HWVS/zhang 1995 - influence of segmentation over feature measurement - ULTIMATE MEASUREMENT ACCURACY - SEGMENTATION - EF.pdf}
}

@book{zhangSpatialUncertaintyProceedings2008,
  title = {Spatial Uncertainty: Proceedings of the 8th {{International}} Symposium on Spatial Accuracy Assessment in Natural Resources and Environmental Sciences, {{Shanghai}}, {{China}}, {{June}} 25 - 27, 28},
  shorttitle = {Spatial Uncertainty},
  editor = {Zhang, Jingxiong and {International Symposium on Spatial Accuracy Assessment in Natural Resources {and} Environmental Sciences}},
  year = {2008},
  series = {Spacial Accuracy 2008},
  number = {the 8th International symposium on spatial accuracy assessment in natural resources and environmental sciences, Shanghai, China, June 25 - 27, 2008 ; Vol. 1},
  publisher = {{World Academic Union (World Academic Press)}},
  address = {{Liverpool}},
  isbn = {978-1-84626-170-1},
  langid = {english},
  annotation = {OCLC: 552441241}
}

@article{zhangWhatGoingGeoprocess2015,
  title = {What's Going on about Geo-Process Modeling in Virtual Geographic Environments ({{VGEs}})},
  author = {Zhang, Chunxiao and Chen, Min and Li, Rongrong and Fang, Chaoyang and Lin, Hui},
  year = {2015},
  month = may,
  journal = {Ecological Modelling},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2015.04.023},
  langid = {english},
  keywords = {guppy,virtual},
  file = {/Users/bill/D/Zotero/storage/WI8TUXBK/zhang et al 2015 - Whats going on about geo-process modeling in virtual geographic environments (VGEs) - VIRTUAL.pdf}
}

@article{zhaoEnhancingRobustnessScalefree2009,
  title = {Enhancing the Robustness of Scale-Free Networks},
  author = {Zhao, Jichang and Xu, Ke},
  year = {2009},
  month = may,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {42},
  number = {19},
  pages = {195003},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/42/19/195003},
  abstract = {Error tolerance and attack vulnerability are two common and important properties of complex networks, which are usually used to evaluate the robustness of a network. Recently, much work has been devoted to determining the network design with optimal robustness. However, little attention has been paid to the problem of how to improve the robustness of existing networks. In this paper, we present a new parameter {$\alpha$}, called enforcing parameter, to guide the process of enhancing the robustness of scale-free networks by gradually adding new links. Intuitively, {$\alpha$} {$<$} 0 means the nodes with lower degrees are selected preferentially while the nodes with higher degrees will be more probably selected when {$\alpha$} {$>$} 0. It is shown both theoretically and experimentally that when {$\alpha$} {$<$} 0 the attack survivability of the network can be enforced apparently. Then we propose new strategies to enhance the network robustness. Through extensive experiments and comparisons, we conclude that establishing new links between nodes with low degrees can drastically enforce the attack survivability of scale-free networks while having little impact on the error tolerance.},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/P3RUS44Y/JPA09.pdf}
}

@article{zhaoWeakTiesSubtle2010,
  title = {Weak Ties: {{Subtle}} Role of Information Diffusion in Online Social Networks},
  shorttitle = {Weak Ties},
  author = {Zhao, Jichang and Wu, Junjie and Xu, Ke},
  year = {2010},
  month = jul,
  journal = {Physical Review E},
  volume = {82},
  number = {1},
  pages = {016105},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.82.016105},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/F3YAGQ59/PRE2010.pdf}
}

@article{zhou2021acm,
  title = {Hiding Solutions in Model {{RB}}: {{Forced}} Instances Are Almost as Hard as Unforced Ones},
  shorttitle = {Hiding Solutions in Model {{RB}}},
  author = {Zhou, Guangyan},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.06649 [cs, math]},
  eprint = {2103.06649},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {In this paper we study the forced instance spaces of model RB, where one or two arbitrary satisfying assignments have been imposed. We prove rigorously that the expected number of solutions of forced RB instances is asymptotically the same with those of unforced ones. Moreover, the distribution of forced RB instances in the corresponding forced instance space is asymptotically the same with that of unforced RB instances in the unforced instance space. These results imply that the hidden assignments will not lead to easily solvable formulas, and the hardness of solving forced RB instances will be the same with unforced RB instances.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {60C05; 68Q87; 68T20,Computer Science - Computational Complexity,Mathematics - Combinatorics},
  file = {/Users/bill/D/Zotero/storage/WCB43232/zhou 2021 - Hiding solutions in model RB - Forced instances are almost as hard as unforced ones - BDPG - ANNO.pdf}
}

@article{zhukovAppliedSpatialStatistics,
  title = {Applied {{Spatial Statistics}} in {{R}}, {{Section}} 6},
  author = {Zhukov, Yuri M},
  pages = {56},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/MD4KD5MI/Spatial6.pdf}
}

@article{zotero-1799,
  type = {Article}
}

@article{zotero-1814,
  type = {Article}
}

@inproceedings{zouLogicDistanceBasedMethod2009,
  title = {A {{Logic Distance-Based Method}} for {{Deploying Probing Sources}} in the {{Topology Discovery}}},
  booktitle = {{{GLOBECOM}} 2009 - 2009 {{IEEE Global Telecommunications Conference}}},
  author = {Zou, Xin and Qiao, Zhongliang and Zhou, Gang and Xu, Ke},
  year = {2009},
  month = nov,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Honolulu, Hawaii}},
  doi = {10.1109/GLOCOM.2009.5426143},
  abstract = {Internet topology plays a vital role in studying network's internal structure and properties. Currently traceroutebased topology discovery is the main approach to map the network. However, the deployments of probing sources are usually quite costly and complex. Even if the total numbers of sources are the same, the overall coverage of the sampled network may vary significantly for different sources. As a result, it is of great importance for a topology discovery project to select a limited set of probing sources to detect more nodes and links. The aim of this paper is to investigate how to select a fixed set of probing sources to maximize the coverage of the sampled network. We propose a novel logic distance-based method to make source placement decisions. Also we evaluate our approach and compare it with other known methods on real network topology and generated topologies.},
  isbn = {978-1-4244-4148-8},
  langid = {english},
  file = {/Users/bill/D/Zotero/storage/RVE68D5W/GLO09.pdf}
}


